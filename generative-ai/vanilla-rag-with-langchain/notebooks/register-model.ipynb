{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfbbe1f3-2a53-44da-bc55-a1151fe47aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "# Configure logger\n",
    "logger: logging.Logger = logging.getLogger(\"run_workflow_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.propagate = False  # Prevent duplicate logs from parent loggers\n",
    "\n",
    "# Set formatter\n",
    "formatter: logging.Formatter = logging.Formatter(\n",
    "    fmt=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "# Configure and attach stream handler\n",
    "stream_handler: logging.StreamHandler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5259bf-a938-4ca6-99c0-8eca066f758e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 22:24:53 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "import uuid\n",
    "import base64\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "from typing import Dict, Any, List\n",
    "import pandas as pd\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline, HuggingFaceEndpoint\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda, RunnableMap\n",
    "from langchain.schema.document import Document\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Add the src directory to the path to import base_service\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"../..\")))\n",
    "from core.chatbot_service.chatbot_service import ChatbotService\n",
    "from src.utils import (\n",
    "    get_context_window, \n",
    "    dynamic_retriever, \n",
    "    format_docs_with_adaptive_context,\n",
    "    login_huggingface,\n",
    "    load_config_and_secrets,\n",
    "    clean_code\n",
    ")\n",
    "from src.prompt_templates import format_rag_chatbot_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7da4487-457c-4018-9b34-5af64c711d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/mlflow/pyfunc/utils/data_validation.py:168: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
      "  color_warning(\n",
      "/opt/conda/lib/python3.12/site-packages/mlflow/pyfunc/utils/data_validation.py:168: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
      "  color_warning(\n"
     ]
    }
   ],
   "source": [
    "# === Standard Library Imports ===\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "# === Third-Party Imports ===\n",
    "import mlflow\n",
    "import torch\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline, HuggingFaceEndpoint\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda, RunnableMap\n",
    "from langchain.schema.document import Document\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import promptquality as pq\n",
    "import galileo_protect as gp\n",
    "from galileo_protect import ProtectTool, ProtectParser, Ruleset\n",
    "\n",
    "# Define the relative path to the 'src' directory (one level up from current working directory)\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "# === Project-Specific Imports (from src.utils) ===\n",
    "from src.utils import (\n",
    "    load_config_and_secrets,\n",
    "    configure_proxy,\n",
    "    initialize_llm,\n",
    "    setup_galileo_environment,\n",
    "    initialize_galileo_evaluator,\n",
    "    initialize_galileo_protect,\n",
    "    initialize_galileo_observer,\n",
    "    configure_hf_cache,\n",
    ")\n",
    "from src.prompt_templates import format_rag_chatbot_prompt\n",
    "\n",
    "# === Import the service class ===\n",
    "from core.chatbot_service.chatbot_service import ChatbotService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c4b8ef6-4669-4b11-acc6-56ea640fa046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters for the RAG chatbot\n",
    "MODEL_SOURCE = \"local\"  # Can be: \"local\", \"hugging-face-local\", \"hugging-face-cloud\"\n",
    "CHATBOT_SERVICE_NAME = \"VanillaRAGChatbot\"\n",
    "\n",
    "# Load configuration and secrets\n",
    "config = load_config(\"../configs/config.yaml\")\n",
    "secrets = load_secrets(\"../configs/secrets.yaml\")\n",
    "\n",
    "print(\"✅ Configuration loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a47787-e57b-42fa-8009-7b0c296ba7d4",
   "metadata": {},
   "source": [
    "# Model Service \n",
    "\n",
    "In this section, we demonstrate how to deploy a RAG-based chatbot service with integrated Galileo Protect and Observe capabilities. This service provides a REST API endpoint that allows users to query the knowledge base with natural language questions, upload new documents to the knowledge base, and manage conversation history, all with built-in safeguards against sensitive information and toxicity. This service encapsulates all the functionality we developed in this notebook, including the document retrieval system, RAG-based question answering capabilities, and Galileo integration for protection, observation and evaluation. It demonstrates how to use our ChatbotService from the src/service directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a93da99-a714-4ee2-8b22-b3af42c04075",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 22:25:02,250 - INFO - PyTorch version 2.7.0 available.\n",
      "2025-07-03 22:25:02,550 - INFO - Use pytorch device_name: cuda\n",
      "2025-07-03 22:25:02,550 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "/opt/conda/lib/python3.12/site-packages/mlflow/pyfunc/__init__.py:3212: UserWarning: \u001b[1;33mAn input example was not provided when logging the model. To ensure the model signature functions correctly, specify the `input_example` parameter. See https://mlflow.org/docs/latest/model/signatures.html#model-input-example for more details about the benefits of using input_example.\u001b[0m\n",
      "  color_warning(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed1c2c693ba641d6a47a620bd4e48696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6e670edd574843b6341073bae7b72c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d799af548e574ab1acfd87be3c6105c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb9144c2386454c9798c050026ba72c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b97ddf082c4993a1de6b3543ca641a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/03 22:27:11 WARNING mlflow.utils.requirements_utils: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - PyPDF (current: uninstalled, required: PyPDF)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n",
      "2025-07-03 22:29:18,470 - INFO - Model and artifacts successfully registered in MLflow.\n",
      "Registered model 'AIStudio-Chatbot-Model' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'AIStudio-Chatbot-Model'.\n",
      "2025-07-03 22:29:19 - INFO - ✅ Model registered successfully with run ID: 589b398f5bb548979a98ec29430fc1c5\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_tracking_uri('/phoenix/mlflow')\n",
    "# === Set MLflow experiment context ===\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\n",
    "\n",
    "# === Validate local model file path ===\n",
    "if not os.path.exists(LOCAL_MODEL_PATH):\n",
    "    logger.info(f\"⚠️ Warning: Model file not found at {LOCAL_MODEL_PATH}. Please verify the path.\")\n",
    "\n",
    "# === Log and register model to MLflow ===\n",
    "with mlflow.start_run(run_name=MLFLOW_RUN_NAME) as run:\n",
    "    \n",
    "    # Log model artifacts using custom ChatbotService\n",
    "    ChatbotService.log_model(\n",
    "        artifact_path=MLFLOW_MODEL_NAME,\n",
    "        config_path=CONFIG_PATH,\n",
    "        secrets_path=SECRETS_PATH,\n",
    "        docs_path=DATA_PATH,\n",
    "        model_path=LOCAL_MODEL_PATH,\n",
    "        demo_folder=DEMO_FOLDER\n",
    "    )\n",
    "\n",
    "    # Construct the URI for the logged model\n",
    "    model_uri = f\"runs:/{run.info.run_id}/{MLFLOW_MODEL_NAME}\"\n",
    "\n",
    "    # Register the model into MLflow Model Registry\n",
    "    mlflow.register_model(\n",
    "        model_uri=model_uri,\n",
    "        name=MLFLOW_MODEL_NAME\n",
    "    )\n",
    "\n",
    "    logger.info(f\"✅ Model registered successfully with run ID: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cb33b25-e9bf-443a-9636-d1fd0385ee80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 22:29:19 - INFO - ⏱️ Total execution time: 4m 25.90s\n",
      "2025-07-03 22:29:19 - INFO - ✅ Notebook execution completed successfully.\n"
     ]
    }
   ],
   "source": [
    "end_time: float = time.time()\n",
    "elapsed_time: float = end_time - start_time\n",
    "elapsed_minutes: int = int(elapsed_time // 60)\n",
    "elapsed_seconds: float = elapsed_time % 60\n",
    "\n",
    "logger.info(f\"⏱️ Total execution time: {elapsed_minutes}m {elapsed_seconds:.2f}s\")\n",
    "logger.info(\"✅ Notebook execution completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184e2983-bd61-4a8d-b76c-6ea5dcf3df72",
   "metadata": {},
   "source": [
    "Built with ❤️ using [**HP AI Studio**](https://hp.com/ai-studio)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
