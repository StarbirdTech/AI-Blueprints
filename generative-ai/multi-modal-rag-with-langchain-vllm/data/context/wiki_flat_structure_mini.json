[
  {
    "path": "Data-Science-Team/Best-Practices-for-HP-AI-Studio-Blueprints-Repository.md",
    "content": "**Best Practices for HP AI Studio Blueprints Repository**\n===========================================================\n\n* * *\n\n**1. Repository Structure & Documentation**\n-------------------------------------------\n\n### **1.1 Consistent Directory Structure**\n\n*   Follow the guidelines defined in the **\"Directory Structure for HP AI Studio Samples Repository\"** to maintain uniformity.\n    \n* Each blueprint project should have a clear and meaningful name that reflects its use case and primary tool, following this format:  \n  `<use-case>-with-<main-tool-used>`\n\n* All folder and file names should use **dashes** (`-`) as word separators instead of underscores (`_`) to ensure better readability.\n    \n\n### **1.2 Required Documentation**\n\n*   Every root directory, category directory, and project folder must contain a `README.md` file.\n    \n*   The content of all `README.md` files must comply with the structure outlined in the **\"Directory Structure for HP AI Studio Samples Repository\"**.\n    \n*   Each notebook must contain clear and informative markdown cells to divide code into logical, easy-to-follow sections.\n    \n\n### **1.3 Notebook Documentation Guidelines**\n\n*   The beginning of each notebook should feature a markdown-based **Table of Contents**, such as:\n    \n        Notebook Overview\n        - User Constants\n        - Imports\n        - Configurations\n        - Verify Assets\n        - Other sections...\n\n*   The **User Constants** section must contain all the variables that users are expected to modify based on their specific inputs and configurations. Similar to input fields in a user interface, these constants should be easily adjustable, enabling users to interact with the notebook in a customized and flexible manner.\n \n*   The **Imports** section must include both `pip install` commands and import statements.\n    \n*   The **Configurations** section should contain:\n    *   Warning suppression and logging setup\n        \n    *   Constants, environment variables, secrets, and config file usage\n        \n    *   Asset verification logic for models and datasets\n        \n*   Every notebook title should use this formatting for clarity:\n    \n        <h1 style=\"text-align: center; font-size: 50px;\"> Notebook Name </h1>\n        \n### **1.4 User Interface Documentation Guidelines**\n\n* Printed PDF pages for **Swagger**, **HTML**, and **Streamlit** user interfaces for each blueprint project should be added to the `docs/` folder within their respective blueprint directories.\n\n* In each user interface, a note should be displayed **above the button** that triggers the API call.  \n  This note will inform users about input limitations caused by the API timeout threshold.\n\n* The message should follow this template:\n\n  > “The input you can use with this interface has the following limitations: `<limitations>`, due to API timeout.  \n  > If you need to work with larger input, please use the Jupyter notebook for this blueprint project instead.”\n\n\n\n\n* * *\n\n**2. Code Quality & Maintainability**\n-------------------------------------\n\n### **2.1 Enforcing Code Style**\n\n*   Use `ruff` for code formatting and linting to replace tools like `flake8`, `black`, and `isort`.\n    \n\n### **2.2 Logging Best Practices**\n\n*   Replace `print()` with the `logging` module for enhanced maintainability:\n    \n        import logging\n        import time\n        \n        logger = logging.getLogger(\"notebook_logger\")\n        logger.setLevel(logging.INFO)\n        \n        formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n        stream_handler = logging.StreamHandler()\n        stream_handler.setFormatter(formatter)\n        logger.addHandler(stream_handler)\n        logger.propagate = False\n        \n    \n*   Add these lines near the **start** of the notebook execution:\n    \n        \n        start_time = time.time()        \n        logger.info(\"Notebook execution started.\")\n        \n    \n*   Add these lines at the **end** of the notebook execution:\n\n        end_time = time.time()\n        elapsed_time = end_time - start_time\n        elapsed_minutes = int(elapsed_time // 60)\n        elapsed_seconds = elapsed_time % 60\n        logger.info(f\"Total execution time: {elapsed_minutes}m {elapsed_seconds:.2f}s\")\n        logger.info(\"Notebook execution completed.\")\n        \n    \n\n### **2.3 Type Safety & Function Documentation**\n\n*   All functions must include:\n    *   Type annotations\n        \n    *   Descriptive docstrings\n        \n    \n        def add_numbers(a: int, b: int) -> int:\n            \"\"\"Adds two integers and returns their sum.\"\"\"\n            return a + b\n        \n    \n\n### **2.4 Proper Import Structure**\n\n*   Place all imports at the beginning of the notebook.\n    \n*   Avoid duplicate import statements.\n    \n\n### **2.5 Concise and Insightful Comments**\n\n*   Provide meaningful comments to improve readability and guide users.\n    \n\n### **2.6 Informative Markdown Cells**\n\n*   Use well-written markdown cells to explain the purpose and content of each section.\n    \n\n### **2.7 Constants Management**\n\n*   Avoid hardcoded values. Define constants for experiment names, paths, and configurations at the top.\n    \n\n### **2.8 Cross-Platform File Paths**\n\n*   Use the `pathlib` library for defining file paths to ensure OS compatibility.\n    \n\n* * *\n\n**3. Configuration & Security**\n-------------------------------\n\n### **3.1 Configuration Management**\n\n*   Store all non-sensitive environment variables in `configs/config.yaml`.\n    \n*   Use uppercase letters with underscores (e.g., `GALILEO_API_KEY`) for naming keys.\n    \n*   Add descriptive comments in both `config.yaml` and `secrets.yaml` explaining each variable.\n    \n\n### **3.2 Secret Handling**\n\n*   Never commit `secrets.yaml` to version control, even if it is empty.\n    \n\n### **3.3 Security Auditing**\n\n*   Run `pip-audit` to identify known vulnerabilities in project dependencies.\n    \n*   Address GitHub Dependabot alerts and verify the project still functions correctly after updates.\n    \n\n* * *\n\n**4. Dependency & Environment Management**\n------------------------------------------\n\n### **4.1 Dependency Management**\n\n*   Always specify exact package versions in `requirements.txt` for reproducibility.\n    \n*   Use `pipreqsnb` (not `pip freeze`) to avoid unnecessary packages in `requirements.txt`.\n    \n*   Ensure the notebook begins with:\n    \n        %pip install -r requirements.txt --quiet\n        \n    \n\n* * *\n\n**5. Development Workflow**\n---------------------------\n\n### **5.1 Git Workflow Conventions**\n\n*   Follow standardized Git branch naming:\n    *   `main`: Production-ready code\n        \n    *   `feat/<feature-name>`: New features\n        \n    *   `fix/<issue-number>`: Bug fixes\n        \n    *   `chore/<chore-name>`: Maintenance\n        \n*   Write clear, meaningful commit messages:\n    \n        [feat] Add GPU detection\n        [fix] Resolve path issue in data loader\n        [docs] Update README replication steps\n        \n    \n\n### **5.2 Notebook Execution and Output Trimming**\n\n*   Execute all cells in order using \"Restart Kernel & Run All\" before committing.\n    \n*   Trim lengthy or irrelevant cell outputs.\n    \n\n### **5.3 Pull Request Guidelines**\n\n*   Clearly describe all changes and confirm they are tested.\n    \n*   Merge the latest `main` branch before submitting.\n    \n*   Assign at least one reviewer and add appropriate tags.\n    \n*   Link each PR to the relevant Azure DevOps item.\n    \n\n* * *\n\n**6. Project-Specific Guidelines**\n----------------------------------\n\n### **6.1 GPU Usage**\n\n*   Always detect and use an NVIDIA GPU when available:\n    \n        import torch\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        print(f\"Using device: {device}\")\n        \n    \n\n### **6.2 Experiment Tracking**\n\n*   Use MLflow for experiment tracking and model versioning.\n    \n*   Use TensorBoard for visualizing deep learning metrics.\n    \n\n### **6.3 Offline AI Models**\n\n*   Prefer using local models rather than downloading from external sources or APIs.\n    \n\n### **6.4 Prompt Engineering**\n\n*   Apply best practices in prompt design to:\n    *   Enhance output quality\n        \n    *   Avoid hallucinations\n        \n    *   Prevent prompt injection\n        \n\n### **6.5 Local GenAI Image and Galileo Integration**\n\n*   For GenAI projects, use the **Local GenAI image** in HP AI Studio.\n    \n*   Integrate observability and evaluation workflows with [Galileo AI](https://docs.galileo.ai/galileo).\n    \n\n### **6.6 User Interfaces**\n\n*   Each project should include a UI created with one of the following:\n    *   Streamlit\n        \n    *   Gradio\n        \n    *   Mercury\n        \n    *   Taipy\n        \n    *   React\n        \n    *   Or at minimum, an `index.html` file\n        \n\n* * *\n\n**7. Ensuring Reproducibility**\n-------------------------------\n\n### **7.1 Replication from `README.md`**\n\n*   Projects should be reproducible with minimal effort by following their `README.md`.\n    \n\n* * *\n\n**Conclusion**\n--------------\n\nFollowing these best practices ensures:\n*   ✅ **Organized and consistent repository structure**\n    \n*   ✅ **High-quality, readable, and maintainable code**\n    \n*   ✅ **Secure and scalable configuration management**\n    \n*   ✅ **Reliable dependency handling**\n    \n*   ✅ **Effective ML/AI project development workflows**\n    \n*   ✅ **Ease of reproducibility for users and contributors**\n    \n\n* * *\n\nFinally, all README and notebook files must include the HP AI Studio signature at the end:\n\n> Built with ❤️ using [**HP AI Studio**](https://www.hp.com/us-en/workstations/ai-studio.html).",
    "images": []
  },
  {
    "path": "Data-Science-Team/🧪-Blueprint-Testing-Guide.md",
    "content": "🧪 Blueprint Testing Guide\n=====================================================\n\n\nThis document outlines the **standard and comprehensive steps** for testing blueprint projects in the [AI-Blueprints GitHub repository](https://github.com/HPInc/AI-Blueprints).\n\n* * *\n\n✅ Testing Workflow\n------------------\n\nPlease follow the steps below when testing any blueprint project:\n\n### 1. Create a Project in AI Studio\n\n*   If the blueprint is published, create a new project in AI Studio using the blueprint directly.\n    \n*   If not published, manually create the project:\n    *   Navigate to the related blueprint folder in the repo.\n        \n    *   Read the `README.md` file carefully.\n        \n    *   Follow the setup instructions in the README to manually configure the environment in AI Studio.\n        \n\n### 2. Complete the Setup\n\n*   Ensure **all steps under the `Setup` section** in the blueprint's README are completed without exception.\n    \n\n### 3. Run the Notebook\n\n*   Open the Jupyter notebook associated with the blueprint.\n    \n*   Execute all cells by clicking **\"Run All\"**, as described in **Step 1 of the `Usage` section**.\n    \n\n### 4. Register the Model and Deploy Locally (if applicable)\n\n*   If the blueprint includes MLflow integration:\n    *   Follow the next usage step to **register the model in MLflow** and deploy it successfully.\n        \n\n### 5. Test the Interfaces (if applicable)\n\n*   If the blueprint includes user interfaces:\n    *   Use the provided **HTML and Streamlit UIs** as described in the `Usage` section.\n        \n\n### 6. Create a Testing Branch\n\n*   Create a **new Git branch** for testing the selected blueprint.\n    \n\n### 7. Push Executed Notebook\n\n*   Push the **fully executed Jupyter notebook**, with all cell outputs visible, to the correct location in your testing branch, following the README structure.\n    \n\n### 8. Push Interface Snapshots\n\n*   **Print and save** the Swagger, HTML, and Streamlit interface pages by right-clicking on the page and selecting **“Print” → “Save as PDF”**. Ensure the captured PDFs clearly show successful outputs.\n\n    \n*   Push these assets to the `docs` folder of the blueprint directory.\n    \n\n### 9. Add Test Cases and Results\n\n*   If not already present, create a `test-cases` folder under the `docs` directory.\n    \n*   Add an Excel file documenting:\n    *   **Test configurations used** (e.g., different model backends like `local`, `huggingface-cloud`, `huggingface-local`)\n        \n    *   **Results of each test** (`Pass`/`Fail`)\n        \n\n### 10. Submit a Pull Request\n\n*   Open a PR with your testing branch.\n    \n*   Your PR will be reviewed to ensure compliance with all steps, and will be approved upon validation.\n    \n\n* * *\n\n🐞 Bug Reporting Guidelines\n---------------------------\n\nIf you encounter issues, ensure you've followed all steps correctly. If not a user error, open a bug in **Azure DevOps (ADO)** and assign the appropriate severity:\n| Severity Level | Description |\n| --- | --- |\n| **Critical** | Notebook execution crashes the app. |\n| **High** | Notebook, deployment, or UI is completely non-functional. |\n| **Medium** | Notebook, deployment, or UI works intermittently. |\n| **Low** | Documentation is unclear or contains minor inaccuracies. |\n\n* * *\n\n**Thank you.**\n\n* * *",
    "images": []
  },
  {
    "path": "Data-Science-Team/Directory-Structure-for-HP-AI-Studio-Blueprints-Repository.md",
    "content": "**Directory Structure for HP AI Studio Blueprints Repository**\n======================================================================\n\n* * *\n\nThe following structure should be followed for every project in the HP AI Studio Samples Repository. This ensures consistency, readability, and scalability across all sample projects.\n\n    # Root Directory\n    ├── category/                           # Folder categorizing projects (e.g., machine-learning, deep-learning)\n    │   │\n    │   ├── sample-project/                 # Specific sample project folder\n    │   │   │\n    │   │   ├── configs/                    # Configuration files\n    │   │   │   ├── config.yaml             # Non-sensitive settings and variables\n    │   │   │   └── secrets.yaml            # Sensitive secrets (must be gitignored)\n    │   │   ├── data/                       # Data assets used in the project\n    │   │   │   ├── sample_doc.pdf          # Example document\n    │   │   │   └── sample_image.png        # Example image\n    │   │   ├── demo/                       # UI-related files (Web apps, dashboards, web components)\n    │   │   ├── docs/                       # Project documentation\n    │   │   │   ├── installation_guide.md   # Step-by-step installation instructions\n    │   │   │   └── architecture.md         # System architecture and workflow overview\n    │   │   ├── notebooks/                  # Jupyter notebooks\n    │   │   │   └── notebook.ipynb          # Main notebook for the project\n    │   │   ├── src/                        # Core Python modules\n    │   │   │   ├── __init__.py             # Marks directory as a package\n    │   │   │   └── utils.py                # Reusable helper functions\n    │   │   ├── .gitignore                  # Ignore logs, cache, virtual envs, and compiled files\n    │   │   ├── requirements.txt            # Python dependencies (used with pip install)\n    │   │   └── README.md                   # Project-specific documentation (see format below)\n    │   │\n    │   └── README.md                       # Overview of all sample projects under this category\n    │\n    ├── .gitignore                          # Root-level .gitignore\n    ├── LICENSE                             # Project license (e.g., MIT, Apache 2.0)\n    └── README.md                           # Root-level repository documentation\n    \n\n* * *\n\n**README Format for Each Sample Project**\n-----------------------------------------\n\nEvery project-level `README.md` file must include the following sections:\n\n### **1. Title & Badges**\n\n*   A clear and descriptive project title\n    \n*   Badges (e.g., license, supported Python version, dependency status)\n    \n\n### **2. Overview**\n\n*   A concise summary of the project’s purpose and core functionalities\n    \n\n### **3. Project Structure**\n\n*   A high-level description of the project’s folder and file organization\n    \n\n### **4. Setup**\n\n*   Step-by-step setup instructions, including:\n    *   **Step 0**: Minimum hardware requirements (e.g., disk space, GPU model, VRAM)\n        \n    *   Asset creation and how to add assets (models/datasets) to the project environment\n        \n\n### **5. Usage**\n\n*   Clear explanation of how to use the project after setup is complete. This section can include:\n    *   How to run the notebooks\n        \n    *   Deployment steps\n\n    *   How to interact with the API post-deployment\n        \n    *   How to interact with the user interface post-deployment\n        \n\n### **6. Support & Troubleshooting**\n\n*   Guidance for users seeking help:\n    *   Where to ask questions (e.g., GitHub Issues tab, support email)\n        \n    *   Common fixes (e.g., restarting the workspace to detect new assets in DataFabric)\n        \n\n* * *\n\nBy following this structure and documentation format, projects in the repository will be:\n\n*   ✅ Easy to navigate and understand\n    \n*   ✅ Reproducible by other users\n    \n*   ✅ Scalable and easy to maintain\n    \n\n---\n\n> Built with ❤️ using [**HP AI Studio**](https://www.hp.com/us-en/workstations/ai-studio.html).",
    "images": []
  },
  {
    "path": "Frontend-Team/React-Hooks-Best-Practices.md",
    "content": "# 🪝 React Hooks Best Practices\n\nThis document outlines best practices for writing maintainable and efficient code using React Hooks in production-grade applications.\n\n---\n\n## 🔄 State Management\n\n- Always colocate state with the component that uses it.\n- Use `useReducer` over `useState` for complex state transitions.\n\n```tsx\nconst [state, dispatch] = useReducer(reducer, initialState);\n```\n\n## 🧠 Memoization\n\n- Use `useMemo` to avoid expensive recalculations.\n- Use `useCallback` to prevent unnecessary re-renders of child components.\n\n```tsx\nconst memoizedValue = useMemo(() => compute(data), [data]);\n```\n\n## 🧪 Side Effects\n\n- Always clean up effects in `useEffect` to prevent memory leaks.\n\n```tsx\nuseEffect(() => {\n  const id = setInterval(() => doSomething(), 1000);\n  return () => clearInterval(id);\n}, []);\n```\n\n---\n\n## ❗ Anti-Patterns to Avoid\n\n- Calling hooks conditionally\n- Mutating state directly\n- Using `useEffect` for logic that belongs in event handlers\n\n> Built with ❤️ by the Frontend Team",
    "images": []
  },
  {
    "path": "DevOps-Team/Kubernetes-Troubleshooting-Guide.md",
    "content": "# 🛠️ Kubernetes Troubleshooting Guide\n\nCommon issues and resolutions when deploying to Kubernetes clusters.\n\n---\n\n## 🔍 Symptoms & Fixes\n\n### Pod CrashLoopBackOff\n- Run `kubectl describe pod <pod-name>`\n- Check for misconfigured environment variables or readiness probes\n\n### ImagePullBackOff\n- Confirm image URL and secret credentials\n- Test with:\n\n```bash\nkubectl get events --sort-by='.lastTimestamp'\n```\n\n### Connection Refused\n- Ensure services are exposed via `ClusterIP`, `NodePort`, or `LoadBalancer`\n- Validate with `kubectl port-forward` or `curl` inside a test pod\n\n---\n\n## 🧼 Logs & Debugging\n\n```bash\nkubectl logs <pod-name>\nkubectl exec -it <pod-name> -- /bin/bash\n```\n\n> For internal clusters, refer to the `internal-ingress` Helm chart documentation.\n\n> Built with ❤️ by the DevOps Team",
    "images": []
  },
  {
    "path": "Data-Team/Feature-Store-Design-Decisions.md",
    "content": "# 🧠 Feature Store Design Decisions\n\nThis document records our architecture decisions around the implementation of a centralized Feature Store for real-time ML features.\n\n---\n\n## ⚙️ Why We Need a Feature Store\n- Reuse features across models\n- Serve features with low latency\n- Improve training/inference consistency\n\n## 🏗️ Chosen Architecture\n- Storage: Redis for online, S3 for offline\n- Orchestration: Apache Airflow\n- Access: Feature SDK + gRPC API\n\n## 📏 Design Tradeoffs\n| Option | Chosen? | Reason |\n|--------|---------|--------|\n| Kafka streaming ingest | ✅ | Enables real-time freshness |\n| Feast as framework | ❌ | Too opinionated for our use cases |\n\n---\n\n## ✅ Next Steps\n- Build ingestion DAGs\n- Define governance for feature ownership\n\n> Built with ❤️ by the Data Engineering Team",
    "images": []
  },
  {
    "path": "Mobile-Team/App-Onboarding-Checklist.md",
    "content": "# 📱 Mobile App Onboarding Checklist\n\nFollow this checklist before pushing a new feature to the production mobile app (Android/iOS).\n\n---\n\n## 🔧 Code\n- [x] Feature flag added\n- [x] Analytics event logged\n- [x] Backward compatibility tested\n\n## 🧪 QA\n- [x] Regression suite passed\n- [x] Manual UI test passed on both platforms\n\n## 🚀 Release\n- [x] App Store metadata updated\n- [x] Screenshots uploaded\n- [x] Version bumped in `build.gradle` / `Info.plist`\n\n## 📋 Post-Release\n- [x] Monitoring dashboard checked\n- [x] Crash logs reviewed (Sentry)\n\n> Built with ❤️ by the Mobile Engineering Team",
    "images": []
  },
  {
    "path": "Security-Team/Secrets-Handling-Playbook.md",
    "content": "# 🔐 Secrets Handling Playbook\n\nThis guide defines how engineering teams should manage secrets (API keys, tokens, credentials) securely.\n\n---\n\n## 🚫 Don't\n- Never commit secrets to Git\n- Never use `.env` files in production\n\n## ✅ Do\n- Use HashiCorp Vault or AWS Secrets Manager\n- Rotate secrets every 90 days\n- Use `dotenv` only for **local** dev with `.env.example` templates\n\n```bash\nvault kv put secret/db-creds username=admin password=securepass123\n```\n\n## 🔍 Auditing\n- Enable logging on all access attempts\n- Run `truffleHog` in CI for secret detection\n\n> Built with ❤️ by the Security Team",
    "images": []
  }
]
