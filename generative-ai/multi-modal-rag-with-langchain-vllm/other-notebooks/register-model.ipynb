{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff193981-f701-4595-9d5f-a08779c65508",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-size: 50px;\"> 🤖 MLFlow Registration for Multimodal RAG with Local Chroma Cache</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e7b230-e638-458a-8104-0a30727b2466",
   "metadata": {},
   "source": [
    "# MLFlow Model Service \n",
    "\n",
    "In this section, we demonstrate how to deploy a RAG-based chatbot service. This service provides a REST API endpoint that allows users to query the knowledge base with natural language questions, use any Azure DevOps Wikis as the knowledge base, and manage conversation history. This service encapsulates all the functionality we developed in this notebook, including the document retrieval system, RAG-based question answering capabilities, and MLFlow integration for observation and evaluation. For this specific service, we store the databases created for embeddings locally, to reduce the processing time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032ab59a-a358-4b11-9ee4-ac38101666bf",
   "metadata": {},
   "source": [
    "## Step 0: Imports and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e5a1883-f877-414a-9ff9-b876a2ba6531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os \n",
    "from pathlib import Path\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "# Create logger\n",
    "logger = logging.getLogger(\"multimodal_rag_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.handlers:\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "    stream_handler = logging.StreamHandler(sys.stdout)\n",
    "    stream_handler.setFormatter(formatter)\n",
    "    logger.addHandler(stream_handler)\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38660c87-34b3-40f2-be21-16c78fb12598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-05 00:49:10 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()  \n",
    "\n",
    "logger.info(\"Notebook execution started.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2607392-6bd0-4950-8ac8-84026da9df0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "622040db-4d03-482f-958f-45cc87492ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 00:49:19.081531: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-05 00:49:19.091308: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754354959.101627    4724 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754354959.104670    4724 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754354959.113605    4724 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754354959.113620    4724 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754354959.113621    4724 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754354959.113622    4724 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-05 00:49:19.116568: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-05 00:49:20 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "# === Standard Library Imports ===\n",
    "import gc\n",
    "import json\n",
    "import base64\n",
    "import tempfile\n",
    "import threading\n",
    "import shutil\n",
    "import warnings\n",
    "import hashlib\n",
    "from rank_bm25 import BM25Okapi\n",
    "from typing import Any, Dict, List, Optional, TypedDict\n",
    "from collections import defaultdict\n",
    "from IPython.display import display, Image, Markdown\n",
    "\n",
    "# === Third-Party Library Imports ===\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import torch\n",
    "from langchain.schema.document import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.types import ColSpec, DataType, Schema\n",
    "from PIL import Image as PILImage\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoImageProcessor, AutoTokenizer, SiglipModel, SiglipProcessor\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# === Project-Specific Imports ===\n",
    "# Add the project root to the system path to allow importing from 'src'\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from src.components import SemanticCache, SiglipEmbeddings\n",
    "from src.wiki_pages_clone import orchestrate_wiki_clone\n",
    "from src.local_genai_judge import LocalGenAIJudge\n",
    "from src.utils import (\n",
    "    configure_hf_cache,\n",
    "    multimodal_rag_asset_status,\n",
    "    load_config,\n",
    "    load_secrets,\n",
    "    load_mm_docs_clean,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0def9a7-d827-4158-be16-302c79bfeaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84f036c1-4922-4389-94d0-406f7e769612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cb6032-d809-4bf9-b9b8-87ef1f3c47ed",
   "metadata": {},
   "source": [
    "## Step 1: Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60df3050-7f7a-4735-af13-67cbadcd0b78",
   "metadata": {},
   "source": [
    "### Verify Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "869f893e-bddd-4f15-8c76-298091f9daee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-05 00:49:22 - INFO - Local Model is properly configured. \n",
      "2025-08-05 00:49:22 - INFO - Config is properly configured. \n",
      "2025-08-05 00:49:22 - INFO - Secrets is properly configured. \n",
      "2025-08-05 00:49:22 - INFO - wiki_flat_structure.json is properly configured. \n",
      "2025-08-05 00:49:22 - INFO - CONTEXT is properly configured. \n",
      "2025-08-05 00:49:22 - INFO - CHROMA is properly configured. \n",
      "2025-08-05 00:49:22 - INFO - CACHE is properly configured. \n",
      "2025-08-05 00:49:22 - INFO - MANIFEST is properly configured. \n"
     ]
    }
   ],
   "source": [
    "CONFIG_PATH = \"../configs/config.yaml\"\n",
    "SECRETS_PATH = \"../configs/secrets.yaml\"\n",
    "\n",
    "LOCAL_MODEL = \"/home/jovyan/datafabric/Qwen2.5-VL-7B-Instruct-GPTQ-Int4-1\"\n",
    "CONTEXT_DIR: Path = Path(\"../data/context\")\n",
    "CHROMA_DIR: Path = Path(\"../data/chroma_store\")     \n",
    "CACHE_DIR: Path = CHROMA_DIR / \"semantic_cache\"\n",
    "MANIFEST_PATH: Path = CHROMA_DIR / \"manifest.json\"\n",
    "\n",
    "IMAGE_DIR = CONTEXT_DIR / \"images\"\n",
    "WIKI_METADATA_DIR = CONTEXT_DIR / \"wiki_flat_structure.json\"\n",
    "\n",
    "CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "multimodal_rag_asset_status(\n",
    "    local_model_path=LOCAL_MODEL,\n",
    "    config_path=CONFIG_PATH,\n",
    "    secrets_path=SECRETS_PATH,\n",
    "    wiki_metadata_dir=WIKI_METADATA_DIR,\n",
    "    context_dir=CONTEXT_DIR,\n",
    "    chroma_dir=CHROMA_DIR,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    manifest_path=MANIFEST_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2f98b97-0338-4107-91ea-797ff3a26072",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bfe368-e9c5-4856-8aad-a93026bacd7a",
   "metadata": {},
   "source": [
    "### Config HuggingFace Caches\n",
    "\n",
    "In the next cell, we configure HuggingFace cache, so that all the models downloaded from them are persisted locally, even after the workspace is closed. This is a future desired feature for AI Studio and the GenAI addon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4576dc32-7e04-48aa-b19d-da1adc92693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "configure_hf_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8125515-1c50-4297-8986-63ba9944a958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HuggingFace Embeddings\n",
    "txt_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/e5-large-v2\",\n",
    "    cache_folder=\"/tmp/hf_cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cebbd13-6e1f-46e9-a710-9621b3010bc8",
   "metadata": {},
   "source": [
    "### MLflow Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82d6964a-0768-4fa0-9da4-e8ecbc5487db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-05 00:49:24 - INFO - Using MLflow tracking URI: /phoenix/mlflow\n",
      "2025-08-05 00:49:24 - INFO - Using MLflow experiment: 'AIStudio-Multimodal-Chatbot-Experiment'\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"AIStudio-Multimodal-Chatbot-Model\"\n",
    "RUN_NAME = f\"Register_{MODEL_NAME}\"\n",
    "EXPERIMENT_NAME = \"AIStudio-Multimodal-Chatbot-Experiment\"\n",
    "\n",
    "# Set MLflow tracking URI and experiment\n",
    "# This should be configured for your environment, e.g., a remote server or local file path\n",
    "mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\", \"/phoenix/mlflow\"))\n",
    "mlflow.set_experiment(experiment_name=EXPERIMENT_NAME)\n",
    "\n",
    "logger.info(f\"Using MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "logger.info(f\"Using MLflow experiment: '{EXPERIMENT_NAME}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1cb8d4-d58a-48bc-96e9-9bd63d43f80f",
   "metadata": {},
   "source": [
    "## Step 2: MLflow Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec72ef2-03e5-40e4-aa2e-580222f79c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalRagModel(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"\n",
    "    An MLflow PythonModel for a dynamic, updatable Multimodal RAG pipeline\n",
    "    using Qwen-VL and a persistent ChromaDB cache.\n",
    "\n",
    "    This model acts as a service with two main commands:\n",
    "    - 'update_kb': Incrementally updates the vector knowledge base from a source.\n",
    "    - 'query': Answers a user's question using the RAG pipeline, with\n",
    "      built-in semantic caching and self-evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 1. Inner Class for the RAG Generation Pipeline\n",
    "    # ==========================================================================\n",
    "    class QwenVLRAGPipeline:\n",
    "        \"\"\"Minimal, self-contained multimodal QA wrapper for Qwen-VL with vLLM.\"\"\"\n",
    "        def __init__(self, llm: \"LLM\", tok: \"AutoTokenizer\", image_processor: \"AutoImageProcessor\", device: str, cache: Any, text_db: Chroma, image_db: Chroma, bm25_index: Optional[BM25Okapi], doc_map: dict):\n",
    "            self.llm = llm\n",
    "            self.tok = tok\n",
    "            self.image_processor = image_processor\n",
    "            self.device = device\n",
    "            self.cache = cache\n",
    "            self.text_db = text_db\n",
    "            self.image_db = image_db\n",
    "            self.bm25_index = bm25_index\n",
    "            self.doc_map = doc_map\n",
    "\n",
    "        @staticmethod\n",
    "        def _reciprocal_rank_fusion(results: list[list[Document]], k: int = 60) -> list[tuple[Document, float]]:\n",
    "            \"\"\"Performs Reciprocal Rank Fusion on multiple ranked lists of documents.\"\"\"\n",
    "            ranked_lists = [{doc.page_content: (doc, i + 1) for i, doc in enumerate(res)} for res in results]\n",
    "            rrf_scores = defaultdict(float)\n",
    "            all_docs = {}\n",
    "            # Iterate through each ranked list and calculate RRF scores\n",
    "            for ranked_list in ranked_lists:\n",
    "                for content, (doc, rank) in ranked_list.items():\n",
    "                    rrf_scores[content] += 1 / (k + rank)\n",
    "                    if content not in all_docs:\n",
    "                        all_docs[content] = doc\n",
    "            fused_results = [(all_docs[content], rrf_scores[content]) for content in sorted(rrf_scores, key=rrf_scores.get, reverse=True)]\n",
    "            return fused_results\n",
    "\n",
    "        def _retrieve_mm(self, query: str, k_text: int = 3, k_img: int = 2, recall_k: int = 20) -> dict[str, any]:\n",
    "            \"\"\"Retrieves relevant documents and images using dense and sparse retrieval.\"\"\"\n",
    "            dense_hits = self.text_db.similarity_search(query, k=recall_k)\n",
    "            sparse_hits = []\n",
    "            \n",
    "            # If BM25 index is available, perform sparse retrieval\n",
    "            if self.bm25_index and list(self.doc_map.keys()):\n",
    "                tokenized_query = query.lower().split(\" \")\n",
    "                sparse_texts = self.bm25_index.get_top_n(tokenized_query, list(self.doc_map.keys()), n=recall_k)\n",
    "                sparse_hits = [self.doc_map[text] for text in sparse_texts]\n",
    "\n",
    "            if not dense_hits and not sparse_hits:\n",
    "                return {\"docs\": [], \"scores\": [], \"images\": []}\n",
    "\n",
    "            # Combine dense and sparse hits, ensuring no duplicates\n",
    "            fused_results = self._reciprocal_rank_fusion([dense_hits, sparse_hits])\n",
    "            final_docs = [doc for doc, score in fused_results[:k_text]]\n",
    "            final_scores = [score for doc, score in fused_results[:k_text]]\n",
    "\n",
    "            # Retrieve images based on the sources of the final documents\n",
    "            retrieved_images = []\n",
    "            if final_docs and self.image_db:\n",
    "                final_sources = list(set(d.metadata[\"source\"] for d in final_docs))\n",
    "                image_hits = self.image_db.similarity_search(query, k=k_img, filter={\"source\": {\"$in\": final_sources}})\n",
    "                retrieved_images = [img.page_content for img in image_hits]\n",
    "\n",
    "            return {\"docs\": final_docs, \"scores\": final_scores, \"images\": retrieved_images}\n",
    "\n",
    "        def generate(self, query: str, force_regenerate: bool = False, **retrieval_kwargs) -> Dict[str, Any]:\n",
    "            \"\"\"\n",
    "            Generates a response using the Qwen-VL RAG pipeline, now with semantic caching restored.\n",
    "            \"\"\"\n",
    "            start_gen_time = time.time()\n",
    "            \n",
    "            # Check if the query is already cached\n",
    "            if not force_regenerate and self.cache:\n",
    "                cached_result = self.cache.get(query, threshold=0.92)\n",
    "                if cached_result:\n",
    "                    logger.info(f\"SEMANTIC CACHE HIT for query: '{query}'\")\n",
    "                    cached_result.setdefault(\"generation_time_seconds\", 0.0)\n",
    "                    return cached_result\n",
    "            \n",
    "            # Restore the cache entry if it exists\n",
    "            if force_regenerate and self.cache:\n",
    "                logger.info(f\"Forced regeneration for query: '{query}'. Clearing old cache entry.\")\n",
    "                self.cache.delete(query)\n",
    "\n",
    "            logger.info(f\"CACHE MISS for query: '{query}'. Running full pipeline.\")\n",
    "\n",
    "            # --- Document & Image Retrieval ---\n",
    "            hits = self._retrieve_mm(query, **retrieval_kwargs)\n",
    "            docs, images = hits[\"docs\"], hits[\"images\"]\n",
    "            \n",
    "            if not docs and not images:\n",
    "                return {\"reply\": \"Based on the provided context, I cannot answer this question.\", \"used_images\": [], \"generation_time_seconds\": 0.0}\n",
    "\n",
    "            # Limit the number of images to 2 for memory efficiency\n",
    "            if len(images) > 2:\n",
    "                logger.warning(f\"Limiting images from {len(images)} to 2 to save memory\")\n",
    "                images = images[:2]\n",
    "                \n",
    "            context_str = \"\\n\\n\".join(\n",
    "                f\"<source_document name=\\\"{d.metadata.get('source', 'unknown')}\\\">\\n{d.page_content}\\n</source_document>\"\n",
    "                for d in docs\n",
    "            )\n",
    "            \n",
    "            system_prompt = \"\"\"You are a Multimodal RAG Assistant. Your task is to answer the user's query using ONLY the provided context...\"\"\" # Your full prompt\n",
    "            \n",
    "            if images:\n",
    "                image_tokens = \"\".join([\"<|vision_start|><|image_pad|><|vision_end|>\" for _ in images])\n",
    "                user_content = f\"{image_tokens}\\n\\n<context>\\n{context_str}\\n</context>\\n\\n<user_query>\\n{query}\\n</user_query>\"\n",
    "            else:\n",
    "                user_content = f\"<context>\\n{context_str}\\n</context>\\n\\n<user_query>\\n{query}\\n</user_query>\"\n",
    "\n",
    "            messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_content}]\n",
    "            prompt_string = self.tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "            try:\n",
    "                # ... Image processing and vLLM call ...\n",
    "                pil_images = []\n",
    "                if images:\n",
    "                    # ... (image processing logic with 512x512 thumbnail) ...\n",
    "                    for i, img_path in enumerate(images):\n",
    "                        try:\n",
    "                            img = PILImage.open(img_path).convert(\"RGB\")\n",
    "                            if img.width > 512 or img.height > 512:\n",
    "                                img.thumbnail((512, 512), PILImage.Resampling.LANCZOS)\n",
    "                            pil_images.append(img)\n",
    "                        except Exception as e:\n",
    "                            logger.warning(f\"Failed to process image {img_path}: {e}\")\n",
    "                \n",
    "                request_payload = {\"prompt\": prompt_string}\n",
    "                if pil_images:\n",
    "                    request_payload[\"multi_modal_data\"] = {\"image\": pil_images}\n",
    "\n",
    "                output_list = self.llm.generate([request_payload], SamplingParams(temperature=0.0, top_p=1.0, max_tokens=2048))\n",
    "                reply = output_list[0].outputs[0].text.strip() if output_list and output_list[0].outputs else \"Error: no output from LLM.\"\n",
    "                \n",
    "                end_gen_time = time.time()\n",
    "                \n",
    "                if reply == \"The provided context does not contain relevant information to answer the query.\":\n",
    "                    images = []\n",
    "                \n",
    "                result_dict = {\"reply\": reply, \"used_images\": images, \"generation_time_seconds\": end_gen_time - start_gen_time}\n",
    "\n",
    "                # --- Semantic Caching ---\n",
    "                if self.cache:\n",
    "                    self.cache.set(query, result_dict)\n",
    "                \n",
    "                return result_dict\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Qwen-VL generation failed: {e}\", exc_info=True)\n",
    "                return {\"reply\": f\"Error during generation: {e}\", \"used_images\": images, \"generation_time_seconds\": 0.0}\n",
    "        \n",
    "        def _clear_cuda(self):\n",
    "            if torch.cuda.is_available():\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 2. MLflow `pyfunc` Life-cycle and Service Methods\n",
    "    # ==========================================================================\n",
    "    def load_context(self, context: mlflow.pyfunc.PythonModelContext) -> None:\n",
    "        \"\"\"Initializes the service, loads models, and sets up persistent storage.\"\"\"\n",
    "        \n",
    "        logger.info(\"--- Initializing Dynamic MultimodalRAG Service (Qwen-VL) ---\")\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Setup persistent storage paths\n",
    "        self.persistent_storage_path = Path(\"/tmp/multimodal_rag_service_data\")\n",
    "        self.persistent_storage_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.chroma_store_path = self.persistent_storage_path / \"chroma_store\"\n",
    "        self.cache_dir = self.persistent_storage_path / \"semantic_cache\"\n",
    "        self.manifest_path = self.persistent_storage_path / \"manifest.json\"\n",
    "        logger.info(f\"Service data will be managed at: {self.persistent_storage_path}\")\n",
    "\n",
    "        # Load embedding models from artifacts\n",
    "        e5_model_path = context.artifacts[\"e5_model_dir\"]\n",
    "        siglip_model_path = context.artifacts[\"siglip_model_dir\"]\n",
    "        self.text_embed_model = HuggingFaceEmbeddings(model_name=e5_model_path, model_kwargs={\"device\": self.device})\n",
    "        self.siglip_embed_model = SiglipEmbeddings(model_id=siglip_model_path, device=self.device)\n",
    "\n",
    "        # Load Qwen-VL model using vLLM\n",
    "        if self.device == \"cuda\":\n",
    "            model_path = Path(context.artifacts[\"local_model_dir\"]).resolve()\n",
    "            base_model_name = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "            self.tok = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "            self.image_processor = AutoImageProcessor.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "            self.llm = LLM(\n",
    "                model=str(model_path),\n",
    "                quantization=\"gptq\",\n",
    "                gpu_memory_utilization=0.80,\n",
    "                max_model_len=4096,\n",
    "                enforce_eager=True,\n",
    "                limit_mm_per_prompt={\"image\": 2},\n",
    "                disable_custom_all_reduce=True,\n",
    "                tensor_parallel_size=1,\n",
    "                dtype=\"float16\",\n",
    "            )\n",
    "            self.judge = LocalGenAIJudge(llm=self.llm, tokenizer=self.tok)\n",
    "            logger.info(\"Qwen-VL LLM and Judge loaded successfully.\")\n",
    "        else:\n",
    "            self.llm, self.tok, self.image_processor, self.judge = None, None, None, None\n",
    "            logger.error(\"vLLM requires a CUDA device. LLM and Judge are disabled.\")\n",
    "        \n",
    "        # Initialize placeholders for RAG components\n",
    "        self.text_db, self.image_db, self.bm25_index, self.doc_map, self.cache, self.mm_llm = [None] * 6\n",
    "        logger.info(\"--- Service initialized. Ready for commands. ---\")\n",
    "\n",
    "    def predict(self, context: mlflow.pyfunc.PythonModelContext, model_input: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Handles 'update_kb' and 'query' commands.\"\"\"\n",
    "        command = model_input[\"command\"].iloc[0]\n",
    "        logger.info(f\"Received command: '{command}'\")\n",
    "\n",
    "        # if the command is \"update_kb\", update the knowledge base\n",
    "        if command == \"update_kb\":\n",
    "            payload = json.loads(model_input[\"payload\"].iloc[0])\n",
    "            result = self.update_knowledge_base(config=payload[\"config\"], secrets=payload[\"secrets\"])\n",
    "            return pd.DataFrame([result])\n",
    "    \n",
    "        # if the command is \"query\", process the query\n",
    "        elif command == \"query\":\n",
    "            if self.mm_llm is None:\n",
    "                logger.info(\"First query received. Initializing RAG pipeline from persistent storage...\")\n",
    "                self._initialize_rag_pipeline()\n",
    "            \n",
    "            query = model_input[\"query\"].iloc[0]\n",
    "            force_regenerate = model_input.get(\"force_regenerate\", pd.Series([False])).iloc[0]\n",
    "            response_dict = self.mm_llm.generate(query, force_regenerate=force_regenerate)\n",
    "            \n",
    "            # Convert image paths to Base64 for JSON response\n",
    "            image_paths = response_dict.get(\"used_images\", [])\n",
    "            base64_images = []\n",
    "            for path in image_paths:\n",
    "                try:\n",
    "                    with open(path, \"rb\") as img_file:\n",
    "                        encoded_string = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "                        base64_images.append(encoded_string)\n",
    "                except FileNotFoundError:\n",
    "                    logger.warning(f\"Image file not found: {path}\")\n",
    "            response_dict[\"used_images\"] = json.dumps(base64_images)\n",
    "    \n",
    "            # Add query to response dictionary for evaluation, using llm judge\n",
    "            if self.judge:\n",
    "                retrieved_info = self.mm_llm._retrieve_mm(query)\n",
    "                context_str = \"\\n\\n\".join(d.page_content for d in retrieved_info[\"docs\"])\n",
    "                eval_df = pd.DataFrame([{\"questions\": query, \"result\": response_dict[\"reply\"], \"source_documents\": context_str}])\n",
    "                response_dict[\"faithfulness\"] = self.judge.evaluate_faithfulness(eval_df).iloc[0]\n",
    "                response_dict[\"relevance\"] = self.judge.evaluate_relevance(eval_df).iloc[0]\n",
    "            else:\n",
    "                response_dict[\"faithfulness\"], response_dict[\"relevance\"] = None, None\n",
    "            \n",
    "            return pd.DataFrame([response_dict])\n",
    "    \n",
    "        else:\n",
    "            return pd.DataFrame([{\"status\": \"error\", \"message\": f\"Unknown command: {command}\"}])\n",
    "\n",
    "    def update_knowledge_base(self, config: dict, secrets: dict) -> dict:\n",
    "        \"\"\"Checks for source data changes and rebuilds the knowledge base if necessary.\"\"\"\n",
    "        logger.info(\"Starting knowledge base update check...\")\n",
    "        processed_data_dir = self.persistent_storage_path / \"processed_data\"\n",
    "        processed_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            # Use a temporary directory for the initial clone to ensure atomicity\n",
    "            with tempfile.TemporaryDirectory() as temp_dir_str:\n",
    "                temp_path = Path(temp_dir_str)\n",
    "\n",
    "                # Now, pass the Path object to the cloning function\n",
    "                orchestrate_wiki_clone(pat=secrets['AIS_ADO_TOKEN'], config=config, output_dir=temp_path)\n",
    "\n",
    "                # Create a manifest for the cloned data\n",
    "                current_manifest = self._create_json_manifest(temp_path)\n",
    "                if not current_manifest:\n",
    "                    return {\"status\": \"error\", \"message\": \"Cloning step failed to produce a manifest file.\"}\n",
    "\n",
    "                old_manifest = json.loads(self.manifest_path.read_text()) if self.manifest_path.exists() else {}\n",
    "\n",
    "                # If the data hasn't changed, no need to re-index\n",
    "                if current_manifest == old_manifest:\n",
    "                    logger.info(\"Knowledge base is already up-to-date.\")\n",
    "                    # Initialize the pipeline if it's the first run\n",
    "                    if not self.mm_llm:\n",
    "                        self._initialize_rag_pipeline()\n",
    "                    return {\"status\": \"success\", \"message\": \"Knowledge base is already up-to-date.\"}\n",
    "\n",
    "                # If data has changed, perform a full re-index\n",
    "                logger.info(\"Knowledge base has changed. Performing a full re-index.\")\n",
    "                if self.chroma_store_path.exists():\n",
    "                    shutil.rmtree(self.chroma_store_path)\n",
    "                self.chroma_store_path.mkdir()\n",
    "\n",
    "                # Move successfully cloned data from the temp to the permanent location\n",
    "                shutil.copytree(temp_path, processed_data_dir, dirs_exist_ok=True)\n",
    "\n",
    "            # Process data from the permanent location\n",
    "            image_dir = processed_data_dir / \"images\"\n",
    "            wiki_metadata_path = processed_data_dir / \"wiki_flat_structure.json\"\n",
    "            all_raw_docs = load_mm_docs_clean(wiki_metadata_path, image_dir)\n",
    "            all_chunks = self._chunk_docs(all_raw_docs)\n",
    "\n",
    "            # Index text chunks\n",
    "            if all_chunks:\n",
    "                logger.info(f\"Indexing {len(all_chunks)} text chunks...\")\n",
    "                Chroma.from_documents(\n",
    "                    documents=all_chunks, embedding=self.text_embed_model,\n",
    "                    persist_directory=str(self.chroma_store_path), collection_name=\"mm_text\"\n",
    "                )\n",
    "\n",
    "            # Index images\n",
    "            img_paths, img_ids, img_meta = self._collect_image_vectors(all_raw_docs, image_dir)\n",
    "            if img_paths:\n",
    "                logger.info(f\"Indexing {len(img_paths)} images...\")\n",
    "                image_db = Chroma(\n",
    "                    collection_name=\"mm_image\", persist_directory=str(self.chroma_store_path),\n",
    "                    embedding_function=self.siglip_embed_model,\n",
    "                )\n",
    "                image_db.add_texts(texts=img_paths, metadatas=img_meta, ids=img_ids)\n",
    "                image_db.persist()\n",
    "\n",
    "            # Clear the old semantic cache and save the new manifest\n",
    "            if self.cache_dir.exists():\n",
    "                shutil.rmtree(self.cache_dir)\n",
    "            self.manifest_path.write_text(json.dumps(current_manifest, indent=2))\n",
    "            \n",
    "            # Re-initialize the full RAG pipeline with the new data\n",
    "            self._initialize_rag_pipeline()\n",
    "\n",
    "            return {\"status\": \"success\", \"message\": \"Knowledge base rebuilt successfully.\"}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Knowledge base update failed: {e}\", exc_info=True)\n",
    "            return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 3. Helper and Class Methods\n",
    "    # ==========================================================================\n",
    "    def _initialize_rag_pipeline(self):\n",
    "        \"\"\"Initializes all RAG components from the persistent storage.\"\"\"\n",
    "        if not self.chroma_store_path.exists():\n",
    "            raise FileNotFoundError(f\"ChromaDB not found. Run 'update_kb' first.\")\n",
    "        \n",
    "        # Setup chromadbs\n",
    "        logger.info(\"Loading RAG components from persistent storage...\")\n",
    "        self.text_db = Chroma(collection_name=\"mm_text\", persist_directory=str(self.chroma_store_path), embedding_function=self.text_embed_model)\n",
    "        self.image_db = Chroma(collection_name=\"mm_image\", persist_directory=str(self.chroma_store_path), embedding_function=self.siglip_embed_model)\n",
    "    \n",
    "        # Load all documents from the text database and deduplicate them\n",
    "        all_docs_data = self.text_db.get(include=[\"documents\", \"metadatas\"])\n",
    "        if not all_docs_data['ids']:\n",
    "             unique_splits = []\n",
    "        else:\n",
    "            all_docs = [Document(page_content=txt, metadata=meta) for txt, meta in zip(all_docs_data['documents'], all_docs_data['metadatas'])]\n",
    "            unique_splits = list({doc.page_content: doc for doc in all_docs}.values())\n",
    "\n",
    "        # Create BM25 index from unique text splits\n",
    "        corpus = [doc.page_content for doc in unique_splits]\n",
    "        self.bm25_index = BM25Okapi([doc.split(\" \") for doc in corpus]) if corpus else None\n",
    "        self.doc_map = {doc.page_content: doc for doc in unique_splits}\n",
    "    \n",
    "        # Initialize the semantic cache\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.cache = SemanticCache(persist_directory=str(self.cache_dir), embedding_function=self.text_embed_model)\n",
    "        \n",
    "        if not self.llm:\n",
    "            raise RuntimeError(\"LLM not loaded (requires CUDA). Cannot initialize RAG pipeline.\")\n",
    "        \n",
    "        # Initialize the multimodal RAG pipeline\n",
    "        self.mm_llm = self.QwenVLRAGPipeline(\n",
    "            llm=self.llm, tok=self.tok, image_processor=self.image_processor, device=self.device, cache=self.cache,\n",
    "            text_db=self.text_db, image_db=self.image_db,\n",
    "            bm25_index=self.bm25_index, doc_map=self.doc_map\n",
    "        )\n",
    "        logger.info(\"✅ RAG pipeline fully initialized with Qwen-VL.\")\n",
    "\n",
    "    def _create_json_manifest(self, context_dir: Path) -> Dict[str, str]:\n",
    "        \"\"\"Creates a manifest by hashing 'wiki_flat_structure.json'.\"\"\"\n",
    "        manifest = {}\n",
    "        json_file = context_dir / \"wiki_flat_structure.json\"\n",
    "        if json_file.exists():\n",
    "            file_bytes = json_file.read_bytes()\n",
    "            manifest[json_file.name] = hashlib.sha256(file_bytes).hexdigest()\n",
    "        return manifest\n",
    "    \n",
    "    def _chunk_docs(self, docs: List[Document]) -> List[Document]:\n",
    "        \"\"\"Chunks raw documents and assigns unique IDs.\"\"\"\n",
    "        # This method chunks documents based on headers and recursive character splitting.\n",
    "        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[(\"#\", \"title\"), (\"##\", \"section\")])\n",
    "        recursive_splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=200)\n",
    "        all_chunks: list[Document] = []\n",
    "        for doc in docs:\n",
    "            page_title = Path(doc.metadata[\"source\"]).stem.replace(\"-\", \" \")\n",
    "            section_docs = header_splitter.split_text(doc.page_content)\n",
    "            doc_chunk_counter = 0\n",
    "            for section in section_docs:\n",
    "                tiny_texts = recursive_splitter.split_text(section.page_content)\n",
    "                for tiny in tiny_texts:\n",
    "                    chunk_metadata = {\n",
    "                        \"title\": page_title,\n",
    "                        \"source\": doc.metadata[\"source\"],\n",
    "                        \"section_header\": section.metadata.get(\"header\", \"\"),\n",
    "                        \"chunk_id\": doc_chunk_counter,\n",
    "                    }\n",
    "                    all_chunks.append(Document(page_content=f\"{page_title}\\n\\n{tiny.strip()}\", metadata=chunk_metadata))\n",
    "                    doc_chunk_counter += 1\n",
    "        return all_chunks\n",
    "        \n",
    "    def _collect_image_vectors(self, mm_raw_docs: List[Document], image_dir: Path) -> tuple:\n",
    "        \"\"\"Collects paths, IDs, and metadata for unique images.\"\"\"\n",
    "        # This method collects image paths, IDs, and metadata from the raw documents.\n",
    "        img_paths, img_ids, img_meta = [], [], []\n",
    "        seen = set()\n",
    "        for doc in mm_raw_docs:\n",
    "            src = doc.metadata[\"source\"]\n",
    "            for name in doc.metadata.get(\"images\", []):\n",
    "                img_id = f\"{src}::{name}\"\n",
    "                if img_id in seen: continue\n",
    "                seen.add(img_id)\n",
    "                img_paths.append(str(image_dir / name))\n",
    "                img_ids.append(img_id)\n",
    "                img_meta.append({\"source\": src, \"image\": name})\n",
    "        return img_paths, img_ids, img_meta\n",
    "\n",
    "    @classmethod\n",
    "    def log_model(cls, model_name: str, local_model: str) -> None:\n",
    "        \"\"\"Logs the model service to MLflow.\"\"\"\n",
    "        logger.info(f\"--- Logging '{model_name}' Service to MLflow ---\")\n",
    "        # This method logs the model service to MLflow, including artifacts and signatures.\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            temp_path = Path(temp_dir)\n",
    "            e5_path = temp_path / \"e5-large-v2\"\n",
    "            SentenceTransformer(\"intfloat/e5-large-v2\").save(str(e5_path))\n",
    "            \n",
    "            siglip_path = temp_path / \"siglip2-base-patch16-224\"\n",
    "            SiglipModel.from_pretrained(\"google/siglip2-base-patch16-224\").save_pretrained(siglip_path)\n",
    "            SiglipProcessor.from_pretrained(\"google/siglip2-base-patch16-224\").save_pretrained(siglip_path)\n",
    "            \n",
    "            artifacts = {\n",
    "                \"local_model_dir\": local_model,\n",
    "                \"e5_model_dir\": str(e5_path),\n",
    "                \"siglip_model_dir\": str(siglip_path),\n",
    "            }\n",
    "            \n",
    "            input_schema = Schema([\n",
    "                ColSpec(DataType.string, \"command\"),\n",
    "                ColSpec(DataType.string, \"query\", required=False),\n",
    "                ColSpec(DataType.string, \"payload\", required=False),\n",
    "                ColSpec(DataType.boolean, \"force_regenerate\", required=False)\n",
    "            ])\n",
    "            output_schema = Schema([\n",
    "                ColSpec(DataType.string, \"reply\", required=False),\n",
    "                ColSpec(DataType.string, \"used_images\", required=False),\n",
    "                ColSpec(DataType.double, \"generation_time_seconds\", required=False),\n",
    "                ColSpec(DataType.double, \"faithfulness\", required=False),\n",
    "                ColSpec(DataType.double, \"relevance\", required=False),\n",
    "                ColSpec(DataType.string, \"status\", required=False),\n",
    "                ColSpec(DataType.string, \"message\", required=False),\n",
    "            ])\n",
    "            signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "\n",
    "            mlflow.pyfunc.log_model(\n",
    "                artifact_path=model_name,\n",
    "                python_model=cls(),\n",
    "                artifacts=artifacts,\n",
    "                pip_requirements=\"../requirements.txt\",\n",
    "                signature=signature,\n",
    "                code_paths=[\"../src\"],\n",
    "            )\n",
    "        logger.info(f\"✅ Successfully logged '{model_name}' service.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfee18b-a495-42c0-aaf4-a37a4d5009eb",
   "metadata": {},
   "source": [
    "## Step 3: Start Run, Log & Register Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97c7b785-0628-4358-8e8d-9d49a0d6f1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-05 00:49:25 - INFO - Started MLflow run: 4bc2abcd34464fa28f185b1b29941ae0\n",
      "2025-08-05 00:49:25 - INFO - --- Logging 'AIStudio-Multimodal-Chatbot-Model' Service to MLflow ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf1679a6ad4481da321608c8ddaede5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f2525e3644042fa9157c4bb5357e904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc32adba9a145c1ac84899bfb266f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-05 00:51:28 - INFO - ✅ Successfully logged 'AIStudio-Multimodal-Chatbot-Model' service.\n",
      "2025-08-05 00:51:28 - INFO - Registering model from URI: runs:/4bc2abcd34464fa28f185b1b29941ae0/AIStudio-Multimodal-Chatbot-Model\n",
      "2025-08-05 00:51:28 - INFO - ✅ Successfully registered model 'AIStudio-Multimodal-Chatbot-Model'\n",
      "CPU times: user 3.77 s, sys: 25.8 s, total: 29.6 s\n",
      "Wall time: 2min 3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'AIStudio-Multimodal-Chatbot-Model' already exists. Creating a new version of this model...\n",
      "Created version '5' of model 'AIStudio-Multimodal-Chatbot-Model'.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# --- Start MLflow Run and Log the Model ---\n",
    "try:\n",
    "    with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "        run_id = run.info.run_id\n",
    "        logger.info(f\"Started MLflow run: {run_id}\")\n",
    "\n",
    "        # Use the class method to log the model and its artifacts\n",
    "        MultimodalRagModel.log_model(model_name=MODEL_NAME, local_model=LOCAL_MODEL)\n",
    "\n",
    "        model_uri = f\"runs:/{run_id}/{MODEL_NAME}\"\n",
    "        logger.info(f\"Registering model from URI: {model_uri}\")\n",
    "        \n",
    "        # Register the model in the MLflow Model Registry\n",
    "        mlflow.register_model(model_uri=model_uri, name=MODEL_NAME)\n",
    "        logger.info(f\"✅ Successfully registered model '{MODEL_NAME}'\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    logger.error(f\"Error: A required file or directory was not found. Please ensure the project structure is correct.\")\n",
    "    logger.error(f\"Details: {e}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred during the MLflow run: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c628e7f-9d9d-4e7a-9790-34ed1b003caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-05 00:51:28 - INFO - Found latest version '5' for model 'AIStudio-Multimodal-Chatbot-Model'.\n"
     ]
    }
   ],
   "source": [
    "# --- Retrieve the latest version from the Model Registry ---\n",
    "try:\n",
    "    client = MlflowClient()\n",
    "    versions = client.get_latest_versions(MODEL_NAME, stages=[\"None\"])\n",
    "    if not versions:\n",
    "        raise RuntimeError(f\"No registered versions found for model '{MODEL_NAME}'.\")\n",
    "    \n",
    "    latest_version = versions[0]\n",
    "    logger.info(f\"Found latest version '{latest_version.version}' for model '{MODEL_NAME}'.\")\n",
    "    model_uri_registry = latest_version.source\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to retrieve model from registry: {e}\", exc_info=True)\n",
    "    model_uri_registry = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c31856b3-5ff5-48b9-98b7-ddbf7dd16be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-05 00:51:28 - INFO - Loading model from: /phoenix/mlflow/594178897322281329/4bc2abcd34464fa28f185b1b29941ae0/artifacts/AIStudio-Multimodal-Chatbot-Model\n",
      "2025-08-05 00:51:28 - INFO - --- Initializing Dynamic MultimodalRAG Service (Qwen-VL) ---\n",
      "2025-08-05 00:51:28 - INFO - Service data will be managed at: /tmp/multimodal_rag_service_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-05 00:51:48 [config.py:841] This model supports multiple tasks: {'generate', 'classify', 'reward', 'embed'}. Defaulting to 'generate'.\n",
      "WARNING 08-05 00:51:48 [config.py:3371] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 08-05 00:51:48 [config.py:1472] Using max model len 4096\n",
      "INFO 08-05 00:51:48 [gptq_marlin.py:174] Detected that the model can run with gptq_marlin, however you specified quantization=gptq explicitly, so forcing gptq. Use quantization=gptq_marlin for faster inference\n",
      "WARNING 08-05 00:51:48 [config.py:960] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 08-05 00:51:48 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 08-05 00:51:48 [cuda.py:102] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 08-05 00:51:48 [__init__.py:2662] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 00:51:50.899674: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-05 00:51:50.907052: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754355110.915721    4922 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754355110.918213    4922 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754355110.924729    4922 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754355110.924752    4922 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754355110.924754    4922 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754355110.924755    4922 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-05 00:51:50.926965: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-05 00:51:52 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 08-05 00:51:53 [core.py:526] Waiting for init message from front-end.\n",
      "INFO 08-05 00:51:53 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/phoenix/mlflow/594178897322281329/4bc2abcd34464fa28f185b1b29941ae0/artifacts/AIStudio-Multimodal-Chatbot-Model/artifacts/Qwen2.5-VL-7B-Instruct-GPTQ-Int4-1', speculative_config=None, tokenizer='/phoenix/mlflow/594178897322281329/4bc2abcd34464fa28f185b1b29941ae0/artifacts/AIStudio-Multimodal-Chatbot-Model/artifacts/Qwen2.5-VL-7B-Instruct-GPTQ-Int4-1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=gptq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/phoenix/mlflow/594178897322281329/4bc2abcd34464fa28f185b1b29941ae0/artifacts/AIStudio-Multimodal-Chatbot-Model/artifacts/Qwen2.5-VL-7B-Instruct-GPTQ-Int4-1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}\n",
      "INFO 08-05 00:51:54 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 08-05 00:51:54 [interface.py:382] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "WARNING 08-05 00:51:54 [profiling.py:271] The sequence length (4096) is smaller than the pre-defined wosrt-case total number of multimodal tokens (32768). This may cause certain multi-modal inputs to fail during inference. To avoid this, you should increase `max_model_len` or reduce `mm_counts`.\n",
      "INFO 08-05 00:51:54 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.\n",
      "INFO 08-05 00:51:54 [gpu_model_runner.py:1770] Starting to load model /phoenix/mlflow/594178897322281329/4bc2abcd34464fa28f185b1b29941ae0/artifacts/AIStudio-Multimodal-Chatbot-Model/artifacts/Qwen2.5-VL-7B-Instruct-GPTQ-Int4-1...\n",
      "INFO 08-05 00:51:55 [gpu_model_runner.py:1775] Loading model from scratch...\n",
      "WARNING 08-05 00:51:55 [vision.py:91] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "INFO 08-05 00:51:55 [cuda.py:284] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:09<00:09,  9.05s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:19<00:00, 10.16s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:19<00:00,  9.99s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-05 00:52:15 [default_loader.py:272] Loading weights took 17.45 seconds\n",
      "INFO 08-05 00:52:16 [gpu_model_runner.py:1801] Model loading took 6.5934 GiB and 18.128455 seconds\n",
      "INFO 08-05 00:52:16 [gpu_model_runner.py:2238] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n",
      "/opt/conda/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-05 00:52:25 [gpu_worker.py:232] Available KV cache memory: 0.60 GiB\n",
      "INFO 08-05 00:52:25 [kv_cache_utils.py:716] GPU KV cache size: 11,280 tokens\n",
      "INFO 08-05 00:52:25 [kv_cache_utils.py:720] Maximum concurrency for 4,096 tokens per request: 2.75x\n",
      "INFO 08-05 00:52:25 [core.py:172] init engine (profile, create kv cache, warmup model) took 9.39 seconds\n",
      "2025-08-05 00:52:26 - INFO - Qwen-VL LLM and Judge loaded successfully.\n",
      "2025-08-05 00:52:26 - INFO - --- Service initialized. Ready for commands. ---\n",
      "2025-08-05 00:52:26 - INFO - ✅ Successfully loaded model from registry.\n"
     ]
    }
   ],
   "source": [
    "if model_uri_registry:\n",
    "    try:\n",
    "        logger.info(f\"Loading model from: {model_uri_registry}\")\n",
    "        # This step will trigger the load_context method and initialize the vLLM engine\n",
    "        loaded_model = mlflow.pyfunc.load_model(model_uri=model_uri_registry)\n",
    "        logger.info(\"✅ Successfully loaded model from registry.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load model from registry URI: {e}\", exc_info=True)\n",
    "        loaded_model = None\n",
    "else:\n",
    "    logger.warning(\"Skipping model loading due to previous errors.\")\n",
    "    loaded_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2d9db6-b2dd-44e5-989d-ebb498372229",
   "metadata": {},
   "source": [
    "## Step 4: Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fa1c9dd-974c-475d-876c-e4621b03a202",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-05 00:52:26 - INFO - Populating the model's knowledge base using the 'update_kb' command...\n",
      "2025-08-05 00:52:26 - INFO - Received command: 'update_kb'\n",
      "2025-08-05 00:52:26 - INFO - Starting knowledge base update check...\n",
      "2025-08-05 00:52:26 - INFO - Starting ADO Wiki clone process...\n",
      "2025-08-05 00:52:26 - INFO - Cloning wiki 'Phoenix-DS-Platform.wiki' to temporary directory: /tmp/tmplomlu0ok\n",
      "2025-08-05 00:52:41 - INFO - Scanning for Markdown files...\n",
      "2025-08-05 00:52:41 - INFO - → Found 575 Markdown pages.\n",
      "2025-08-05 00:52:41 - INFO - Copying referenced images to /tmp/tmpbqg13flv/images...\n",
      "2025-08-05 00:52:42 - INFO - → 792 unique images copied.\n",
      "2025-08-05 00:52:42 - INFO - Assembling flat JSON structure...\n",
      "2025-08-05 00:52:42 - INFO - ✅ Wiki data successfully cloned to /tmp/tmpbqg13flv\n",
      "2025-08-05 00:52:42 - INFO - Cleaned up temporary directory: /tmp/tmplomlu0ok\n",
      "2025-08-05 00:52:42 - INFO - Knowledge base is already up-to-date.\n",
      "2025-08-05 00:52:42 - INFO - Loading RAG components from persistent storage...\n",
      "2025-08-05 00:52:43 - INFO - ✅ RAG pipeline fully initialized with Qwen-VL.\n",
      "2025-08-05 00:52:43 - INFO - Update Status: Knowledge base is already up-to-date.\n",
      "CPU times: user 1.23 s, sys: 718 ms, total: 1.95 s\n",
      "Wall time: 17.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if loaded_model:\n",
    "    logger.info(\"Populating the model's knowledge base using the 'update_kb' command...\")\n",
    "    try:\n",
    "        config = load_config(CONFIG_PATH)\n",
    "        secrets = load_secrets(SECRETS_PATH)\n",
    "        \n",
    "        # 1. Construct the payload with credentials and config\n",
    "        update_payload_dict = {\n",
    "            \"config\": config,\n",
    "            \"secrets\": {\"AIS_ADO_TOKEN\": secrets.get('AIS_ADO_TOKEN')}\n",
    "        }\n",
    "        \n",
    "        # 2. Create the DataFrame for the 'update_kb' command\n",
    "        update_df = pd.DataFrame([{\n",
    "            \"command\": \"update_kb\",\n",
    "            \"payload\": json.dumps(update_payload_dict)\n",
    "        }])\n",
    "        \n",
    "        # 3. Send the command to the model to build/verify the database\n",
    "        update_status = loaded_model.predict(update_df)\n",
    "        logger.info(f\"Update Status: {update_status['message'].iloc[0]}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to update knowledge base: {e}\", exc_info=True)\n",
    "else:\n",
    "    logger.warning(\"Skipping knowledge base update because the model was not loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "279582c0-74e3-4b98-9d1d-6007c98f109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(query: str, result_df: pd.DataFrame):\n",
    "    \"\"\"Helper to neatly print the query, reply, and display Base64 images.\"\"\"\n",
    "    if result_df.empty:\n",
    "        print(\"Received an empty result.\")\n",
    "        return\n",
    "\n",
    "    row = result_df.iloc[0]\n",
    "    reply = row.get(\"reply\", \"No reply generated.\")\n",
    "    used_images_json = row.get(\"used_images\", \"[]\")\n",
    "    gen_time = row.get(\"generation_time_seconds\", 0)\n",
    "    faithfulness = row.get(\"faithfulness\", -1)\n",
    "    relevance = row.get(\"relevance\", -1)\n",
    "\n",
    "    try:\n",
    "        base64_images = json.loads(used_images_json)\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        base64_images = []\n",
    "\n",
    "    # Display the output\n",
    "    print(\"---\" * 20)\n",
    "    print(f\"❓ Query:\\n{query}\\n\")\n",
    "    print(\"🤖 Reply:\")\n",
    "    display(Markdown(reply))\n",
    "    \n",
    "    print(f\"\\n📊 Faithfulness: {faithfulness:.4f} | Relevance: {relevance:.4f}\")\n",
    "    print(f\"⏱️ Generation Time: {gen_time:.2f}s\\n\")\n",
    "\n",
    "    if base64_images:\n",
    "        print(f\"🖼️ Displaying {len(base64_images)} retrieved image(s):\")\n",
    "        for b64_string in base64_images:\n",
    "            display(Image(data=base64.b64decode(b64_string), width=400))\n",
    "    else:\n",
    "        print(\"▶ No images were retrieved for this query.\")\n",
    "    print(\"---\" * 20 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1230ed-5e16-4c18-8115-47dffd40bc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "if loaded_model:\n",
    "    logger.info(\"Running sample inference with the loaded model...\")\n",
    "    sample_queries = [\n",
    "        \"What are the AI Blueprints Repository best practices?\",\n",
    "        \"What are some feature flags that i can enable in AIStudio?\",\n",
    "        \"How do i manually clean my environment without hooh?\",\n",
    "    ]\n",
    "\n",
    "    for query in sample_queries:\n",
    "        try:\n",
    "            input_payload = pd.DataFrame([{\n",
    "                \"command\": \"query\",\n",
    "                \"query\": query,\n",
    "                \"force_regenerate\": False\n",
    "            }])\n",
    "            \n",
    "            result_df = loaded_model.predict(input_payload)\n",
    "            result_df['query'] = query\n",
    "            display_results(query, result_df)\n",
    "            all_results.append(result_df)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Prediction failed for query '{query}': {e}\", exc_info=True)\n",
    "\n",
    "    final_results_df = pd.concat(all_results, ignore_index=True) if all_results else pd.DataFrame()\n",
    "else:\n",
    "    logger.warning(\"Skipping sample inference because the model was not loaded.\")\n",
    "    final_results_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83806df-79a8-4273-ab62-96c4bf1c2ded",
   "metadata": {},
   "source": [
    "## Step 5: Log Hallucinations & Relevance Evaluations to MlFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65476b0a-e314-44a6-8e51-856e237b00b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-05 00:53:08 - INFO - --- Reopening original run (4bc2abcd34464fa28f185b1b29941ae0) to log evaluations ---\n",
      "2025-08-05 00:53:08 - INFO - Successfully reopened run. Logging metrics and artifacts...\n",
      "2025-08-05 00:53:08 - INFO - ✅ Successfully logged metrics and artifacts.\n"
     ]
    }
   ],
   "source": [
    "# Check if the model was loaded, run_id exists, AND we have results to log\n",
    "if loaded_model and 'run_id' in locals() and not final_results_df.empty:\n",
    "    logger.info(f\"--- Reopening original run ({run_id}) to log evaluations ---\")\n",
    "    \n",
    "    # Reopen the existing run using its ID\n",
    "    with mlflow.start_run(run_id=run_id) as run:\n",
    "        logger.info(\"Successfully reopened run. Logging metrics and artifacts...\")\n",
    "\n",
    "        # Calculate average scores from the DataFrame of results\n",
    "        avg_faithfulness = final_results_df[\"faithfulness\"].astype(float).mean()\n",
    "        avg_relevance = final_results_df[\"relevance\"].astype(float).mean()\n",
    "\n",
    "        # Log the average scores as summary metrics\n",
    "        mlflow.log_metrics({\n",
    "            \"avg_faithfulness\": avg_faithfulness,\n",
    "            \"avg_relevance\": avg_relevance\n",
    "        })\n",
    "\n",
    "        # Log the detailed results as a table artifact for inspection\n",
    "        mlflow.log_table(\n",
    "            data=final_results_df[['query', 'reply', 'faithfulness', 'relevance']], \n",
    "            artifact_file=\"inline_evaluation_results.json\"\n",
    "        )\n",
    "        \n",
    "        logger.info(\"✅ Successfully logged metrics and artifacts.\")\n",
    "else:\n",
    "    logger.warning(\"Skipping evaluation logging: model not loaded, run_id not found, or no results were generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "725e1d0b-333c-43fd-95a7-5ce13fdfe3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-05 00:53:08 - INFO - ⏱️ Total execution time: 3m 58.02s\n",
      "2025-08-05 00:53:08 - INFO - ✅ Notebook execution completed.\n"
     ]
    }
   ],
   "source": [
    "end_time: float = time.time()\n",
    "elapsed_time: float = end_time - start_time\n",
    "elapsed_minutes: int = int(elapsed_time // 60)\n",
    "elapsed_seconds: float = elapsed_time % 60\n",
    "\n",
    "logger.info(f\"⏱️ Total execution time: {elapsed_minutes}m {elapsed_seconds:.2f}s\")\n",
    "logger.info(\"✅ Notebook execution completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e89eee-6171-446a-8da6-9381d3fe10c0",
   "metadata": {},
   "source": [
    "Built with ❤️ using Z by HP AI Studio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
