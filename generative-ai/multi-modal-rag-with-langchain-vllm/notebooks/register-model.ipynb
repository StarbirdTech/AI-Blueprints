{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff193981-f701-4595-9d5f-a08779c65508",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-size: 50px;\"> 🤖 MLFlow Registration for Multimodal RAG Cacheless\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e7b230-e638-458a-8104-0a30727b2466",
   "metadata": {},
   "source": [
    "# MLFlow Model Service \n",
    "\n",
    "In this section, we demonstrate how to deploy a RAG-based chatbot service. This service provides a REST API endpoint that allows users to query the knowledge base with natural language questions, use any Azure DevOps Wikis as the knowledge base, and manage conversation history. This service encapsulates all the functionality we developed in this notebook, including the document retrieval system, RAG-based question answering capabilities, and MLFlow integration for observation and evaluation. For this specific service, we handle each new request with a refresh of the database for our cloud streamlit deployment, ensuring privacy across all users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032ab59a-a358-4b11-9ee4-ac38101666bf",
   "metadata": {},
   "source": [
    "## Step 0: Imports and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e5a1883-f877-414a-9ff9-b876a2ba6531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os \n",
    "from pathlib import Path\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "# Create logger\n",
    "logger = logging.getLogger(\"multimodal_rag_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.handlers:\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "    stream_handler = logging.StreamHandler(sys.stdout)\n",
    "    stream_handler.setFormatter(formatter)\n",
    "    logger.addHandler(stream_handler)\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38660c87-34b3-40f2-be21-16c78fb12598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-05 03:08:05 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()  \n",
    "\n",
    "logger.info(\"Notebook execution started.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2607392-6bd0-4950-8ac8-84026da9df0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "622040db-4d03-482f-958f-45cc87492ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 03:08:15.724055: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-05 03:08:15.739360: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754363295.758101    1300 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754363295.763724    1300 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754363295.778315    1300 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754363295.778353    1300 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754363295.778354    1300 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754363295.778356    1300 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-05 03:08:15.783282: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-05 03:08:18 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "# === Standard Library Imports ===\n",
    "import gc\n",
    "import json\n",
    "import base64\n",
    "import tempfile\n",
    "import threading\n",
    "import shutil\n",
    "import warnings\n",
    "from rank_bm25 import BM25Okapi\n",
    "from typing import Any, Dict, List, Optional, TypedDict\n",
    "from collections import defaultdict\n",
    "from IPython.display import display, Image, Markdown\n",
    "\n",
    "# === Third-Party Library Imports ===\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import torch\n",
    "from langchain.schema.document import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "import chromadb\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.types import ColSpec, DataType, Schema\n",
    "from PIL import Image as PILImage\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoImageProcessor, AutoTokenizer, SiglipModel, SiglipProcessor\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# === Project-Specific Imports ===\n",
    "# Add the project root to the system path to allow importing from 'src'\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from src.components import SiglipEmbeddings\n",
    "from src.wiki_pages_clone import orchestrate_wiki_clone\n",
    "from src.local_genai_judge import LocalGenAIJudge\n",
    "from src.utils import (\n",
    "    configure_hf_cache,\n",
    "    multimodal_rag_asset_status,\n",
    "    load_config,\n",
    "    load_secrets,\n",
    "    load_mm_docs_clean,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0def9a7-d827-4158-be16-302c79bfeaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84f036c1-4922-4389-94d0-406f7e769612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-05 03:08:20 - INFO - Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "logger.info(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cb6032-d809-4bf9-b9b8-87ef1f3c47ed",
   "metadata": {},
   "source": [
    "## Step 1: Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60df3050-7f7a-4735-af13-67cbadcd0b78",
   "metadata": {},
   "source": [
    "### Verify Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "869f893e-bddd-4f15-8c76-298091f9daee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-05 03:08:20 - INFO - Local Model is properly configured. \n",
      "2025-08-05 03:08:20 - INFO - Config is properly configured. \n",
      "2025-08-05 03:08:20 - INFO - Secrets is properly configured. \n",
      "2025-08-05 03:08:20 - INFO - wiki_flat_structure.json is properly configured. \n",
      "2025-08-05 03:08:20 - INFO - CONTEXT is properly configured. \n",
      "2025-08-05 03:08:20 - INFO - CHROMA is properly configured. \n",
      "2025-08-05 03:08:20 - INFO - CACHE is properly configured. \n",
      "2025-08-05 03:08:20 - INFO - MANIFEST is properly configured. \n"
     ]
    }
   ],
   "source": [
    "CONFIG_PATH = \"../configs/config.yaml\"\n",
    "SECRETS_PATH = \"../configs/secrets.yaml\"\n",
    "\n",
    "LOCAL_MODEL = \"/home/jovyan/datafabric/Qwen2.5-VL-7B-Instruct-GPTQ-Int4\"\n",
    "CONTEXT_DIR: Path = Path(\"../data/context\")\n",
    "CHROMA_DIR: Path = Path(\"../data/chroma_store\")     \n",
    "CACHE_DIR: Path = CHROMA_DIR / \"semantic_cache\"\n",
    "MANIFEST_PATH: Path = CHROMA_DIR / \"manifest.json\"\n",
    "\n",
    "IMAGE_DIR = CONTEXT_DIR / \"images\"\n",
    "WIKI_METADATA_DIR = CONTEXT_DIR / \"wiki_flat_structure.json\"\n",
    "\n",
    "CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "multimodal_rag_asset_status(\n",
    "    local_model_path=LOCAL_MODEL,\n",
    "    config_path=CONFIG_PATH,\n",
    "    secrets_path=SECRETS_PATH,\n",
    "    wiki_metadata_dir=WIKI_METADATA_DIR,\n",
    "    context_dir=CONTEXT_DIR,\n",
    "    chroma_dir=CHROMA_DIR,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    manifest_path=MANIFEST_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2f98b97-0338-4107-91ea-797ff3a26072",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bfe368-e9c5-4856-8aad-a93026bacd7a",
   "metadata": {},
   "source": [
    "### Config HuggingFace Caches\n",
    "\n",
    "In the next cell, we configure HuggingFace cache, so that all the models downloaded from them are persisted locally, even after the workspace is closed. This is a future desired feature for AI Studio and the GenAI addon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4576dc32-7e04-48aa-b19d-da1adc92693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "configure_hf_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8125515-1c50-4297-8986-63ba9944a958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HuggingFace Embeddings\n",
    "txt_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/e5-large-v2\",\n",
    "    cache_folder=\"/tmp/hf_cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cebbd13-6e1f-46e9-a710-9621b3010bc8",
   "metadata": {},
   "source": [
    "### MLflow Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82d6964a-0768-4fa0-9da4-e8ecbc5487db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/05 03:08:23 INFO mlflow.tracking.fluent: Experiment with name 'AIStudio-Multimodal-Chatbot-Experiment' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-05 03:08:23 - INFO - Using MLflow tracking URI: /phoenix/mlflow\n",
      "2025-08-05 03:08:23 - INFO - Using MLflow experiment: 'AIStudio-Multimodal-Chatbot-Experiment'\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"AIStudio-Multimodal-Chatbot-Model\"\n",
    "RUN_NAME = f\"Register_{MODEL_NAME}\"\n",
    "EXPERIMENT_NAME = \"AIStudio-Multimodal-Chatbot-Experiment\"\n",
    "\n",
    "# Set MLflow tracking URI and experiment\n",
    "# This should be configured for your environment, e.g., a remote server or local file path\n",
    "mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\", \"/phoenix/mlflow\"))\n",
    "mlflow.set_experiment(experiment_name=EXPERIMENT_NAME)\n",
    "\n",
    "logger.info(f\"Using MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "logger.info(f\"Using MLflow experiment: '{EXPERIMENT_NAME}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1cb8d4-d58a-48bc-96e9-9bd63d43f80f",
   "metadata": {},
   "source": [
    "## Step 2: MLflow Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cec72ef2-03e5-40e4-aa2e-580222f79c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalRagModel(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"\n",
    "    An MLflow PythonModel for a stateless, in-memory Multimodal RAG pipeline.\n",
    "    This model uses Qwen-VL via vLLM for generation.\n",
    "    \"\"\"\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 1. Inner Class for the RAG Generation Pipeline\n",
    "    # ==========================================================================\n",
    "    class QwenVLMM:\n",
    "        \"\"\"Minimal, self-contained multimodal QA wrapper.\"\"\"\n",
    "        def __init__(self, llm: LLM, tok: AutoTokenizer, image_processor: AutoImageProcessor, device: str, text_db: Chroma, image_db: Chroma, bm25_index: Optional[BM25Okapi], doc_map: dict):\n",
    "            self.llm = llm\n",
    "            self.tok = tok\n",
    "            self.image_processor = image_processor\n",
    "            self.device = device\n",
    "            self.text_db = text_db\n",
    "            self.image_db = image_db\n",
    "            self.bm25_index = bm25_index\n",
    "            self.doc_map = doc_map\n",
    "        \n",
    "        @staticmethod\n",
    "        def _reciprocal_rank_fusion(results: list[list[Document]], k: int = 60) -> list[tuple[Document, float]]:\n",
    "            \"\"\"Performs Reciprocal Rank Fusion on multiple ranked lists of documents.\"\"\"\n",
    "            ranked_lists = [ {doc.page_content: (doc, i + 1) for i, doc in enumerate(res)} for res in results]\n",
    "            rrf_scores = defaultdict(float)\n",
    "            all_docs = {}\n",
    "            \n",
    "            for ranked_list in ranked_lists:\n",
    "                # Iterate through each ranked list and calculate RRF scores\n",
    "                for content, (doc, rank) in ranked_list.items():\n",
    "                    rrf_scores[content] += 1 / (k + rank)\n",
    "                    if content not in all_docs: all_docs[content] = doc\n",
    "            fused_results = [(all_docs[content], rrf_scores[content]) for content in sorted(rrf_scores, key=rrf_scores.get, reverse=True)]\n",
    "            return fused_results\n",
    "\n",
    "        def _retrieve_mm(self, query: str, k_text: int = 3, k_img: int = 2, recall_k: int = 20) -> dict[str, any]:\n",
    "            \"\"\"Retrieves relevant documents and images based on the query using both dense and sparse retrieval methods.\"\"\"\n",
    "            dense_hits = self.text_db.similarity_search(query, k=recall_k)\n",
    "            \n",
    "            # If no dense hits, try sparse retrieval with BM25\n",
    "            sparse_hits = []\n",
    "            if self.bm25_index and list(self.doc_map.keys()):\n",
    "                tokenized_query = query.lower().split(\" \")\n",
    "                sparse_texts = self.bm25_index.get_top_n(tokenized_query, list(self.doc_map.keys()), n=recall_k)\n",
    "                sparse_hits = [self.doc_map[text] for text in sparse_texts]\n",
    "\n",
    "            if not dense_hits and not sparse_hits:\n",
    "                return {\"docs\": [], \"scores\": [], \"images\": []}\n",
    "\n",
    "            # Perform Reciprocal Rank Fusion on the hits\n",
    "            fused_results = self._reciprocal_rank_fusion([dense_hits, sparse_hits])\n",
    "            \n",
    "            # Limit to top k_text results\n",
    "            final_docs = [doc for doc, score in fused_results[:k_text]]\n",
    "            final_scores = [score for doc, score in fused_results[:k_text]]\n",
    "\n",
    "            # Retrieve images based on the sources of the final documents\n",
    "            retrieved_images = []\n",
    "            if final_docs and self.image_db:\n",
    "                final_sources = list(set(d.metadata[\"source\"] for d in final_docs))\n",
    "                image_hits = self.image_db.similarity_search(query, k=k_img, filter={\"source\": {\"$in\": final_sources}})\n",
    "                retrieved_images = [img.page_content for img in image_hits]\n",
    "\n",
    "            return {\"docs\": final_docs, \"scores\": final_scores, \"images\": retrieved_images}\n",
    "\n",
    "        def generate(self, query: str, **retrieval_kwargs) -> Dict[str, Any]:\n",
    "            \"\"\"Generates a response using the Qwen-VL RAG pipeline.\"\"\"\n",
    "            start_gen_time = time.time()\n",
    "            \n",
    "            # Retrieve relevant documents and images\n",
    "            hits = self._retrieve_mm(query, **retrieval_kwargs)\n",
    "            docs, images = hits[\"docs\"], hits[\"images\"]\n",
    "            \n",
    "            if not docs and not images:\n",
    "                return {\"reply\": \"Based on the provided context, I cannot answer this question.\", \"used_images\": [], \"generation_time_seconds\": 0.0}\n",
    "            \n",
    "            # Limit the number of images to 2 to save memory\n",
    "            if len(images) > 2:\n",
    "                logger.warning(f\"Limiting images from {len(images)} to 2 to save memory\")\n",
    "                images = images[:2]\n",
    "                \n",
    "            context_str = \"\\n\\n\".join(\n",
    "                f\"<source_document name=\\\"{d.metadata.get('source', 'unknown')}\\\">\\n{d.page_content}\\n</source_document>\"\n",
    "                for d in docs\n",
    "            )\n",
    "            \n",
    "            # Prepare the system prompt for the LLM\n",
    "            system_prompt = \"\"\"You are a Multimodal RAG Assistant. Your task is to answer the user's query using ONLY the provided context from retrieved documents and images.\n",
    "            \n",
    "            **Instructions:**\n",
    "            1. **Analyze Context:** Carefully examine the retrieved images and text documents provided in the context.\n",
    "            2. **Answer Directly:** Provide a clear, comprehensive answer to the user's query by synthesizing information from both text and image sources.\n",
    "            3. **Stay Focused:** Do not include unnecessary sections or verbose explanations. Answer the question directly and concisely.\n",
    "            4. **No Hallucination:** Use ONLY the information provided in the context. Do not make up facts or add information not present in the retrieved materials.\n",
    "            \n",
    "            **Output Format:**\n",
    "            - If the context is relevant: Provide a direct answer using the retrieved context.\n",
    "            - If the context is irrelevant: Respond with \"The provided context does not contain relevant information to answer the query.\"\n",
    "            \"\"\"\n",
    "            \n",
    "            # Prepare user content with images if available\n",
    "            if images:\n",
    "                # Use the standard Qwen2.5-VL image token format\n",
    "                image_tokens = \"\"\n",
    "                for i in range(len(images)):\n",
    "                    image_tokens += f\"<|vision_start|><|image_pad|><|vision_end|>\"\n",
    "                \n",
    "                user_content = f\"\"\"{image_tokens}\n",
    "    \n",
    "                <context>\n",
    "                {context_str}\n",
    "                </context>\n",
    "                \n",
    "                <user_query>\n",
    "                {query}\n",
    "                </user_query>\"\"\"\n",
    "            else:\n",
    "                user_content = f\"\"\"<context>\n",
    "                {context_str}\n",
    "                </context>\n",
    "                \n",
    "                <user_query>\n",
    "                {query}\n",
    "                </user_query>\"\"\"\n",
    "\n",
    "            messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_content}]\n",
    "            prompt_string = self.tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "            try:\n",
    "                self._clear_cuda()\n",
    "                sampling_params = SamplingParams(temperature=0.0, top_p=1.0, max_tokens=2048)\n",
    "                \n",
    "                # Process images if available\n",
    "                if images:\n",
    "                    pil_images = []\n",
    "                    for i, img_path in enumerate(images):\n",
    "                        try:\n",
    "                            img = PILImage.open(img_path).convert(\"RGB\")\n",
    "                            # Resize large images while preserving aspect ratio\n",
    "                            if img.size[0] > 512 or img.size[1] > 512:\n",
    "                                img.thumbnail((512, 512), PILImage.Resampling.LANCZOS)\n",
    "                            pil_images.append(img)\n",
    "                            logger.info(f\"Processed image {i+1}: {img_path} -> new size {img.size}\")\n",
    "                        except Exception as e:\n",
    "                            logger.warning(f\"Failed to process image {img_path}: {e}\")\n",
    "                            continue\n",
    "                    \n",
    "                    # If no images were successfully processed, proceed with text-only request\n",
    "                    if not pil_images:\n",
    "                        logger.warning(\"No images successfully processed, proceeding text-only\")\n",
    "                        request_payload = {\"prompt\": prompt_string}\n",
    "                    else:\n",
    "                        request_payload = {\n",
    "                            \"prompt\": prompt_string,\n",
    "                            \"multi_modal_data\": {\"image\": pil_images},\n",
    "                        }\n",
    "                else:\n",
    "                    request_payload = {\"prompt\": prompt_string}\n",
    "\n",
    "                # Generate the response using the LLM\n",
    "                output_list = self.llm.generate(request_payload, sampling_params=sampling_params)\n",
    "                reply = output_list[0].outputs[0].text.strip() if output_list and output_list[0].outputs else \"Error: no output from LLM.\"\n",
    "                \n",
    "                self._clear_cuda()\n",
    "                end_gen_time = time.time()\n",
    "                \n",
    "                # If the reply is empty or indicates no relevant context, handle it gracefully\n",
    "                if reply == \"The provided context does not contain relevant information to answer the query.\":\n",
    "                    images = []\n",
    "                    \n",
    "                return {\"reply\": reply, \"used_images\": images, \"generation_time_seconds\": end_gen_time - start_gen_time}\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                logger.error(\"Qwen-VL generation failed: %s\", e)\n",
    "                return {\"reply\": f\"Error during generation: {e}\", \"used_images\": images, \"generation_time_seconds\": 0.0}\n",
    "\n",
    "        def _clear_cuda(self):\n",
    "            \"\"\"Clears CUDA memory.\"\"\"\n",
    "            if torch.cuda.is_available():\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 2. MLflow `pyfunc` Life-cycle and Service Methods\n",
    "    # ==========================================================================\n",
    "    def load_context(self, context: mlflow.pyfunc.PythonModelContext) -> None:\n",
    "        \"\"\"Initializes the model and loads all necessary components into memory.\"\"\"\n",
    "        logger.info(\"--- Initializing Stateless MultimodalRAG Service (Qwen-VL) ---\")\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        logger.info(\"--- Service initialized with a single locked collection. ---\")\n",
    "        model_path = Path(context.artifacts[\"local_model_dir\"]).resolve()\n",
    "        e5_model_path = context.artifacts[\"e5_model_dir\"]\n",
    "        siglip_model_path = context.artifacts[\"siglip_model_dir\"]\n",
    "        \n",
    "        logger.info(\"Loading text embedding model (E5)...\")\n",
    "        self.text_embed_model = HuggingFaceEmbeddings(model_name=e5_model_path, model_kwargs={\"device\": self.device})\n",
    "        \n",
    "        logger.info(\"Loading image embedding model (SigLIP)...\")\n",
    "        self.siglip_embed_model = SiglipEmbeddings(model_id=siglip_model_path, device=self.device)\n",
    "\n",
    "        logger.info(\"Loading main LLM (Qwen-VL via vLLM)...\")\n",
    "        base_model_name = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "        self.tok = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "        if self.tok.pad_token is None: self.tok.pad_token = self.tok.eos_token\n",
    "        self.image_processor = AutoImageProcessor.from_pretrained(base_model_name, trust_remote_code=True, use_fast=True)\n",
    "        \n",
    "        if self.device == \"cuda\":\n",
    "            self.llm = LLM(\n",
    "                model=str(model_path),\n",
    "                quantization=\"gptq\",\n",
    "                gpu_memory_utilization=0.80,\n",
    "                max_model_len=4096,\n",
    "                enforce_eager=True,\n",
    "                limit_mm_per_prompt={\"image\": 2},\n",
    "                disable_custom_all_reduce=True,\n",
    "                tensor_parallel_size=1,\n",
    "                dtype=\"float16\",\n",
    "            )\n",
    "            logger.info(\"Initializing LocalGenAIJudge for self-evaluation...\")\n",
    "            self.judge = LocalGenAIJudge(llm=self.llm, tokenizer=self.tok)\n",
    "        else:\n",
    "            self.llm = None\n",
    "            self.judge = None # Judge is None if no CUDA LLM\n",
    "            logger.error(\"Qwen-VL with vLLM requires a CUDA device. LLM and Judge not loaded.\")\n",
    "        self.db_lock = threading.Lock()\n",
    "\n",
    "        self.text_collection_name = \"rag_text_collection\"\n",
    "        self.image_collection_name = \"rag_image_collection\"\n",
    "\n",
    "        # Use a persistent client that lives with the model\n",
    "        self.chroma_client = chromadb.Client()\n",
    "        self.text_vector_store = Chroma(\n",
    "            client=self.chroma_client,\n",
    "            collection_name=self.text_collection_name,\n",
    "            embedding_function=self.text_embed_model,\n",
    "        )\n",
    "        self.image_vector_store = Chroma(\n",
    "            client=self.chroma_client,\n",
    "            collection_name=self.image_collection_name,\n",
    "            embedding_function=self.siglip_embed_model,\n",
    "        )\n",
    "        \n",
    "        logger.info(\"--- Service initialized with all models loaded. Ready for queries. ---\")\n",
    "\n",
    "    def _build_transient_kb(self, config: dict, secrets: dict, temp_path: Path) -> Dict[str, Any]:\n",
    "        \"\"\"Fetches, processes, and indexes data entirely in memory from a given temp path.\"\"\"\n",
    "        logger.info(\"Cloning wiki to temporary directory...\")\n",
    "        orchestrate_wiki_clone(pat=secrets['AIS_ADO_TOKEN'], config=config, output_dir=temp_path)\n",
    "        \n",
    "        image_dir = temp_path / \"images\"\n",
    "        wiki_metadata_path = temp_path / \"wiki_flat_structure.json\"\n",
    "        \n",
    "        if not wiki_metadata_path.exists():\n",
    "            raise FileNotFoundError(\"Cloning failed: 'wiki_flat_structure.json' not found.\")\n",
    "        \n",
    "        all_raw_docs = load_mm_docs_clean(wiki_metadata_path, image_dir)\n",
    "        all_chunks = self._chunk_docs(all_raw_docs)\n",
    "\n",
    "        # 1. Wipe the text collection by deleting all existing documents by ID\n",
    "        existing_text_ids = self.text_vector_store._collection.get(include=[])['ids']\n",
    "        if existing_text_ids:\n",
    "            logger.info(f\"Wiping {len(existing_text_ids)} documents from text collection.\")\n",
    "            self.text_vector_store._collection.delete(ids=existing_text_ids)\n",
    "\n",
    "        # 2. Add new documents. LangChain handles the embeddings automatically.\n",
    "        if all_chunks:\n",
    "            self.text_vector_store.add_documents(documents=all_chunks)\n",
    "        logger.info(f\"Populated text collection with {len(all_chunks)} chunks.\")\n",
    "    \n",
    "        # 3. Wipe the image collection\n",
    "        img_paths, img_ids, img_meta = self._collect_image_vectors(all_raw_docs, image_dir)\n",
    "        existing_image_ids = self.image_vector_store._collection.get(include=[])['ids']\n",
    "        if existing_image_ids:\n",
    "            logger.info(f\"Wiping {len(existing_image_ids)} images from image collection.\")\n",
    "            self.image_vector_store._collection.delete(ids=existing_image_ids)\n",
    "\n",
    "        # 4. Add new images\n",
    "        if img_paths:\n",
    "            self.image_vector_store.add_texts(texts=img_paths, metadatas=img_meta, ids=img_ids)\n",
    "        logger.info(f\"Populated image collection with {len(img_paths)} images.\")\n",
    "\n",
    "        # BM25 index is still built in memory per request\n",
    "        unique_splits = list({doc.page_content: doc for doc in all_chunks}.values())\n",
    "        corpus = [doc.page_content for doc in unique_splits]\n",
    "        bm25_index = BM25Okapi([doc.split(\" \") for doc in corpus]) if corpus else None\n",
    "        doc_map = {doc.page_content: doc for doc in unique_splits}\n",
    "    \n",
    "        return {\n",
    "            \"text_db\": self.text_vector_store, \n",
    "            \"image_db\": self.image_vector_store, \n",
    "            \"bm25_index\": bm25_index, \n",
    "            \"doc_map\": doc_map\n",
    "        }\n",
    "\n",
    "    def predict(self, context: mlflow.pyfunc.PythonModelContext, model_input: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Processes a query using the stateless RAG pipeline.\"\"\"\n",
    "        with self.db_lock:\n",
    "            logger.info(\"Received new query. Lock acquired. Processing new request...\")\n",
    "            pipeline_start_time = time.time()\n",
    "            \n",
    "            # Validate input DataFrame\n",
    "            query = model_input[\"query\"].iloc[0]\n",
    "            payload = json.loads(model_input[\"payload\"].iloc[0])\n",
    "            transient_kb, rag_pipeline = None, None\n",
    "\n",
    "            # Create a temporary directory for transient KB\n",
    "            with tempfile.TemporaryDirectory() as temp_dir:\n",
    "                try:\n",
    "                    # Ensure the temp directory exists\n",
    "                    temp_path = Path(temp_dir)\n",
    "                    transient_kb = self._build_transient_kb(config=payload[\"config\"], secrets=payload[\"secrets\"], temp_path=temp_path)\n",
    "                    \n",
    "                    if not self.llm:\n",
    "                        raise RuntimeError(\"LLM not loaded. Cannot proceed with generation.\")\n",
    "                    \n",
    "                    # Initialize the RAG pipeline with the transient KB\n",
    "                    rag_pipeline = self.QwenVLMM(\n",
    "                        llm=self.llm, tok=self.tok, image_processor=self.image_processor,\n",
    "                        device=self.device, **transient_kb\n",
    "                    )\n",
    "                    \n",
    "                    # Perform the generation\n",
    "                    response_dict = rag_pipeline.generate(query)\n",
    "                    \n",
    "                    logger.info(\"Performing self-evaluation with LocalGenAIJudge...\")\n",
    "                    # Use the LocalGenAIJudge for self-evaluation\n",
    "                    if self.judge:\n",
    "                        context_str = \"\\n\\n\".join(d.page_content for d in transient_kb[\"text_db\"].similarity_search(query, k=3))\n",
    "                        eval_df = pd.DataFrame([{\"questions\": query, \"result\": response_dict[\"reply\"], \"source_documents\": context_str}])\n",
    "                        \n",
    "                        response_dict[\"faithfulness\"] = self.judge.evaluate_faithfulness(eval_df).iloc[0]\n",
    "                        response_dict[\"relevance\"] = self.judge.evaluate_relevance(eval_df).iloc[0]\n",
    "                    else:\n",
    "                        response_dict[\"faithfulness\"] = -1.0\n",
    "                        response_dict[\"relevance\"] = -1.0\n",
    "                    \n",
    "                    # Encode images to Base64 for the response\n",
    "                    base64_images = []\n",
    "                    for path in response_dict.get(\"used_images\", []):\n",
    "                        try:\n",
    "                            with open(path, \"rb\") as img_file:\n",
    "                                base64_images.append(base64.b64encode(img_file.read()).decode('utf-8'))\n",
    "                        except FileNotFoundError:\n",
    "                            logger.warning(f\"Image file not found at temp path: {path}\")\n",
    "                    response_dict[\"used_images\"] = json.dumps(base64_images)\n",
    "                    \n",
    "                    pipeline_end_time = time.time()\n",
    "                    response_dict[\"total_pipeline_time_seconds\"] = pipeline_end_time - pipeline_start_time\n",
    "    \n",
    "                    logger.info(\"Request finished. Releasing lock.\")\n",
    "                    return pd.DataFrame([response_dict])\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Stateless RAG pipeline failed while lock was held: {e}\", exc_info=True)\n",
    "                    return pd.DataFrame([{\"status\": \"error\", \"message\": str(e)}])\n",
    "                \n",
    "                finally:\n",
    "                    logger.info(\"Cleaning up transient KB objects and VRAM...\")\n",
    "                    del transient_kb, rag_pipeline\n",
    "                    gc.collect()\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                        torch.cuda.synchronize()\n",
    "                    logger.info(\"Cleanup complete.\")\n",
    "        \n",
    "    # ==========================================================================\n",
    "    # 3. Helper and Class Methods\n",
    "    # ==========================================================================\n",
    "    def _chunk_docs(self, docs: List[Document]) -> List[Document]:\n",
    "        \"\"\"Takes a list of raw docs and performs chunking with unique IDs per doc.\"\"\"\n",
    "        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[(\"#\", \"title\"), (\"##\", \"section\")])\n",
    "        recursive_splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=200)\n",
    "        all_chunks: list[Document] = []\n",
    "        \n",
    "        # Process each document, splitting by headers and then recursively splitting sections\n",
    "        for doc in docs:\n",
    "            page_title = Path(doc.metadata[\"source\"]).stem.replace(\"-\", \" \")\n",
    "            section_docs = header_splitter.split_text(doc.page_content)\n",
    "            doc_chunk_counter = 0\n",
    "            for section in section_docs:\n",
    "                tiny_texts = recursive_splitter.split_text(section.page_content)\n",
    "                for tiny in tiny_texts:\n",
    "                    chunk_metadata = {\"title\": page_title, \"source\": doc.metadata[\"source\"], \"section_header\": section.metadata.get(\"header\", \"\"),\"chunk_id\": doc_chunk_counter}\n",
    "                    all_chunks.append(Document(page_content=f\"{page_title}\\n\\n{tiny.strip()}\", metadata=chunk_metadata))\n",
    "                    doc_chunk_counter += 1\n",
    "        return all_chunks\n",
    "        \n",
    "    def _collect_image_vectors(self, mm_raw_docs: List[Document], image_dir: Path):\n",
    "        \"\"\"Scans raw docs and returns paths, IDs, and metadata for unique images.\"\"\"\n",
    "        img_paths, img_ids, img_meta = [], [], []\n",
    "        seen = set()\n",
    "        \n",
    "        # Ensure the image directory exists, process all images in the directory\n",
    "        for doc in mm_raw_docs:\n",
    "            src = doc.metadata[\"source\"]\n",
    "            for name in doc.metadata.get(\"images\", []):\n",
    "                img_id = f\"{src}::{name}\"\n",
    "                if img_id in seen: continue\n",
    "                seen.add(img_id)\n",
    "                img_path = image_dir / name\n",
    "                if img_path.is_file():\n",
    "                    img_paths.append(str(img_path))\n",
    "                    img_ids.append(img_id)\n",
    "                    img_meta.append({\"source\": src, \"image\": name})\n",
    "        return img_paths, img_ids, img_meta\n",
    "\n",
    "    @classmethod\n",
    "    def log_model(cls, model_name: str, local_model: str) -> None:\n",
    "        \"\"\"Logs the Multimodal RAG model to MLflow with all necessary artifacts.\"\"\"\n",
    "        logger.info(f\"--- Logging '{model_name}' Service to MLflow ---\")\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            temp_path = Path(temp_dir)\n",
    "            e5_path = temp_path / \"e5-large-v2\"\n",
    "            SentenceTransformer(\"intfloat/e5-large-v2\").save(str(e5_path))\n",
    "            \n",
    "            siglip_path = temp_path / \"siglip2-base-patch16-224\"\n",
    "            SiglipModel.from_pretrained(\"google/siglip2-base-patch16-224\").save_pretrained(siglip_path)\n",
    "            SiglipProcessor.from_pretrained(\"google/siglip2-base-patch16-224\").save_pretrained(siglip_path)\n",
    "            \n",
    "            artifacts = {\"local_model_dir\": local_model, \"e5_model_dir\": str(e5_path), \"siglip_model_dir\": str(siglip_path)}\n",
    "            \n",
    "            input_schema = Schema([\n",
    "                ColSpec(DataType.string, \"query\"),\n",
    "                ColSpec(DataType.string, \"payload\"),\n",
    "            ])\n",
    "            # Updated output schema - removed faithfulness and relevance\n",
    "            output_schema = Schema([\n",
    "                ColSpec(DataType.string, \"reply\", required=False),\n",
    "                ColSpec(DataType.string, \"used_images\", required=False),\n",
    "                ColSpec(DataType.double, \"total_pipeline_time_seconds\", required=False),\n",
    "                ColSpec(DataType.double, \"generation_time_seconds\", required=False),\n",
    "                ColSpec(DataType.double, \"faithfulness\", required=False),\n",
    "                ColSpec(DataType.double, \"relevance\", required=False),\n",
    "                ColSpec(DataType.string, \"status\", required=False),\n",
    "                ColSpec(DataType.string, \"message\", required=False),\n",
    "            ])\n",
    "            \n",
    "            signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "\n",
    "            mlflow.pyfunc.log_model(\n",
    "                artifact_path=model_name, python_model=cls(), artifacts=artifacts,\n",
    "                pip_requirements=\"../requirements.txt\", signature=signature, code_paths=[\"../src\"]\n",
    "            )\n",
    "        logger.info(f\"✅ Successfully logged '{model_name}' service and cleaned up.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfee18b-a495-42c0-aaf4-a37a4d5009eb",
   "metadata": {},
   "source": [
    "## Step 3: Start Run, Log & Register Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97c7b785-0628-4358-8e8d-9d49a0d6f1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-05 03:08:23 - INFO - Started MLflow run: 22a5e70839ea457797fc3067e52d7152\n",
      "2025-08-05 03:08:23 - INFO - --- Logging 'AIStudio-Multimodal-Chatbot-Model' Service to MLflow ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a6d63e20f4c4af599762f256658459a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d12678cb6e4462ae10e9e0ca15ec60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c77b229d3eb24415a12b5879fa0bb2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c33a0a80e50f4fa6b0b88b85f009e483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a447e8646be4833b1658c2de1ebf25c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f83c54c3b09f46d58c853ec2ca1b04a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10004148c096400dac4b01ec5032de33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfdd14b333434bd89d857104c5d17f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "017c1db4fdf3412cb9fccce35817dc1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c42b010a00cc4220afdddb3f70ed5c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/201 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "949488915b324d12a48d17e487274016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "000c1e2498754866b7d3683e20b65aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73406775a08b42828a13ec376747f5fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-05 03:12:16 - INFO - ✅ Successfully logged 'AIStudio-Multimodal-Chatbot-Model' service and cleaned up.\n",
      "2025-08-05 03:12:16 - INFO - Registering model from URI: runs:/22a5e70839ea457797fc3067e52d7152/AIStudio-Multimodal-Chatbot-Model\n",
      "2025-08-05 03:12:16 - INFO - ✅ Successfully registered model 'AIStudio-Multimodal-Chatbot-Model'\n",
      "CPU times: user 9.59 s, sys: 45.1 s, total: 54.6 s\n",
      "Wall time: 3min 53s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'AIStudio-Multimodal-Chatbot-Model'.\n",
      "Created version '1' of model 'AIStudio-Multimodal-Chatbot-Model'.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# --- Start MLflow Run and Log the Model ---\n",
    "try:\n",
    "    with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "        run_id = run.info.run_id\n",
    "        logger.info(f\"Started MLflow run: {run_id}\")\n",
    "\n",
    "        # Use the class method to log the model and its artifacts\n",
    "        MultimodalRagModel.log_model(model_name=MODEL_NAME, local_model=LOCAL_MODEL)\n",
    "\n",
    "        model_uri = f\"runs:/{run_id}/{MODEL_NAME}\"\n",
    "        logger.info(f\"Registering model from URI: {model_uri}\")\n",
    "        \n",
    "        # Register the model in the MLflow Model Registry\n",
    "        mlflow.register_model(model_uri=model_uri, name=MODEL_NAME)\n",
    "        logger.info(f\"✅ Successfully registered model '{MODEL_NAME}'\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    logger.error(f\"Error: A required file or directory was not found. Please ensure the project structure is correct.\")\n",
    "    logger.error(f\"Details: {e}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred during the MLflow run: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c628e7f-9d9d-4e7a-9790-34ed1b003caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-05 03:12:16 - INFO - Found latest version '1' for model 'AIStudio-Multimodal-Chatbot-Model' in stage 'None'.\n",
      "CPU times: user 6.82 ms, sys: 1.29 ms, total: 8.11 ms\n",
      "Wall time: 50.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Retrieve the latest version from the Model Registry ---\n",
    "try:\n",
    "    client = MlflowClient()\n",
    "    versions = client.get_latest_versions(MODEL_NAME, stages=[\"None\"])\n",
    "    if not versions:\n",
    "        raise RuntimeError(f\"No registered versions found for model '{MODEL_NAME}'.\")\n",
    "    \n",
    "    latest_version = versions[0]\n",
    "    logger.info(f\"Found latest version '{latest_version.version}' for model '{MODEL_NAME}' in stage '{latest_version.current_stage}'.\")\n",
    "    model_uri_registry = latest_version.source\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to retrieve model from registry: {e}\", exc_info=True)\n",
    "    model_uri_registry = None # Ensure variable exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c31856b3-5ff5-48b9-98b7-ddbf7dd16be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-05 03:12:16 - INFO - Loading model from: /phoenix/mlflow/532448835017385957/22a5e70839ea457797fc3067e52d7152/artifacts/AIStudio-Multimodal-Chatbot-Model\n",
      "2025-08-05 03:12:16 - INFO - --- Initializing Stateless MultimodalRAG Service (Qwen-VL) ---\n",
      "2025-08-05 03:12:16 - INFO - --- Service initialized with a single locked collection. ---\n",
      "2025-08-05 03:12:16 - INFO - Loading text embedding model (E5)...\n",
      "2025-08-05 03:12:24 - INFO - Loading image embedding model (SigLIP)...\n",
      "2025-08-05 03:12:37 - INFO - Loading main LLM (Qwen-VL via vLLM)...\n",
      "INFO 08-05 03:12:51 [config.py:841] This model supports multiple tasks: {'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.\n",
      "WARNING 08-05 03:12:51 [config.py:3371] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 08-05 03:12:51 [config.py:1472] Using max model len 4096\n",
      "WARNING 08-05 03:12:51 [config.py:960] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "WARNING 08-05 03:12:51 [arg_utils.py:1735] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
      "WARNING 08-05 03:12:51 [cuda.py:102] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 08-05 03:12:51 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.2) with config: model='/phoenix/mlflow/532448835017385957/22a5e70839ea457797fc3067e52d7152/artifacts/AIStudio-Multimodal-Chatbot-Model/artifacts/Qwen2.5-VL-7B-Instruct-GPTQ-Int4', speculative_config=None, tokenizer='/phoenix/mlflow/532448835017385957/22a5e70839ea457797fc3067e52d7152/artifacts/AIStudio-Multimodal-Chatbot-Model/artifacts/Qwen2.5-VL-7B-Instruct-GPTQ-Int4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=gptq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/phoenix/mlflow/532448835017385957/22a5e70839ea457797fc3067e52d7152/artifacts/AIStudio-Multimodal-Chatbot-Model/artifacts/Qwen2.5-VL-7B-Instruct-GPTQ-Int4, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "WARNING 08-05 03:12:52 [interface.py:382] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 08-05 03:12:52 [cuda.py:311] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 08-05 03:12:52 [cuda.py:360] Using XFormers backend.\n",
      "INFO 08-05 03:12:53 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 08-05 03:12:53 [model_runner.py:1171] Starting to load model /phoenix/mlflow/532448835017385957/22a5e70839ea457797fc3067e52d7152/artifacts/AIStudio-Multimodal-Chatbot-Model/artifacts/Qwen2.5-VL-7B-Instruct-GPTQ-Int4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc8df0e5a284db1be95866b07a52d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-05 03:13:23 [default_loader.py:272] Loading weights took 29.08 seconds\n",
      "INFO 08-05 03:13:24 [model_runner.py:1203] Model loading took 6.5963 GiB and 29.846687 seconds\n",
      "WARNING 08-05 03:13:25 [profiling.py:271] The sequence length (4096) is smaller than the pre-defined wosrt-case total number of multimodal tokens (32768). This may cause certain multi-modal inputs to fail during inference. To avoid this, you should increase `max_model_len` or reduce `mm_counts`.\n",
      "WARNING 08-05 03:13:25 [model_runner.py:1368] Computed max_num_seqs (min(256, 5120 // 49152)) to be less than 1. Setting it to the minimum value of 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-05 03:13:30 [profiling.py:237] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 5120) is too short to hold the multi-modal embeddings in the worst case (49152 tokens in total, out of which {'image': 32768, 'video': 16384} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "INFO 08-05 03:14:20 [worker.py:294] Memory profiling takes 56.07 seconds\n",
      "INFO 08-05 03:14:20 [worker.py:294] the current vLLM instance can use total_gpu_memory (24.00GiB) x gpu_memory_utilization (0.80) = 19.20GiB\n",
      "INFO 08-05 03:14:20 [worker.py:294] model weights take 6.60GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 7.52GiB; the rest of the memory reserved for KV Cache is 5.04GiB.\n",
      "INFO 08-05 03:14:21 [executor_base.py:113] # cuda blocks: 5894, # CPU blocks: 4681\n",
      "INFO 08-05 03:14:21 [executor_base.py:118] Maximum concurrency for 4096 tokens per request: 23.02x\n",
      "INFO 08-05 03:14:22 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 57.97 seconds\n",
      "2025-08-05 03:14:22 - INFO - Initializing LocalGenAIJudge for self-evaluation...\n",
      "2025-08-05 03:14:22 - INFO - --- Service initialized with all models loaded. Ready for queries. ---\n",
      "2025-08-05 03:14:22 - INFO - ✅ Successfully loaded model from registry.\n",
      "CPU times: user 1min 19s, sys: 30.6 s, total: 1min 49s\n",
      "Wall time: 2min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if model_uri_registry:\n",
    "    try:\n",
    "        logger.info(f\"Loading model from: {model_uri_registry}\")\n",
    "        loaded_model = mlflow.pyfunc.load_model(model_uri=model_uri_registry)\n",
    "        logger.info(\"✅ Successfully loaded model from registry.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load model from registry URI: {e}\", exc_info=True)\n",
    "        loaded_model = None\n",
    "else:\n",
    "    logger.warning(\"Skipping model loading due to previous errors.\")\n",
    "    loaded_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2d9db6-b2dd-44e5-989d-ebb498372229",
   "metadata": {},
   "source": [
    "## Step 4: Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "279582c0-74e3-4b98-9d1d-6007c98f109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(query: str, result_df: pd.DataFrame):\n",
    "    \"\"\"Helper to neatly print the query, reply, and display Base64 images.\"\"\"\n",
    "    if result_df.empty:\n",
    "        print(\"Received an empty result.\")\n",
    "        return\n",
    "\n",
    "    # Extract results from the DataFrame\n",
    "    row = result_df.iloc[0]\n",
    "    reply = row.get(\"reply\", \"No reply generated.\")\n",
    "    # This field now contains a JSON string of a list of Base64 strings\n",
    "    used_images_json = row.get(\"used_images\", \"[]\")\n",
    "    gen_time = row.get(\"generation_time_seconds\", 0)\n",
    "    total_pipeline_time_seconds = row.get(\"total_pipeline_time_seconds\", 0)\n",
    "    faithfulness = row.get(\"faithfulness\", 0)\n",
    "    relevance = row.get(\"relevance\", 0)\n",
    "\n",
    "    # Safely parse the JSON string of Base64 images\n",
    "    base64_images = []\n",
    "    try:\n",
    "        # Use json.loads to parse the string into a list\n",
    "        base64_images = json.loads(used_images_json)\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        print(\"Warning: Could not parse image data from the API response.\")\n",
    "\n",
    "    # Display the output\n",
    "    print(\"---\" * 20)\n",
    "    print(f\"❓ Query:\\n{query}\\n\")\n",
    "    print(f\"🤖 Reply:\")\n",
    "    display(Markdown(reply)) # Render markdown for better formatting\n",
    "    \n",
    "    print(f\"\\n📊 Faithfulness: {faithfulness:.4f} | Relevance: {relevance:.4f}\")\n",
    "    print(f\"⏱️ Generation Time: {gen_time:.2f}s\\n\")\n",
    "    print(f\" Total Pipeline Time: {total_pipeline_time_seconds:.2f}s\\n\")\n",
    "\n",
    "    if base64_images and isinstance(base64_images, list):\n",
    "        print(f\"🖼️ Displaying {len(base64_images)} retrieved image(s):\")\n",
    "        for b64_string in base64_images:\n",
    "            try:\n",
    "                # Decode the Base64 string into bytes\n",
    "                image_bytes = base64.b64decode(b64_string)\n",
    "                # Display the image directly from the bytes data\n",
    "                display(Image(data=image_bytes, width=400))\n",
    "            except Exception as e:\n",
    "                print(f\"  - Could not decode or display an image: {e}\")\n",
    "    else:\n",
    "        print(\"▶ No images were retrieved for this query.\")\n",
    "    print(\"---\" * 20 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1230ed-5e16-4c18-8115-47dffd40bc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "all_results = []\n",
    "\n",
    "if loaded_model:\n",
    "    logger.info(\"--- Running sample inference with the loaded model ---\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Load config and secrets ONCE before making queries.\n",
    "        config = load_config(CONFIG_PATH)\n",
    "        ADO_PAT = os.getenv(\"AIS_ADO_TOKEN\")\n",
    "        if not ADO_PAT:\n",
    "            logger.info(\"Environment variable not found. Falling back to secrets.yaml.\")\n",
    "            secrets = load_secrets(SECRETS_PATH)\n",
    "            ADO_PAT = secrets.get('AIS_ADO_TOKEN')\n",
    "\n",
    "        # 2. Construct the payload dictionary that will be sent with each request.\n",
    "        request_payload_dict = {\n",
    "            \"config\": config,\n",
    "            \"secrets\": {\"AIS_ADO_TOKEN\": ADO_PAT}\n",
    "        }\n",
    "        \n",
    "        sample_queries = [\n",
    "            \"What are the AI Blueprints Repository best practices?\",\n",
    "            \"What are some feature flags that I can enable in AIStudio?\",\n",
    "            \"How do I manually clean my environment without hooh?\",\n",
    "        ]\n",
    "\n",
    "        # 3. For each query, send BOTH the query and the payload.\n",
    "        for query in sample_queries:\n",
    "            logger.info(f\"Processing query: '{query}'...\")\n",
    "            \n",
    "            # Create the DataFrame that matches the model's signature\n",
    "            prediction_df = pd.DataFrame([{\n",
    "                \"query\": query,\n",
    "                \"payload\": json.dumps(request_payload_dict),\n",
    "            }])\n",
    "            \n",
    "            result_df = loaded_model.predict(prediction_df)\n",
    "            result_df['query'] = query\n",
    "            display_results(query, result_df)\n",
    "\n",
    "            all_results.append(result_df)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Prediction failed: {e}\", exc_info=True)\n",
    "\n",
    "    # --- ADD: Combine all individual results into one DataFrame ---\n",
    "    if all_results:\n",
    "        final_results_df = pd.concat(all_results, ignore_index=True)\n",
    "    else:\n",
    "        final_results_df = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    logger.warning(\"Skipping sample inference because the model was not loaded.\")\n",
    "    # --- ADD: Ensure the variable exists ---\n",
    "    final_results_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83806df-79a8-4273-ab62-96c4bf1c2ded",
   "metadata": {},
   "source": [
    "## Step 5: Log Hallucinations & Relevance Evaluations to MlFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c6845fe-be61-4a87-b42f-b4e9ac057066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-05 03:20:23 - INFO - --- Reopening original run (22a5e70839ea457797fc3067e52d7152) to log pre-computed evaluations ---\n",
      "2025-08-05 03:20:23 - INFO - Successfully reopened existing run. Logging metrics and artifacts...\n",
      "2025-08-05 03:20:23 - INFO - ✅ Successfully logged metrics and artifacts to the original model run.\n"
     ]
    }
   ],
   "source": [
    "if loaded_model and 'run_id' in locals() and not final_results_df.empty:\n",
    "    logger.info(f\"--- Reopening original run ({run_id}) to log pre-computed evaluations ---\")\n",
    "\n",
    "    # The results_df already contains the scores from the `predict` calls\n",
    "    results_df = final_results_df\n",
    "    \n",
    "    # Reopen the existing run using its ID\n",
    "    with mlflow.start_run(run_id=run_id) as run:\n",
    "        logger.info(\"Successfully reopened existing run. Logging metrics and artifacts...\")\n",
    "\n",
    "        # Calculate average scores from the DataFrame\n",
    "        avg_faithfulness = results_df[\"faithfulness\"].astype(float).mean()\n",
    "        avg_relevance = results_df[\"relevance\"].astype(float).mean()\n",
    "        avg_pipeline_time = results_df[\"total_pipeline_time_seconds\"].astype(float).mean()\n",
    "\n",
    "        # Log the average scores as metrics\n",
    "        mlflow.log_metrics({\n",
    "            \"avg_faithfulness\": avg_faithfulness,\n",
    "            \"avg_relevance\": avg_relevance,\n",
    "            \"avg_pipeline_time_seconds\": avg_pipeline_time,\n",
    "        })\n",
    "\n",
    "        # Log the detailed results as a table artifact\n",
    "        mlflow.log_table(\n",
    "            data=results_df[['query', 'reply', 'faithfulness', 'relevance', 'total_pipeline_time_seconds']], \n",
    "            artifact_file=\"stateless_evaluation_results.json\"\n",
    "        )\n",
    "        \n",
    "        logger.info(\"✅ Successfully logged metrics and artifacts to the original model run.\")\n",
    "\n",
    "else:\n",
    "    logger.warning(\"Skipping evaluation logging because the model was not loaded, run_id was not found, or no results were generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "725e1d0b-333c-43fd-95a7-5ce13fdfe3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-05 03:20:23 - INFO - ⏱️ Total execution time: 12m 17.42s\n",
      "2025-08-05 03:20:23 - INFO - ✅ Notebook execution completed.\n"
     ]
    }
   ],
   "source": [
    "end_time: float = time.time()\n",
    "elapsed_time: float = end_time - start_time\n",
    "elapsed_minutes: int = int(elapsed_time // 60)\n",
    "elapsed_seconds: float = elapsed_time % 60\n",
    "\n",
    "logger.info(f\"⏱️ Total execution time: {elapsed_minutes}m {elapsed_seconds:.2f}s\")\n",
    "logger.info(\"✅ Notebook execution completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e89eee-6171-446a-8da6-9381d3fe10c0",
   "metadata": {},
   "source": [
    "Built with ❤️ using Z by HP AI Studio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
