{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9ff4be-1fee-47eb-8d09-611c29a7a83f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h1 style=\"text-align: center; font-size: 50px;\">Multimodal RAG Chatbot with Langchain and VLLM</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbb3e26-125f-4151-9ed3-c84a3fd9081d",
   "metadata": {},
   "source": [
    "Retrieval-Augmented Generation (RAG) is an architectural approach that can enhance the effectiveness of large language model (LLM) applications using customized data. In this example, we use LangChain, an orchestrator for language pipelines, to build an assistant capable of loading information from a web page and use it for answering user questions. We'll leverage torch and transformers for multimodal model support in Python. We'll also use the MLFlow platform to evaluate and trace the LLM responses (in `register-workflow.ipynb`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a55c55-a1dc-4697-b920-2a81f1bb0f74",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "- Configuring the Environment\n",
    "- Data Loading & Cleaning\n",
    "- Setup Embeddings & Vector Store\n",
    "- Retrieval Function\n",
    "- Model Setup & Chain Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f78b36-56e2-4596-8325-c75901f30c76",
   "metadata": {},
   "source": [
    "## Step 0: Configuring the Environment\n",
    "\n",
    "In this step, we import all the necessary libraries and internal components required to run the RAG pipeline, including modules for notebook parsing, embedding generation, vector storage, and code generation with LLMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7156d10f-d930-4be7-a9e8-15606d466460",
   "metadata": {},
   "source": [
    "By using our Local GenAI workspace image, many of the necessary libraries to work with RAG already come pre-installed - in our case, we just need to extra support for multimodal processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "022490c4-8d8a-4c53-92ac-c62ea70a8209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os \n",
    "from pathlib import Path\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "# Create logger\n",
    "logger = logging.getLogger(\"multimodal_rag_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\") \n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f912d15a-7aab-4620-8c89-33c8f07104f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 02:56:44 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()  \n",
    "\n",
    "logger.info('Notebook execution started.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e700f51-9011-401c-90d3-a07ea8238955",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77853772-0239-40d0-94be-7a62fcb465c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 03:02:02.832750: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-05 03:02:02.846118: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754362922.861433     603 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754362922.866025     603 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754362922.878492     603 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754362922.878508     603 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754362922.878510     603 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754362922.878511     603 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-05 03:02:02.882429: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-05 03:02:06 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "# === Standard Library Imports ===\n",
    "import gc\n",
    "import json\n",
    "import math\n",
    "import hashlib\n",
    "import shutil\n",
    "import warnings\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from rank_bm25 import BM25Okapi\n",
    "from statistics import mean\n",
    "from typing import Any, Dict, List, Optional, TypedDict\n",
    "from IPython.display import display, Markdown\n",
    "from collections import defaultdict\n",
    "\n",
    "# === Third-Party Library Imports ===\n",
    "import mlflow\n",
    "import torch\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain.schema.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from PIL import Image as PILImage\n",
    "from transformers import AutoImageProcessor, AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# === Project-Specific Imports ===\n",
    "from src.components import SemanticCache, SiglipEmbeddings\n",
    "from src.wiki_pages_clone import orchestrate_wiki_clone\n",
    "from src.utils import (\n",
    "    configure_hf_cache,\n",
    "    multimodal_rag_asset_status,\n",
    "    load_config,\n",
    "    load_secrets,\n",
    "    load_mm_docs_clean,\n",
    "    display_images,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd3b8d2c-d0e1-4409-86dc-470c8454fff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7384967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1c0c3a0-9264-4916-844f-6cab88bd9de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Container PyTorch version: 2.7.0+cu126\n",
      "--- Container CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"--- Container PyTorch version: {torch.__version__}\")\n",
    "print(f\"--- Container CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e80960-54f9-4ec2-b062-6d49526494d1",
   "metadata": {},
   "source": [
    "\n",
    "### Verify Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75f2111d-5508-41ba-967c-256b40a9fb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 03:02:08 - INFO - Local Model is properly configured. \n",
      "2025-08-05 03:02:08 - INFO - Config is properly configured. \n",
      "2025-08-05 03:02:08 - INFO - Secrets is properly configured. \n",
      "2025-08-05 03:02:08 - INFO - wiki_flat_structure.json is not properly configured. Place JSON Wiki Pages in data/\n",
      "2025-08-05 03:02:08 - INFO - CONTEXT is properly configured. \n",
      "2025-08-05 03:02:08 - INFO - CHROMA is properly configured. \n",
      "2025-08-05 03:02:08 - INFO - CACHE is properly configured. \n",
      "2025-08-05 03:02:08 - INFO - MANIFEST is not properly configured. Please check if the MANIFEST path was properly configured in your project on AI Studio.\n"
     ]
    }
   ],
   "source": [
    "CONFIG_PATH = \"../configs/config.yaml\"\n",
    "SECRETS_PATH = \"../configs/secrets.yaml\"\n",
    "\n",
    "LOCAL_MODEL_PATH: Path = Path(\"/home/jovyan/datafabric/Qwen2.5-VL-7B-Instruct-GPTQ-Int4\")\n",
    "CONTEXT_DIR: Path = Path(\"../data/context\")             \n",
    "CHROMA_DIR: Path = Path(\"../data/chroma_store\")     \n",
    "CACHE_DIR: Path = CHROMA_DIR / \"semantic_cache\"\n",
    "MANIFEST_PATH: Path = CHROMA_DIR / \"manifest.json\"\n",
    "\n",
    "IMAGE_DIR = CONTEXT_DIR / \"images\"\n",
    "WIKI_METADATA_DIR = CONTEXT_DIR / \"wiki_flat_structure.json\"\n",
    "\n",
    "DEMO_FOLDER = \"../demo\"\n",
    "\n",
    "CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "multimodal_rag_asset_status(\n",
    "    local_model_path=LOCAL_MODEL_PATH,\n",
    "    config_path=CONFIG_PATH,\n",
    "    secrets_path=SECRETS_PATH,\n",
    "    wiki_metadata_dir=WIKI_METADATA_DIR,\n",
    "    context_dir=CONTEXT_DIR,\n",
    "    chroma_dir=CHROMA_DIR,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    manifest_path=MANIFEST_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7062ee0",
   "metadata": {},
   "source": [
    "### Config Loading\n",
    "\n",
    "In this section, we load configuration parameters from the YAML file in the configs folder.\n",
    "\n",
    "- **config.yaml**: Contains non-sensitive configuration parameters like model sources and URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e0b7464-ad76-47f2-94f0-e70c4d4cc380",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ca6361-dd3c-4778-9a9d-b11da7b20151",
   "metadata": {},
   "source": [
    "### Config HuggingFace Caches\n",
    "\n",
    "In the next cell, we configure HuggingFace cache, so that all the models downloaded from them are persisted locally, even after the workspace is closed. This is a future desired feature for AI Studio and the GenAI addon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965e4ea4-084a-4f32-9f51-9ead89dd16fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure HuggingFace cache\n",
    "configure_hf_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41d47161-f197-409b-9a43-a3f59f1f695c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0296471e55c94af284f2bffb399a1771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb0109f087214356a1d1a1acf816f53f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a91434e7c1344022a426c21789876b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a10c52469fe4e0ba52d92324f9e1fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cdd5c4727f14073a99e5771c7f2b682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1671ce3da4254930a92b20ef438bb8c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb22242400a4d68923f8f57a1f446c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a9df8e126245a182674773c826caf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1347c5856d98477982c870fd34793cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec1d442918c44eab8f55cabf2251d178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/201 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.56 s, sys: 2.36 s, total: 4.92 s\n",
      "Wall time: 22.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Initialize HuggingFace Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/e5-large-v2\",\n",
    "    cache_folder=\"/tmp/hf_cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d92659e-23ea-4965-bb27-58cd1dd0f3ec",
   "metadata": {},
   "source": [
    "## Step 1: Data Loading & Cleaning\n",
    "\n",
    "`wiki_flat_structure.json` is a custom json metadata for ADO Wiki data. It is flatly structured, with keys for filepath, md content, and a list of images. We also have a image folder that contains all the images for every md page. We directly scrape this data from ADO and perform any cleanup if necessary.\n",
    "\n",
    "- **secrets.yaml**: For Freemium users, use secrets.yaml to store your sensitive data like API Keys. If you are a Premium user, you can use secrets manager.\n",
    "- **AIS Secrets Manager**: For Paid users, use the secrets manager in the `Project Setup` tab to configure your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "915bdc8a-0cb9-4b17-8d8f-a56acb7ffcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 03:02:30 - INFO - Environment variable not found... Secrets Manager not properly set. Falling to secrets.yaml.\n",
      "2025-08-05 03:02:30 - INFO - Starting ADO Wiki clone process...\n",
      "2025-08-05 03:02:30 - INFO - Cloning wiki 'Phoenix-DS-Platform.wiki' to temporary directory: /tmp/tmpd9whd777\n",
      "2025-08-05 03:02:44 - INFO - Scanning for Markdown files...\n",
      "2025-08-05 03:02:45 - INFO - → Found 575 Markdown pages.\n",
      "2025-08-05 03:02:45 - INFO - Copying referenced images to ../data/context/images...\n",
      "2025-08-05 03:02:56 - INFO - → 792 unique images copied.\n",
      "2025-08-05 03:02:56 - INFO - Assembling flat JSON structure...\n",
      "2025-08-05 03:02:56 - INFO - ✅ Wiki data successfully cloned to ../data/context\n",
      "2025-08-05 03:02:56 - INFO - Cleaned up temporary directory: /tmp/tmpd9whd777\n",
      "2025-08-05 03:02:56 - INFO - ✅ Wiki data preparation step completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.2 s, sys: 1.36 s, total: 2.56 s\n",
      "Wall time: 25.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ADO_PAT = os.getenv(\"AIS_ADO_TOKEN\")\n",
    "if not ADO_PAT:\n",
    "    logger.info(\"Environment variable not found... Secrets Manager not properly set. Falling to secrets.yaml.\")\n",
    "    try:\n",
    "        secrets = load_secrets(SECRETS_PATH)\n",
    "        ADO_PAT = secrets.get('AIS_ADO_TOKEN')\n",
    "    except NameError:\n",
    "        logger.error(\"The 'secrets' object is not defined or available.\")\n",
    "\n",
    "try:\n",
    "    orchestrate_wiki_clone(\n",
    "        pat=ADO_PAT,\n",
    "        config=config,\n",
    "        output_dir=CONTEXT_DIR\n",
    "    )\n",
    "    logger.info(\"✅ Wiki data preparation step completed successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(\"Halting notebook execution due to a critical error in the wiki preparation step.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb4c1589-285f-4e72-ab8e-5ae9fbd4043d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 03:02:57 - WARNING - ⚠️ 95 broken image refs filtered out\n",
      "2025-08-05 03:02:57 - INFO - Docs loaded: 575 docs, avg_tokens=3138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 67.5 ms, sys: 81.6 ms, total: 149 ms\n",
      "Wall time: 1.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "WIKI_METADATA_DIR   = Path(WIKI_METADATA_DIR)\n",
    "IMAGE_DIR = Path(IMAGE_DIR)\n",
    "\n",
    "mm_raw_docs = load_mm_docs_clean(WIKI_METADATA_DIR, Path(IMAGE_DIR))\n",
    "\n",
    "def log_stage(name: str, docs: List[Document]):\n",
    "    logger.info(f\"{name}: {len(docs)} docs, avg_tokens={sum(len(d.page_content) for d in docs)/len(docs):.0f}\")\n",
    "log_stage(\"Docs loaded\", mm_raw_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc63ed05-192e-4f59-856c-cf9bff160fc8",
   "metadata": {},
   "source": [
    "## Step 2: Creation of Chunks\n",
    "\n",
    "Here, we split the loaded documents into chunks, so we have smaller and more specific texts to add to our vector database. \n",
    "\n",
    "We chunk based on header style, and then within each header style we futher chunk based on the provided chunk size. Each chunk retains the page name, which preserves the relevance of each chunk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3dbd381-58b8-4569-aa75-b26d07556b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 03:02:57 - INFO - Chunking complete: 575 docs → 2678 chunks (avg 721 chars)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 133 ms, sys: 3.89 ms, total: 137 ms\n",
      "Wall time: 136 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def chunk_documents(\n",
    "    docs,\n",
    "    chunk_size: int = 1200,\n",
    "    overlap: int = 200,\n",
    ") -> list[Document]:\n",
    "    \"\"\"\n",
    "    1) Split each wiki page on Markdown headers (#, ## …) to keep logical\n",
    "       sections together.\n",
    "    2) Recursively break long sections to <= `chunk_size` chars with `overlap`.\n",
    "    3) Prefix every chunk with its page-title and store the title in metadata.\n",
    "    \"\"\"\n",
    "    header_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=[(\"#\", \"title\"), (\"##\", \"section\")]\n",
    "    )\n",
    "    recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "    )\n",
    "\n",
    "    all_chunks: list[Document] = []\n",
    "    for doc in docs:\n",
    "        page_title = Path(doc.metadata[\"source\"]).stem.replace(\"-\", \" \")\n",
    "\n",
    "        # 1️. section‑level split (returns list[Document])\n",
    "        section_docs = header_splitter.split_text(doc.page_content)\n",
    "\n",
    "        for section in section_docs:\n",
    "            # 2. size‑based split inside each section\n",
    "            tiny_texts = recursive_splitter.split_text(section.page_content)\n",
    "\n",
    "            for idx, tiny in enumerate(tiny_texts):\n",
    "                all_chunks.append(\n",
    "                    Document(\n",
    "                        page_content=f\"{page_title}\\n\\n{tiny.strip()}\",\n",
    "                        metadata={\n",
    "                            \"title\": page_title,\n",
    "                            \"source\": doc.metadata[\"source\"],\n",
    "                            \"section_header\": section.metadata.get(\"header\", \"\"),\n",
    "                            \"chunk_id\": idx,\n",
    "                        },\n",
    "                    )\n",
    "                )\n",
    "    if all_chunks:\n",
    "        avg_len = int(mean(len(c.page_content) for c in all_chunks))\n",
    "        logger.info(\n",
    "            \"Chunking complete: %d docs → %d chunks (avg %d chars)\",\n",
    "            len(docs),\n",
    "            len(all_chunks),\n",
    "            avg_len,\n",
    "        )\n",
    "    else:\n",
    "        logger.warning(\"Chunking produced zero chunks for %d docs\", len(docs))\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "splits = chunk_documents(mm_raw_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33fda9b-e514-4ecd-b480-f07955e08233",
   "metadata": {},
   "source": [
    "## Step 3: Setup Embeddings & Vector Store\n",
    "Here we setup Siglip for Image embeddings, and also transform our cleaned text chunks into embeddings to be stored in Chroma. We store the chroma data locally on the disk to reduce memory usage. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cd5fbc",
   "metadata": {},
   "source": [
    "### Setup Text ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e83d3d3f-87a8-4209-a805-59983479d629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 03:02:57 - INFO - Chroma directory or manifest not found. A rebuild is required.\n",
      "2025-08-05 03:02:57 - WARNING - REBUILDING: Wiping old ChromaDB store at ../data/chroma_store\n",
      "2025-08-05 03:02:58 - INFO - Creating new text context index in ../data/chroma_store ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min, sys: 1.95 s, total: 1min 2s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 1) TEXT store\n",
    "def _current_manifest() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Returns a dictionary mapping every context JSON file to its SHA256 content hash.\n",
    "    This allows detecting changes in file content, not just filenames.\n",
    "    \"\"\"\n",
    "    manifest = {}\n",
    "    json_files = sorted(CONTEXT_DIR.rglob(\"*.json\"))\n",
    "\n",
    "    for file_path in json_files:\n",
    "        try:\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                file_bytes = f.read()\n",
    "                file_hash = hashlib.sha256(file_bytes).hexdigest()\n",
    "                manifest[str(file_path.resolve())] = file_hash\n",
    "        except IOError as e:\n",
    "            logger.error(f\"Could not read file {file_path} for hashing: {e}\")\n",
    "    return manifest\n",
    "\n",
    "def _needs_rebuild() -> bool:\n",
    "    \"\"\"\n",
    "    Determines if the ChromaDB needs to be rebuilt.\n",
    "    A rebuild is needed if:\n",
    "    1. The Chroma directory or manifest file doesn't exist.\n",
    "    2. The manifest is unreadable.\n",
    "    3. The stored file hashes in the manifest do not match the current file hashes.\n",
    "    \"\"\"\n",
    "    if not CHROMA_DIR.exists() or not MANIFEST_PATH.exists():\n",
    "        logger.info(\"Chroma directory or manifest not found. A rebuild is required.\")\n",
    "        return True\n",
    "    try:\n",
    "        old_manifest = json.loads(MANIFEST_PATH.read_text())\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not read manifest file. A rebuild is required. Error: {e}\")\n",
    "        return True\n",
    "\n",
    "    current_manifest = _current_manifest()\n",
    "    if old_manifest != current_manifest:\n",
    "        logger.info(\"Data content has changed. A rebuild is required.\")\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def _save_manifest(manifest: Dict[str, str]) -> None:\n",
    "    \"\"\"Saves the current data manifest (mapping file paths to hashes) to disk.\"\"\"\n",
    "    CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    MANIFEST_PATH.write_text(json.dumps(manifest, indent=2))\n",
    "\n",
    "def _build_text_db() -> Chroma:\n",
    "    collection = \"mm_text\"\n",
    "    # The rebuild check is now done outside this function.\n",
    "    # We check if the directory exists. If not, we build.\n",
    "    if not CHROMA_DIR.exists() or not (CHROMA_DIR / \"chroma.sqlite3\").exists():\n",
    "        logger.info(\"Creating new text context index in %s ...\", CHROMA_DIR)\n",
    "        chroma = Chroma.from_documents(\n",
    "            documents          = splits,\n",
    "            embedding          = embeddings,\n",
    "            collection_name    = collection,\n",
    "            persist_directory  = str(CHROMA_DIR),\n",
    "        )\n",
    "        return chroma\n",
    "\n",
    "    logger.info(\"Loading existing Chroma index from %s\", CHROMA_DIR)\n",
    "    return Chroma(\n",
    "        collection_name   = collection,\n",
    "        persist_directory = str(CHROMA_DIR),\n",
    "        embedding_function= embeddings,\n",
    "    )\n",
    "    \n",
    "# Check if a rebuild is needed and wipe the old DB if so.\n",
    "# This ensures both the text and image databases are rebuilt from scratch.\n",
    "if _needs_rebuild():\n",
    "    logger.warning(\"REBUILDING: Wiping old ChromaDB store at %s\", CHROMA_DIR)\n",
    "    if CHROMA_DIR.exists():\n",
    "        shutil.rmtree(CHROMA_DIR)\n",
    "    # Save the new manifest immediately after deciding to rebuild\n",
    "    _save_manifest(_current_manifest())\n",
    "\n",
    "# Now, initialize your databases. They will be created fresh if they were just deleted.\n",
    "text_db = _build_text_db()\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2c64bd",
   "metadata": {},
   "source": [
    "### Setup Image ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50a39492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e054a569462d47c19686821afc1963ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/253 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf4eeb6129f4bf883191c09564c024d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0569ef6a77fe4ab19a093784a808048c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/394 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a622b9231c43028e835c0b0244e5c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e75ad75f532a4dfe8ea6b759957e31e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0411b01cb8b54537a3a293da51b062e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a808e6ff62c1431ebf0bb6f37aeed616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 03:04:59 - INFO - Indexed 806 unique images.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.8 s, sys: 16.5 s, total: 46.4 s\n",
      "Wall time: 59.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#  Helper: walk all docs once and gather *unique* image vectors + metadata\n",
    "def _collect_image_vectors():\n",
    "    \"\"\"\n",
    "    Scans every wiki page for image references and returns three parallel lists:\n",
    "        img_paths : list[str]   → full file-system paths (for SigLIP)\n",
    "        img_ids   : list[str]   → unique key per (page, image) pair\n",
    "        img_meta  : list[dict]  → {\"source\": wiki_page, \"image\": file_name}\n",
    "    Runs in < 1s even for thousands of docs.\n",
    "    \"\"\"\n",
    "    img_paths, img_ids, img_meta = [], [], []\n",
    "    seen = set()\n",
    "\n",
    "    for doc in mm_raw_docs:                         # raw wiki pages\n",
    "        src = doc.metadata[\"source\"]\n",
    "        for name in doc.metadata.get(\"images\", []): # list[str]\n",
    "            img_id = f\"{src}::{name}\"\n",
    "            if img_id in seen:\n",
    "                continue                            # de‑dupe\n",
    "            seen.add(img_id)\n",
    "\n",
    "            img_paths.append(str(IMAGE_DIR / name))\n",
    "            img_ids.append(img_id)\n",
    "            img_meta.append({\"source\": src, \"image\": name})\n",
    "\n",
    "    return img_paths, img_ids, img_meta\n",
    "\n",
    "siglip_embeddings = SiglipEmbeddings(\"google/siglip2-base-patch16-224\", DEVICE)\n",
    "\n",
    "# 2) IMAGE store\n",
    "image_db = Chroma(\n",
    "    collection_name    = \"mm_image\",\n",
    "    persist_directory  = str(CHROMA_DIR),   # SAME dir as text db\n",
    "    embedding_function = siglip_embeddings, # <-- class you kept\n",
    ")\n",
    "\n",
    "# Populate vectors *only* if it is empty\n",
    "if not image_db._collection.count():\n",
    "    img_paths, img_ids, img_meta = _collect_image_vectors()\n",
    "    image_db.add_texts(texts=img_paths, metadatas=img_meta, ids=img_ids)\n",
    "    image_db.persist()\n",
    "    logger.info(\"Indexed %d unique images.\", len(img_paths))\n",
    "else:\n",
    "    logger.info(\"Loaded existing image index (%d vectors).\",\n",
    "                image_db._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b7a32f-81e5-49bb-94c3-04a480e4be89",
   "metadata": {},
   "source": [
    "### Setup Memory Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9473fbc-bbbb-40b2-b344-decd9322a303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the semantic cache\n",
    "semantic_cache = SemanticCache(persist_directory=CACHE_DIR, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fd4358-0385-4cd3-bd2d-7da53a627418",
   "metadata": {},
   "source": [
    "## Step 4: Retrieval Function\n",
    "\n",
    "This code implements a hybrid retrieval process that combines two powerful search techniques to find the most relevant text documents and associated images.\n",
    "\n",
    "1.  **Initial Recall (Hybrid Search)**: The system performs two searches in parallel:\n",
    "    * **Dense Search**: A vector similarity search against `text_db` (ChromaDB) to find semantically related documents.\n",
    "    * **Sparse Search**: A keyword-based search using a `BM25` index to find documents with exact term matches.\n",
    "\n",
    "2.  **Fusion (RRF)**: The results from both searches are combined into a single, more robust ranked list using **Reciprocal Rank Fusion (RRF)**. This method intelligently merges the rankings without needing complex parameter tuning.\n",
    "\n",
    "3.  **Image Retrieval**: Using the top text documents from the fused list, the system performs a targeted search in the `image_db` to find images that are on the same source pages, ensuring contextual relevance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbf1f37f-11ff-4bf7-bae6-5e15470a8ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 03:05:00 - INFO - De-duplicated 2678 chunks down to 2667 unique chunks.\n"
     ]
    }
   ],
   "source": [
    "# This is necessary because the chunking process can sometimes create identical chunks.\n",
    "unique_docs_map = {doc.page_content: doc for doc in splits}\n",
    "unique_splits = list(unique_docs_map.values())\n",
    "\n",
    "logger.info(f\"De-duplicated {len(splits)} chunks down to {len(unique_splits)} unique chunks.\")\n",
    "\n",
    "# Now, build the BM25 index and the final doc_map using only the unique documents.\n",
    "# This ensures the index and the search corpus are perfectly aligned.\n",
    "corpus = [doc.page_content for doc in unique_splits]\n",
    "tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "doc_map = {doc.page_content: doc for doc in unique_splits}\n",
    "\n",
    "# %%\n",
    "# Helper function for Reciprocal Rank Fusion\n",
    "def reciprocal_rank_fusion(\n",
    "    results: list[list[Document]], k: int = 60\n",
    ") -> list[tuple[Document, float]]:\n",
    "    \"\"\"Performs RRF on multiple lists of ranked documents.\"\"\"\n",
    "    ranked_lists = [\n",
    "        {doc.page_content: (doc, i + 1) for i, doc in enumerate(res)}\n",
    "        for res in results\n",
    "    ]\n",
    "    rrf_scores = defaultdict(float)\n",
    "    all_docs = {}\n",
    "    for ranked_list in ranked_lists:\n",
    "        for content, (doc, rank) in ranked_list.items():\n",
    "            rrf_scores[content] += 1 / (k + rank)\n",
    "            if content not in all_docs:\n",
    "                all_docs[content] = doc\n",
    "    fused_results = [\n",
    "        (all_docs[content], rrf_scores[content])\n",
    "        for content in sorted(rrf_scores, key=rrf_scores.get, reverse=True)\n",
    "    ]\n",
    "    return fused_results\n",
    "\n",
    "\n",
    "def retrieve_mm(\n",
    "    query: str,\n",
    "    text_db: Chroma,\n",
    "    image_db: Chroma,\n",
    "    bm25_index: BM25Okapi,\n",
    "    doc_map: dict,\n",
    "    k_text: int = 3,\n",
    "    k_img: int = 2,\n",
    "    recall_k: int = 20,\n",
    ") -> dict[str, any]:\n",
    "    \"\"\"\n",
    "    Performs hybrid search for text and retrieves contextually relevant images.\n",
    "    \"\"\"\n",
    "    # 1. Hybrid Search for Text\n",
    "    dense_hits = text_db.similarity_search(query, k=recall_k)\n",
    "    tokenized_query = query.lower().split(\" \")\n",
    "    sparse_texts = bm25_index.get_top_n(tokenized_query, list(doc_map.keys()), n=recall_k)\n",
    "    sparse_hits = [doc_map[text] for text in sparse_texts]\n",
    "\n",
    "    if not dense_hits and not sparse_hits:\n",
    "        return {\"docs\": [], \"scores\": [], \"images\": []}\n",
    "\n",
    "    fused_results = reciprocal_rank_fusion([dense_hits, sparse_hits])\n",
    "    final_docs = [doc for doc, score in fused_results[:k_text]]\n",
    "    final_scores = [score for doc, score in fused_results[:k_text]]\n",
    "\n",
    "    # 2. Retrieve Relevant Images\n",
    "    retrieved_images = []\n",
    "    if final_docs:\n",
    "        # Get the source pages of the top text results\n",
    "        final_sources = list(set(d.metadata[\"source\"] for d in final_docs))\n",
    "\n",
    "        # Perform a vector search for images, filtered by the relevant sources\n",
    "        # The image_db's embedding function (SigLIP) will automatically handle the text query.\n",
    "        image_hits = image_db.similarity_search(\n",
    "            query,\n",
    "            k=k_img,\n",
    "            filter={\"source\": {\"$in\": final_sources}}\n",
    "        )\n",
    "        # The `page_content` of an image document is its path/name\n",
    "        retrieved_images = [img.page_content for img in image_hits]\n",
    "\n",
    "    return {\n",
    "        \"docs\": final_docs,\n",
    "        \"scores\": final_scores,\n",
    "        \"images\": retrieved_images,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58329084-640e-4b25-be70-ac1ec90467e9",
   "metadata": {},
   "source": [
    "## Step 5: Model Setup & Chain Creation\n",
    "\n",
    "In this section, we set up our local Large Language Model (LLM) and integrate it into a Question Answering (QA) pipeline. We're using `Qwen2.5VL-7B-Instruct` as our multimodal model, which can process both text and images. This setup is encapsulated within the QwenVLLM class, designed for efficient and robust multimodal interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fcccbf",
   "metadata": {},
   "source": [
    "### Cleanup Previous Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3679775b-14ad-44bc-8eb7-c24a3a9a88bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 03:05:00 - INFO - ✅ Embeddings and vector stores are ready. Offloading embedding models to free up VRAM.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"✅ Embeddings and vector stores are ready. Offloading embedding models to free up VRAM.\")\n",
    "\n",
    "# Explicitly delete the objects to free memory\n",
    "del embeddings\n",
    "del siglip_embeddings\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# For PyTorch, you can also empty the CUDA cache\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63929d26",
   "metadata": {},
   "source": [
    "### QwenVLMM QA Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "910ccb5a-fb0c-4f7c-bc2a-69f464f3ffa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 03:05:01 - INFO - Loading Qwen2.5-VL via vLLM...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bda813630044d31a62ef895e65bfab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4b92128400047c6a1dcd9ee725670f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d1c3f8900d4126a5eafa2f88b7c4cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a58d5981500b43f89127a1fc5086e7a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c72bc0ad95493dbba54bd45e2fa962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-05 03:05:13 [config.py:841] This model supports multiple tasks: {'classify', 'generate', 'reward', 'embed'}. Defaulting to 'generate'.\n",
      "WARNING 08-05 03:05:13 [config.py:3371] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 08-05 03:05:13 [config.py:1472] Using max model len 4096\n",
      "WARNING 08-05 03:05:13 [config.py:960] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "WARNING 08-05 03:05:13 [arg_utils.py:1735] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
      "WARNING 08-05 03:05:13 [cuda.py:102] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 08-05 03:05:13 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.2) with config: model='/home/jovyan/datafabric/Qwen2.5-VL-7B-Instruct-GPTQ-Int4', speculative_config=None, tokenizer='/home/jovyan/datafabric/Qwen2.5-VL-7B-Instruct-GPTQ-Int4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=gptq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/jovyan/datafabric/Qwen2.5-VL-7B-Instruct-GPTQ-Int4, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "WARNING 08-05 03:05:14 [interface.py:382] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 08-05 03:05:14 [cuda.py:311] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 08-05 03:05:14 [cuda.py:360] Using XFormers backend.\n",
      "INFO 08-05 03:05:15 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 08-05 03:05:15 [model_runner.py:1171] Starting to load model /home/jovyan/datafabric/Qwen2.5-VL-7B-Instruct-GPTQ-Int4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d064dbc9ce074438be7a5cc016c3f87a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-05 03:05:51 [default_loader.py:272] Loading weights took 34.98 seconds\n",
      "INFO 08-05 03:05:52 [model_runner.py:1203] Model loading took 6.5939 GiB and 35.508739 seconds\n",
      "WARNING 08-05 03:05:53 [profiling.py:271] The sequence length (4096) is smaller than the pre-defined wosrt-case total number of multimodal tokens (32768). This may cause certain multi-modal inputs to fail during inference. To avoid this, you should increase `max_model_len` or reduce `mm_counts`.\n",
      "WARNING 08-05 03:05:53 [model_runner.py:1368] Computed max_num_seqs (min(256, 5120 // 49152)) to be less than 1. Setting it to the minimum value of 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-05 03:05:57 [profiling.py:237] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 5120) is too short to hold the multi-modal embeddings in the worst case (49152 tokens in total, out of which {'image': 32768, 'video': 16384} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "INFO 08-05 03:06:45 [worker.py:294] Memory profiling takes 52.84 seconds\n",
      "INFO 08-05 03:06:45 [worker.py:294] the current vLLM instance can use total_gpu_memory (24.00GiB) x gpu_memory_utilization (0.70) = 16.80GiB\n",
      "INFO 08-05 03:06:45 [worker.py:294] model weights take 6.59GiB; non_torch_memory takes 0.02GiB; PyTorch activation peak memory takes 7.51GiB; the rest of the memory reserved for KV Cache is 2.68GiB.\n",
      "INFO 08-05 03:06:45 [executor_base.py:113] # cuda blocks: 3139, # CPU blocks: 4681\n",
      "INFO 08-05 03:06:45 [executor_base.py:118] Maximum concurrency for 4096 tokens per request: 12.26x\n",
      "INFO 08-05 03:06:46 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 54.17 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 03:06:46 - INFO - vLLM model loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 16s, sys: 25.4 s, total: 1min 41s\n",
      "Wall time: 1min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "class QwenVLMM:\n",
    "    \"\"\"\n",
    "    Multimodal QA wrapper around the quantized Qwen2.5-VL model using vLLM.\n",
    "    Requires:\n",
    "      * `vllm` installed and importable.\n",
    "      * `qwen_vl_utils.process_vision_info` for multimodal image handling.\n",
    "      * HuggingFace transformers for tokenizer / image processor.\n",
    "      * External retrieval function (e.g., `retrieve_mm`) and a `SemanticCache`-like cache.\n",
    "    Expects the quantized safetensors model `RedHatAI/Qwen2.5-VL-7B-Instruct-quantized.w8a8`\n",
    "    to be accessible (vLLM will pull it from HuggingFace).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cache,\n",
    "        text_db,\n",
    "        image_db,\n",
    "        bm25_index,\n",
    "        doc_map: dict,\n",
    "        model_name: str = \"RedHatAI/Qwen2.5-VL-7B-Instruct-quantized.w8a8\",\n",
    "        base_for_tokenizer: str = \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "        device: str = \"cuda\",\n",
    "    ):\n",
    "        self.cache = cache\n",
    "        self.text_db = text_db\n",
    "        self.image_db = image_db\n",
    "        self.bm25_index = bm25_index\n",
    "        self.doc_map = doc_map\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.base_for_tokenizer = base_for_tokenizer\n",
    "        self.device = device\n",
    "\n",
    "        self.tok = None\n",
    "        self.image_processor = None\n",
    "        self.llm = None  # vLLM instance\n",
    "\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "        self._load()\n",
    "\n",
    "    # ---------- public function ----------\n",
    "    def generate(self, query: str, force_regenerate: bool = False, **retrieval_kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Run retrieval, prompt assembly, and model generation via vLLM.\n",
    "        \"\"\"\n",
    "        # 1. Cache check\n",
    "        if not force_regenerate:\n",
    "            cached_result = self.cache.get(query, threshold=0.92)\n",
    "            if cached_result:\n",
    "                logger.info(f\"SEMANTIC CACHE HIT for query: '{query}'\")\n",
    "                return cached_result\n",
    "        if force_regenerate:\n",
    "            logger.info(f\"Forced regeneration for query: '{query}'. Clearing old cache entry.\")\n",
    "            self.cache.delete(query)\n",
    "        logger.info(f\"CACHE MISS for query: '{query}'. Running full pipeline.\")\n",
    "\n",
    "        if self.llm is None or self.tok is None:\n",
    "            return {\"reply\": \"Error: model not initialised.\", \"used_images\": []}\n",
    "\n",
    "        # 2. Retrieval\n",
    "        hits = retrieve_mm(\n",
    "            query,\n",
    "            text_db=self.text_db,\n",
    "            image_db=self.image_db,\n",
    "            bm25_index=self.bm25_index,\n",
    "            doc_map=self.doc_map,\n",
    "            **retrieval_kwargs\n",
    "        )\n",
    "        docs = hits.get(\"docs\", [])\n",
    "        images = hits.get(\"images\", [])\n",
    "\n",
    "        if not docs and not images:\n",
    "            return {\n",
    "                \"reply\": \"Based on the provided context, I cannot answer this question.\", \n",
    "                \"used_images\": [],\n",
    "                \"retrieved_sources\": {\"text_documents\": [], \"images\": []},\n",
    "            }\n",
    "\n",
    "        # Limit number of images to reduce memory usage\n",
    "        if len(images) > 2:\n",
    "            logger.warning(f\"Limiting images from {len(images)} to 2 to save memory\")\n",
    "            images = images[:2]\n",
    "\n",
    "        # 3. Build prompt\n",
    "        context_str = \"\\n\\n\".join(\n",
    "            f\"<source_document name=\\\"{d.metadata.get('source', 'unknown')}\\\">\\n{d.page_content}\\n</source_document>\"\n",
    "            for d in docs\n",
    "        )\n",
    "\n",
    "        system_prompt = \"\"\"You are a Multimodal RAG Assistant. Your task is to answer the user's query using ONLY the provided context from retrieved documents and images.\n",
    "            \n",
    "            **Instructions:**\n",
    "            1. **Analyze Context:** Carefully examine the retrieved images and text documents provided in the context.\n",
    "            2. **Answer Directly:** Provide a clear, comprehensive answer to the user's query by synthesizing information from both text and image sources.\n",
    "            3. **Stay Focused:** Do not include unnecessary sections or verbose explanations. Answer the question directly and concisely.\n",
    "            4. **No Hallucination:** Use ONLY the information provided in the context. Do not make up facts or add information not present in the retrieved materials.\n",
    "            \n",
    "            **Output Format:**\n",
    "            - If the context is relevant: Provide a direct answer using the retrieved context.\n",
    "            - If the context is irrelevant: Respond with \"The provided context does not contain relevant information to answer the query.\"\n",
    "            \"\"\"\n",
    "                    \n",
    "        # Build user content with proper image placeholders for Qwen2.5-VL\n",
    "        if images:\n",
    "            # Use the standard Qwen2.5-VL image token format\n",
    "            image_tokens = \"\"\n",
    "            for i in range(len(images)):\n",
    "                image_tokens += f\"<|vision_start|><|image_pad|><|vision_end|>\"\n",
    "            \n",
    "            user_content = f\"\"\"{image_tokens}\n",
    "\n",
    "            <context>\n",
    "            {context_str}\n",
    "            </context>\n",
    "            \n",
    "            <user_query>\n",
    "            {query}\n",
    "            </user_query>\"\"\"\n",
    "        else:\n",
    "            user_content = f\"\"\"<context>\n",
    "            {context_str}\n",
    "            </context>\n",
    "            \n",
    "            <user_query>\n",
    "            {query}\n",
    "            </user_query>\"\"\"\n",
    "\n",
    "        # Use chat template if available\n",
    "        if hasattr(self.tok, 'apply_chat_template') and self.tok.chat_template:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_content}\n",
    "            ]\n",
    "            try:\n",
    "                prompt_string = self.tok.apply_chat_template(\n",
    "                    messages, \n",
    "                    tokenize=False, \n",
    "                    add_generation_prompt=True\n",
    "                )\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Chat template failed: {e}, using fallback\")\n",
    "                prompt_string = f\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n<|im_start|>user\\n{user_content}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "        else:\n",
    "            # Fallback to manual template\n",
    "            prompt_string = f\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n<|im_start|>user\\n{user_content}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "        # 4. Generation via vLLM\n",
    "        try:\n",
    "            self._clear_cuda()\n",
    "\n",
    "            # More conservative sampling parameters\n",
    "            sampling_params = SamplingParams(\n",
    "                temperature=0.0,     # Deterministic\n",
    "                top_p=1.0,\n",
    "                max_tokens=2048,\n",
    "            )\n",
    "\n",
    "            if images:\n",
    "                # Process images with size limit\n",
    "                pil_images = []\n",
    "                for i, img_path in enumerate(images):\n",
    "                    try:\n",
    "                        img = PILImage.open(img_path).convert(\"RGB\")\n",
    "                        # Resize large images to save memory\n",
    "                        if img.size[0] > 512 or img.size[1] > 512:\n",
    "                            img.thumbnail((512, 512), PILImage.Resampling.LANCZOS)\n",
    "                        pil_images.append(img)\n",
    "                        logger.info(f\"Processed image {i+1}: {img_path}\")\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Failed to process image {img_path}: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                if not pil_images:\n",
    "                    logger.warning(\"No images successfully processed, proceeding text-only\")\n",
    "                    request_payload = {\"prompt\": prompt_string}\n",
    "                else:\n",
    "                    request_payload = {\n",
    "                        \"prompt\": prompt_string,\n",
    "                        \"multi_modal_data\": {\n",
    "                            \"image\": pil_images\n",
    "                        },\n",
    "                    }\n",
    "            else:\n",
    "                request_payload = {\"prompt\": prompt_string}\n",
    "            \n",
    "            output_list = self.llm.generate(request_payload, sampling_params=sampling_params)\n",
    "            \n",
    "            if output_list and output_list[0].outputs:\n",
    "                reply = output_list[0].outputs[0].text.strip()\n",
    "            else:\n",
    "                reply = \"Error: no output from LLM.\"\n",
    "\n",
    "            self._clear_cuda()\n",
    "            \n",
    "            # Prepare retrieved sources for programmatic return\n",
    "            retrieved_sources = {\n",
    "                \"text_documents\": [\n",
    "                    {\n",
    "                        \"source\": d.metadata.get('source', 'unknown'),\n",
    "                        \"content\": d.page_content[:500] + \"...\" if len(d.page_content) > 500 else d.page_content,\n",
    "                        \"metadata\": d.metadata\n",
    "                    }\n",
    "                    for d in docs\n",
    "                ],\n",
    "                \"images\": [\n",
    "                    {\n",
    "                        \"path\": img_path,\n",
    "                        \"filename\": img_path.split('/')[-1] if '/' in img_path else img_path\n",
    "                    }\n",
    "                    for img_path in images\n",
    "                ]\n",
    "            }\n",
    "\n",
    "            if reply == \"The provided context does not contain relevant information to answer the query.\":\n",
    "                images = []\n",
    "            \n",
    "            result = {\n",
    "                \"reply\": reply, \n",
    "                \"used_images\": images,\n",
    "                \"retrieved_sources\": retrieved_sources,\n",
    "            }\n",
    "            self.cache.set(query, result)\n",
    "            return result\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            msg = str(e).lower()\n",
    "            if \"cuda\" in msg or \"out of memory\" in msg:\n",
    "                logger.warning(\"CUDA error – resetting model: %s\", e)\n",
    "                self._reset()\n",
    "                error_reply = \"I ran into a GPU memory error – please try again.\"\n",
    "            else:\n",
    "                logger.error(\"Runtime error: %s\", e)\n",
    "                error_reply = f\"Error: {e}\"\n",
    "            return {\n",
    "                \"reply\": error_reply, \n",
    "                \"used_images\": images,\n",
    "                \"retrieved_sources\": {\"text_documents\": [], \"images\": []},\n",
    "            }\n",
    "\n",
    "    # ---------- internal helpers ----------\n",
    "\n",
    "    def _load(self):\n",
    "        \"\"\"Load tokenizer, image_processor, & vLLM model.\"\"\"\n",
    "        logger.info(\"Loading Qwen2.5-VL via vLLM...\")\n",
    "        gc.collect()\n",
    "        self._clear_cuda()\n",
    "    \n",
    "        # Tokenizer & image processor (base model)\n",
    "        self.tok = AutoTokenizer.from_pretrained(\n",
    "            self.base_for_tokenizer, trust_remote_code=True\n",
    "        )\n",
    "        if self.tok.pad_token is None:\n",
    "            self.tok.pad_token = self.tok.eos_token\n",
    "    \n",
    "        self.image_processor = AutoImageProcessor.from_pretrained(\n",
    "            self.base_for_tokenizer, trust_remote_code=True, use_fast=True\n",
    "        )\n",
    "    \n",
    "        # Load vLLM with the quantized safetensors model (no use_mlock)\n",
    "        self.llm = LLM(\n",
    "            model=self.model_name,\n",
    "            quantization=\"gptq\",\n",
    "            gpu_memory_utilization=0.70,    # Leave headroom for image tensors\n",
    "            max_model_len=4096,\n",
    "            enforce_eager=True,\n",
    "            limit_mm_per_prompt={\"image\": 2},  # No more than 2 images\n",
    "            disable_custom_all_reduce=True,\n",
    "            tensor_parallel_size=1,\n",
    "            dtype=\"float16\",\n",
    "        )\n",
    "\n",
    "        logger.info(\"vLLM model loaded.\")\n",
    "\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Free everything and reload on error.\"\"\"\n",
    "        logger.warning(\"Resetting InternQwenVLMM model …\")\n",
    "        try:\n",
    "            del self.llm, self.tok, self.image_processor\n",
    "        except Exception:\n",
    "            pass\n",
    "        self.llm = self.tok = self.image_processor = None\n",
    "        gc.collect()\n",
    "        self._clear_cuda()\n",
    "        time.sleep(1)\n",
    "        self._load()\n",
    "\n",
    "    @staticmethod\n",
    "    def _clear_cuda():\n",
    "        try:\n",
    "            import torch\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "        except ImportError:\n",
    "            pass\n",
    "\n",
    "\n",
    "# Initalize mm llm\n",
    "mm = QwenVLMM(\n",
    "    cache=semantic_cache,\n",
    "    text_db=text_db,\n",
    "    image_db=image_db,\n",
    "    bm25_index=bm25,\n",
    "    doc_map=doc_map,\n",
    "    model_name=str(LOCAL_MODEL_PATH),  # quantized safetensors model\n",
    "    base_for_tokenizer=\"Qwen/Qwen2.5-VL-7B-Instruct\",  # for tokenizer / image processor\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0057f873",
   "metadata": {},
   "source": [
    "## Step 6: Test Generation and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a24f50-8384-488e-87e0-eafae305f2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the AI Blueprints Repository best practices?\"\n",
    "results = mm.generate(question, force_regenerate=True)\n",
    "print(\"--- MODEL CONTEXT ---\")\n",
    "print(results[\"retrieved_sources\"])\n",
    "\n",
    "print(\"\\n--- MODEL RESPONSE ---\")\n",
    "display(Markdown(results[\"reply\"]))\n",
    "print(\"----------------------\\n\")\n",
    "\n",
    "display_images(results[\"used_images\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c98ead24-203d-41fd-b60a-74f64445e800",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 03:07:03 - INFO - Forced regeneration for query: 'What is the capital of paris?'. Clearing old cache entry.\n",
      "2025-08-05 03:07:03 - INFO - CACHE MISS for query: 'What is the capital of paris?'. Running full pipeline.\n",
      "2025-08-05 03:07:03 - INFO - Processed image 1: ../data/context/images/image-21f1f2bf-917f-4a1c-af46-3f71d885aad6.png\n",
      "2025-08-05 03:07:03 - INFO - Processed image 2: ../data/context/images/image-68e01678-f7a5-4be7-9284-6355854edb03.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a52d9bd5a66943baa6cf1a60df936eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6964d48e88c34038bc605fdec34aa55e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- MODEL CONTEXT ---\n",
      "{'text_documents': [{'source': 'Dogfooding-Experiences/How-to-add-a-bug-for-dogfooding.md', 'content': \"How to add a bug for dogfooding\\n\\n![image.png](/.attachments/image-68e01678-f7a5-4be7-9284-6355854edb03.png)\\nYou should now see a blank bug template:\\n![image.png](/.attachments/image-68354047-38d8-4ca9-9e6e-d942698b6dc0.png)  \\n`\\nIMPORTANT ADO WILL NOT AUTO SAVE. Remember to hit the save button a lot. Do not close with out saving. The save button is in the upper right hand corner.  \\n`  \\n3. Fill in the basics:(to make sure we have basic info you can't save unless you get rid of all the fields with ...\", 'metadata': {'chunk_id': 3, 'section_header': '', 'source': 'Dogfooding-Experiences/How-to-add-a-bug-for-dogfooding.md', 'title': 'How to add a bug for dogfooding'}}, {'source': 'Development/RFCs/Private-Projects.md', 'content': 'Private Projects\\n\\n![groups-details.png](/.attachments/groups-details-297d6616-fa4f-4e69-b451-2dc37d366219.png)', 'metadata': {'chunk_id': 0, 'section_header': '', 'source': 'Development/RFCs/Private-Projects.md', 'title': 'Private Projects'}}, {'source': 'Incident-Management-and-Incident-Response/Postmortem.md', 'content': 'Postmortem\\n\\nInclude a description of what solved the problem. If there was a temporary fix in place, describe that along with the long-term solution.', 'metadata': {'title': 'Postmortem', 'source': 'Incident-Management-and-Incident-Response/Postmortem.md', 'section_header': '', 'chunk_id': 0}}], 'images': [{'path': '../data/context/images/image-21f1f2bf-917f-4a1c-af46-3f71d885aad6.png', 'filename': 'image-21f1f2bf-917f-4a1c-af46-3f71d885aad6.png'}, {'path': '../data/context/images/image-68e01678-f7a5-4be7-9284-6355854edb03.png', 'filename': 'image-68e01678-f7a5-4be7-9284-6355854edb03.png'}]}\n",
      "\n",
      "--- MODEL RESPONSE ---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The provided context does not contain relevant information to answer the query."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "\n",
      "▶ No images to display.\n",
      "CPU times: user 1.21 s, sys: 88.3 ms, total: 1.29 s\n",
      "Wall time: 1.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "question1 = \"What is the capital of paris?\"\n",
    "results = mm.generate(question1, force_regenerate=True)\n",
    "print(\"--- MODEL CONTEXT ---\")\n",
    "print(results[\"retrieved_sources\"])\n",
    "\n",
    "print(\"\\n--- MODEL RESPONSE ---\")\n",
    "display(Markdown(results[\"reply\"]))\n",
    "print(\"----------------------\\n\")\n",
    "\n",
    "display_images(results[\"used_images\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a3a494-8f3c-4bce-8494-5c8bd7e150cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "question2 = \"What is ITG, STG, and Prod?\"\n",
    "results = mm.generate(question2, force_regenerate=True)\n",
    "print(\"--- MODEL CONTEXT ---\")\n",
    "print(results[\"retrieved_sources\"])\n",
    "\n",
    "print(\"\\n--- MODEL RESPONSE ---\")\n",
    "display(Markdown(results[\"reply\"]))\n",
    "print(\"----------------------\\n\")\n",
    "\n",
    "display_images(results[\"used_images\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dbca87f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 03:07:07 - INFO - ⏱️ Total execution time: 10m 23.51s\n",
      "2025-08-05 03:07:07 - INFO - ✅ Notebook execution completed successfully.\n"
     ]
    }
   ],
   "source": [
    "end_time: float = time.time()\n",
    "elapsed_time: float = end_time - start_time\n",
    "elapsed_minutes: int = int(elapsed_time // 60)\n",
    "elapsed_seconds: float = elapsed_time % 60\n",
    "\n",
    "logger.info(f\"⏱️ Total execution time: {elapsed_minutes}m {elapsed_seconds:.2f}s\")\n",
    "logger.info(\"✅ Notebook execution completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8147f34-6e3b-47f2-9a68-7e8077f71618",
   "metadata": {},
   "source": [
    "Built with ❤️ using Z by HP AI Studio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
