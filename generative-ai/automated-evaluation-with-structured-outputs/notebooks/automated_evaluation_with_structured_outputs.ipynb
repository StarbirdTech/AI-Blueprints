{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccbe15ed",
   "metadata": {},
   "source": [
    "<h1 style=\\\"text-align: center; font-size: 50px;\\\"> Automated Evaluation  with Structured Outputs </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39419e57",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "- Imports\n",
    "- Configurations\n",
    "- Verify Assets\n",
    "- Load Data\n",
    "- Load Data & Validate Columns\n",
    "- Sort TotalScore and scores\n",
    "- Logging Model to MLflow\n",
    "- Fetching the Latest Model Version from MLflow\n",
    "- Loading the Model and Running Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e5a4d6",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a291384-cd71-4a8d-b77f-c8be54073bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0041982-b220-41fb-b9a2-11bbf3334d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────── Standard Library ───────\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "import httpx\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from llama_cpp import Llama, LlamaGrammar\n",
    "import multiprocessing\n",
    "\n",
    "# ─────── MLflow Integration ───────\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types import Schema, ColSpec, DataType, ParamSpec, ParamSchema\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# === Internal modules ===\n",
    "\n",
    "# Add 'src' directory to system path (2 levels up)\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5889a7a1",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d66393e",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4af9a5-ef38-4497-a29c-931329c31892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure total runtime\n",
    "start_notebook = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1391c180-f33a-406e-9e4b-bda20c04405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Constants & Configuration\n",
    "INPUT_PATH    = \"../data/2025 ISEF Project Abstracts.csv\"  # <-- set your input CSV here\n",
    "OUTPUT_PATH   = \"../Sorted_by_Score.csv\"             # <-- set your output CSV here\n",
    "KEY_COLUMN    = \"BoothNumber\"                     # <-- unique ID column\n",
    "EVAL_COLUMN   = \"AbstractText\"                    # <-- text column to evaluate\n",
    "CRITERIA      = [                                  # <-- list your evaluation criteria\n",
    "    \"Originality\",\n",
    "    \"ScientificRigor\",\n",
    "    \"Clarity\",\n",
    "    \"Relevance\",\n",
    "    \"Feasibility\",\n",
    "    \"Brevity\",\n",
    "]\n",
    "BATCH_SIZE    = 5\n",
    "# ─────── MLflow Experiment Configuration ───────\n",
    "EXPERIMENT_NAME = \"LLaMA_Evaluator_Experiment\"\n",
    "RUN_NAME        = \"LLaMA_Evaluator_Run\"\n",
    "MODEL_NAME      = \"LlamaEvaluatorModel\"\n",
    "# ─────── PATHS ───────\n",
    "#LLAMA_GGUF_PATH = \"/home/jovyan/datafabric/Meta-Llama-3.1-8B-Instruct-Q8_0/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\"\n",
    "LLAMA_GGUF_PATH = \"/home/jovyan/datafabric/llama2-7b/ggml-model-f16-Q5_K_M.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89e3f1b-e6ad-4e48-9130-757b282e4543",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 3. Load & Configure Local LLaMA\n",
    "#local_model_path = \"/home/jovyan/datafabric/Meta-Llama-3-8B-Instruct-Q8_0/Meta-Llama-3-8B-Instruct-Q8_0.gguf\"\n",
    "local_model_path = LLAMA_GGUF_PATH\n",
    "\n",
    "llm = Llama(\n",
    "            model_path=local_model_path,\n",
    "            n_gpu_layers=-1,\n",
    "            n_batch=128,\n",
    "            n_ctx=8192,\n",
    "            max_tokens=512,\n",
    "            f16_kv=True,\n",
    "            use_mmap=True,\n",
    "            low_vram=True,\n",
    "            rope_scaling=None,\n",
    "            temperature=0.0,\n",
    "            repeat_penalty=1.0,\n",
    "            streaming=False,\n",
    "            stop=None,\n",
    "            seed=42,\n",
    "            num_threads=multiprocessing.cpu_count(),\n",
    "            verbose=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea62050a-4ecf-4b54-81fe-6d1c74947157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have:\n",
    "#   KEY_COLUMN = \"BoothNumber\"\n",
    "#   CRITERIA   = [\"Originality\",\"ScientificRigor\",\"Clarity\",\"Relevance\",\"Feasibility\"]\n",
    "\n",
    "# Build the bullet list dynamically:\n",
    "criteria_bullets = \"\\n\".join(f\"- {c}\" for c in CRITERIA)\n",
    "\n",
    "# Build the example-object fields (all with dummy value 7):\n",
    "example_fields = \", \".join(f'\"{c}\": 7' for c in CRITERIA)\n",
    "\n",
    "SYSTEM_INSTRUCTIONS = (\n",
    "    \"You are an expert evaluator.  \"\n",
    "    f\"For each input record, score 1–10 on these criteria:\\n\"\n",
    "    f\"{criteria_bullets}\\n\\n\"\n",
    "    \"Respond *only* with a valid JSON object of the form:\\n\"\n",
    "    \"{\\n\"\n",
    "    '  \"results\": [\\n'\n",
    "    f'    {{ \"{KEY_COLUMN}\": \"...\", {example_fields} }}\\n'\n",
    "    \"  ]\\n\"\n",
    "    \"}\\n\"\n",
    "    \"Do not include any other text, explanation, or markup.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd37fa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"automated_evaluation_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d368761",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c9403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Notebook execution started.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed642c0",
   "metadata": {},
   "source": [
    "## Verify Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d388f3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_asset_status(asset_path: str, asset_name: str, success_message: str, failure_message: str) -> None:\n",
    "    \"\"\"\n",
    "    Logs the status of a given asset based on its existence.\n",
    "\n",
    "    Parameters:\n",
    "        asset_path (str): File or directory path to check.\n",
    "        asset_name (str): Name of the asset for logging context.\n",
    "        success_message (str): Message to log if asset exists.\n",
    "        failure_message (str): Message to log if asset does not exist.\n",
    "    \"\"\"\n",
    "    if Path(asset_path).exists():\n",
    "        logger.info(f\"{asset_name} is properly configured. {success_message}\")\n",
    "    else:\n",
    "        logger.info(f\"{asset_name} is not properly configured. {failure_message}\")\n",
    "\n",
    "\n",
    "# Check and log status for model\n",
    "log_asset_status(\n",
    "    asset_path=local_model_path,\n",
    "    asset_name=\"LLaMA Local model\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please create and download the required assets in your project on AI Studio.\"\n",
    ")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=LLAMA_GGUF_PATH,\n",
    "    asset_name=\"LLaMA model\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please create and download the required assets in your project on AI Studio.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929bcfab",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37962aac-8800-4b53-b890-1f9bdbfa6dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_list(lst: List[int], size: int) -> List[List[int]]:\n",
    "    return [lst[i : i + size] for i in range(0, len(lst), size)]\n",
    "\n",
    "# Load the “json_arr” grammar for a top‐level JSON array\n",
    "GRAMMAR_URL = \"https://raw.githubusercontent.com/ggerganov/llama.cpp/master/grammars/json_arr.gbnf\"\n",
    "grammar_text = httpx.get(GRAMMAR_URL).text\n",
    "json_arr_grammar = LlamaGrammar.from_string(grammar_text)\n",
    "\n",
    "def evaluate_batch(batch_df: pd.DataFrame) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Scores a batch of rows under the json_arr grammar,\n",
    "    returning a flat list of dicts with KEY_COLUMN + CRITERIA keys.\n",
    "    \"\"\"\n",
    "    payload = [\n",
    "        {KEY_COLUMN: str(r[KEY_COLUMN]), EVAL_COLUMN: r[EVAL_COLUMN]}\n",
    "        for _, r in batch_df.iterrows()\n",
    "    ]\n",
    "    prompt = SYSTEM_INSTRUCTIONS + \"\\n\\n\" + json.dumps(payload, indent=2)\n",
    "\n",
    "    resp: Dict[str, Any] = llm(\n",
    "        prompt,\n",
    "        grammar=json_arr_grammar,\n",
    "        #grammar=objarr_grammar,   # ← now only allows [ { … }, { … } ]\n",
    "        max_tokens=-1,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    # Extract text\n",
    "    text = resp[\"choices\"][0][\"text\"]\n",
    "    data = json.loads(text)\n",
    "    if isinstance(data, list):\n",
    "        return data\n",
    "    else:\n",
    "        raise RuntimeError(f\"Expected JSON array, got {type(data)}:\\n{text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80129d5",
   "metadata": {},
   "source": [
    "# Load Data & Validate Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776b6999-a68e-440b-8c53-4db63a17cf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(INPUT_PATH)\n",
    "for col in (KEY_COLUMN, EVAL_COLUMN):\n",
    "    if col not in df.columns:\n",
    "        raise KeyError(f\"Required column '{col}' not found in input CSV\")\n",
    "\n",
    "df[KEY_COLUMN] = df[KEY_COLUMN].astype(str)\n",
    "\n",
    "# 6. Batch Evaluation Loop\n",
    "results: List[Dict[str, Any]] = []\n",
    "for batch_idxs in tqdm(\n",
    "    chunk_list(df.index.tolist(), BATCH_SIZE),\n",
    "    desc=\"Scoring batches\",\n",
    "    unit=\"batch\"\n",
    "):\n",
    "    batch_df      = df.loc[batch_idxs]\n",
    "    batch_results = evaluate_batch(batch_df)\n",
    "    results.extend(batch_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818a4a7b",
   "metadata": {},
   "source": [
    "# Sort TotalScore and scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c289ad70-280a-418f-996a-9096d0fe9270",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_results: List[Dict[str, Any]] = []\n",
    "for batch in results:\n",
    "    # If each batch is a dict with a \"results\" key, use that list\n",
    "    if isinstance(batch, dict) and \"results\" in batch and isinstance(batch[\"results\"], list):\n",
    "        flat_results.extend(batch[\"results\"])\n",
    "    # If somehow you ended up with lists directly, handle those too\n",
    "    elif isinstance(batch, list):\n",
    "        flat_results.extend(batch)\n",
    "    else:\n",
    "        # Ignore anything else (e.g. stray floats)\n",
    "        logging.warning(f\"Ignoring unexpected batch entry: {batch!r}\")\n",
    "\n",
    "# 8. Build the scores DataFrame\n",
    "scores_df = pd.DataFrame(flat_results)\n",
    "\n",
    "# 9. Sanity check: ensure your key column is present\n",
    "if KEY_COLUMN not in scores_df.columns:\n",
    "    raise KeyError(\n",
    "        f\"Expected column '{KEY_COLUMN}' in scores_df, but got: {scores_df.columns.tolist()}\"\n",
    "    )\n",
    "\n",
    "# 10. Cast keys to string on both sides\n",
    "scores_df[KEY_COLUMN] = scores_df[KEY_COLUMN].astype(str)\n",
    "df[KEY_COLUMN]      = df[KEY_COLUMN].astype(str)\n",
    "\n",
    "# 11. Merge, compute TotalScore, sort, and export\n",
    "combined = df.merge(scores_df, on=KEY_COLUMN, how=\"left\")\n",
    "combined[\"TotalScore\"] = combined[CRITERIA].sum(axis=1)\n",
    "combined.sort_values(\"TotalScore\", ascending=False, inplace=True)\n",
    "combined.reset_index(drop=True, inplace=True)\n",
    "\n",
    "combined.to_csv(OUTPUT_PATH, index=False)\n",
    "elapsed = time.time() - start_notebook\n",
    "print(f\"✅ Done in {elapsed:.1f}s — output saved to '{OUTPUT_PATH}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0423bf9-f0ba-42f0-843f-10acee4c44dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ea10da",
   "metadata": {},
   "source": [
    "# Logging Model to MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd745cae-26ae-42e1-8658-4a77a35cd337",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAMMAR_URL = (\n",
    "    \"https://raw.githubusercontent.com/ggerganov/llama.cpp/\"\n",
    "    \"master/grammars/json_arr.gbnf\"\n",
    ")\n",
    "\n",
    "class LlamaEvaluatorModel(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"\n",
    "    A PythonModel that uses a local LLaMA model to score texts on multiple criteria.\n",
    "\n",
    "    Predict signature:\n",
    "      predict(self, context, model_input: DataFrame, params: Dict[str,Any])\n",
    "    where `params` must include:\n",
    "      - key_column (str)\n",
    "      - eval_column (str)\n",
    "      - criteria (JSON-encoded list of str)\n",
    "      - batch_size (int)\n",
    "    \"\"\"\n",
    "    def load_context(self, context):\n",
    "        # 1. Load LLaMA model\n",
    "        model_path = context.artifacts[\"llama_model_path\"]\n",
    "        self.llm = Llama(\n",
    "            model_path=model_path,\n",
    "            n_gpu_layers=-1,\n",
    "            n_batch=128,\n",
    "            n_ctx=8192,\n",
    "            max_tokens=512,\n",
    "            f16_kv=True,\n",
    "            use_mmap=True,\n",
    "            low_vram=True,\n",
    "            rope_scaling=None,\n",
    "            temperature=0.0,\n",
    "            repeat_penalty=1.0,\n",
    "            streaming=False,\n",
    "            stop=None,\n",
    "            seed=42,\n",
    "            num_threads=multiprocessing.cpu_count(),\n",
    "            verbose=False,\n",
    "        )\n",
    "        # 2. Load JSON-array grammar\n",
    "        grammar_text = httpx.get(GRAMMAR_URL).text\n",
    "        self.grammar = LlamaGrammar.from_string(grammar_text)\n",
    "\n",
    "    def predict(self, context, model_input: pd.DataFrame, params: Dict[str,Any]) -> pd.DataFrame:\n",
    "        # 1. Extract config from params\n",
    "        try:\n",
    "            key_column   = params[\"key_column\"]\n",
    "            eval_column  = params[\"eval_column\"]\n",
    "            criteria     = params[\"criteria\"]\n",
    "            batch_size   = int(params[\"batch_size\"])\n",
    "        except KeyError as e:\n",
    "            raise KeyError(f\"Missing required param: {e}\")\n",
    "\n",
    "        # If criteria passed as JSON string, parse it\n",
    "        if isinstance(criteria, str):\n",
    "            criteria = json.loads(criteria)\n",
    "        # 2. Validate DataFrame columns\n",
    "        for col in (key_column, eval_column):\n",
    "            if col not in model_input.columns:\n",
    "                raise KeyError(f\"Input DataFrame must contain column '{col}'\")\n",
    "\n",
    "        df = model_input.copy()\n",
    "        df[key_column] = df[key_column].astype(str)\n",
    "        # 3. Build prompt template\n",
    "        bullets = \"\\n\".join(f\"- {c}\" for c in criteria)\n",
    "        example_fields = \", \".join(f'\"{c}\": 7' for c in criteria)\n",
    "        prompt_template = (\n",
    "            \"You are an expert evaluator. For each input record, \"\n",
    "            \"score 1–10 on these criteria:\\n\"\n",
    "            f\"{bullets}\\n\\n\"\n",
    "            \"Respond *ONLY* with a JSON array of objects. Each element *MUST* be an object containing the *EXACT* fields shown below;\"\n",
    "            \"*NEVER* output numbers, strings, or null values.\\n\"\n",
    "            \"[\\n\"\n",
    "            f'  {{ \"{key_column}\": \"...\", {example_fields} }},\\n'\n",
    "            \"  { … }\\n\"\n",
    "            \"]\\n\"\n",
    "            \"No wrapper, no extra text.\"\n",
    "            \"Do not include any other text, explanation, or markup.\"\n",
    "            \"Return *ONLY* with a JSON array of objects.\"\n",
    "        )\n",
    "\n",
    "        prompt_template = (\n",
    "            \"You are an expert evaluator.  \"\n",
    "            f\"For each input record, score 1–10 on these criteria:\\n\"\n",
    "            f\"{bullets}\\n\\n\"\n",
    "            \"Respond *only* with a valid JSON object of the form:\\n\"\n",
    "            \"{\\n\"\n",
    "            '  \"results\": [\\n'\n",
    "            f'    {{ \"{key_column}\": \"...\", {example_fields} }}\\n'\n",
    "            \"  ]\\n\"\n",
    "            \"}\\n\"\n",
    "            \"Do not include any other text, explanation, or markup.\"\n",
    "        )\n",
    "        # 4. Helper to chunk indices\n",
    "        def chunk_list(lst: List[int], n: int):\n",
    "            for i in range(0, len(lst), n):\n",
    "                yield lst[i : i + n]\n",
    "        # 5. Score in batches\n",
    "        scored: List[Dict[str,Any]] = []\n",
    "        for idxs in chunk_list(df.index.tolist(), batch_size):\n",
    "            batch = df.loc[idxs]\n",
    "            payload = [\n",
    "                {key_column: r[key_column], eval_column: r[eval_column]}\n",
    "                for _, r in batch.iterrows()\n",
    "            ]\n",
    "            prompt = prompt_template + \"\\n\\n\" + json.dumps(payload, indent=2)\n",
    "            resp = self.llm(prompt, grammar=self.grammar, max_tokens=-1, temperature=0.0)\n",
    "            arr = json.loads(resp[\"choices\"][0][\"text\"])\n",
    "            if not isinstance(arr, list):\n",
    "                raise RuntimeError(f\"Expected JSON array, got {type(arr)}:\\n{arr!r}\")\n",
    "            scored.extend(arr)\n",
    "        # 6. Flatten & clean model output\n",
    "        flat: List[Dict[str, Any]] = []\n",
    "        for item in scored:\n",
    "            if isinstance(item, dict) and \"results\" in item:\n",
    "                flat.extend(item[\"results\"])\n",
    "            elif isinstance(item, dict):\n",
    "                flat.append(item)\n",
    "            elif isinstance(item, list):\n",
    "                # e.g. model returned nested array: flatten one level\n",
    "                flat.extend(obj for obj in item if isinstance(obj, dict))\n",
    "            else:\n",
    "                # Skip numbers, nulls, etc.\n",
    "                logging.warning(\"Discarding non‑object item from model output: %r\", item)\n",
    "        \n",
    "        if not flat:\n",
    "            raise RuntimeError(\"Model returned no valid score objects; check prompt/grammar.\")\n",
    "        # 7. Build scores DataFrame\n",
    "        scores_df = pd.DataFrame(flat)\n",
    "        if key_column not in scores_df.columns:\n",
    "            raise KeyError(f\"Missing '{key_column}' in scored output\")\n",
    "\n",
    "        scores_df[key_column] = scores_df[key_column].astype(str)\n",
    "        # 8. Merge & compute TotalScore\n",
    "        combined = df.merge(scores_df, on=key_column, how=\"left\")\n",
    "        combined[\"TotalScore\"] = combined[criteria].sum(axis=1)\n",
    "        return combined\n",
    "\n",
    "    @classmethod\n",
    "    def log_model(\n",
    "        cls,\n",
    "        model_name: str,\n",
    "        llama_model_path: str,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Logs the model to MLflow with signature requiring:\n",
    "          - DataFrame input with any columns\n",
    "          - params: key_column (str), eval_column (str), criteria (JSON string), batch_size (int)\n",
    "        \"\"\"\n",
    "        DEMO_PATH = \"../demo\"\n",
    "        artifacts = {\n",
    "            \"llama_model_path\": llama_model_path,\n",
    "            \"demo\": DEMO_PATH,\n",
    "                    }\n",
    "\n",
    "        # Input schema: DataFrame only\n",
    "        input_schema = None  # allow arbitrary columns\n",
    "\n",
    "        # Output schema: will match input DF plus criteria columns + TotalScore\n",
    "        # we omit explicit output schema for flexibility\n",
    "\n",
    "        # Params schema: four required params\n",
    "        params_schema = ParamSchema([\n",
    "            ParamSpec(\"key_column\",  DataType.string,  None),\n",
    "            ParamSpec(\"eval_column\", DataType.string,  None),\n",
    "            ParamSpec(\"criteria\",    DataType.string,  '[\"Originality\",\"Clarity\",\"Relevance\",\"Feasibility\",\"Feasibility\"]'),\n",
    "            ParamSpec(\"batch_size\",  DataType.long,    5),\n",
    "        ])\n",
    "\n",
    "        signature = ModelSignature(inputs=input_schema, outputs=None, params=params_schema)\n",
    "\n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=model_name,\n",
    "            python_model=cls(),\n",
    "            artifacts=artifacts,\n",
    "            signature=signature,\n",
    "            registered_model_name=model_name,\n",
    "        )\n",
    "        logging.info(f\"Logged LlamaEvaluatorModel '{model_name}' requiring key_column, eval_column, criteria, batch_size\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c200dfb2-97a6-4907-94e9-39e3469fd094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 ──────────────────────────────────────────────────────────────────────────\n",
    "# global settings\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"llm‑mlflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5715c4b-0c00-4ca1-b199-b49fb9b4398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 2 ──────────────────────────────────────────────────────────────────────────\n",
    "# Log and register the model\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "logger.info(f\"Starting experiment: {EXPERIMENT_NAME}\")\n",
    "mlflow.set_tracking_uri('/phoenix/mlflow')\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "    run_id = run.info.run_id\n",
    "    logger.info(\"Run ID: %s\", run_id)\n",
    "\n",
    "    # log_model now only needs model_name & gguf path (all runtime settings come as params at inference)\n",
    "    LlamaEvaluatorModel.log_model(\n",
    "        model_name      = MODEL_NAME,\n",
    "        llama_model_path= LLAMA_GGUF_PATH,\n",
    "    )\n",
    "\n",
    "    # Ensure the model is registered (in case autolog didn't)\n",
    "    mlflow.register_model(\n",
    "        model_uri=f\"runs:/{run_id}/{MODEL_NAME}\",\n",
    "        name     =MODEL_NAME\n",
    "    )\n",
    "    logger.info(\"Registered model: %s\", MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0e36ce",
   "metadata": {},
   "source": [
    "# Fetching the Latest Model Version from MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cd9b62-51b8-4ca0-8257-e85fd012bf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 ──────────────────────────────────────────────────────────────────────────\n",
    "# Retrieve the latest version & signature\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "client = MlflowClient()\n",
    "latest_version = client.get_latest_versions(MODEL_NAME, stages=[\"None\"])[0].version\n",
    "logger.info(\"Latest model version: %s\", latest_version)\n",
    "\n",
    "mi = mlflow.models.get_model_info(f\"models:/{MODEL_NAME}/{latest_version}\")\n",
    "logger.info(\"Model signature:\\n%s\", mi.signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204663e3",
   "metadata": {},
   "source": [
    "# Loading the Model and Running Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da553bf5-b034-41bb-b84b-9b3968808ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 4 ──────────────────────────────────────────────────────────────────────────\n",
    "# Load the model\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "model_uri = f\"models:/{MODEL_NAME}/{latest_version}\"\n",
    "model     = mlflow.pyfunc.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec726daf-a05c-42b0-b7d7-1c4d13018f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 5 ──────────────────────────────────────────────────────────────────────────\n",
    "# Run inference\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "sample_df = pd.DataFrame({\n",
    "    \"BoothNumber\": [\"TEST001\", \"TEST002\"],\n",
    "    \"AbstractText\": [\n",
    "        \"Investigating the effects of microplastics on marine life populations.\",\n",
    "        \"Developing a low‑cost solar charger for off‑grid applications.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "predictions = model.predict(\n",
    "    sample_df,\n",
    "    params={\n",
    "        \"key_column\":  \"BoothNumber\",\n",
    "        \"eval_column\": \"AbstractText\",\n",
    "        \"criteria\":    json.dumps(\n",
    "            [\"Originality\", \"ScientificRigor\", \"Clarity\",\n",
    "             \"Relevance\", \"Feasibility\", \"Brevity\"]\n",
    "        ),\n",
    "        \"batch_size\":  5\n",
    "    }\n",
    ")\n",
    "\n",
    "logger.info(\"Inference results:\")\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2eb880",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Notebook execution completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beec62f",
   "metadata": {},
   "source": [
    "Built with ❤️ using Z by HP AI Studio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
