{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31714085-065e-47da-bb2c-3c80ff88f6c8",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-size: 50px;\">Scientific Presentation Script Generator with Local LLM & ChromaDB</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438b733e",
   "metadata": {},
   "source": [
    "## üéØ **Overview**\n",
    "\n",
    "This notebook demonstrates how to build a comprehensive **Scientific Presentation Script Generator** using:\n",
    "\n",
    "- **arXiv Paper Retrieval**: Search and download academic papers  \n",
    "- **Document Processing**: Text extraction and chunking for optimal processing  \n",
    "- **Vector Database**: ChromaDB for semantic search and retrieval  \n",
    "- **Local LLM Integration**: Meta Llama 3.1 model for analysis and generation  \n",
    "- **Interactive Generation**: Step-by-step script creation with user approval  \n",
    "\n",
    "**Pipeline Flow**: arXiv ‚Üí Text Extraction ‚Üí Vector Storage ‚Üí Analysis ‚Üí Script Generation ‚Üí Interactive Refinement\n",
    "\n",
    "---\n",
    "\n",
    "## üõ† **What You'll Learn**\n",
    "\n",
    "- Paper retrieval from arXiv using search queries  \n",
    "- Text extraction and chunking strategies  \n",
    "- Vector database setup with ChromaDB  \n",
    "- LLM configuration for local inference  \n",
    "- Script generation and evaluation workflows  \n",
    "- MLflow model registration and deployment  \n",
    "\n",
    "---\n",
    "\n",
    "## üìã **Prerequisites**\n",
    "\n",
    "- LangChain setup and configuration  \n",
    "- Vector database fundamentals  \n",
    "- Basic understanding of embeddings and retrieval systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb6d677-8e73-411d-828e-a63f70edb461",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "This step installs the necessary libraries for local LLM processing and document analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6521a055-e62f-45bd-ba11-ff3104cdbafa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdf4a81-6b8c-41c9-a107-aad52d5583ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import mlflow\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import torch\n",
    "\n",
    "# Add the src directory to the path to import utils\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from src.utils import configure_hf_cache\n",
    "from src.utils import configure_proxy\n",
    "from src.utils import load_config_and_secrets\n",
    "from src.utils import initialize_llm\n",
    "\n",
    "# Import transformers from huggingface\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Import components of notebook\n",
    "from core.extract_text.arxiv_search import ArxivSearcher\n",
    "from core.generator.script_generator import ScriptGenerator\n",
    "from core.analyzer.scientific_paper_analyzer import ScientificPaperAnalyzer\n",
    "from core.deploy.text_generation_service import TextGenerationService\n",
    "\n",
    "# Import langchain libraries\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_huggingface import HuggingFacePipeline, HuggingFaceEndpoint\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "# Libraries from python\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686a593c",
   "metadata": {},
   "source": [
    "## Configurations and Secrets Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073ab4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress Python warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150df526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Create logger ===\n",
    "logger = logging.getLogger(\"text-generation-notebook\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", \n",
    "                             datefmt=\"%Y-%m-%d %H:%M:%S\") \n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a7b078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ML and data processing\n",
    "import mlflow\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from operator import itemgetter\n",
    "\n",
    "# === Project-Specific Imports (from src.utils) ===\n",
    "from src.utils import (\n",
    "    load_config_and_secrets,\n",
    "    configure_proxy,\n",
    "    initialize_llm,\n",
    "    login_huggingface,\n",
    "    clean_code,\n",
    "    generate_code_with_retries,\n",
    "    get_model_context_window,\n",
    "    get_context_window,\n",
    "    dynamic_retriever,\n",
    "    format_docs_with_adaptive_context,\n",
    "    estimate_tokens_accurate\n",
    ")\n",
    "\n",
    "# === Core Module Imports ===\n",
    "from core.extract_text.arxiv_search import ArxivSearcher\n",
    "from core.analyzer.scientific_paper_analyzer import ScientificPaperAnalyzer\n",
    "from core.generator.script_generator import ScriptGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac48c2a7-503f-4e9b-b7cb-1523277b952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config, secrets = load_config_and_secrets(CONFIG_PATH, SECRETS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596cf12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging for notebook execution\n",
    "logger.info('Notebook execution started - Local text generation pipeline')\n",
    "logger.info('All dependencies loaded successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4754b897",
   "metadata": {},
   "source": [
    "### Verify Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b15966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration and secrets\n",
    "config, secrets = load_config_and_secrets()\n",
    "\n",
    "# Configure proxy if specified in config\n",
    "configure_proxy(config)\n",
    "\n",
    "print(\"‚úÖ Configuration loaded successfully.\")\n",
    "print(f\"üìÅ Model source: {config.get('model_source', 'local')}\")\n",
    "\n",
    "# Setup HuggingFace authentication if available\n",
    "if \"HUGGINGFACE_API_KEY\" in secrets:\n",
    "    try:\n",
    "        login_huggingface(secrets)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è HuggingFace login failed: {e}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No HuggingFace API key found - using models without authentication\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb301df",
   "metadata": {},
   "source": [
    "### Proxy Configuration\n",
    "\n",
    "For certain enterprise networks, you might need to configure proxy settings to access external services. If this is your case, set up the \"proxy\" field in your config.yaml and the following cell will configure the necessary environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488a1d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "configure_proxy(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e98e41",
   "metadata": {},
   "source": [
    "### Configuration of Hugging face caches\n",
    "\n",
    "In the next cell, we configure HuggingFace cache, so that all the models downloaded from them are persisted locally, even after the workspace is closed. This is a future desired feature for AI Studio and the GenAI addon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bce2146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure HuggingFace cache\n",
    "configure_hf_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739ef1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HuggingFace Embeddings\n",
    "embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c24c88-59aa-4d2b-a7fa-674e9130e2c7",
   "metadata": {},
   "source": [
    "## üéØ Step 4: Script Generation and Evaluation\n",
    "\n",
    "### Interactive Script Generation\n",
    "\n",
    "The ScriptGenerator orchestrates the prompt flow, allowing users to generate each section of the presentation interactively, with built-in approval workflows for quality control.\n",
    "\n",
    "**Key Features:**\n",
    "- **Interactive Approval**: Review and approve each generated section  \n",
    "- **Iterative Refinement**: Regenerate content until satisfied  \n",
    "- **Structured Output**: Organized presentation script format  \n",
    "- **Context-Aware Generation**: Uses analyzed content for accurate scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9034bdfe-71a7-47fa-85d0-dae9ad26f53f",
   "metadata": {},
   "source": [
    "### 1. ‚úÖ **Local LLM Initialization**\n",
    "\n",
    "Initialize the local language model for content analysis and script generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a678462e-7d6b-4c77-9b69-b6029239522e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration is already loaded - no additional setup needed\n",
    "print(\"‚úÖ Environment configured for local LLM processing\")\n",
    "print(\"üîß Ready to proceed with document analysis and script generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef9286a-6f07-489f-bd77-2275deed3ad7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the language model for script generation\n",
    "print(\"ü§ñ Initializing local language model...\")\n",
    "\n",
    "try:\n",
    "    # Load LLM using configuration\n",
    "    llm = initialize_llm(\n",
    "        model_source=config.get(\"model_source\", \"local\"),\n",
    "        secrets=secrets\n",
    "    )\n",
    "    print(\"‚úÖ Language model loaded successfully!\")\n",
    "    print(f\"üìä Context window: {get_context_window(llm)} tokens\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading language model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc84e63-bc1d-4a4f-8eea-a315b9ce4100",
   "metadata": {},
   "source": [
    "### üß± Step 2: Processing and Embedding Generation\n",
    "In this step, we transform the raw text extracted from the papers into structured embeddings that can be stored and retrieved efficiently in the RAG pipeline.\n",
    "\n",
    "The flow includes three main stages:\n",
    "\n",
    "1. **üìÑ Create Document Objects**\n",
    "The full text of each paper is wrapped into Document objects ‚Äî a standard structure used by LangChain to manage and manipulate textual data.\n",
    "\n",
    "2. **‚úÇÔ∏è Split Text into Chunks**\n",
    "Using LangChain's RecursiveCharacterTextSplitter, the documents are segmented into smaller blocks (chunks) based on character limits. This makes the downstream embedding and retrieval process more effective.\n",
    "\n",
    "The chunk_size parameter defines the maximum length of each chunk.\n",
    "\n",
    "3. **üìä Generate Embeddings**\n",
    "Each text chunk is converted into a vector representation (embedding) using HuggingFaceEmbeddings. These embeddings are later used to populate the vector store and serve as the foundation for similarity-based retrieval in the generation step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064759c4-6e03-4653-8dd1-6dd845aa1ed2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creates a list of Document objects from the scientific articles in the `papers` variable.\n",
    "# Each `Document` is created with the article content and a metadata dictionary containing the title.\n",
    "documents = [Document(page_content=paper['text'], metadata={\"title\": paper['title']}) for paper in papers]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=400)\n",
    "splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25966f82-3b88-4067-86a8-9ebe143b535b",
   "metadata": {},
   "source": [
    "### üß© Step 3: Vector Data Storage and Retrieval\n",
    "This step handles the storage of embeddings into a vector database and configures a retriever to enable similarity-based search ‚Äî a key component in the RAG pipeline.\n",
    "\n",
    "üß† Store Embeddings with Chroma\n",
    "The segmented text chunks, previously converted into embeddings, are stored in a local vector store using ChromaDB. This enables efficient access to semantically similar information later on.\n",
    "\n",
    "üîé Configure the Retriever\n",
    "After storing the embeddings, a retriever is set up to perform similarity search queries. This retriever is responsible for:\n",
    "\n",
    "- Receiving a user query or prompt\n",
    "\n",
    "- Searching through the stored embeddings\n",
    "\n",
    "- Returning the most relevant chunks based on vector similarity\n",
    "\n",
    "> üì¶ This mechanism allows the generation model to work with only the most relevant information, improving accuracy and reducing hallucinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1486a1-1c3a-4c93-b6e3-6a7685ad0f13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Our vector database\n",
    "vectordb = Chroma.from_documents(documents=splits, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d5dc9e-0632-4f61-bc9b-43bbb144a09e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04e8faf-43be-44f5-bd21-94be9efb799c",
   "metadata": {},
   "source": [
    "## üß† Chapter 2: Building a Prompt Flow for Generating Scientific Presentation Scripts\n",
    "In this chapter, we build a prompt flow to generate a complete scientific presentation script using LLMs. Each section of the script (e.g., title, introduction, methodology) is created individually through dedicated prompt templates.\n",
    "\n",
    "The process is composed of four main steps:\n",
    "\n",
    "1. üß† **Model Selection**\n",
    "Choose the best-suited LLM for the generation task, depending on performance or local availability.\n",
    "\n",
    "2. üîç **Analysis with ScientificPaperAnalyzer**\n",
    "Using the component ScientificPaperAnalyzer, a custom LangChain chain is built to analyze the scientific paper and generate context-aware responses.\n",
    "\n",
    "3. üßæ **Script Generation**\n",
    "The ScriptGenerator orchestrates the prompt flow, allowing users to generate each section of the presentation interactively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b928d0d-7e83-4f5d-b1d5-9792f28fd76a",
   "metadata": {},
   "source": [
    "#### ‚öôÔ∏è Step 4: Config Enviroment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b13463-62ba-4cb5-9ff5-89bf8253cca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for the script generation project\n",
    "PROJECT_NAME = 'Academic Script Generator'\n",
    "print(f\"‚úÖ Project configured: {PROJECT_NAME}\")\n",
    "print(\"üöÄ Ready for script generation pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc352519-9241-4b14-a9a6-53747473f038",
   "metadata": {},
   "source": [
    "## Local Environment Setup\n",
    "\n",
    "This section configures the local environment for script generation. The following steps will:\n",
    "\n",
    "1. ‚úÖ **Initialize Local Configuration**\n",
    "2. ‚úÖ **Set Up Script Generator**\n",
    "3. ‚úÖ **Configure Content Generation Parameters**\n",
    "\n",
    "The ScriptGenerator orchestrates the prompt flow, allowing users to generate each section of the presentation interactively, with all processing done locally using the configured LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca598c3-80c5-4940-895a-f296765ef75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure environment for local development\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up working directory and logging\n",
    "WORK_DIR = os.getcwd()\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "print(f\"‚úÖ Working directory: {WORK_DIR}\")\n",
    "print(f\"‚úÖ Session timestamp: {TIMESTAMP}\")\n",
    "print(\"üîß Environment ready for script generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67775ce6-cb3c-4959-8051-5f2f19da7e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Script Generator\n",
    "from core.generator.script_generator import ScriptGenerator\n",
    "\n",
    "# Instantiate generator\n",
    "generator = ScriptGenerator()\n",
    "\n",
    "# Initialize\n",
    "print(\"‚úÖ Script generator initialized successfully\")\n",
    "print(\"üöÄ Ready to generate academic scripts with LLM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cc0349-e689-4497-a02d-98d50e97483c",
   "metadata": {},
   "source": [
    "### ‚úÖ Step 6: Run and Approve\n",
    "The ScriptGenerator component is responsible for generating each section of the scientific presentation script in an interactive and human-in-the-loop fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ced59fe-c099-49e5-a4b8-20867ad1bf16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Configure content generation parameters\n",
    "generation_config = {\n",
    "    \"topic\": \"The Impact of Artificial Intelligence on Modern Education\",\n",
    "    \"script_type\": \"academic_presentation\",\n",
    "    \"duration_minutes\": 10,\n",
    "    \"target_audience\": \"university_students\",\n",
    "    \"tone\": \"informative_engaging\"\n",
    "}\n",
    "\n",
    "# Display configuration\n",
    "print(\"üìù Content Generation Configuration:\")\n",
    "for key, value in generation_config.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "    \n",
    "print(\"\\n‚úÖ Configuration set - ready to generate script content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33cac41-2020-40f8-8be7-f330b7abfaaf",
   "metadata": {},
   "source": [
    "## Model Service\n",
    "\n",
    "In this section, we implement the **Model Service**, a REST API responsible for serving the language model. The API is automatically documented using Swagger (via FastAPI), enabling interactive testing and clear documentation of the endpoints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3260db63-2353-4fef-b7ea-619db50079bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate academic script content\n",
    "print(\"üöÄ Starting script generation...\")\n",
    "\n",
    "try:\n",
    "    # Generate script using the configured parameters\n",
    "    generated_script = generator.generate_script(\n",
    "        topic=generation_config[\"topic\"],\n",
    "        script_type=generation_config[\"script_type\"],\n",
    "        duration_minutes=generation_config[\"duration_minutes\"],\n",
    "        target_audience=generation_config[\"target_audience\"],\n",
    "        tone=generation_config[\"tone\"]\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Script generation completed successfully!\")\n",
    "    print(f\"üìÑ Generated script length: {len(generated_script)} characters\")\n",
    "    \n",
    "    # Display first 500 characters as preview\n",
    "    print(\"\\nüìñ Script Preview:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(generated_script[:500] + \"...\" if len(generated_script) > 500 else generated_script)\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during script generation: {str(e)}\")\n",
    "    print(\"Please check your configuration and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91617048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze generated script locally\n",
    "print(\"üìä Local Script Analysis\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if 'generated_script' in locals():\n",
    "    # Basic text analysis\n",
    "    word_count = len(generated_script.split())\n",
    "    char_count = len(generated_script)\n",
    "    estimated_reading_time = word_count / 150  # Average reading speed\n",
    "    \n",
    "    print(f\"üìà Word count: {word_count}\")\n",
    "    print(f\"üìà Character count: {char_count}\")\n",
    "    print(f\"‚è±Ô∏è  Estimated reading time: {estimated_reading_time:.1f} minutes\")\n",
    "    print(f\"üéØ Target duration: {generation_config['duration_minutes']} minutes\")\n",
    "    \n",
    "    # Check if content meets target duration\n",
    "    duration_diff = abs(estimated_reading_time - generation_config['duration_minutes'])\n",
    "    if duration_diff <= 1:\n",
    "        print(\"‚úÖ Script duration matches target well!\")\n",
    "    elif estimated_reading_time < generation_config['duration_minutes']:\n",
    "        print(\"‚ö†Ô∏è  Script may be shorter than target duration\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Script may be longer than target duration\")\n",
    "        \n",
    "    print(\"\\nüéâ Local analysis completed!\")\n",
    "else:\n",
    "    print(\"‚ùå No generated script found. Please run the generation cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1887916",
   "metadata": {},
   "source": [
    "Built with ‚ù§Ô∏è using Z by HP AI Studio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
