{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7c007e5",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-size: 50px;\"> Register Model </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdcb43f",
   "metadata": {},
   "source": [
    "# Notebook Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756db15a",
   "metadata": {},
   "source": [
    "- Configure the Environment\n",
    "- Define Constants and Paths\n",
    "- Load Configuratons and Secrets\n",
    "- Register Model \n",
    "- Evaluate Model\n",
    "- Log Execution Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a1d3d1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure logger\n",
    "logger: logging.Logger = logging.getLogger(\"register_markdown_model_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.propagate = False\n",
    "\n",
    "# Set formatter\n",
    "formatter: logging.Formatter = logging.Formatter(\n",
    "    fmt=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "# Configure and attach stream handler\n",
    "if not logger.handlers:\n",
    "    stream_handler: logging.StreamHandler = logging.StreamHandler()\n",
    "    stream_handler.setFormatter(formatter)\n",
    "    logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0722b2d8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "logger.info(\"Notebook execution started.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49769695",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# File Paths\n",
    "CONFIG_PATH = Path(\"../configs/configs.yaml\")\n",
    "SECRETS_PATH = Path(\"../configs/secrets.yaml\")\n",
    "REQUIREMENTS_PATH = Path(\"../requirements.txt\")\n",
    "CODE_PATH = Path(\"../src\")\n",
    "LOCAL_MODEL_PATH = Path(\"/home/jovyan/datafabric/llama3.1-8b-instruct/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\")\n",
    "\n",
    "# Evaluation Data Path\n",
    "EVAL_DATA_PATH = Path(\"./results.json\")\n",
    "\n",
    "# MLflow Configuration\n",
    "EXPERIMENT_NAME = \"markdown-correction-experiment\"\n",
    "RUN_NAME = f\"registration-run-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "MODEL_NAME = \"MarkdownCorrector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96297375",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# %pip install -r {REQUIREMENTS_PATH} --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b00c95",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Standard & Third-Party Libraries\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from mlflow.models import evaluate, ModelSignature\n",
    "from mlflow.types import Schema, ColSpec\n",
    "from mlflow.metrics import ari_grade_level, exact_match, rouge1, rougeL\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# ✅ Add src directory to system path. This allows the notebook to find\n",
    "# your custom modules for interactive development.\n",
    "sys.path.append(str(CODE_PATH.resolve()))\n",
    "\n",
    "# Internal Modules (now imported correctly from the src directory)\n",
    "from markdown_correction_service import MarkdownCorrectionService\n",
    "from utils import load_config_and_secrets\n",
    "from llm_metrics import (\n",
    "    semantic_similarity_metric,\n",
    "    grammar_error_count_metric,\n",
    "    grammar_error_rate_metric,\n",
    "    grammar_improvement_metric,\n",
    "    grammar_score_metric,\n",
    "    readability_improvement_metric,\n",
    "    llm_judge_metric_local,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c3b585",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def log_asset_status(asset_path: Path, asset_name: str) -> None:\n",
    "    \"\"\"Logs the status of a given asset based on its existence.\"\"\"\n",
    "    if asset_path.exists():\n",
    "        logger.info(f\"✅ {asset_name} is properly configured at: {asset_path}\")\n",
    "    else:\n",
    "        logger.warning(f\"⚠️ {asset_name} not found at: {asset_path}. Please ensure it is correctly configured.\")\n",
    "\n",
    "# --- Validate Assets ---\n",
    "log_asset_status(CONFIG_PATH, \"Config file\")\n",
    "log_asset_status(SECRETS_PATH, \"Secrets file\")\n",
    "log_asset_status(LOCAL_MODEL_PATH, \"Local LLaMA model\")\n",
    "log_asset_status(EVAL_DATA_PATH, \"Evaluation data JSON\")\n",
    "log_asset_status(REQUIREMENTS_PATH, \"Requirements file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbe3a09",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load evaluation results from the JSON file\n",
    "with open(EVAL_DATA_PATH, \"r\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# Extract original markdown texts and create evaluation DataFrame\n",
    "original_texts = [item[\"original\"] for item in results]\n",
    "eval_df = pd.DataFrame(original_texts, columns=[\"markdown\"])\n",
    "\n",
    "logger.info(f\"Loaded {len(eval_df)} records for evaluation.\")\n",
    "print(eval_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0118e140",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "    run_id = run.info.run_id\n",
    "    logger.info(f\"MLflow Run Started. Run ID: {run_id}\")\n",
    "\n",
    "    # Use the classmethod from your module to log and register the model\n",
    "    MarkdownCorrectionService.log_model(\n",
    "        model_name=MODEL_NAME,\n",
    "        llm_artifact_path=str(LOCAL_MODEL_PATH),\n",
    "        config_path=str(CONFIG_PATH),\n",
    "        secrets_path=str(SECRETS_PATH),\n",
    "        requirements_path=str(REQUIREMENTS_PATH),\n",
    "        # ✅ This parameter is the key fix. It tells MLflow to bundle the\n",
    "        # entire contents of the '../src' directory with the model artifact.\n",
    "        code_paths=[str(CODE_PATH)]\n",
    "    )\n",
    "    \n",
    "    model_uri = f\"runs:/{run_id}/{MODEL_NAME}\"\n",
    "    logger.info(f\"Model URI: {model_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842614ff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "client = MlflowClient()\n",
    "try:\n",
    "    latest_version_info = client.get_latest_versions(MODEL_NAME, stages=[\"None\"])[0]\n",
    "    latest_version = latest_version_info.version\n",
    "    logger.info(f\"Successfully registered model '{MODEL_NAME}' version {latest_version}.\")\n",
    "    \n",
    "    # Load model for a test prediction\n",
    "    model_uri_latest = f\"models:/{MODEL_NAME}/{latest_version}\"\n",
    "    loaded_model = mlflow.pyfunc.load_model(model_uri_latest)\n",
    "    \n",
    "    # Perform a sample prediction\n",
    "    sample_input = eval_df.head(1)\n",
    "    logger.info(f\"Performing sample prediction on: \\n{sample_input['markdown'].iloc[0][:100]}...\")\n",
    "    \n",
    "    prediction = loaded_model.predict(sample_input)\n",
    "    logger.info(f\"✅ Sample prediction successful. Output:\\n{prediction.iloc[0][:100]}...\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to validate registered model. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2b8504",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "logger.info(\"Starting model evaluation with mlflow.evaluate...\")\n",
    "\n",
    "# mlflow.evaluate requires a run to be active\n",
    "with mlflow.start_run(run_id=run_id):\n",
    "    results = mlflow.evaluate(\n",
    "        model=model_uri,\n",
    "        data=eval_df,\n",
    "        targets=\"markdown\",\n",
    "        feature_names=[\"markdown\"],\n",
    "        evaluators=\"default\",\n",
    "        extra_metrics=[\n",
    "            ari_grade_level(),\n",
    "            exact_match(),\n",
    "            rouge1(),\n",
    "            rougeL(),\n",
    "            semantic_similarity_metric,\n",
    "            grammar_error_count_metric,\n",
    "            grammar_error_rate_metric,\n",
    "            grammar_improvement_metric,\n",
    "            grammar_score_metric,\n",
    "            readability_improvement_metric,\n",
    "            llm_judge_metric_local\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    logger.info(\"✅ Evaluation complete.\")\n",
    "    logger.info(\"Evaluation Metrics:\")\n",
    "    for key, value in results.metrics.items():\n",
    "        logger.info(f\"  - {key}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1c85b7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "end_time: float = time.time()\n",
    "elapsed_time: float = end_time - start_time\n",
    "elapsed_minutes: int = int(elapsed_time // 60)\n",
    "elapsed_seconds: float = elapsed_time % 60\n",
    "\n",
    "logger.info(f\"⏱️ Total execution time: {elapsed_minutes}m {elapsed_seconds:.2f}s\")\n",
    "logger.info(\"✅ Notebook execution completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
