{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7c007e5",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-size: 50px;\"> Register Model </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdcb43f",
   "metadata": {},
   "source": [
    "# Notebook Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756db15a",
   "metadata": {},
   "source": [
    "- Configure the Environment\n",
    "- Define Constants and Paths\n",
    "- Define Necessary Modules\n",
    "- Define MLflow Model Class\n",
    "- Log and Register Model\n",
    "- Prepare Eval Data\n",
    "- Evaluate Model\n",
    "- Log Execution Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5423f7b3",
   "metadata": {},
   "source": [
    "## Configure the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a1d3d1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "# Configure logger\n",
    "logger: logging.Logger = logging.getLogger(\"register_model_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.propagate = False  # Prevent duplicate logs\n",
    "\n",
    "# Set formatter\n",
    "formatter: logging.Formatter = logging.Formatter(\n",
    "    fmt=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "# Configure and attach stream handler\n",
    "stream_handler: logging.StreamHandler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "start_time = time.time()\n",
    "logger.info(\"✅ Notebook execution started.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c1f447",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "%pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be896c9a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Standard & Third-Party Libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from mlflow.models import ModelSignature, evaluate\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "from typing import List\n",
    "\n",
    "# Add src directory to system path\n",
    "sys.path.append(str(Path(\"..\").resolve() / \"src\"))\n",
    "\n",
    "# --- Imports for Evaluation ---\n",
    "# Import standard MLflow metrics\n",
    "from mlflow.metrics import exact_match, rouge1, rougeL\n",
    "\n",
    "logger.info(\"✅ All libraries and metrics imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964f2b67",
   "metadata": {},
   "source": [
    "## Define Constants and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49769695",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration Paths\n",
    "CONFIG_PATH = Path(\"../configs/configs.yaml\")\n",
    "SECRETS_PATH = Path(\"../configs/secrets.yaml\")\n",
    "REQUIREMENTS_PATH = Path(\"../requirements.txt\")\n",
    "LOCAL_MODEL_PATH = Path(\"/home/jovyan/datafabric/llama3.1-8b-instruct/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\")\n",
    "\n",
    "# MLflow Configuration\n",
    "MLFLOW_EXPERIMENT_NAME = \"Markdown_Correction_Service\"\n",
    "MLFLOW_MODEL_NAME = \"markdown-corrector\"\n",
    "\n",
    "logger.info(\"Constants and paths defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69633861",
   "metadata": {},
   "source": [
    "## Define Necessary Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c842bde",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "###--------GITHUB EXTRACTOR START--------###\n",
    "############################################\n",
    "\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import sys\n",
    "import base64 \n",
    "import re \n",
    "from markdown_it import MarkdownIt\n",
    "from markdown_it.token import Token \n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Optional, Tuple, Dict, Union\n",
    "\n",
    "class GitHubMarkdownProcessor:\n",
    "    \"\"\"\n",
    "    Processor for extracting and parsing Markdown files from GitHub repositories.\n",
    "\n",
    "    This class fetches `.md` files from a public or private GitHub repository,\n",
    "    replaces structural and inline components with labeled placeholders, and \n",
    "    saves the resulting structure locally.\n",
    "\n",
    "    Attributes:\n",
    "        repo_url (str): GitHub repository URL.\n",
    "        repo_owner (str): Owner of the GitHub repository.\n",
    "        repo_name (str): Name of the GitHub repository.\n",
    "        access_token (Optional[str]): GitHub Personal Access Token (if needed).\n",
    "        save_dir (str): Directory to save the processed files.\n",
    "        api_base_url (str): Base GitHub API URL for the repository.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, repo_url: str, access_token: Optional[str] = None, save_dir: str = './parsed_repo'):\n",
    "        \"\"\"\n",
    "        Initializes the Markdown processor with a GitHub repo URL.\n",
    "\n",
    "        Args:\n",
    "            repo_url (str): Full URL to the GitHub repository.\n",
    "            access_token (Optional[str]): GitHub token for private repo access.\n",
    "            save_dir (str): Output directory to store processed Markdown files.\n",
    "        \"\"\"\n",
    "        self.repo_url = repo_url\n",
    "        self.access_token = access_token\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        owner, repo, error = self.parse_url()\n",
    "        if error:\n",
    "            raise ValueError(error)\n",
    "        \n",
    "        self.repo_owner = owner\n",
    "        self.repo_name = repo\n",
    "        self.api_base_url = f\"https://api.github.com/repos/{self.repo_owner}/{self.repo_name}\"\n",
    "\n",
    "    def parse_url(self) -> Tuple[Optional[str], Optional[str], Optional[str]] :\n",
    "        \"\"\"\n",
    "        Parses a GitHub URL and extracts the repository owner and name.\n",
    "    \n",
    "        Args:\n",
    "            github_url (str): The full GitHub URL to parse.\n",
    "    \n",
    "        Returns:\n",
    "            Tuple[Optional[str], Optional[str], Optional[str]]:\n",
    "                - The repository owner (e.g., 'openai')\n",
    "                - The repository name (e.g., 'whisper')\n",
    "                - An error message string if parsing fails; otherwise, None\n",
    "        \"\"\"\n",
    "        parsed_url = urlparse(self.repo_url)\n",
    "        path_parts = parsed_url.path.strip(\"/\").split(\"/\")\n",
    "    \n",
    "        # Validate URL format\n",
    "        if len(path_parts) < 2:\n",
    "            return None, None, \"Invalid GitHub URL format.\"\n",
    "        \n",
    "        # Return owner and repo\n",
    "        return path_parts[0], path_parts[1], None\n",
    "    \n",
    "    def check_repo(self) -> str:\n",
    "        \"\"\"\n",
    "        Determines the visibility (public or private) of a GitHub repository.\n",
    "    \n",
    "        Parses the provided GitHub URL, queries the GitHub API to fetch repository details,\n",
    "        and returns a string indicating its visibility or an error message if access fails.\n",
    "    \n",
    "        Args:\n",
    "            github_url (str): The URL of the GitHub repository to check.\n",
    "            access_token (Optional[str], optional): GitHub personal access token for authenticated requests. Defaults to None.\n",
    "    \n",
    "        Returns:\n",
    "            str: \"private\" or \"public\" if the repository is accessible;\n",
    "                 otherwise, a descriptive error message.\n",
    "        \"\"\"\n",
    "        # Parse url into components\n",
    "        owner, name, error = self.parse_url()\n",
    "        if error:\n",
    "            return error\n",
    "        \n",
    "        # Build GitHub URL\n",
    "        url = f\"https://api.github.com/repos/{owner}/{name}\"\n",
    "    \n",
    "        # Build authentication header\n",
    "        headers = {}\n",
    "        if self.access_token:\n",
    "            headers[\"Authorization\"] = f\"Bearer {self.access_token}\"\n",
    "    \n",
    "        response = requests.get(url, headers=headers)\n",
    "    \n",
    "        # Determine privacy of repo\n",
    "        if response.status_code == 200:\n",
    "            repo_data = response.json()\n",
    "            return \"private\" if repo_data.get(\"private\") else \"public\"\n",
    "        elif response.status_code == 404:\n",
    "            return \"Repository is inaccessible. Please authenticate.\"\n",
    "        else:\n",
    "            return f\"Error: {response.status_code}, {response.text}\"\n",
    "        \n",
    "    def extract_md_files(self) -> Tuple[Optional[Dict], Optional[str]]:\n",
    "        \"\"\"\n",
    "        Traverses a GitHub repository to extract all Markdown (.md) files and organize them in a nested directory structure.\n",
    "    \n",
    "        Connects to the GitHub API, retrieves the file tree of the default branch, downloads any Markdown files,\n",
    "        and reconstructs their paths locally in a dictionary format. Supports optional authentication with a personal access token.\n",
    "    \n",
    "        Args:\n",
    "            github_url (str): The GitHub repository URL (e.g., \"https://github.com/user/repo\").\n",
    "            access_token (Optional[str], optional): GitHub personal access token for authenticated requests. Defaults to None.\n",
    "    \n",
    "        Returns:\n",
    "            Tuple[Optional[Dict], Optional[str]]:\n",
    "                - A nested dictionary representing the directory structure and Markdown file contents.\n",
    "                - An error message string if any step fails; otherwise, None.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Parse url into components\n",
    "        owner, name, error = self.parse_url()\n",
    "        if error:\n",
    "            return None, error\n",
    "        \n",
    "        # Build GitHub URL\n",
    "        url = f\"https://api.github.com/repos/{owner}/{name}\"\n",
    "    \n",
    "        # Build authentication header\n",
    "        headers = {}\n",
    "        if self.access_token:\n",
    "            headers[\"Authorization\"] = f\"Bearer {self.access_token}\"\n",
    "    \n",
    "        response = requests.get(url, headers=headers)\n",
    "        if not response.ok:\n",
    "            return None, f\"Error: {response.status_code}, {response.text}\"\n",
    "    \n",
    "        # Get default branch\n",
    "        default_branch = response.json().get(\"default_branch\", \"main\")\n",
    "    \n",
    "        # Build GitHub URL for file tree\n",
    "        tree_url = f\"https://api.github.com/repos/{owner}/{name}/git/trees/{default_branch}?recursive=1\"\n",
    "        tree_response = requests.get(tree_url, headers=headers)\n",
    "        if not tree_response.ok:\n",
    "            return None, f\"Error: {tree_response.status_code}, {tree_response.text}\"\n",
    "        \n",
    "        # Dictionary to hold directory structure\n",
    "        dir_structure = {}\n",
    "    \n",
    "        # Iterate through repo tree structure\n",
    "        for item in tree_response.json().get(\"tree\", []):\n",
    "            path=item[\"path\"]\n",
    "        \n",
    "            # Skip all non md files\n",
    "            if item[\"type\"] != \"blob\" or not path.endswith(\".md\"):\n",
    "                continue\n",
    "    \n",
    "            # Fetch md file content\n",
    "            content_url = f\"https://api.github.com/repos/{owner}/{name}/contents/{path}\"\n",
    "            content_response = requests.get(content_url, headers=headers)\n",
    "            if not content_response.ok:\n",
    "                continue\n",
    "    \n",
    "            # Decode content response\n",
    "            file_data = content_response.json()\n",
    "            try:\n",
    "                content = base64.b64decode(file_data[\"content\"]).decode(\"utf-8\")\n",
    "            except Exception as e:\n",
    "                content = f\"Error decoding content: {e}\"\n",
    "    \n",
    "            # Build directory structure\n",
    "            parts = path.split(\"/\")\n",
    "            current = dir_structure \n",
    "            for part in parts[:-1]:\n",
    "                current = current.setdefault(part, {})\n",
    "            current[parts[-1]] = content \n",
    "    \n",
    "        return dir_structure, None\n",
    "    \n",
    "    def run(self) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        High-level method to:\n",
    "        1. Check repository access\n",
    "        2. Extract markdown files\n",
    "        3. Return raw markdown content by file path\n",
    "    \n",
    "        Returns:\n",
    "            Dict[str, str]: Mapping from file paths to raw markdown content.\n",
    "        \"\"\"\n",
    "        visibility = self.check_repo()\n",
    "        print(f\"Repository visibility: {visibility}\")\n",
    "    \n",
    "        if visibility == \"Repository is inaccessible. Please authenticate.\":\n",
    "            raise PermissionError(\"Cannot access repository. Check your access token.\")\n",
    "    \n",
    "        structure, error = self.extract_md_files()\n",
    "        if error:\n",
    "            raise RuntimeError(f\"Markdown extraction failed: {error}\")\n",
    "    \n",
    "        raw_data = {}\n",
    "    \n",
    "        def process_structure(structure: Dict[str, Union[str, dict]], path: str = \"\") -> None:\n",
    "            \"\"\"\n",
    "            Recursively flattens a nested directory structure of markdown files.\n",
    "        \n",
    "            Args:\n",
    "                structure (Dict[str, Union[str, dict]]): Nested dictionary representing directories and markdown file contents.\n",
    "                path (str, optional): Current path used to build the full file path during traversal. Defaults to \"\".\n",
    "        \n",
    "            Returns:\n",
    "                None: Updates the outer `raw_data` dictionary in-place with path-to-content mappings.\n",
    "            \"\"\"\n",
    "            for name, content in structure.items():\n",
    "                current_path = os.path.join(path, name)\n",
    "                if isinstance(content, dict):\n",
    "                    process_structure(content, current_path)\n",
    "                else:\n",
    "                    raw_data[current_path] = content\n",
    "    \n",
    "        process_structure(structure)\n",
    "        print(\"Raw markdown extraction complete.\")\n",
    "        return raw_data\n",
    "\n",
    "############################################\n",
    "###---------GITHUB EXTRACTOR END---------###\n",
    "############################################\n",
    "\n",
    "############################################\n",
    "###-------------PARSER START-------------###\n",
    "############################################\n",
    "\n",
    "import re\n",
    "from typing import Tuple, Dict\n",
    "from markdown_it import MarkdownIt\n",
    "\n",
    "def parse_md_for_grammar_correction(md_content: str) -> Tuple[Dict[str, str], str]:\n",
    "    \"\"\"\n",
    "    Parses Markdown content by replacing non-prose elements with placeholders.\n",
    "    This version uses a simplified, robust method for HTML blocks while keeping\n",
    "    all other original logic intact, including newline placeholders.\n",
    "\n",
    "    Args:\n",
    "        md_content (str): Raw markdown content to process.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[str, str], str]: \n",
    "            - A dictionary mapping placeholder keys to original markdown blocks.\n",
    "            - The transformed markdown with placeholders in place of structure.\n",
    "    \"\"\"\n",
    "\n",
    "    md = MarkdownIt()\n",
    "        \n",
    "    placeholder_map = {}\n",
    "    counter = 0\n",
    "\n",
    "    def get_next_placeholder(value: str, prefix=\"PH\") -> str:\n",
    "        \"\"\"\n",
    "        Generates a unique placeholder token for the given value and stores the mapping.\n",
    "    \n",
    "        If the input contains any previously assigned placeholders, they are unwrapped and reassigned\n",
    "        as a new single placeholder. This helps deduplicate and simplify complex inline structures.\n",
    "    \n",
    "        Args:\n",
    "            value (str): The text content to be replaced by a placeholder.\n",
    "            prefix (str): The placeholder prefix (e.g., \"PH\", \"BULLET\").\n",
    "    \n",
    "        Returns:\n",
    "            str: The generated placeholder token (e.g., \"<<PH3>>\" or \"[[BULLET2]]\").\n",
    "        \"\"\"\n",
    "        nonlocal counter\n",
    "        value = re.sub(\n",
    "            r'<<(PH|BULLET|SEP)\\d*>>|\\[\\[BULLET\\d+\\]\\]',\n",
    "            lambda m: placeholder_map.get(m.group(0).strip('<>[]'), m.group(0)),\n",
    "            value\n",
    "        )\n",
    "        counter += 1\n",
    "        key = f\"{prefix}{counter}\"\n",
    "        placeholder_map[key] = value\n",
    "        \n",
    "        if prefix == \"BULLET\":\n",
    "            return f\"[[{key}]]\"\n",
    "        else:\n",
    "            return f\"<<{key}>>\"\n",
    "    \n",
    "    def protect_tables(content: str) -> str:\n",
    "        \"\"\"\n",
    "        Detects Markdown tables and replaces them with placeholders.\n",
    "\n",
    "        Args:\n",
    "            content (str): Input markdown content.\n",
    "\n",
    "        Returns:\n",
    "            str: Markdown with tables replaced by placeholders.\n",
    "        \"\"\"\n",
    "        lines = content.splitlines()\n",
    "        protected_lines = []\n",
    "        in_table = False\n",
    "        table_buffer = []\n",
    "        \n",
    "        for line in lines:\n",
    "            if '|' in line and line.strip().startswith('|') and line.strip().endswith('|'):\n",
    "                if not in_table:\n",
    "                    in_table = True\n",
    "                    table_buffer = [line]\n",
    "                else:\n",
    "                    table_buffer.append(line)\n",
    "            elif in_table and re.match(r'^\\s*\\|[\\s\\-\\|:]+\\|\\s*$', line):\n",
    "                table_buffer.append(line)\n",
    "            elif in_table:\n",
    "                if len(table_buffer) >= 2:\n",
    "                    table_content = '\\n'.join(table_buffer)\n",
    "                    placeholder = get_next_placeholder(table_content)\n",
    "                    protected_lines.append(placeholder)\n",
    "                else:\n",
    "                    protected_lines.extend(table_buffer)\n",
    "                \n",
    "                table_buffer = []\n",
    "                in_table = False\n",
    "                protected_lines.append(line)\n",
    "            else:\n",
    "                protected_lines.append(line)\n",
    "\n",
    "        # Handle final table\n",
    "        if in_table and len(table_buffer) >= 2:\n",
    "            table_content = '\\n'.join(table_buffer)\n",
    "            placeholder = get_next_placeholder(table_content)\n",
    "            protected_lines.append(placeholder)\n",
    "        elif in_table:\n",
    "            protected_lines.extend(table_buffer)\n",
    "        \n",
    "        return '\\n'.join(protected_lines)\n",
    "\n",
    "    def is_low_prose_line(line: str, threshold: float = 0.1) -> bool:\n",
    "        \"\"\"\n",
    "        Determines if a line contains mostly structure and little natural language prose.\n",
    "\n",
    "        Args:\n",
    "            line (str): A single markdown line.\n",
    "            threshold (float): Minimum proportion of prose content.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if it's mostly non-prose.\n",
    "        \"\"\"\n",
    "        # Treat empty lines as low prose\n",
    "        if not line.strip():\n",
    "            return True \n",
    "\n",
    "        # Create a \"clean\" version by removing all known non-prose elements\n",
    "        no_placeholders = re.sub(r'<<(PH|BULLET|SEP)\\d*>>|\\[\\[BULLET\\d+\\]\\]', '', line)\n",
    "        \n",
    "        # Remove common markdown characters\n",
    "        no_markdown = re.sub(r'[*_`[\\]()#|-]', '', no_placeholders)\n",
    "        \n",
    "        # What's left is considered \"prose\"\n",
    "        prose_content = no_markdown.strip()\n",
    "        \n",
    "        # If the ratio of prose to the total line length is below the threshold, protect it\n",
    "        if len(line) > 0 and (len(prose_content) / len(line)) < threshold:\n",
    "            return True\n",
    "            \n",
    "        return False\n",
    "    \n",
    "    md_content = protect_tables(md_content)\n",
    "    \n",
    "    tokens = md.parse(md_content)\n",
    "    lines = md_content.splitlines()\n",
    "    block_replacements = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        token = tokens[i]\n",
    "\n",
    "        # First, check for the special case of an HTML block followed by a heading.\n",
    "        if (token.type == \"html_block\" \n",
    "            and i + 3 < len(tokens) \n",
    "            and tokens[i + 1].type == \"heading_open\"):\n",
    "            \n",
    "            html_start, _ = token.map\n",
    "            _, heading_end = tokens[i + 1].map \n",
    "\n",
    "            raw_block = '\\n'.join(lines[html_start:heading_end])\n",
    "            placeholder = get_next_placeholder(raw_block)\n",
    "            block_replacements.append((html_start, heading_end, placeholder))\n",
    "            \n",
    "            i += 4 \n",
    "            continue\n",
    "\n",
    "        elif token.type == \"fence\" or token.type == \"html_block\":\n",
    "            start, end = token.map\n",
    "            raw_block = '\\n'.join(lines[start:end])\n",
    "            placeholder = get_next_placeholder(raw_block)\n",
    "            block_replacements.append((start, end, placeholder))\n",
    "            i += 1\n",
    "            continue\n",
    "            \n",
    "        elif token.type == \"heading_open\":\n",
    "            level = int(token.tag[1])\n",
    "            inline_token = tokens[i + 1]\n",
    "            header_text = inline_token.content.strip()\n",
    "            placeholder = get_next_placeholder(header_text)\n",
    "            start, end = token.map\n",
    "            markdown_prefix = \"#\" * level\n",
    "            block_replacements.append((start, end, f\"{markdown_prefix} {placeholder}\"))\n",
    "            i += 3\n",
    "            continue\n",
    "\n",
    "        elif token.type == \"blockquote_open\":\n",
    "            start = token.map[0] if token.map else i\n",
    "            j = i + 1\n",
    "            blockquote_content = []\n",
    "            while j < len(tokens) and tokens[j].type != \"blockquote_close\":\n",
    "                if tokens[j].type == \"paragraph_open\" and j + 1 < len(tokens) and tokens[j+1].type == \"inline\":\n",
    "                    blockquote_content.append(tokens[j+1].content)\n",
    "                j += 1\n",
    "            \n",
    "            if j < len(tokens):\n",
    "                end = tokens[j].map[1] if tokens[j].map else start + 1\n",
    "                if blockquote_content:\n",
    "                    text_content = \" \".join(blockquote_content)\n",
    "                    placeholder = get_next_placeholder(text_content)\n",
    "                    block_replacements.append((start, end, f\"> {placeholder}\"))\n",
    "                i = j + 1\n",
    "            else:\n",
    "                i += 1\n",
    "            continue\n",
    "            \n",
    "        i += 1\n",
    "\n",
    "    for start, end, replacement in sorted(block_replacements, reverse=True):\n",
    "        lines[start:end] = [replacement]\n",
    "\n",
    "    # Process inline links, code, and URLs\n",
    "    def replace_md_links(match):\n",
    "        \"\"\"\n",
    "        Replaces standard Markdown links with placeholders for the URL target.\n",
    "        \"\"\"\n",
    "        text, url = match.group(1), match.group(2)\n",
    "        if re.match(r'^<<PH\\d+>>$', url):\n",
    "            return match.group(0)\n",
    "        return f\"[{text}]({get_next_placeholder(url)})\"\n",
    "\n",
    "    def replace_internal_links(match):\n",
    "        \"\"\"\n",
    "        Replaces internal anchor links with placeholders for the anchor target.\n",
    "        \"\"\"\n",
    "        text, anchor = match.group(1), f\"#{match.group(2)}\"\n",
    "        if re.match(r'^<<PH\\d+>>$', anchor):\n",
    "            return match.group(0)\n",
    "        return f\"[{text}]({get_next_placeholder(anchor)})\"\n",
    "\n",
    "    processed_lines = []\n",
    "    for line in lines:\n",
    "        line = re.sub(r'`([^`]+)`', lambda m: f\"`{get_next_placeholder(m.group(1))}`\", line)\n",
    "        line = re.sub(r'https?://[^\\s)\\]}]+', lambda m: get_next_placeholder(m.group(0)), line)\n",
    "        line = re.sub(r'\\[([^\\]]+)]\\(([^)]+)\\)', replace_md_links, line)\n",
    "        line = re.sub(r'\\[([^\\]]+)]\\(#([^)]+)\\)', replace_internal_links, line)\n",
    "        processed_lines.append(line)\n",
    "\n",
    "    def is_title_line(content: str) -> bool:\n",
    "        \"\"\"Heuristic check to determine if a line is a title-style phrase.\"\"\"\n",
    "        clean = re.sub(r'<<[^>]+>>', '', content)\n",
    "        clean = re.sub(r'[*_`[\\]\\(\\)]', '', clean).strip()\n",
    "        words = re.findall(r\"[A-Za-z]+(?:-[A-Za-z]+)*\", clean)\n",
    "        if len(words) < 2: return False\n",
    "        alpha = re.sub(r'[^A-Za-z]', '', clean)\n",
    "        if alpha.isupper(): return True\n",
    "        upper_words = [w for w in words if w[0].isupper()]\n",
    "        if len(upper_words) / len(words) >= 0.75: return True\n",
    "        return False\n",
    "\n",
    "    bullet_placeholder_lines = []\n",
    "    for line in processed_lines:\n",
    "        if is_low_prose_line(line):\n",
    "            placeholder = get_next_placeholder(line)\n",
    "            bullet_placeholder_lines.append(placeholder)\n",
    "            continue\n",
    "\n",
    "        m = re.match(r'^(\\s*)([-*+]|\\d+\\.)\\s+(.*)$', line)\n",
    "        if m:\n",
    "            indent, bullet, content = m.groups()\n",
    "            if is_title_line(content):\n",
    "                ph = get_next_placeholder(content)\n",
    "                bullet_placeholder_lines.append(f\"{indent}{bullet} {ph}\")\n",
    "            else:\n",
    "                bph = get_next_placeholder(bullet, prefix=\"BULLET\")\n",
    "                bullet_placeholder_lines.append(f\"{indent}{bph} {content}\")\n",
    "        else:\n",
    "            bullet_placeholder_lines.append(line)\n",
    "\n",
    "    # Handle newline preservation\n",
    "    raw_processed = \"\\n\".join(bullet_placeholder_lines)\n",
    "    raw_processed = re.sub(r'^\\s*---\\s*$', lambda m: get_next_placeholder(m.group(0)), raw_processed, flags=re.MULTILINE)\n",
    "\n",
    "    final_lines = []\n",
    "    for line in raw_processed.splitlines(keepends=True):\n",
    "        if line.endswith('\\n'):\n",
    "            content = line.rstrip('\\n')\n",
    "            trailing_newline = True\n",
    "        else:\n",
    "            content = line\n",
    "            trailing_newline = False\n",
    "        \n",
    "        newline_placeholder = get_next_placeholder(\"\\n\", prefix=\"PH\")\n",
    "        final_lines.append(content + (newline_placeholder if trailing_newline else ''))\n",
    "\n",
    "    processed_content = ''.join(final_lines)\n",
    "\n",
    "    # Merge back-to-back placeholders\n",
    "    merged_placeholder_map = {}\n",
    "    pattern = re.compile(r'(?:<<PH\\d+>>){2,}')\n",
    "    \n",
    "    while True:\n",
    "        match = pattern.search(processed_content)\n",
    "        if not match: break\n",
    "    \n",
    "        ph_sequence = re.findall(r'<<PH\\d+>>', match.group(0))\n",
    "        keys = [ph.strip('<>') for ph in ph_sequence]\n",
    "        merged_value = ''.join(placeholder_map.get(k, '') for k in keys)\n",
    "    \n",
    "        counter += 1\n",
    "        new_key = f\"PH{counter}\"\n",
    "        new_ph = f\"<<{new_key}>>\"\n",
    "        placeholder_map[new_key] = merged_value\n",
    "    \n",
    "        processed_content = processed_content[:match.start()] + new_ph + processed_content[match.end():]\n",
    "        merged_placeholder_map[new_key] = keys\n",
    "\n",
    "    # Add <<SEP>> marker between prose and placeholders if needed\n",
    "    processed_content = re.sub(r'(<<PH\\d+>>)(?!<<)(?=\\w)', r'\\1<<SEP>>', processed_content)\n",
    "    placeholder_map[\"SEP\"] = \"\"\n",
    "    \n",
    "    return placeholder_map, processed_content\n",
    "\n",
    "\n",
    "def restore_placeholders(corrected_text: str, placeholder_map: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Replaces placeholders in the corrected markdown content with their original values.\n",
    "\n",
    "    Args:\n",
    "        corrected_text (str): Markdown content containing placeholders.\n",
    "        placeholder_map (Dict[str, str]): Map of placeholders to original content.\n",
    "\n",
    "    Returns:\n",
    "        str: Fully restored markdown with original formatting.\n",
    "    \"\"\"\n",
    "    restored_text = corrected_text\n",
    "\n",
    "    # Replace placeholder tokens with original values\n",
    "    for placeholder, original in sorted(placeholder_map.items(), key=lambda x: -len(x[0])):\n",
    "        if placeholder.startswith(\"BULLET\"):\n",
    "            restored_text = restored_text.replace(f\"[[{placeholder}]]\", original)\n",
    "        else:\n",
    "            restored_text = restored_text.replace(f\"<<{placeholder}>>\", original)\n",
    "\n",
    "    # Remove SEP markers\n",
    "    restored_text = restored_text.replace('<<SEP>>', '')\n",
    "\n",
    "    return restored_text\n",
    "\n",
    "############################################\n",
    "###--------------PARSER END--------------###\n",
    "############################################\n",
    "\n",
    "############################################\n",
    "###------------CHUNKER  START------------###\n",
    "############################################\n",
    "\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "def estimate_token_count(text: str) -> int:\n",
    "    '''\n",
    "    Estimate the number of tokens in a given string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "        int: Approximate number of tokens, assuming 4 characters per token\n",
    "    '''\n",
    "    return len(text) // 4 \n",
    "\n",
    "def split_by_top_level_headers(markdown: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split markdown into sections using top-level headers (e.g., #, ##, ..., ######).\n",
    "\n",
    "    Args:\n",
    "        markdown (str): The input markdown text.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of sections split by top-level headers.\n",
    "    \"\"\"\n",
    "    # Find all headers from # to ###### at the beginning of a line\n",
    "    matches = list(re.finditer(r'^\\s*#{1,6}\\s.*', markdown, flags=re.MULTILINE))\n",
    "    if not matches:\n",
    "        # No headers found, return whole content\n",
    "        return [markdown]\n",
    "\n",
    "    sections = []\n",
    "    for i, match in enumerate(matches):\n",
    "        start = match.start()\n",
    "        end = matches[i + 1].start() if i + 1 < len(matches) else len(markdown)\n",
    "        sections.append(markdown[start:end])\n",
    "\n",
    "    return sections\n",
    "\n",
    "def smart_sentence_split(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text at sentence boundaries or placeholder boundaries.\n",
    "\n",
    "    Args: \n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of sentence-level parts, preserving placeholder boundaries.\n",
    "    \"\"\"\n",
    "    # Pattern to capture sentence boundaries and placeholder boundaries\n",
    "    pattern = r'([.!?]\\s+(?=[A-Z]))|(__PLACEHOLDER\\d+__)'\n",
    "    matches = re.split(pattern, text)\n",
    "\n",
    "    # Reconstruct complete sentence or placeholder segments\n",
    "    parts = []\n",
    "    buffer = ''\n",
    "    for chunk in matches:\n",
    "        if chunk is None:\n",
    "            continue\n",
    "        buffer += chunk\n",
    "        if re.match(r'[.!?]\\s+$', chunk) or re.match(r'__PLACEHOLDER\\d+__', chunk.strip()):\n",
    "            parts.append(buffer.strip())\n",
    "            buffer = ''\n",
    "    if buffer.strip():\n",
    "        parts.append(buffer.strip())\n",
    "\n",
    "    return parts\n",
    "\n",
    "def chunk_large_section(section: str, max_tokens: int = 1000) -> List[str]:\n",
    "    \"\"\"\n",
    "    Chunk a section while avoiding breaks mid-sentence or mid-placeholder.\n",
    "\n",
    "    Args:\n",
    "        section (str): A section of text to be chunked.\n",
    "        max_tokens (int): Maximum token count per chunk.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of token-limited chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_token_count = 0\n",
    "\n",
    "    # Split section into sentence-safe pieces\n",
    "    parts = smart_sentence_split(section)\n",
    "\n",
    "    for part in parts:\n",
    "        part = part.strip()\n",
    "        if not part:\n",
    "            continue\n",
    "\n",
    "        part_token_count = estimate_token_count(part)\n",
    "\n",
    "        # Finalize current chunk if adding this part would exceed max tokens\n",
    "        if current_token_count + part_token_count > max_tokens:\n",
    "            if current_chunk:\n",
    "                chunks.append(' '.join(current_chunk).strip())\n",
    "                current_chunk = []\n",
    "                current_token_count = 0\n",
    "\n",
    "        current_chunk.append(part)\n",
    "        current_token_count += part_token_count\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk).strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def chunk_markdown(markdown: str, max_tokens: int = 100) -> List[str]:\n",
    "    '''\n",
    "    Chunk a full markdown document into smaller parts based on headers and token limits.\n",
    "\n",
    "    Args:\n",
    "        markdown (str): The complete markdown content.\n",
    "        max_tokens (int): Maximum allowed tokens per chunk.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of markdown chunks that are token-limited and structured.\n",
    "    '''\n",
    "    # Split by top level headers\n",
    "    sections = split_by_top_level_headers(markdown)\n",
    "    final_chunks = []\n",
    "\n",
    "    for section in sections:\n",
    "        # If the section is small enough, keep it as is\n",
    "        if estimate_token_count(section) <= max_tokens:\n",
    "            final_chunks.append(section.strip())\n",
    "        else:\n",
    "            # Otherwise, chunk it further based on sentence boundaries\n",
    "            final_chunks.extend(chunk_large_section(section, max_tokens=max_tokens))\n",
    "\n",
    "    def split_long_chunk(chunk: str, max_chars: int = 3500) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split a long chunk into smaller character-limited subchunks, preferring newline or sentence boundaries.\n",
    "\n",
    "        Args:\n",
    "            chunk (str): A markdown chunk that may be too long.\n",
    "            max_chars (int): Maximum number of characters per subchunk.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: Subchunks of the input chunk that fit the character limit.\n",
    "        \"\"\"\n",
    "        if len(chunk) <= max_chars:\n",
    "            return [chunk]\n",
    "\n",
    "        # Try splitting on newlines first\n",
    "        parts = re.split(r'(?<=\\n)', chunk)\n",
    "        subchunks = []\n",
    "        buffer = \"\"\n",
    "\n",
    "        for part in parts:\n",
    "            if len(buffer) + len(part) > max_chars:\n",
    "                if buffer:\n",
    "                    subchunks.append(buffer.strip())\n",
    "                buffer = part\n",
    "            else:\n",
    "                buffer += part\n",
    "\n",
    "        if buffer.strip():\n",
    "            subchunks.append(buffer.strip())\n",
    "\n",
    "        # If subchunks are still too long, split on sentence boundary\n",
    "        final_subchunks = []\n",
    "        for sub in subchunks:\n",
    "            if len(sub) <= max_chars:\n",
    "                final_subchunks.append(sub)\n",
    "            else:\n",
    "                sentences = re.split(r'(?<=[.!?])\\s+', sub)\n",
    "                sentence_buffer = \"\"\n",
    "                for s in sentences:\n",
    "                    if len(sentence_buffer) + len(s) > max_chars:\n",
    "                        final_subchunks.append(sentence_buffer.strip())\n",
    "                        sentence_buffer = s\n",
    "                    else:\n",
    "                        sentence_buffer += (\" \" if sentence_buffer else \"\") + s\n",
    "                if sentence_buffer.strip():\n",
    "                    final_subchunks.append(sentence_buffer.strip())\n",
    "\n",
    "        return final_subchunks\n",
    "\n",
    "    # Apply post-splitting to each chunk to enforce character limits\n",
    "    adjusted_chunks = []\n",
    "    for chunk in final_chunks:\n",
    "        adjusted_chunks.extend(split_long_chunk(chunk))\n",
    "\n",
    "    return adjusted_chunks\n",
    "\n",
    "############################################\n",
    "###-------------CHUNKER  END-------------###\n",
    "############################################\n",
    "\n",
    "############################################\n",
    "###-------------PROMPT START-------------###\n",
    "############################################\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Template for llama3-instruct format\n",
    "MARKDOWN_CORRECTION_TEMPLATE_LLAMA3 = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a markdown grammar correction assistant. Your job is to correct only grammatical errors in the user's Markdown content.\n",
    "\n",
    "Strictly follow these rules:\n",
    "- Do **not** modify any placeholders (e.g., <<PH1>>, <<PH93>>, [[BULLET3]], <<SEP>>). Leave them **exactly as they appear**, including spacing and underscores.\n",
    "- Do **not** remove, reword, rename, reformat, or relocate any placeholder.\n",
    "- Do **not** alter Markdown formatting (e.g., headings, links, lists, or indentation).\n",
    "- Do **not** remove Markdown styling characters (e.g., **, *, _, __, `, [, ]).\n",
    "- Do **not** add or remove extra content from the original text.\n",
    "- Only correct grammar **within natural language sentences**, leaving structure unchanged.\n",
    "- **Always** maintain title case wherever it is is present in the original text. \n",
    "\n",
    "If a sentence spans multiple lines or has placeholders in it, correct the grammar but preserve formatting and placeholders **as-is**.\n",
    "\n",
    "Example:\n",
    "- Original: \"<SEP>We use <<PH4>> to **builds** model **likke** this:<<PH17>><<PH18>>\"\n",
    "- Corrected: \"<SEP>We use <<PH4>> to **build** models **like** this:<<PH17>><<PH18_>>\"\n",
    "\n",
    "Example:\n",
    "- Original: \"[[BULLET1]] **It Will Be More Profitablr<PH12>>**\"\n",
    "- Corrected: \"[[BULLET1]] **It Will Be More Profitable<PH12>>**\"\n",
    "\n",
    "Example:\n",
    "- Original: \"This methd is **not necessary** the way ti build *AI* agents <<PH32>>\"\n",
    "- Corrected: \"This method is **not necessary** the way to build *AI* agents <<PH32>>\"\n",
    "\n",
    "All placeholders are present and stay exactly the same with no additional spaces — only grammar is corrected.\n",
    "\n",
    "Respond only with the corrected Markdown content. Do not explain anything.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "Original markdown:\n",
    "{markdown}\n",
    "\n",
    "Corrected markdown:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_markdown_correction_prompt() -> PromptTemplate:\n",
    "    \"\"\"\n",
    "    Get the markdown correction prompt formatted for LLaMA 3 instruct.\n",
    "\n",
    "    Returns:\n",
    "        PromptTemplate: Ready to use in LangChain with LLaMA 3 format.\n",
    "    \"\"\"\n",
    "    return PromptTemplate.from_template(MARKDOWN_CORRECTION_TEMPLATE_LLAMA3)\n",
    "\n",
    "############################################\n",
    "###--------------PROMPT END--------------###\n",
    "############################################\n",
    "\n",
    "############################################\n",
    "###-------------UTILS  START-------------###\n",
    "############################################\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import importlib.util\n",
    "import multiprocessing\n",
    "from typing import Dict, Any, Optional, Union, List, Tuple\n",
    "\n",
    "#Default models to be loaded in our examples:\n",
    "DEFAULT_MODELS = {\n",
    "    \"local\": \"/home/jovyan/datafabric/llama2-7b/ggml-model-f16-Q5_K_M.gguf\",\n",
    "    \"tensorrt\": \"\",\n",
    "    \"hugging-face-local\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    \"hugging-face-cloud\": \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "}\n",
    "\n",
    "# Context window sizes for various models\n",
    "MODEL_CONTEXT_WINDOWS = {\n",
    "    # LlamaCpp models\n",
    "    'ggml-model-f16-Q5_K_M.gguf': 4096,\n",
    "    'ggml-model-7b-q4_0.bin': 4096,\n",
    "    'gguf-model-7b-4bit.bin': 4096,\n",
    "\n",
    "    # HuggingFace models\n",
    "    'mistralai/Mistral-7B-Instruct-v0.3': 8192,\n",
    "    'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B': 4096,\n",
    "    'meta-llama/Llama-2-7b-chat-hf': 4096,\n",
    "    'meta-llama/Llama-3-8b-chat-hf': 8192,\n",
    "    'google/flan-t5-base': 512,\n",
    "    'google/flan-t5-large': 512,\n",
    "    'TheBloke/WizardCoder-Python-7B-V1.0-GGUF': 4096,\n",
    "\n",
    "    # OpenAI models\n",
    "    'gpt-3.5-turbo': 16385,\n",
    "    'gpt-4': 8192,\n",
    "    'gpt-4-32k': 32768,\n",
    "    'gpt-4-turbo': 128000,\n",
    "    'gpt-4o': 128000,\n",
    "\n",
    "    # Anthropic models\n",
    "    'claude-3-opus-20240229': 200000,\n",
    "    'claude-3-sonnet-20240229': 180000,\n",
    "    'claude-3-haiku-20240307': 48000,\n",
    "\n",
    "    # Other models\n",
    "    'qwen/Qwen-7B': 8192,\n",
    "    'microsoft/phi-2': 2048,\n",
    "    'tiiuae/falcon-7b': 4096,\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\": 128000,\n",
    "}\n",
    "\n",
    "def load_config_and_secrets(\n",
    "    config_path: str = \"../../configs/config.yaml\",\n",
    "    secrets_path: str = \"../../configs/secrets.yaml\"\n",
    ") -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load configuration and secrets from YAML files.\n",
    "\n",
    "    Args:\n",
    "        config_path: Path to the configuration YAML file.\n",
    "        secrets_path: Path to the secrets YAML file.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing (config, secrets) as dictionaries.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If either the config or secrets file is not found.\n",
    "    \"\"\"\n",
    "    # Convert to absolute paths if needed\n",
    "    config_path = os.path.abspath(config_path)\n",
    "    secrets_path = os.path.abspath(secrets_path)\n",
    "\n",
    "    if not os.path.exists(secrets_path):\n",
    "        raise FileNotFoundError(f\"secrets.yaml file not found in path: {secrets_path}\")\n",
    "\n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f\"config.yaml file not found in path: {config_path}\")\n",
    "\n",
    "    with open(config_path) as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "    with open(secrets_path) as file:\n",
    "        secrets = yaml.safe_load(file)\n",
    "\n",
    "    return config, secrets\n",
    "\n",
    "def initialize_llm(\n",
    "    model_source: str = \"local\",\n",
    "    secrets: Optional[Dict[str, Any]] = None,\n",
    "    local_model_path: str = DEFAULT_MODELS[\"local\"],\n",
    "    hf_repo_id: str = \"\"\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Initialize a language model based on specified source.\n",
    "\n",
    "    Args:\n",
    "        model_source: Source of the model. Options are \"local\", \"hugging-face-local\", or \"hugging-face-cloud\".\n",
    "        secrets: Dictionary containing API keys for cloud services.\n",
    "        local_model_path: Path to local model file.\n",
    "\n",
    "    Returns:\n",
    "        Initialized language model object.\n",
    "\n",
    "    Raises:\n",
    "        ImportError: If required libraries are not installed.\n",
    "        ValueError: If an unsupported model_source is provided.\n",
    "    \"\"\"\n",
    "    # Check dependencies\n",
    "    missing_deps = []\n",
    "    for module in [\"langchain_huggingface\", \"langchain_core.callbacks\", \"langchain_community.llms\"]:\n",
    "        if not importlib.util.find_spec(module):\n",
    "            missing_deps.append(module)\n",
    "    \n",
    "    if missing_deps:\n",
    "        raise ImportError(f\"Missing required dependencies: {', '.join(missing_deps)}\")\n",
    "    \n",
    "    # Import required libraries\n",
    "    from langchain_huggingface import HuggingFacePipeline, HuggingFaceEndpoint\n",
    "    from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "    from langchain_community.llms import LlamaCpp\n",
    "\n",
    "    model = None\n",
    "    context_window = None\n",
    "    \n",
    "    # Initialize based on model source\n",
    "    if model_source == \"hugging-face-cloud\":\n",
    "        if hf_repo_id == \"\":\n",
    "            repo_id = DEFAULT_MODELS[\"hugging-face-cloud\"]\n",
    "        else:\n",
    "            repo_id = hf_repo_id  \n",
    "        if not secrets or \"HUGGINGFACE_API_KEY\" not in secrets:\n",
    "            raise ValueError(\"HuggingFace API key is required for cloud model access\")\n",
    "            \n",
    "        huggingfacehub_api_token = secrets[\"HUGGINGFACE_API_KEY\"]\n",
    "        # Get context window from our lookup table\n",
    "        if repo_id in MODEL_CONTEXT_WINDOWS:\n",
    "            context_window = MODEL_CONTEXT_WINDOWS[repo_id]\n",
    "\n",
    "        model = HuggingFaceEndpoint(\n",
    "            huggingfacehub_api_token=huggingfacehub_api_token,\n",
    "            repo_id=repo_id,\n",
    "        )\n",
    "\n",
    "    elif model_source == \"hugging-face-local\":\n",
    "        from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "        if \"HUGGINGFACE_API_KEY\" in secrets:\n",
    "            os.environ[\"HF_TOKEN\"] = secrets[\"HUGGINGFACE_API_KEY\"]\n",
    "        if hf_repo_id == \"\":\n",
    "            model_id = DEFAULT_MODELS[\"hugging-face-local\"]\n",
    "        else:\n",
    "            model_id = hf_repo_id        \n",
    "        # Get context window from our lookup table\n",
    "        if model_id in MODEL_CONTEXT_WINDOWS:\n",
    "            context_window = MODEL_CONTEXT_WINDOWS[model_id]\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        hf_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "        # If tokenizer has model_max_length, that's our context window\n",
    "        if hasattr(tokenizer, 'model_max_length') and tokenizer.model_max_length not in (None, -1):\n",
    "            context_window = tokenizer.model_max_length\n",
    "\n",
    "        pipe = pipeline(\"text-generation\", model=hf_model, tokenizer=tokenizer, max_new_tokens=100, device=0)\n",
    "        model = HuggingFacePipeline(pipeline=pipe)\n",
    "        \n",
    "    elif model_source == \"tensorrt\":\n",
    "        #If a Hugging Face model is specified, it will be used - otherwise, it will try loading the model from local_path\n",
    "        try:\n",
    "            import tensorrt_llm\n",
    "            sampling_params = tensorrt_llm.SamplingParams(temperature=0.1, top_p=0.95, max_tokens=512) \n",
    "            if hf_repo_id != \"\":\n",
    "                return TensorRTLangchain(model_path = hf_repo_id, sampling_params = sampling_params)\n",
    "            else:\n",
    "                model_config = os.path.join(local_model_path, config.json)\n",
    "                if os.path.isdir(local_model_path) and os.path.isfile(model_config):\n",
    "                    return TensorRTLangchain(model_path = local_model_path, sampling_params = sampling_params)\n",
    "                else:\n",
    "                    raise Exception(\"Model format incompatible with TensorRT LLM\")\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"Could not import tensorrt-llm library. \"\n",
    "                \"Please make sure tensorrt-llm is installed properly, or \"\n",
    "                \"consider using workspaces based on the NeMo Framework\"\n",
    "            )\n",
    "    elif model_source == \"local\":\n",
    "        callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "        # For LlamaCpp, get the context window from the filename\n",
    "        model_filename = os.path.basename(local_model_path)\n",
    "        if model_filename in MODEL_CONTEXT_WINDOWS:\n",
    "            context_window = MODEL_CONTEXT_WINDOWS[model_filename]\n",
    "        else:  \n",
    "            # Default context window for LlamaCpp models (explicitly set)\n",
    "            context_window = 4096\n",
    "\n",
    "        model = LlamaCpp(\n",
    "            model_path=local_model_path,\n",
    "            n_gpu_layers=-1,                             \n",
    "            n_batch=512,                                 \n",
    "            n_ctx=32000,\n",
    "            max_tokens=1024,\n",
    "            f16_kv=True,\n",
    "            use_mmap=False,                             \n",
    "            low_vram=False,                            \n",
    "            rope_scaling=None,\n",
    "            temperature=0.0,\n",
    "            repeat_penalty=1.0,\n",
    "            streaming=False,\n",
    "            stop=None,\n",
    "            seed=42,\n",
    "            num_threads=multiprocessing.cpu_count(),\n",
    "            verbose=False #\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model source: {model_source}\")\n",
    "\n",
    "    # Store context window as model attribute for easy access\n",
    "    if model and hasattr(model, '__dict__'):\n",
    "        model.__dict__['_context_window'] = context_window\n",
    "\n",
    "    return model\n",
    "\n",
    "############################################\n",
    "###--------------UTILS  END--------------###\n",
    "############################################\n",
    "\n",
    "############################################\n",
    "###-----------LLM METRIC START-----------###\n",
    "############################################\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from typing import List, Optional\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from mlflow.metrics import make_metric\n",
    "from llama_cpp import Llama\n",
    "import multiprocessing\n",
    "\n",
    "# Initialize TF-IDF vertorizer for semantic similarity\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "\n",
    "# Path to local judge model\n",
    "LOCAL_LLAMA_JUDGE_PATH = \"/home/jovyan/datafabric/llama2-7b/ggml-model-f16-Q5_K_M.gguf\" \n",
    "\n",
    "class LocalJudgeLlamaClient:\n",
    "    \"\"\"Singleton wrapper for local judge-specific LLaMA.\"\"\"\n",
    "    _client = None\n",
    "\n",
    "    @classmethod\n",
    "    def get_client(cls, model_path: Optional[str] = None) -> Llama:\n",
    "        \"\"\"\n",
    "        Get or initialize the singleton LLaMA client.\n",
    "\n",
    "        Args:\n",
    "            model_path (Optional[str]): Path to the local gguf LLaMA model.\n",
    "\n",
    "        Returns:\n",
    "            Llama: Loaded LLaMA instance.\n",
    "        \"\"\"\n",
    "        if cls._client is None:\n",
    "            if model_path is None:\n",
    "                raise ValueError(\"Must provide model_path to initialize local LLaMA judge.\")\n",
    "\n",
    "            cls._client = Llama(\n",
    "                model_path=model_path,\n",
    "                n_gpu_layers=-1,                             \n",
    "                n_batch=512,                                 \n",
    "                n_ctx=32000,\n",
    "                max_tokens=1024,\n",
    "                f16_kv=True,\n",
    "                use_mmap=False,                             \n",
    "                low_vram=False,                            \n",
    "                rope_scaling=None,\n",
    "                temperature=0.0,\n",
    "                repeat_penalty=1.0,\n",
    "                streaming=False,\n",
    "                stop=None,\n",
    "                seed=42,\n",
    "                num_threads=multiprocessing.cpu_count(),\n",
    "                verbose=False\n",
    "            )\n",
    "        return cls._client\n",
    "\n",
    "# Preload model at module load\n",
    "LocalJudgeLlamaClient.get_client(model_path=LOCAL_LLAMA_JUDGE_PATH)\n",
    "\n",
    "def simple_grammar_check(text: str) -> int:\n",
    "    \"\"\"\n",
    "    Basic grammar checking without external libraries.\n",
    "    Detects repeated words, double spaces, and capitalization issues.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input sentence to analyze.\n",
    "\n",
    "    Returns:\n",
    "        int: Count of potential grammar issues.\n",
    "    \"\"\"\n",
    "    issues = 0\n",
    "    text = str(text).strip()\n",
    "    \n",
    "    # Check for basic issues\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "            \n",
    "        # Check for sentences not starting with capital letter\n",
    "        if sentence and not sentence[0].isupper():\n",
    "            issues += 1\n",
    "            \n",
    "        # Check for double spaces\n",
    "        if '  ' in sentence:\n",
    "            issues += 1\n",
    "            \n",
    "        # Check for common grammar patterns\n",
    "        words = sentence.lower().split()\n",
    "        for i, word in enumerate(words):\n",
    "            # Basic subject-verb agreement checks\n",
    "            if word == 'i' and i < len(words) - 1:\n",
    "                if words[i + 1] in ['are', 'were']:\n",
    "                    issues += 1  \n",
    "                    \n",
    "            # Check for repeated words\n",
    "            if i > 0 and word == words[i-1]:\n",
    "                issues += 1\n",
    "                \n",
    "    return issues\n",
    "\n",
    "def semantic_similarity_eval_fn(predictions: List[str], targets: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Compute semantic similarity between predictions and targets using TF-IDF and cosine similarity.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): Model-generated texts.\n",
    "        targets (List[str]): Ground truth references.\n",
    "\n",
    "    Returns:\n",
    "        float: Mean cosine similarity between matched pairs.\n",
    "    \"\"\"\n",
    "    # Combine all texts to fit the vectorizer\n",
    "    all_texts = list(targets) + list(predictions)\n",
    "    \n",
    "    # Handle empty texts\n",
    "    all_texts = [str(text) if text else \"\" for text in all_texts]\n",
    "    \n",
    "    if len(set(all_texts)) < 2:  # All texts are identical or empty\n",
    "        return 1.0\n",
    "    \n",
    "    # Fit and transform\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(all_texts)\n",
    "    \n",
    "    # Split back into targets and predictions\n",
    "    n_targets = len(targets)\n",
    "    target_vectors = tfidf_matrix[:n_targets]\n",
    "    pred_vectors = tfidf_matrix[n_targets:]\n",
    "    \n",
    "    # Calculate cosine similarity for each pair\n",
    "    similarities = []\n",
    "    for i in range(len(targets)):\n",
    "        similarity = cosine_similarity(target_vectors[i:i+1], pred_vectors[i:i+1])[0][0]\n",
    "        similarities.append(similarity)\n",
    "    \n",
    "    return np.mean(similarities)\n",
    "\n",
    "def grammar_error_count_eval_fn(predictions, targets):\n",
    "    \"\"\"\n",
    "    Count grammar issues in the predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): Model outputs.\n",
    "        targets (List[str]): Reference texts (unused here).\n",
    "\n",
    "    Returns:\n",
    "        float: Average number of issues per prediction.\n",
    "    \"\"\"\n",
    "    error_counts = []\n",
    "    for pred in predictions:\n",
    "        error_count = simple_grammar_check(str(pred))\n",
    "        error_counts.append(error_count)\n",
    "    \n",
    "    return np.mean(error_counts)\n",
    "\n",
    "def grammar_error_rate_eval_fn(predictions: List[str], targets: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate grammar error rate (issues per word) for predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): Model outputs.\n",
    "        targets (List[str]): Reference texts (unused here).\n",
    "\n",
    "    Returns:\n",
    "        float: Mean error rate.\n",
    "    \"\"\"\n",
    "    error_rates = []\n",
    "    for pred in predictions:\n",
    "        error_count = simple_grammar_check(str(pred))\n",
    "        word_count = len(str(pred).split())\n",
    "        error_rate = error_count / max(word_count, 1)  # Avoid division by zero\n",
    "        error_rates.append(error_rate)\n",
    "    \n",
    "    return np.mean(error_rates)\n",
    "\n",
    "def grammar_improvement_eval_fn(predictions: List[str], targets: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Measure improvement in grammar (fewer errors) from targets to predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): Corrected text.\n",
    "        targets (List[str]): Original input text.\n",
    "\n",
    "    Returns:\n",
    "        float: Mean improvement (positive = fewer errors in prediction).\n",
    "    \"\"\"\n",
    "    improvements = []\n",
    "    for pred, target in zip(predictions, targets):\n",
    "        input_errors = simple_grammar_check(str(target))\n",
    "        output_errors = simple_grammar_check(str(pred))\n",
    "        improvement = input_errors - output_errors  \n",
    "        improvements.append(improvement)\n",
    "    \n",
    "    return np.mean(improvements)\n",
    "\n",
    "def grammar_score_eval_fn(predictions: List[str], targets: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Assign grammar score from 0–100 based on number of issues.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): Model outputs.\n",
    "        targets (List[str]): Reference texts (unused here).\n",
    "\n",
    "    Returns:\n",
    "        float: Mean score where 100 = perfect grammar.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for pred in predictions:\n",
    "        error_count = simple_grammar_check(str(pred))\n",
    "        word_count = len(str(pred).split())\n",
    "        if word_count == 0:\n",
    "            scores.append(0)\n",
    "        else:\n",
    "            # Simple scoring: start at 100, subtract points for errors\n",
    "            error_penalty = min(error_count * 10, 100)  \n",
    "            score = max(100 - error_penalty, 0)\n",
    "            scores.append(score)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "def readability_improvement_eval_fn(predictions: List[str], targets: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Estimate improvement in readability using sentence length as proxy.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): Corrected output.\n",
    "        targets (List[str]): Original input.\n",
    "\n",
    "    Returns:\n",
    "        float: Average improvement in readability score.\n",
    "    \"\"\"\n",
    "    def calculate_readability_score(text: str) -> float:\n",
    "        \"\"\"\n",
    "        Estimate a basic readability score for a given text.\n",
    "    \n",
    "        Uses average sentence length as a simple heuristic. Shorter sentences are considered more readable.\n",
    "    \n",
    "        Args:\n",
    "            text (str): The input text to evaluate.\n",
    "    \n",
    "        Returns:\n",
    "            float: Readability score where higher is better.\n",
    "        \"\"\"\n",
    "        sentences = text.split('.')\n",
    "        words = text.split()\n",
    "        if len(sentences) == 0 or len(words) == 0:\n",
    "            return 0\n",
    "        \n",
    "        avg_sentence_length = len(words) / len(sentences)\n",
    "        # Simple readability: prefer shorter sentences and common words\n",
    "        readability = max(20 - avg_sentence_length, 0)  \n",
    "        return readability\n",
    "    \n",
    "    improvements = []\n",
    "    for pred, target in zip(predictions, targets):\n",
    "        input_readability = calculate_readability_score(str(target))\n",
    "        output_readability = calculate_readability_score(str(pred))\n",
    "        improvement = output_readability - input_readability\n",
    "        improvements.append(improvement)\n",
    "    \n",
    "    return np.mean(improvements)\n",
    "\n",
    "def llm_judge_eval_fn_local(predictions: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Use a local LLaMA model to rate grammar of predictions from 1 to 10.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): Model outputs.\n",
    "\n",
    "    Returns:\n",
    "        float: Average LLaMA-generated grammar rating.\n",
    "    \"\"\"\n",
    "    llama = LocalJudgeLlamaClient.get_client()\n",
    "    scores = []\n",
    "\n",
    "    for pred in predictions:\n",
    "        prompt = f\"\"\"Rate the following text solely on grammar. Respond with a single digit from 1 to 10. DO NOT include any explanation, label, or punctuation. Reply with just the number.\n",
    "\n",
    "Text: I has a apple.\n",
    "Answer: 3\n",
    "\n",
    "Text: The dog chased the ball across the yard.\n",
    "Answer: 9\n",
    "\n",
    "Text: Him don't know where she is.\n",
    "Answer: 2\n",
    "\n",
    "Text: {pred}\n",
    "Answer:\"\"\"\n",
    "\n",
    "        try:\n",
    "            result = llama(prompt, stop=[\"\\n\"])\n",
    "            text = result[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "            try:\n",
    "                import os\n",
    "                os.makedirs(\"llm_eval_logs\", exist_ok=True)\n",
    "                with open(\"llm_eval_logs/local_llama_responses.txt\", 'a', encoding='utf-8') as f:\n",
    "                    f.write(f\"Response: {text}\\n\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            score = float(re.findall(r\"\\d+\", text)[0])\n",
    "            scores.append(score)\n",
    "        except Exception as e:\n",
    "            print(f\"[LLaMA judge error]: {e}\")\n",
    "            scores.append(5.0)\n",
    "\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "# ---- Create all the metric wrappers for MLflow ----\n",
    "    \n",
    "semantic_similarity_metric = make_metric(\n",
    "    eval_fn=semantic_similarity_eval_fn,\n",
    "    greater_is_better=True,\n",
    "    name=\"semantic_similarity\"\n",
    ")\n",
    "\n",
    "grammar_error_count_metric = make_metric(\n",
    "    eval_fn=grammar_error_count_eval_fn,\n",
    "    greater_is_better=False,\n",
    "    name=\"grammar_error_count\"\n",
    ")\n",
    "\n",
    "grammar_error_rate_metric = make_metric(\n",
    "    eval_fn=grammar_error_rate_eval_fn,\n",
    "    greater_is_better=False,\n",
    "    name=\"grammar_error_rate\"\n",
    ")\n",
    "\n",
    "grammar_improvement_metric = make_metric(\n",
    "    eval_fn=grammar_improvement_eval_fn,\n",
    "    greater_is_better=True,\n",
    "    name=\"grammar_improvement\"\n",
    ")\n",
    "\n",
    "grammar_score_metric = make_metric(\n",
    "    eval_fn=grammar_score_eval_fn,\n",
    "    greater_is_better=True,\n",
    "    name=\"grammar_score\"\n",
    ")\n",
    "\n",
    "readability_improvement_metric = make_metric(\n",
    "    eval_fn=readability_improvement_eval_fn,\n",
    "    greater_is_better=True,\n",
    "    name=\"readability_improvement\"\n",
    ")\n",
    "\n",
    "llm_judge_metric_local = make_metric(\n",
    "    eval_fn=llm_judge_eval_fn_local,\n",
    "    greater_is_better=True,\n",
    "    name=\"llm_judge_local_score\"\n",
    ")\n",
    "\n",
    "############################################\n",
    "###------------LLM METRIC END------------###\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4287a10d",
   "metadata": {},
   "source": [
    "## Define MLflow Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96297375",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class MarkdownCorrectorModel(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"An MLflow model that encapsulates the entire markdown correction pipeline.\"\"\"\n",
    "\n",
    "    def load_context(self, context: mlflow.pyfunc.PythonModelContext) -> None:\n",
    "        \"\"\"Initializes the model, loading configurations, secrets, and the LLM.\"\"\"\n",
    "        \n",
    "        # Load configurations and secrets from model artifacts\n",
    "        config_path = context.artifacts[\"config\"]\n",
    "        secrets_path = context.artifacts[\"secrets\"]\n",
    "        self.config, self.secrets = load_config_and_secrets(config_path, secrets_path)\n",
    "\n",
    "        # Initialize the LLM\n",
    "        model_source = self.config.get(\"model_source\", \"local\")\n",
    "        local_model_path = context.artifacts.get(\"local_model\")\n",
    "        llm = initialize_llm(model_source, self.secrets, local_model_path)\n",
    "\n",
    "        # Create the processing chain\n",
    "        correction_prompt = get_markdown_correction_prompt()\n",
    "        self.llm_chain = correction_prompt | llm\n",
    "        logger.info(\"✅ Model context loaded and LLM chain initialized.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _safe_join_chunks(chunks: List[str]) -> str:\n",
    "        \"\"\"Rejoins text chunks while preserving formatting.\"\"\"\n",
    "        joined = \"\"\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            if i == 0:\n",
    "                joined += chunk\n",
    "            else:\n",
    "                prev = chunks[i - 1].rstrip()\n",
    "                curr = chunk\n",
    "                if prev.endswith('.') and re.match(r'^[A-Z\\\"]', curr.lstrip()):\n",
    "                    joined += ' ' + curr.lstrip()\n",
    "                else:\n",
    "                    joined += curr\n",
    "        return joined\n",
    "\n",
    "    def predict(self, context: mlflow.pyfunc.PythonModelContext, model_input: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"Executes the full workflow for each repository URL in the input.\"\"\"\n",
    "\n",
    "        results = []\n",
    "        for repo_url in model_input[\"repo_url\"]:\n",
    "            logger.info(f\"Processing repository: {repo_url}\")\n",
    "            access_token = self.secrets.get(\"GITHUB_ACCESS_TOKEN\")\n",
    "            \n",
    "            # 1. Extract Markdown\n",
    "            processor = GitHubMarkdownProcessor(repo_url=repo_url, access_token=access_token)\n",
    "            markdowns = processor.run()\n",
    "\n",
    "            # 2. Parse & Chunk\n",
    "            parsed_markdowns, placeholder_maps, all_chunks = {}, {}, {}\n",
    "            for filename, content in markdowns.items():\n",
    "                placeholder_map, processed_content = parse_md_for_grammar_correction(content)\n",
    "                parsed_markdowns[filename] = processed_content\n",
    "                placeholder_maps[filename] = placeholder_map\n",
    "                all_chunks[filename] = chunk_markdown(processed_content)\n",
    "            \n",
    "            # 3. Invoke Model\n",
    "            corrected_chunks_by_file = defaultdict(list)\n",
    "            for file_name, chunks in all_chunks.items():\n",
    "                for chunk in chunks:\n",
    "                    response = self.llm_chain.invoke({\"markdown\": chunk})\n",
    "                    corrected_chunks_by_file[file_name].append(response)\n",
    "\n",
    "            # 4. Rebuild and Restore\n",
    "            final_corrected_files = {}\n",
    "            for file_name, corrected_chunks in corrected_chunks_by_file.items():\n",
    "                rebuilt_content = self._safe_join_chunks(corrected_chunks)\n",
    "                placeholder_map = placeholder_maps.get(file_name, {})\n",
    "                restored_content = restore_placeholders(rebuilt_content, placeholder_map)\n",
    "                final_corrected_files[file_name] = restored_content\n",
    "            \n",
    "            results.append(json.dumps(final_corrected_files, indent=2))\n",
    "            logger.info(f\"✅ Finished processing for {repo_url}.\")\n",
    "\n",
    "        return pd.Series(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334790c0",
   "metadata": {},
   "source": [
    "## Verify Assets and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b00c95",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def log_asset_status(asset_path: Path, asset_name: str) -> None:\n",
    "    \"\"\"Logs the existence status of a given file or directory.\"\"\"\n",
    "    if asset_path.exists():\n",
    "        logger.info(f\"✅ {asset_name} is properly configured at: {asset_path}\")\n",
    "    else:\n",
    "        logger.warning(f\"⚠️ {asset_name} not found at: {asset_path}.\")\n",
    "\n",
    "log_asset_status(CONFIG_PATH, \"Config file\")\n",
    "log_asset_status(SECRETS_PATH, \"Secrets file\")\n",
    "log_asset_status(LOCAL_MODEL_PATH, \"Local LLaMA model\")\n",
    "log_asset_status(EVAL_DATA_PATH, \"Evaluation data JSON\")\n",
    "log_asset_status(REQUIREMENTS_PATH, \"Requirements file\")\n",
    "\n",
    "with open(EVAL_DATA_PATH, \"r\") as f:\n",
    "    results = json.load(f)\n",
    "original_texts = [item[\"original\"] for item in results]\n",
    "eval_df = pd.DataFrame(original_texts, columns=[\"markdown\"])\n",
    "logger.info(f\"Loaded {len(eval_df)} records for evaluation.\")\n",
    "print(eval_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21960a9",
   "metadata": {},
   "source": [
    "## Log and Register Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c3b585",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "    run_id = run.info.run_id\n",
    "    logger.info(f\"MLflow Run Started. Run ID: {run_id}\")\n",
    "\n",
    "    artifacts = {\n",
    "        \"config\": str(CONFIG_PATH),\n",
    "        \"secrets\": str(SECRETS_PATH),\n",
    "        \"llm\": str(LOCAL_MODEL_PATH),\n",
    "    }\n",
    "    \n",
    "    signature = ModelSignature(\n",
    "        inputs=Schema([ColSpec(\"string\", \"markdown\")]),\n",
    "        outputs=Schema([ColSpec(\"string\", \"corrected\")]),\n",
    "    )\n",
    "    \n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=MODEL_NAME,\n",
    "        python_model=MarkdownCorrectionService(),\n",
    "        artifacts=artifacts,\n",
    "        signature=signature,\n",
    "        registered_model_name=MODEL_NAME,\n",
    "        pip_requirements=str(REQUIREMENTS_PATH),\n",
    "    )\n",
    "    \n",
    "    model_uri = f\"runs:/{run_id}/{MODEL_NAME}\"\n",
    "    logger.info(f\"Model URI: {model_uri}\")\n",
    "\n",
    "# Validate Registered Model\n",
    "client = MlflowClient()\n",
    "try:\n",
    "    latest_version_info = client.get_latest_versions(MODEL_NAME, stages=[\"None\"])[0]\n",
    "    latest_version = latest_version_info.version\n",
    "    logger.info(f\"Successfully registered model '{MODEL_NAME}' version {latest_version}.\")\n",
    "    \n",
    "    model_uri_latest = f\"models:/{MODEL_NAME}/{latest_version}\"\n",
    "    loaded_model = mlflow.pyfunc.load_model(model_uri_latest)\n",
    "    \n",
    "    sample_input = eval_df.head(1)\n",
    "    logger.info(f\"Performing sample prediction on: \\n{sample_input['markdown'].iloc[0][:100]}...\")\n",
    "    \n",
    "    prediction = loaded_model.predict(sample_input)\n",
    "    logger.info(f\"✅ Sample prediction successful. Output:\\n{prediction.iloc[0][:100]}...\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to validate registered model. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b506c45",
   "metadata": {},
   "source": [
    "## Log and Register the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbe3a09",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define artifacts to be packaged with the model\n",
    "artifacts = {\n",
    "    \"config\": str(CONFIG_PATH),\n",
    "    \"secrets\": str(SECRETS_PATH),\n",
    "    \"local_model\": str(LOCAL_MODEL_PATH)\n",
    "}\n",
    "\n",
    "# Define the model's signature\n",
    "input_schema = Schema([ColSpec(\"string\", \"repo_url\")])\n",
    "output_schema = Schema([ColSpec(\"string\")]) # Output is a JSON string\n",
    "signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "\n",
    "# Set up MLflow experiment\n",
    "mlflow.set_tracking_uri('/phoenix/mlflow') # Adjust if your tracking server is elsewhere\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\n",
    "\n",
    "with mlflow.start_run(run_name=\"MarkdownCorrectorRegistry\") as run:\n",
    "    run_id = run.info.run_id\n",
    "    logger.info(f\"Starting MLflow run with ID: {run_id}\")\n",
    "    \n",
    "    # Log the model to MLflow\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=MLFLOW_MODEL_NAME,\n",
    "        python_model=MarkdownCorrectorModel(),\n",
    "        artifacts=artifacts,\n",
    "        pip_requirements=\"../requirements.txt\",\n",
    "        code_paths=[\"../src\"],\n",
    "        signature=signature,\n",
    "        registered_model_name=MLFLOW_MODEL_NAME,\n",
    "        input_example=pd.DataFrame([{\"repo_url\": \"https://github.com/hp-david/test\"}])\n",
    "    )\n",
    "    logger.info(f\"✅ Model '{MLFLOW_MODEL_NAME}' logged and registered successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966b18e7",
   "metadata": {},
   "source": [
    "## Prepare Eval Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0118e140",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "logger.info(\"Preparing data for evaluation...\")\n",
    "\n",
    "# Load the newly registered model\n",
    "model_uri = f\"models:/{MLFLOW_MODEL_NAME}/latest\"\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "# Get original content from the test repo\n",
    "test_repo_url = \"https://github.com/hp-david/test\"\n",
    "config, secrets = load_config_and_secrets(CONFIG_PATH, SECRETS_PATH)\n",
    "processor = GitHubMarkdownProcessor(repo_url=test_repo_url, access_token=secrets.get(\"GITHUB_ACCESS_TOKEN\"))\n",
    "original_markdowns = processor.run()\n",
    "\n",
    "# Get the model's corrected predictions\n",
    "test_input = pd.DataFrame([{\"repo_url\": test_repo_url}])\n",
    "prediction_result = loaded_model.predict(test_input)\n",
    "corrected_markdowns = json.loads(prediction_result[0])\n",
    "\n",
    "# Align originals and predictions into a single DataFrame\n",
    "eval_data = []\n",
    "for filename, original_content in original_markdowns.items():\n",
    "    if filename in corrected_markdowns:\n",
    "        eval_data.append({\n",
    "            \"original\": original_content,\n",
    "            \"corrected\": corrected_markdowns[filename]\n",
    "        })\n",
    "\n",
    "evaluation_df = pd.DataFrame(eval_data)\n",
    "logger.info(f\"✅ Created evaluation DataFrame with {len(evaluation_df)} file(s).\")\n",
    "display(evaluation_df.head())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3e6d9f",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229e8f32",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "with mlflow.start_run(run_id=run_id):\n",
    "    logger.info(f\"Starting evaluation for run ID: {run_id}\")\n",
    "    \n",
    "    results = mlflow.evaluate(\n",
    "        data=evaluation_df,\n",
    "        targets=\"original\",\n",
    "        predictions=\"corrected\",\n",
    "        extra_metrics=[\n",
    "            # Standard metrics, now treated as extra metrics\n",
    "            exact_match(),\n",
    "            rouge1(),\n",
    "            rougeL(),\n",
    "            # Your custom metrics\n",
    "            semantic_similarity_metric,\n",
    "            grammar_improvement_metric,\n",
    "            llm_judge_metric_local,\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    logger.info(\"✅ Evaluation complete.\")\n",
    "    print(json.dumps(results.metrics, indent=2))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624dad49",
   "metadata": {},
   "source": [
    "## Log Execution Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299e0a3d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "end_time: float = time.time()\n",
    "elapsed_time: float = end_time - start_time\n",
    "elapsed_minutes: int = int(elapsed_time // 60)\n",
    "elapsed_seconds: float = elapsed_time % 60\n",
    "\n",
    "logger.info(f\"⏱️ Total execution time: {elapsed_minutes}m {elapsed_seconds:.2f}s\")\n",
    "logger.info(\"✅ Notebook execution completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
