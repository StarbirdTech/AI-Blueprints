{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7c007e5",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-size: 50px;\"> Register Model </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdcb43f",
   "metadata": {},
   "source": [
    "# Notebook Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756db15a",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "- Configure the Environment\n",
    "- Define Constants\n",
    "- Define Module Dependancies\n",
    "- Verify Assets and Load Data\n",
    "- Log and Register Model\n",
    "- Evaluate Model\n",
    "- Log Execution Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5423f7b3",
   "metadata": {},
   "source": [
    "## Configure the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a1d3d1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import yaml\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import importlib.util\n",
    "import multiprocessing\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, List, Dict, Tuple, Optional\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlflow.models import evaluate, ModelSignature\n",
    "from mlflow.types import Schema, ColSpec\n",
    "from mlflow.metrics import ari_grade_level, exact_match, rouge1, rougeL, make_metric\n",
    "from mlflow.tracking import MlflowClient\n",
    "from langchain.prompts import PromptTemplate\n",
    "from llama_cpp import Llama\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Logger Configuration\n",
    "logger: logging.Logger = logging.getLogger(\"register_markdown_model_logger\")\n",
    "if not logger.handlers:\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.propagate = False\n",
    "    formatter = logging.Formatter(fmt=\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "    stream_handler = logging.StreamHandler()\n",
    "    stream_handler.setFormatter(formatter)\n",
    "    logger.addHandler(stream_handler)\n",
    "\n",
    "# Start Execution\n",
    "start_time: float = time.time()\n",
    "logger.info(\"Notebook execution started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964f2b67",
   "metadata": {},
   "source": [
    "## Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49769695",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Path and Model Configuration \n",
    "CONFIG_PATH = Path(\"../configs/configs.yaml\")\n",
    "SECRETS_PATH = Path(\"../configs/secrets.yaml\")\n",
    "REQUIREMENTS_PATH = Path(\"../requirements.txt\")\n",
    "LOCAL_MODEL_PATH = Path(\"/home/jovyan/datafabric/llama3.1-8b-instruct/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\")\n",
    "EVAL_DATA_PATH = Path(\"./results.json\")\n",
    "\n",
    "# MLflow Configuration\n",
    "EXPERIMENT_NAME = \"markdown-correction-experiment\"\n",
    "RUN_NAME = f\"registration-run-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "MODEL_NAME = \"MarkdownCorrector\"\n",
    "\n",
    "# Model and Path \n",
    "DEFAULT_MODELS: Dict[str, str] = {\n",
    "    \"local\": \"/home/jovyan/datafabric/llama2-7b/ggml-model-f16-Q5_K_M.gguf\",\n",
    "}\n",
    "LOCAL_LLAMA_JUDGE_PATH: str = \"/home/jovyan/datafabric/llama2-7b/ggml-model-f16-Q5_K_M.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c842bde",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%pip install -r {REQUIREMENTS_PATH} --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4287a10d",
   "metadata": {},
   "source": [
    "## Define Module Dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96297375",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def get_markdown_correction_prompt() -> PromptTemplate:\n",
    "    \"\"\"Creates and returns the LangChain PromptTemplate for markdown correction.\"\"\"\n",
    "    template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a markdown grammar correction assistant. Your job is to correct only grammatical errors in the user's Markdown content.\n",
    "Strictly follow these rules:\n",
    "- Do **not** modify any placeholders (e.g., <<PH1>>, <<PH93>>, [[BULLET3]], <<SEP>>). Leave them **exactly as they appear**.\n",
    "- Do **not** alter Markdown formatting (e.g., headings, links, lists).\n",
    "- Do **not** add or remove extra content.\n",
    "- Only correct grammar **within natural language sentences**.\n",
    "- **Always** maintain title case where present.\n",
    "Respond only with the corrected Markdown content. Do not explain anything.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "Original markdown:\n",
    "{markdown}\n",
    "Corrected markdown:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "    return PromptTemplate.from_template(template)\n",
    "\n",
    "def load_config_and_secrets(config_path: Path, secrets_path: Path) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
    "    \"\"\"Loads configuration and secrets from specified YAML files.\"\"\"\n",
    "    if not secrets_path.exists(): raise FileNotFoundError(f\"secrets.yaml not found: {secrets_path}\")\n",
    "    if not config_path.exists(): raise FileNotFoundError(f\"config.yaml not found: {config_path}\")\n",
    "    with open(config_path) as file: config = yaml.safe_load(file)\n",
    "    with open(secrets_path) as file: secrets = yaml.safe_load(file)\n",
    "    return config, secrets\n",
    "\n",
    "def initialize_llm(\n",
    "    model_source: str, \n",
    "    secrets: Optional[Dict[str, Any]], \n",
    "    local_model_path: str\n",
    ") -> Any:\n",
    "    \"\"\"Initializes and returns a LlamaCpp language model instance.\"\"\"\n",
    "    from langchain_community.llms import LlamaCpp\n",
    "    if model_source == \"local\":\n",
    "        model = LlamaCpp(\n",
    "            model_path=local_model_path, n_gpu_layers=-1, n_batch=512, n_ctx=32000,\n",
    "            max_tokens=1024, f16_kv=True, use_mmap=False, temperature=0.0,\n",
    "            num_threads=multiprocessing.cpu_count(), verbose=False\n",
    "        )\n",
    "        return model\n",
    "    raise ValueError(f\"Unsupported model source: {model_source}\")\n",
    "\n",
    "# --- Custom MLflow Model Definition ---\n",
    "class MarkdownCorrectionService(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"An MLflow PythonModel for correcting grammar in Markdown content.\"\"\"\n",
    "    def load_context(self, context: mlflow.pyfunc.PythonModelContext) -> None:\n",
    "        \"\"\"Loads the model artifacts and initializes the LLM chain.\"\"\"\n",
    "        config_path = Path(context.artifacts[\"config\"])\n",
    "        secrets_path = Path(context.artifacts[\"secrets\"])\n",
    "        model_path = context.artifacts[\"llm\"]\n",
    "        config, secrets = load_config_and_secrets(config_path, secrets_path)\n",
    "        self.prompt = get_markdown_correction_prompt()\n",
    "        self.llm = initialize_llm(config[\"model_source\"], secrets, model_path)\n",
    "        self.llm_chain = self.prompt | self.llm\n",
    "        logger.info(\"MarkdownCorrectionService context loaded successfully.\")\n",
    "\n",
    "    def predict(self, context: Any, model_input: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"Applies the correction pipeline to the input DataFrame.\"\"\"\n",
    "        if \"markdown\" not in model_input.columns:\n",
    "            raise KeyError(\"Input DataFrame is missing the required 'markdown' column.\")\n",
    "        corrected = [self.llm_chain.invoke({\"markdown\": row[\"markdown\"]}) for _, row in model_input.iterrows()]\n",
    "        return pd.Series(corrected, name=\"corrected\")\n",
    "\n",
    "# --- Custom Metric Definitions ---\n",
    "class LocalJudgeLlamaClient:\n",
    "    _client = None\n",
    "    @classmethod\n",
    "    def get_client(cls, model_path: Optional[str] = None) -> Llama:\n",
    "        if cls._client is None:\n",
    "            if model_path is None: raise ValueError(\"Must provide model_path to initialize local LLaMA judge.\")\n",
    "            cls._client = Llama(\n",
    "                model_path=model_path, n_gpu_layers=-1, n_batch=512, n_ctx=32000, max_tokens=1024,\n",
    "                f16_kv=True, use_mmap=False, temperature=0.0, seed=42, verbose=False,\n",
    "                num_threads=multiprocessing.cpu_count()\n",
    "            )\n",
    "        return cls._client\n",
    "\n",
    "LocalJudgeLlamaClient.get_client(model_path=LOCAL_LLAMA_JUDGE_PATH)\n",
    "\n",
    "def simple_grammar_check(text: str) -> int:\n",
    "    \"\"\"Counts simple grammatical issues like repeated words and double spaces.\"\"\"\n",
    "    issues = 0\n",
    "    text = str(text).strip()\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence: continue\n",
    "        if '  ' in sentence: issues += 1\n",
    "        words = sentence.lower().split()\n",
    "        for i, word in enumerate(words):\n",
    "            if i > 0 and word == words[i-1]:\n",
    "                issues += 1\n",
    "    return issues\n",
    "\n",
    "def semantic_similarity_eval_fn(predictions: List[str], targets: List[str]) -> float:\n",
    "    \"\"\"Calculates the mean cosine similarity between predictions and targets.\"\"\"\n",
    "    all_texts = [str(text) if text else \"\" for text in list(targets) + list(predictions)]\n",
    "    if len(set(all_texts)) < 2: return 1.0\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(all_texts)\n",
    "    n_targets = len(targets)\n",
    "    target_vectors, pred_vectors = tfidf_matrix[:n_targets], tfidf_matrix[n_targets:]\n",
    "    similarities = [cosine_similarity(target_vectors[i:i+1], pred_vectors[i:i+1])[0][0] for i in range(n_targets)]\n",
    "    return np.mean(similarities)\n",
    "\n",
    "def grammar_error_count_eval_fn(predictions: List[str], targets: List[str]) -> float:\n",
    "    \"\"\"Calculates the average number of grammar errors in predictions.\"\"\"\n",
    "    return np.mean([simple_grammar_check(str(p)) for p in predictions])\n",
    "\n",
    "def grammar_error_rate_eval_fn(predictions: List[str], targets: List[str]) -> float:\n",
    "    \"\"\"Calculates grammar errors per word.\"\"\"\n",
    "    rates = [(simple_grammar_check(str(p)) / max(len(str(p).split()), 1)) for p in predictions]\n",
    "    return np.mean(rates)\n",
    "\n",
    "def grammar_improvement_eval_fn(predictions: List[str], targets: List[str]) -> float:\n",
    "    \"\"\"Measures the reduction in grammar errors from target to prediction.\"\"\"\n",
    "    improvements = [(simple_grammar_check(str(t)) - simple_grammar_check(str(p))) for p, t in zip(predictions, targets)]\n",
    "    return np.mean(improvements)\n",
    "\n",
    "def grammar_score_eval_fn(predictions: List[str], targets: List[str]) -> float:\n",
    "    \"\"\"Assigns a grammar score from 0-100.\"\"\"\n",
    "    scores = [max(100 - (simple_grammar_check(str(p)) * 10), 0) for p in predictions]\n",
    "    return np.mean(scores)\n",
    "\n",
    "def readability_improvement_eval_fn(predictions: List[str], targets: List[str]) -> float:\n",
    "    \"\"\"Measures the change in a simple readability score.\"\"\"\n",
    "    def _readability(text: str) -> float:\n",
    "        words = text.split()\n",
    "        sentences = text.split('.')\n",
    "        if not words or not sentences: return 0\n",
    "        return max(20 - (len(words) / len(sentences)), 0)\n",
    "    improvements = [_readability(str(p)) - _readability(str(t)) for p, t in zip(predictions, targets)]\n",
    "    return np.mean(improvements)\n",
    "\n",
    "def llm_judge_eval_fn_local(predictions: List[str]) -> float:\n",
    "    \"\"\"Uses a local LLM to rate the grammar of predictions on a scale of 1-10.\"\"\"\n",
    "    llama = LocalJudgeLlamaClient.get_client()\n",
    "    scores = []\n",
    "    for pred in predictions:\n",
    "        prompt = f\"Rate the grammar of the following text on a scale of 1 to 10. Respond with only a single digit.\\nText: {pred}\\nAnswer:\"\n",
    "        try:\n",
    "            result = llama(prompt, max_tokens=3, stop=[\"\\n\"])\n",
    "            score_text = result[\"choices\"][0][\"text\"].strip()\n",
    "            score = float(re.findall(r\"\\d+\", score_text)[0])\n",
    "            scores.append(score)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"[LLaMA judge error]: {e}. Appending default score of 5.0.\")\n",
    "            scores.append(5.0)\n",
    "    return np.mean(scores) if scores else 0.0\n",
    "\n",
    "# --- Create MLflow Metric Objects ---\n",
    "semantic_similarity_metric = make_metric(eval_fn=semantic_similarity_eval_fn, greater_is_better=True, name=\"semantic_similarity\")\n",
    "grammar_error_count_metric = make_metric(eval_fn=grammar_error_count_eval_fn, greater_is_better=False, name=\"grammar_error_count\")\n",
    "grammar_error_rate_metric = make_metric(eval_fn=grammar_error_rate_eval_fn, greater_is_better=False, name=\"grammar_error_rate\")\n",
    "grammar_improvement_metric = make_metric(eval_fn=grammar_improvement_eval_fn, greater_is_better=True, name=\"grammar_improvement\")\n",
    "grammar_score_metric = make_metric(eval_fn=grammar_score_eval_fn, greater_is_better=True, name=\"grammar_score\")\n",
    "readability_improvement_metric = make_metric(eval_fn=readability_improvement_eval_fn, greater_is_better=True, name=\"readability_improvement\")\n",
    "llm_judge_metric_local = make_metric(eval_fn=llm_judge_eval_fn_local, greater_is_better=True, name=\"llm_judge_local_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334790c0",
   "metadata": {},
   "source": [
    "## Verify Assets and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b00c95",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def log_asset_status(asset_path: Path, asset_name: str) -> None:\n",
    "    \"\"\"Logs the existence status of a given file or directory.\"\"\"\n",
    "    if asset_path.exists():\n",
    "        logger.info(f\"✅ {asset_name} is properly configured at: {asset_path}\")\n",
    "    else:\n",
    "        logger.warning(f\"⚠️ {asset_name} not found at: {asset_path}.\")\n",
    "\n",
    "log_asset_status(CONFIG_PATH, \"Config file\")\n",
    "log_asset_status(SECRETS_PATH, \"Secrets file\")\n",
    "log_asset_status(LOCAL_MODEL_PATH, \"Local LLaMA model\")\n",
    "log_asset_status(EVAL_DATA_PATH, \"Evaluation data JSON\")\n",
    "log_asset_status(REQUIREMENTS_PATH, \"Requirements file\")\n",
    "\n",
    "with open(EVAL_DATA_PATH, \"r\") as f:\n",
    "    results = json.load(f)\n",
    "original_texts = [item[\"original\"] for item in results]\n",
    "eval_df = pd.DataFrame(original_texts, columns=[\"markdown\"])\n",
    "logger.info(f\"Loaded {len(eval_df)} records for evaluation.\")\n",
    "print(eval_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21960a9",
   "metadata": {},
   "source": [
    "## Log and Register Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c3b585",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "    run_id = run.info.run_id\n",
    "    logger.info(f\"MLflow Run Started. Run ID: {run_id}\")\n",
    "\n",
    "    artifacts = {\n",
    "        \"config\": str(CONFIG_PATH),\n",
    "        \"secrets\": str(SECRETS_PATH),\n",
    "        \"llm\": str(LOCAL_MODEL_PATH),\n",
    "    }\n",
    "    \n",
    "    signature = ModelSignature(\n",
    "        inputs=Schema([ColSpec(\"string\", \"markdown\")]),\n",
    "        outputs=Schema([ColSpec(\"string\", \"corrected\")]),\n",
    "    )\n",
    "    \n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=MODEL_NAME,\n",
    "        python_model=MarkdownCorrectionService(),\n",
    "        artifacts=artifacts,\n",
    "        signature=signature,\n",
    "        registered_model_name=MODEL_NAME,\n",
    "        pip_requirements=str(REQUIREMENTS_PATH),\n",
    "    )\n",
    "    \n",
    "    model_uri = f\"runs:/{run_id}/{MODEL_NAME}\"\n",
    "    logger.info(f\"Model URI: {model_uri}\")\n",
    "\n",
    "# Validate Registered Model\n",
    "client = MlflowClient()\n",
    "try:\n",
    "    latest_version_info = client.get_latest_versions(MODEL_NAME, stages=[\"None\"])[0]\n",
    "    latest_version = latest_version_info.version\n",
    "    logger.info(f\"Successfully registered model '{MODEL_NAME}' version {latest_version}.\")\n",
    "    \n",
    "    model_uri_latest = f\"models:/{MODEL_NAME}/{latest_version}\"\n",
    "    loaded_model = mlflow.pyfunc.load_model(model_uri_latest)\n",
    "    \n",
    "    sample_input = eval_df.head(1)\n",
    "    logger.info(f\"Performing sample prediction on: \\n{sample_input['markdown'].iloc[0][:100]}...\")\n",
    "    \n",
    "    prediction = loaded_model.predict(sample_input)\n",
    "    logger.info(f\"✅ Sample prediction successful. Output:\\n{prediction.iloc[0][:100]}...\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to validate registered model. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b506c45",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbe3a09",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "logger.info(\"Starting model evaluation with mlflow.evaluate...\")\n",
    "\n",
    "with mlflow.start_run(run_id=run_id):\n",
    "    results = mlflow.evaluate(\n",
    "        model=model_uri,\n",
    "        data=eval_df,\n",
    "        targets=\"markdown\",\n",
    "        feature_names=[\"markdown\"],\n",
    "        evaluators=\"default\",\n",
    "        extra_metrics=[\n",
    "            ari_grade_level(),\n",
    "            exact_match(),\n",
    "            rouge1(),\n",
    "            rougeL(),\n",
    "            semantic_similarity_metric,\n",
    "            grammar_error_count_metric,\n",
    "            grammar_error_rate_metric,\n",
    "            grammar_improvement_metric,\n",
    "            grammar_score_metric,\n",
    "            readability_improvement_metric,\n",
    "            llm_judge_metric_local\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    logger.info(\"✅ Evaluation complete.\")\n",
    "    logger.info(\"Evaluation Metrics:\")\n",
    "    for key, value in results.metrics.items():\n",
    "        logger.info(f\"  - {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966b18e7",
   "metadata": {},
   "source": [
    "## Log Execution Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0118e140",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "end_time: float = time.time()\n",
    "elapsed_time: float = end_time - start_time\n",
    "elapsed_minutes: int = int(elapsed_time // 60)\n",
    "elapsed_seconds: float = elapsed_time % 60\n",
    "\n",
    "logger.info(f\"⏱️ Total execution time: {elapsed_minutes}m {elapsed_seconds:.2f}s\")\n",
    "logger.info(\"✅ Notebook execution completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
