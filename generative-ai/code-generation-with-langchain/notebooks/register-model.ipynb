{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c83fc22-12be-4e7e-90c6-9195a4de3a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "# Configure logger\n",
    "logger: logging.Logger = logging.getLogger(\"register_model_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.propagate = False  # Prevent duplicate logs from parent loggers\n",
    "\n",
    "# Set formatter\n",
    "formatter: logging.Formatter = logging.Formatter(\n",
    "    fmt=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "# Configure and attach stream handler\n",
    "stream_handler: logging.StreamHandler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc46ab2b-a3ac-40b8-9be3-0f07d14ae585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 01:47:03 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()  \n",
    "\n",
    "logger.info(\"Notebook execution started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f6740b-1817-4c30-baf7-169a4f37ae8f",
   "metadata": {},
   "source": [
    "## Model Service for Code Generation \n",
    "\n",
    "In this section, we demonstrate how to deploy a code generation service that provides a REST API endpoint for generating code based on natural language queries. This service provides a REST API endpoint for generating code based on natural language queries, \n",
    "The service includes performance optimization features to handle large repositories:\n",
    "- **Parameter-based operation control** with `metadata_only` mode for fast processing \n",
    "- **Resource optimization** with batched processing and configurable timeouts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b156d7-0b39-4b73-8277-8448ddb9fbed",
   "metadata": {},
   "source": [
    "### API Usage Examples\n",
    "\n",
    "The optimized code generation service can be invoked through its REST API using the following patterns:\n",
    "\n",
    "#### Details\n",
    "\n",
    "The API includes some features like:\n",
    "\n",
    "1. **metadata_only mode**: Allows extracting only basic metadata without running the resource-intensive LLM analysis\n",
    "2. **Timeout controls**: Prevents worker timeouts with configurable processing limits\n",
    "\n",
    "```bash\n",
    "# Example 1: Direct code generation (no repository)\n",
    "curl -X 'POST' \\\n",
    "  'https://endpoint/invocations' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "  \"inputs\": {\n",
    "    \"question\": \"Write Python code to load the LLM model using Ollama with \\'llama3\\' and generate an inspirational quote.\"\n",
    "  }\n",
    "}'\n",
    "\n",
    "# Example 2: Repository-enhanced code generation with metadata-only mode (FAST)\n",
    "curl -X 'POST' \\\n",
    "  'https://endpoint/invocations' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "  \"inputs\": {\n",
    "    \"question\": \"Write Python code to scrape a website and extract all links\",\n",
    "    \"repository_url\": \"https://github.com/MunGell/awesome-for-beginners\"\n",
    "  }\n",
    "}'\n",
    "\n",
    "# Example 3: Repository-enhanced code generation with full processing (DEEP UNDERSTANDING)\n",
    "curl -X 'POST' \\\n",
    "  'https://endpoint/invocations' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "  \"inputs\": {\n",
    "    \"question\": \"Write Python code to scrape a website and extract all links\",\n",
    "    \"repository_url\": \"https://github.com/MunGell/awesome-for-beginners\"\n",
    "  }\n",
    "}'\n",
    "```\n",
    "\n",
    "The service will automatically adapt to the parameters provided:\n",
    "- With `metadata_only=true`, it performs lightweight processing ideal for large repositories\n",
    "- With full processing, it performs deep analysis but take longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0fd9146-acac-4735-84b1-9c2ab7342afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/mlflow/pyfunc/utils/data_validation.py:168: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
      "  color_warning(\n",
      "/opt/conda/lib/python3.12/site-packages/mlflow/pyfunc/model.py:172: UserWarning: \u001b[31mType hint used in the model's predict function is not supported for MLflow's schema validation. Type hints must be wrapped in list[...] because MLflow assumes the predict method to take multiple input instances. Specify your type hint as `list[typing.Dict[str, typing.Any]]` for a valid signature. Remove the type hint to disable this warning. To enable validation for the input data, specify input example or model signature when logging the model. \u001b[0m\n",
      "  func_info = _get_func_info_if_type_hint_supported(predict_attr)\n",
      "2025/07/09 01:47:13 INFO mlflow.tracking.fluent: Experiment with name 'Code-Generation-Model' does not exist. Creating a new experiment.\n",
      "/opt/conda/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2025-07-09 01:47:29 - INFO - PyTorch version 2.7.0 available.\n",
      "2025-07-09 01:47:30 - INFO - Downloading and saving embedding model to /home/jovyan/local/hugging_face/embedding_models/all-MiniLM-L6-v2...\n",
      "2025-07-09 01:47:30 - INFO - Use pytorch device_name: cuda\n",
      "2025-07-09 01:47:30 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-07-09 01:47:40 - INFO - Save model to /home/jovyan/local/hugging_face/embedding_models/all-MiniLM-L6-v2\n",
      "2025-07-09 01:47:41 - INFO - Embedding model saved successfully.\n",
      "2025-07-09 01:47:42 - INFO - Using local embedding model from: /home/jovyan/local/hugging_face/embedding_models/all-MiniLM-L6-v2\n",
      "2025/07/09 01:47:42 INFO mlflow.models.signature: Inferring model signature from type hints\n",
      "/opt/conda/lib/python3.12/site-packages/mlflow/pyfunc/__init__.py:3139: UserWarning: Failed to infer signature from type hint: Type hints must be wrapped in list[...] because MLflow assumes the predict method to take multiple input instances. Specify your type hint as `list[typing.Dict[str, typing.Any]]` for a valid signature.\n",
      "  signature_from_type_hints = _infer_signature_from_type_hints(\n",
      "/opt/conda/lib/python3.12/site-packages/mlflow/pyfunc/__init__.py:3212: UserWarning: \u001b[1;33mAn input example was not provided when logging the model. To ensure the model signature functions correctly, specify the `input_example` parameter. See https://mlflow.org/docs/latest/model/signatures.html#model-input-example for more details about the benefits of using input_example.\u001b[0m\n",
      "  color_warning(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ade9f58e35684f939db2fd525c07c339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe38eb2db244288a8e7bdf64ef20e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c282cf45ba4175b926bdeea5aebc70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1184edf473114e3fa6e1a388f4400071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/09 01:50:45 WARNING mlflow.utils.requirements_utils: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - mlflow (current: 2.21.2, required: mlflow==2.9.2)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n",
      "2025-07-09 01:53:34 - INFO - Model and artifacts successfully registered in MLflow.\n",
      "Successfully registered model 'Code-Generation-Model'.\n",
      "Created version '1' of model 'Code-Generation-Model'.\n",
      "2025-07-09 01:53:35 - INFO - ✅ Model registered successfully with run ID: 91b457ec0fac4e3187f3e4b49359cb45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.5 s, sys: 39.1 s, total: 58.6 s\n",
      "Wall time: 6min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# === Imports ===\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import mlflow\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# === Extend system path to include parent directory for module resolution ===\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../../\")))\n",
    "\n",
    "# Import utility functions and CodeGenerationService\n",
    "from src.utils import configure_hf_cache\n",
    "from core.code_generation_service import CodeGenerationService\n",
    "\n",
    "# === Configuration settings ===\n",
    "CONFIG_PATH = \"../configs/config.yaml\"\n",
    "SECRETS_PATH = \"../configs/secrets.yaml\"\n",
    "MLFLOW_MODEL_NAME = \"Code-Generation-Model\"\n",
    "LOCAL_MODEL_PATH = \"/home/jovyan/datafabric/meta-llama3.1-8b-Q8/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\"\n",
    "\n",
    "# Configure HuggingFace cache for the embedding model\n",
    "configure_hf_cache()\n",
    "\n",
    "# Define embedding model parameters\n",
    "EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "EMBEDDING_MODEL_PATH = os.path.join(\n",
    "    os.environ.get(\"HF_HOME\", \"/home/jovyan/local/hugging_face\"),\n",
    "    \"embedding_models\", \n",
    "    EMBEDDING_MODEL_NAME\n",
    ")\n",
    "\n",
    "mlflow.set_tracking_uri('/phoenix/mlflow')\n",
    "# Set up the MLflow experiment\n",
    "mlflow.set_experiment(MLFLOW_MODEL_NAME)\n",
    "\n",
    "# Check if the model file exists\n",
    "if not os.path.exists(LOCAL_MODEL_PATH):\n",
    "    logger.info(f\"⚠️ Warning: Model file not found at {LOCAL_MODEL_PATH}. Please verify the path.\")\n",
    "\n",
    "# Verify the locally saved embedding model exists\n",
    "if not os.path.exists(EMBEDDING_MODEL_PATH):\n",
    "    # If the embedding model wasn't saved earlier, do it now\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        os.makedirs(os.path.dirname(EMBEDDING_MODEL_PATH), exist_ok=True)\n",
    "        logger.info(f\"Downloading and saving embedding model to {EMBEDDING_MODEL_PATH}...\")\n",
    "        model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "        model.save(EMBEDDING_MODEL_PATH)\n",
    "        logger.info(f\"Embedding model saved successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving embedding model: {str(e)}\")\n",
    "        logger.warning(\"Will proceed without a local embedding model. The service will download it during initialization.\")\n",
    "        EMBEDDING_MODEL_PATH = None\n",
    "\n",
    "# Use the CodeGenerationService's log_model method to register the model in MLflow\n",
    "with mlflow.start_run(run_name=MLFLOW_MODEL_NAME) as run:\n",
    "    # Log and register the model using the service's classmethod\n",
    "    CodeGenerationService.log_model(\n",
    "        secrets_path=SECRETS_PATH,\n",
    "        config_path=CONFIG_PATH,\n",
    "        model_path=LOCAL_MODEL_PATH,\n",
    "        embedding_model_path=EMBEDDING_MODEL_PATH,\n",
    "        delay_async_init=True  # Since this service requires parallelism we use this parameter to avoid error when picling the model\n",
    "    )\n",
    "    \n",
    "    # Register the model in MLflow Model Registry\n",
    "    model_uri = f\"runs:/{run.info.run_id}/code_generation_service\"\n",
    "    mlflow.register_model(\n",
    "        model_uri=model_uri,\n",
    "        name=MLFLOW_MODEL_NAME\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"✅ Model registered successfully with run ID: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90e6d558-3a8b-4926-82e5-e59635186b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 01:53:35 - INFO - ⏱️ Total execution time: 6m 32.32s\n",
      "2025-07-09 01:53:35 - INFO - ✅ Notebook execution completed successfully.\n"
     ]
    }
   ],
   "source": [
    "end_time: float = time.time()\n",
    "elapsed_time: float = end_time - start_time\n",
    "elapsed_minutes: int = int(elapsed_time // 60)\n",
    "elapsed_seconds: float = elapsed_time % 60\n",
    "\n",
    "logger.info(f\"⏱️ Total execution time: {elapsed_minutes}m {elapsed_seconds:.2f}s\")\n",
    "logger.info(\"✅ Notebook execution completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7d4c40-b619-45ea-bc58-e93459b58faf",
   "metadata": {},
   "source": [
    "Built with ❤️ using [**HP AI Studio**](https://hp.com/ai-studio)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
