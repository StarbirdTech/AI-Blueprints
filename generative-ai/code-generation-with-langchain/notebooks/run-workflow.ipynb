{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbb953ed-f738-4492-b322-94220613a4f8",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-size: 50px;\"> Code Generation RAG with Langchain </h1>\n",
    "\n",
    "This notebook uses the **RAG (Retrieval-Augmented Generation)** technique to enhance code generation using context-aware prompts. It extracts code and documentation from GitHub repositories (including Python files, Jupyter notebooks, and other programming languages), transforms them into vector embeddings, and stores them in a vector database. When a user submits a prompt, the system retrieves the most relevant code snippets and provides them as context to a language model (LLM), which then generates new code based on that context.\n",
    "\n",
    "## Notebook Overview\n",
    "\n",
    "- Start Execution\n",
    "- Install and Import Libraries\n",
    "- Configure Settings\n",
    "- Cloning and Extracting Code from Github Repositories\n",
    "- Generating Metadata with LLM\n",
    "- Generate Embeddings and Structure Data\n",
    "- Store and Query Documents in ChromaDB\n",
    "- Code Generation Chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5493725e",
   "metadata": {},
   "source": [
    "# Start Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcef64c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "# Configure logger\n",
    "logger: logging.Logger = logging.getLogger(\"run_workflow_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.propagate = False  # Prevent duplicate logs from parent loggers\n",
    "\n",
    "# Set formatter\n",
    "formatter: logging.Formatter = logging.Formatter(\n",
    "    fmt=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "# Configure and attach stream handler\n",
    "stream_handler: logging.StreamHandler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "589228b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 17:40:13 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()  \n",
    "\n",
    "logger.info(\"Notebook execution started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb696e77-2b43-4599-91ee-32e70fe81af6",
   "metadata": {},
   "source": [
    "# Install and Import Libraries\n",
    "\n",
    "In this step, we import all the necessary libraries and internal components required to run the RAG pipeline, including modules for notebook parsing, embedding generation, vector storage, and code generation with LLMs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30141d79-eda3-4c81-a8f9-444ea3cb9857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 125 ms, sys: 20.7 ms, total: 145 ms\n",
      "Wall time: 3.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "%pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fe25d49-a8b5-47f7-acf0-57c4057b28db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Built-in ===\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "# === Third-party libraries ===\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import yaml\n",
    "from IPython import get_ipython\n",
    "from IPython.display import display, Markdown, Code\n",
    "\n",
    "# === Langchain ===\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import Document, StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# === Internal modules ===\n",
    "\n",
    "# Add 'src' directory to system path (2 levels up)\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    " \n",
    "# Utils\n",
    "from src.utils import (\n",
    "    load_config,\n",
    "    load_secrets,\n",
    "    load_secrets_to_env,\n",
    "    configure_proxy,\n",
    "    initialize_llm,\n",
    "    configure_hf_cache,\n",
    "    clean_code,\n",
    "    generate_code_with_retries,\n",
    "    get_context_window,\n",
    "    dynamic_retriever,\n",
    "    format_docs_with_adaptive_context\n",
    ")\n",
    "\n",
    "# Core components\n",
    "from core.extract_text.github_repository_extractor import GitHubRepositoryExtractor\n",
    "from core.generate_metadata.llm_context_updater import LLMContextUpdater\n",
    "from core.dataflow.dataflow import EmbeddingUpdater, DataFrameConverter\n",
    "from core.vector_database.vector_store_writer import VectorStoreWriter\n",
    "from core.extract_text.rag_utils import (\n",
    "    identify_question_type,\n",
    "    retriever,\n",
    "    format_multi_doc_context,\n",
    "    process_repository_question\n",
    ")\n",
    "from core.prompt_templates import (\n",
    "    get_dynamic_repository_prompt,\n",
    "    get_code_description_prompt,\n",
    "    get_code_generation_prompt,\n",
    "    get_specialized_prompt,\n",
    "    get_metadata_generation_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177a8e86",
   "metadata": {},
   "source": [
    "# Configure Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "887e07a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress Python warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef0370ed-0d2d-42b7-8694-163de8b18ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"../configs/config.yaml\"\n",
    "SECRETS_PATH = \"../configs/secrets.yaml\"\n",
    "MLFLOW_EXPERIMENT_NAME = \"Code-Generation-Experiment\"\n",
    "MLFLOW_RUN_NAME = \"Code-Generation-Run\"\n",
    "LOCAL_MODEL_PATH = \"/home/jovyan/datafabric/meta-llama3.1-8b-Q8/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\"\n",
    "MLFLOW_MODEL_NAME = \"Code-Generation-Model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b64aa5",
   "metadata": {},
   "source": [
    "## Configuration and Secrets Loading\n",
    "\n",
    "In this section, we load configuration parameters and API keys from separate YAML files. This separation helps maintain security by keeping sensitive information (API keys) separate from configuration settings.\n",
    "\n",
    "- **config.yaml**: Contains non-sensitive configuration parameters like model sources and URLs\n",
    "- **secrets.yaml**: Contains sensitive API keys for services like HuggingFace\n",
    "- *(Optional for Premium users)* Secrets such as API keys for services like HuggingFace can be stored as environment variables for the project and loaded into the notebook (see the project's README file for steps on how to save secrets in Secrets Manager)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3aa7b4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 1 secrets into environment variables.\n",
      "âœ… Configuration loaded successfully\n",
      "âœ… Secrets loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load secrets from secrets.yaml file (if it exists) into environment\n",
    "if Path(SECRETS_PATH).exists():\n",
    "    load_secrets_to_env(SECRETS_PATH)\n",
    "else:\n",
    "    print(f\"No secrets file found at {SECRETS_PATH}; relying on preexisting environment\")\n",
    "\n",
    "# Retrieve secrets from environment\n",
    "try:\n",
    "    secrets = load_secrets()\n",
    "except ValueError:\n",
    "    secrets = {}\n",
    "\n",
    "# Load configuration and secrets\n",
    "config = load_config(CONFIG_PATH)\n",
    "\n",
    "print(\"âœ… Configuration loaded successfully\")\n",
    "print(\"âœ… Secrets loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351a13bc",
   "metadata": {},
   "source": [
    "### Environment Configuration\n",
    "The following cell configures any necessary environment variables for the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f744d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure environment settings if needed\n",
    "configure_proxy(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ebe5b0-bfd1-4ce7-99d9-f3fb779676fb",
   "metadata": {},
   "source": [
    "### Configuration of Hugging face caches\n",
    "\n",
    "In the next cell, we configure HuggingFace cache, so that all the models downloaded from them are persisted locally, even after the workspace is closed. This is a future desired feature for AI Studio and the GenAI addon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79634199-4abb-414b-988a-fa376ec07e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure HuggingFace cache\n",
    "configure_hf_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1947d480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datasets:PyTorch version 2.7.1 available.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489a58904d504f4dabb803e04307cefc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c123a72de1af4920902cfc3fee1a0e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca61f15f6fff42b2a01616bd9b09c349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff882c6ef2bb4f7f9e67cc2bb9c618f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b467f8b2dc46b9991b73441b18cdb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "586b80688dd14904b5c63b179e29d51c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3505c93a9bb4044bc974ba46212e10a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5aa8b0d6d9415d93dd0bf3a5c8bb0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "791dcf61440346fc9e7570e1ecc07bf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb466f6080eb490986c1647eb0e3932a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c43723dd3764a208c03936344464c3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize HuggingFace Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9221d332-9889-4d45-a9d3-fb9b66c7c1e4",
   "metadata": {},
   "source": [
    "## Step 1: Cloning and Extracting Code from GitHub Repositories\n",
    "\n",
    "In this step, we clone a GitHub repository, locate relevant code files, and extract code snippets along with their documentation context.\n",
    "\n",
    "Using the `GitHubReposito\n",
    "ryExtractor`, the process includes:\n",
    "- Cloning the repository.\n",
    "- Recursively searching for supported code files (Python, Jupyter notebooks, JavaScript, etc.).\n",
    "- Extracting code snippets along with their documentation context.\n",
    "- Structuring the extracted data with fields like `id`, `code`, `context`, `filename`, and a placeholder for `embedding`.\n",
    "\n",
    "Our optimized extractor also handles context window overflow issues:\n",
    "- Limits the maximum file size to 500KB to skip very large files\n",
    "- Breaks large files into chunks of 100 lines for easier processing\n",
    "- Uses pattern-based exclusion for minified/bundled files that would exceed token limits\n",
    "- Provides detailed context information with line numbers for chunked files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a9126f1-d6c1-4db5-aae0-02b32ffb2633",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Processing repository: MunGell/awesome-for-beginners\n",
      "[INFO] Created directory: ./repo_files\n",
      "[INFO] Downloaded: ./repo_files/.github/scripts/render-readme.py\n",
      "[INFO] Downloaded: ./repo_files/CONTRIBUTING.md\n",
      "[INFO] Downloaded: ./repo_files/README.md\n",
      "[INFO] Downloaded: ./repo_files/data.json\n",
      "[INFO] Extracted 30 code snippets from repository\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 84.6 ms, sys: 37.8 ms, total: 122 ms\n",
      "Wall time: 2.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "extractor = GitHubRepositoryExtractor(\n",
    "    repo_url=\"https://github.com/MunGell/awesome-for-beginners\",\n",
    "    save_dir=\"./repo_files\",\n",
    "    verbose=True,\n",
    "    max_file_size_kb=500,\n",
    "    max_chunk_size=100,\n",
    "    supported_extensions=('.py', '.ipynb', '.md', '.txt', '.json', '.js', '.ts')\n",
    ")\n",
    "extracted_data = extractor.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7714591-6f9f-448f-a759-9bbff0ddd1b7",
   "metadata": {},
   "source": [
    "## Step 2: Generating Metadata with LLM ðŸ”¢\n",
    "\n",
    "In this step, we use a language model (LLM) to generate concise explanations for each extracted code snippet, enriching the original data with human-readable context.\n",
    "\n",
    "### What Happens:\n",
    "\n",
    "- A prompt template is defined to guide the LLM in describing what each code snippet does, using the code, file name, and optional context.\n",
    "- A `PromptTemplate` object is built from this structure.\n",
    "- The selected model (e.g., Meta LLaMA 3.1 8B) is initialized using `initialize_llm`.\n",
    "- The function `update_context_with_llm` runs the model for each code snippet and updates the `context` field with the generated explanation.\n",
    "\n",
    "This process transforms raw code into meaningful metadata, which improves downstream tasks like search, summarization, or generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f8a1fd6-8c43-42b2-b699-a313e9fe7f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.61 s, sys: 5.1 s, total: 6.71 s\n",
      "Wall time: 2min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if \"model_source\" in config:\n",
    "    model_source = config[\"model_source\"]\n",
    "\n",
    "# Get the metadata generation prompt with model-specific formatting\n",
    "from core.prompt_templates import get_metadata_generation_prompt\n",
    "metadata_prompt = get_metadata_generation_prompt(model_source)\n",
    "\n",
    "llm = initialize_llm(model_source, secrets, LOCAL_MODEL_PATH)\n",
    "\n",
    "# Create the LLM chain with the metadata prompt\n",
    "llm_chain = metadata_prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213104ca-f28c-4d8b-b0db-579a6d0fc68e",
   "metadata": {},
   "source": [
    "### Generate Metadata with Local LLM\n",
    "\n",
    "âš ï¸ Generating metadata using a local language model may be time-consuming.  \n",
    "Whenever possible, we recommend using a hosted API for faster responses and better performance.\n",
    "\n",
    "Note: The quality of the generated metadata may be lower when using quantized or compact local models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85ba720e-cea4-4535-805a-b20e7f717bc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating Contexts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [01:25<00:00,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 21s, sys: 1.7 s, total: 1min 23s\n",
      "Wall time: 1min 25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "updater = LLMContextUpdater(\n",
    "    llm_chain=llm_chain,\n",
    "    prompt_template=metadata_prompt.template if hasattr(metadata_prompt, 'template') else str(metadata_prompt),\n",
    "    verbose=False,  # Set to True to enable detailed execution logs\n",
    "    print_prompt=False  # Set to True to display the generated prompt before inference\n",
    ")\n",
    "\n",
    "updated_data = updater.update(extracted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6436a808-52a4-406c-8f2b-d3a4283fffc9",
   "metadata": {},
   "source": [
    "## Step 3: Generate Embeddings and Structure Data\n",
    "\n",
    "In this step, we generate semantic embeddings for each code snippet's context and structure the results into a Pandas DataFrame for further processing.\n",
    "\n",
    "### What Happens:\n",
    "\n",
    "- **Embedding Generation**  \n",
    "  We use the HuggingFace model `\"all-MiniLM-L6-v2\"` to convert each snippet's context into an embedding vector.  \n",
    "  The `EmbeddingUpdater` class handles this process, updating the `embedding` field for each item in the data structure.\n",
    "\n",
    "- **Data Structuring**  \n",
    "  The `DataFrameConverter` class is then used to convert the enriched data into a standardized format.  \n",
    "  Each entry includes:\n",
    "  - `id`: Unique identifier\n",
    "  - `embedding`: Vector representation of the context\n",
    "  - `code`: Extracted code\n",
    "  - `metadata`: Including filename and updated context\n",
    "\n",
    "- **DataFrame Creation**  \n",
    "  The structured data is converted into a Pandas DataFrame, making it easier to visualize, manipulate, and persist for downstream use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "528c975c-bd47-476e-9791-73feafc3b7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "updater = EmbeddingUpdater(embedding_model=embeddings, verbose=False)\n",
    "updated_data = updater.update(updated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccb3ca2e-2508-4efc-a9ae-d41feaf977ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 30 snippets from repository.\n"
     ]
    }
   ],
   "source": [
    "converter = DataFrameConverter(verbose=False)\n",
    "df = converter.to_dataframe(updated_data)\n",
    "# Summary of processed snippets\n",
    "print(f\"Processed {len(df)} snippets from repository.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5eab11d1-128d-4f19-9c86-cd416ed9cb9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>code</th>\n",
       "      <th>metadatas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8632d794-e845-4158-b2dc-6c7a8d9d51e0</td>\n",
       "      <td>[-0.07924529165029526, 0.034437816590070724, -...</td>\n",
       "      <td>from jinja2 import Environment, FileSystemLoad...</td>\n",
       "      <td>{'filenames': '.github/scripts/render-readme.p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9f6a994e-20b3-4cc2-ab5b-234072c809a1</td>\n",
       "      <td>[-0.09278706461191177, -0.0659610703587532, 0....</td>\n",
       "      <td># Contribution Guide &amp; Guidelines ðŸš€\\n\\nWelcome...</td>\n",
       "      <td>{'filenames': 'CONTRIBUTING.md', 'context': 'T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e6e30fba-4ef3-4c46-9010-aa3731065ce5</td>\n",
       "      <td>[-0.11133313179016113, 0.02457975037395954, -0...</td>\n",
       "      <td>&lt;!-- DO NOT EDIT THIS FILE (`README.md`) --&gt;\\n...</td>\n",
       "      <td>{'filenames': 'README.md', 'context': 'This is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8fea5c79-8230-4690-8b91-e4820ac3d4ad</td>\n",
       "      <td>[-0.10935547202825546, -0.043452225625514984, ...</td>\n",
       "      <td>- [Alda](https://github.com/alda-lang/alda) _(...</td>\n",
       "      <td>{'filenames': 'README.md', 'context': 'Based o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23c0858a-7250-4fd8-8f25-75c20c95e228</td>\n",
       "      <td>[-0.04330722242593765, 0.025186164304614067, -...</td>\n",
       "      <td>- [stryker](https://github.com/stryker-mutator...</td>\n",
       "      <td>{'filenames': 'README.md', 'context': 'The cod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2f6dbfae-0714-4dde-ba70-ba0ed5de0bed</td>\n",
       "      <td>[-0.10661730170249939, 0.051007699221372604, -...</td>\n",
       "      <td>- [PyMC](https://github.com/pymc-devs/pymc) _(...</td>\n",
       "      <td>{'filenames': 'README.md', 'context': 'This is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6f5b9529-3726-4817-835f-d450a5a9fdf7</td>\n",
       "      <td>[-0.0949971154332161, 0.11657942086458206, -0....</td>\n",
       "      <td>\\nTo the extent possible under law, the author...</td>\n",
       "      <td>{'filenames': 'README.md', 'context': 'This co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>57c2ff28-4fee-44ba-9bff-5797c36b9ab8</td>\n",
       "      <td>[-0.12217269092798233, 0.08077014982700348, -0...</td>\n",
       "      <td>{\\n    \"sponsors\": [\\n        {\\n            \"...</td>\n",
       "      <td>{'filenames': 'data.json', 'context': 'Based o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>24e02bee-be97-491e-8376-e44967d21f0e</td>\n",
       "      <td>[-0.09646370261907578, 0.048626430332660675, -...</td>\n",
       "      <td>\"technologies\": [\\n               ...</td>\n",
       "      <td>{'filenames': 'data.json', 'context': 'The cod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10a0028e-d040-4c94-89e9-40fe5e35b803</td>\n",
       "      <td>[-0.0904029905796051, 0.0739266648888588, -0.0...</td>\n",
       "      <td>\"technologies\": [\\n               ...</td>\n",
       "      <td>{'filenames': 'data.json', 'context': 'The pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0a7ae33c-aa04-4b47-9c85-e218109708f4</td>\n",
       "      <td>[-0.09222058206796646, 0.10129266232252121, -0...</td>\n",
       "      <td>],\\n            \"description\": \"A ...</td>\n",
       "      <td>{'filenames': 'data.json', 'context': 'The cod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>be77dbe5-976d-4fb0-878c-4516cfdd22fb</td>\n",
       "      <td>[-0.10843536257743835, 0.09783437848091125, -0...</td>\n",
       "      <td>},\\n        {\\n            \"name\": \"XW...</td>\n",
       "      <td>{'filenames': 'data.json', 'context': 'The cod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1b3826d7-86ec-4592-b20a-2ba0b3637184</td>\n",
       "      <td>[-0.07527705281972885, 0.07016406953334808, -0...</td>\n",
       "      <td>{\\n            \"name\": \"Kinto.js\",\\n  ...</td>\n",
       "      <td>{'filenames': 'data.json', 'context': 'Based o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2258a7ad-c7e2-453d-b417-c7da66a73dec</td>\n",
       "      <td>[-0.09947903454303741, 0.07687284797430038, -0...</td>\n",
       "      <td>{\\n            \"name\": \"serverless\",\\n...</td>\n",
       "      <td>{'filenames': 'data.json', 'context': 'This is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>071e8a34-2f77-4084-8142-4cc2efcaa92b</td>\n",
       "      <td>[-0.09001032263040543, 0.08275430649518967, -0...</td>\n",
       "      <td>{\\n            \"name\": \"Brave Browser\"...</td>\n",
       "      <td>{'filenames': 'data.json', 'context': 'Based o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0725437b-f4c0-4984-b92f-aa27b6624d5c</td>\n",
       "      <td>[-0.08375752717256546, 0.09703469276428223, -0...</td>\n",
       "      <td>\"name\": \"Meteor\",\\n            \"li...</td>\n",
       "      <td>{'filenames': 'data.json', 'context': 'The pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>37458408-b2a2-4a72-8ff8-f23727b43165</td>\n",
       "      <td>[-0.10816269367933273, 0.06331613659858704, -0...</td>\n",
       "      <td>\"link\": \"https://github.com/reactj...</td>\n",
       "      <td>{'filenames': 'data.json', 'context': 'The pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>74135050-575e-4a78-b82d-3c513c712e1d</td>\n",
       "      <td>[-0.11075174808502197, 0.1033211275935173, -0....</td>\n",
       "      <td>\"label\": \"good first issue\",\\n    ...</td>\n",
       "      <td>{'filenames': 'data.json', 'context': 'This co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>47a1888d-e8c0-4eec-b418-676f06048b36</td>\n",
       "      <td>[-0.1056225597858429, 0.0635622963309288, -0.0...</td>\n",
       "      <td>\"label\": \"good first issue\",\\n    ...</td>\n",
       "      <td>{'filenames': 'data.json', 'context': 'The pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>cb4d28e4-1f52-4df4-b3b2-1b3f0b8068a3</td>\n",
       "      <td>[-0.10818754136562347, 0.08132652193307877, -0...</td>\n",
       "      <td>\"technologies\": [\\n               ...</td>\n",
       "      <td>{'filenames': 'data.json', 'context': 'This co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>69ac4572-2821-401e-be0e-0809e31d3dac</td>\n",
       "      <td>[-0.02817457541823387, 0.034941889345645905, -...</td>\n",
       "      <td>],\\n            \"description\": \"Th...</td>\n",
       "      <td>{'filenames': 'data.json', 'context': 'The cod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>b425b2ba-2617-4967-a0f2-7c43cd92bb59</td>\n",
       "      <td>[-0.11198246479034424, 0.09426416456699371, -0...</td>\n",
       "      <td>\"description\": \"An implementation ...</td>\n",
       "      <td>{'filenames': 'data.json', 'context': 'Based o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>68c1cf35-0758-4931-bcea-4bb2ca2de714</td>\n",
       "      <td>[-0.11715361475944519, 0.07217627763748169, -0...</td>\n",
       "      <td>},\\n        {\\n            \"name\": \"Bo...</td>\n",
       "      <td>{'filenames': 'data.json', 'context': 'Based o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>413dba7b-89f0-495e-8966-16d8cbe9fda1</td>\n",
       "      <td>[-0.07222107797861099, 0.012909922748804092, -...</td>\n",
       "      <td>{\\n            \"name\": \"osem\",\\n      ...</td>\n",
       "      <td>{'filenames': 'data.json', 'context': 'The cod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>78e72f67-5153-44aa-90b6-0d6b85de668c</td>\n",
       "      <td>[-0.11793312430381775, 0.09583167731761932, -0...</td>\n",
       "      <td>\"name\": \"TensorZero\",\\n           ...</td>\n",
       "      <td>{'filenames': 'data.json', 'context': 'Unfortu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>f14bdee4-d421-4b43-ab43-29566e1f9019</td>\n",
       "      <td>[-0.10397642850875854, 0.11628702282905579, -0...</td>\n",
       "      <td>\"label\": \"good first issue\",\\n    ...</td>\n",
       "      <td>{'filenames': 'data.json', 'context': 'Based o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>e37120b0-c8d0-44cc-b5c3-b0b9450a8d8a</td>\n",
       "      <td>[-0.07352858036756516, 0.04429339990019798, -0...</td>\n",
       "      <td>\"technologies\": [\\n               ...</td>\n",
       "      <td>{'filenames': 'data.json', 'context': 'Unfortu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>32bc4529-a48b-48c5-bef7-298cec3992d3</td>\n",
       "      <td>[-0.08134632557630539, 0.02194899693131447, -0...</td>\n",
       "      <td>\"name\": \"Legerity for Uno Platform...</td>\n",
       "      <td>{'filenames': 'data.json', 'context': 'Based o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>e205c19a-5615-40d7-8738-d5aaec313935</td>\n",
       "      <td>[-0.09448446333408356, 0.11170770227909088, -0...</td>\n",
       "      <td>],\\n            \"description\": \"ðŸ”®S...</td>\n",
       "      <td>{'filenames': 'data.json', 'context': 'There i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5b8b37c1-dc7c-4803-b1f4-0b6e8d6f2db5</td>\n",
       "      <td>[-0.10063247382640839, 0.08786283433437347, -0...</td>\n",
       "      <td>\"description\": \"The open source Fi...</td>\n",
       "      <td>{'filenames': 'data.json', 'context': 'This co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     ids  \\\n",
       "0   8632d794-e845-4158-b2dc-6c7a8d9d51e0   \n",
       "1   9f6a994e-20b3-4cc2-ab5b-234072c809a1   \n",
       "2   e6e30fba-4ef3-4c46-9010-aa3731065ce5   \n",
       "3   8fea5c79-8230-4690-8b91-e4820ac3d4ad   \n",
       "4   23c0858a-7250-4fd8-8f25-75c20c95e228   \n",
       "5   2f6dbfae-0714-4dde-ba70-ba0ed5de0bed   \n",
       "6   6f5b9529-3726-4817-835f-d450a5a9fdf7   \n",
       "7   57c2ff28-4fee-44ba-9bff-5797c36b9ab8   \n",
       "8   24e02bee-be97-491e-8376-e44967d21f0e   \n",
       "9   10a0028e-d040-4c94-89e9-40fe5e35b803   \n",
       "10  0a7ae33c-aa04-4b47-9c85-e218109708f4   \n",
       "11  be77dbe5-976d-4fb0-878c-4516cfdd22fb   \n",
       "12  1b3826d7-86ec-4592-b20a-2ba0b3637184   \n",
       "13  2258a7ad-c7e2-453d-b417-c7da66a73dec   \n",
       "14  071e8a34-2f77-4084-8142-4cc2efcaa92b   \n",
       "15  0725437b-f4c0-4984-b92f-aa27b6624d5c   \n",
       "16  37458408-b2a2-4a72-8ff8-f23727b43165   \n",
       "17  74135050-575e-4a78-b82d-3c513c712e1d   \n",
       "18  47a1888d-e8c0-4eec-b418-676f06048b36   \n",
       "19  cb4d28e4-1f52-4df4-b3b2-1b3f0b8068a3   \n",
       "20  69ac4572-2821-401e-be0e-0809e31d3dac   \n",
       "21  b425b2ba-2617-4967-a0f2-7c43cd92bb59   \n",
       "22  68c1cf35-0758-4931-bcea-4bb2ca2de714   \n",
       "23  413dba7b-89f0-495e-8966-16d8cbe9fda1   \n",
       "24  78e72f67-5153-44aa-90b6-0d6b85de668c   \n",
       "25  f14bdee4-d421-4b43-ab43-29566e1f9019   \n",
       "26  e37120b0-c8d0-44cc-b5c3-b0b9450a8d8a   \n",
       "27  32bc4529-a48b-48c5-bef7-298cec3992d3   \n",
       "28  e205c19a-5615-40d7-8738-d5aaec313935   \n",
       "29  5b8b37c1-dc7c-4803-b1f4-0b6e8d6f2db5   \n",
       "\n",
       "                                           embeddings  \\\n",
       "0   [-0.07924529165029526, 0.034437816590070724, -...   \n",
       "1   [-0.09278706461191177, -0.0659610703587532, 0....   \n",
       "2   [-0.11133313179016113, 0.02457975037395954, -0...   \n",
       "3   [-0.10935547202825546, -0.043452225625514984, ...   \n",
       "4   [-0.04330722242593765, 0.025186164304614067, -...   \n",
       "5   [-0.10661730170249939, 0.051007699221372604, -...   \n",
       "6   [-0.0949971154332161, 0.11657942086458206, -0....   \n",
       "7   [-0.12217269092798233, 0.08077014982700348, -0...   \n",
       "8   [-0.09646370261907578, 0.048626430332660675, -...   \n",
       "9   [-0.0904029905796051, 0.0739266648888588, -0.0...   \n",
       "10  [-0.09222058206796646, 0.10129266232252121, -0...   \n",
       "11  [-0.10843536257743835, 0.09783437848091125, -0...   \n",
       "12  [-0.07527705281972885, 0.07016406953334808, -0...   \n",
       "13  [-0.09947903454303741, 0.07687284797430038, -0...   \n",
       "14  [-0.09001032263040543, 0.08275430649518967, -0...   \n",
       "15  [-0.08375752717256546, 0.09703469276428223, -0...   \n",
       "16  [-0.10816269367933273, 0.06331613659858704, -0...   \n",
       "17  [-0.11075174808502197, 0.1033211275935173, -0....   \n",
       "18  [-0.1056225597858429, 0.0635622963309288, -0.0...   \n",
       "19  [-0.10818754136562347, 0.08132652193307877, -0...   \n",
       "20  [-0.02817457541823387, 0.034941889345645905, -...   \n",
       "21  [-0.11198246479034424, 0.09426416456699371, -0...   \n",
       "22  [-0.11715361475944519, 0.07217627763748169, -0...   \n",
       "23  [-0.07222107797861099, 0.012909922748804092, -...   \n",
       "24  [-0.11793312430381775, 0.09583167731761932, -0...   \n",
       "25  [-0.10397642850875854, 0.11628702282905579, -0...   \n",
       "26  [-0.07352858036756516, 0.04429339990019798, -0...   \n",
       "27  [-0.08134632557630539, 0.02194899693131447, -0...   \n",
       "28  [-0.09448446333408356, 0.11170770227909088, -0...   \n",
       "29  [-0.10063247382640839, 0.08786283433437347, -0...   \n",
       "\n",
       "                                                 code  \\\n",
       "0   from jinja2 import Environment, FileSystemLoad...   \n",
       "1   # Contribution Guide & Guidelines ðŸš€\\n\\nWelcome...   \n",
       "2   <!-- DO NOT EDIT THIS FILE (`README.md`) -->\\n...   \n",
       "3   - [Alda](https://github.com/alda-lang/alda) _(...   \n",
       "4   - [stryker](https://github.com/stryker-mutator...   \n",
       "5   - [PyMC](https://github.com/pymc-devs/pymc) _(...   \n",
       "6   \\nTo the extent possible under law, the author...   \n",
       "7   {\\n    \"sponsors\": [\\n        {\\n            \"...   \n",
       "8               \"technologies\": [\\n               ...   \n",
       "9               \"technologies\": [\\n               ...   \n",
       "10              ],\\n            \"description\": \"A ...   \n",
       "11          },\\n        {\\n            \"name\": \"XW...   \n",
       "12          {\\n            \"name\": \"Kinto.js\",\\n  ...   \n",
       "13          {\\n            \"name\": \"serverless\",\\n...   \n",
       "14          {\\n            \"name\": \"Brave Browser\"...   \n",
       "15              \"name\": \"Meteor\",\\n            \"li...   \n",
       "16              \"link\": \"https://github.com/reactj...   \n",
       "17              \"label\": \"good first issue\",\\n    ...   \n",
       "18              \"label\": \"good first issue\",\\n    ...   \n",
       "19              \"technologies\": [\\n               ...   \n",
       "20              ],\\n            \"description\": \"Th...   \n",
       "21              \"description\": \"An implementation ...   \n",
       "22          },\\n        {\\n            \"name\": \"Bo...   \n",
       "23          {\\n            \"name\": \"osem\",\\n      ...   \n",
       "24              \"name\": \"TensorZero\",\\n           ...   \n",
       "25              \"label\": \"good first issue\",\\n    ...   \n",
       "26              \"technologies\": [\\n               ...   \n",
       "27              \"name\": \"Legerity for Uno Platform...   \n",
       "28              ],\\n            \"description\": \"ðŸ”®S...   \n",
       "29              \"description\": \"The open source Fi...   \n",
       "\n",
       "                                            metadatas  \n",
       "0   {'filenames': '.github/scripts/render-readme.p...  \n",
       "1   {'filenames': 'CONTRIBUTING.md', 'context': 'T...  \n",
       "2   {'filenames': 'README.md', 'context': 'This is...  \n",
       "3   {'filenames': 'README.md', 'context': 'Based o...  \n",
       "4   {'filenames': 'README.md', 'context': 'The cod...  \n",
       "5   {'filenames': 'README.md', 'context': 'This is...  \n",
       "6   {'filenames': 'README.md', 'context': 'This co...  \n",
       "7   {'filenames': 'data.json', 'context': 'Based o...  \n",
       "8   {'filenames': 'data.json', 'context': 'The cod...  \n",
       "9   {'filenames': 'data.json', 'context': 'The pro...  \n",
       "10  {'filenames': 'data.json', 'context': 'The cod...  \n",
       "11  {'filenames': 'data.json', 'context': 'The cod...  \n",
       "12  {'filenames': 'data.json', 'context': 'Based o...  \n",
       "13  {'filenames': 'data.json', 'context': 'This is...  \n",
       "14  {'filenames': 'data.json', 'context': 'Based o...  \n",
       "15  {'filenames': 'data.json', 'context': 'The pro...  \n",
       "16  {'filenames': 'data.json', 'context': 'The pro...  \n",
       "17  {'filenames': 'data.json', 'context': 'This co...  \n",
       "18  {'filenames': 'data.json', 'context': 'The pro...  \n",
       "19  {'filenames': 'data.json', 'context': 'This co...  \n",
       "20  {'filenames': 'data.json', 'context': 'The cod...  \n",
       "21  {'filenames': 'data.json', 'context': 'Based o...  \n",
       "22  {'filenames': 'data.json', 'context': 'Based o...  \n",
       "23  {'filenames': 'data.json', 'context': 'The cod...  \n",
       "24  {'filenames': 'data.json', 'context': 'Unfortu...  \n",
       "25  {'filenames': 'data.json', 'context': 'Based o...  \n",
       "26  {'filenames': 'data.json', 'context': 'Unfortu...  \n",
       "27  {'filenames': 'data.json', 'context': 'Based o...  \n",
       "28  {'filenames': 'data.json', 'context': 'There i...  \n",
       "29  {'filenames': 'data.json', 'context': 'This co...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b43d8f8-09ed-49c7-8504-2574a78bfb5a",
   "metadata": {},
   "source": [
    "## Step 4: Store and Query Documents in ChromaDB\n",
    "\n",
    "In this step, we store the code snippets, metadata, and embeddings in **ChromaDB**, a vector database, and implement a function to query them.\n",
    "\n",
    "### What Happens:\n",
    "\n",
    "- Initialize the ChromaDB client and create or retrieve the collection `\"my_collection\"`.\n",
    "- Extract `ids`, `documents`, `metadatas`, and `embeddings` from the DataFrame and upsert them into the collection.\n",
    "- Use the `retriever` function to perform semantic searches and return the most relevant code snippets as `Document` objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcd315bc-77db-48ab-af1d-3c732a25b92d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "INFO:core.vector_database.vector_store_writer:ChromaDB client initialized with persistent storage at ./chroma_db\n",
      "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "INFO:core.vector_database.vector_store_writer:ChromaDB collection 'my_collection' initialized with persistent storage.\n",
      "INFO:core.vector_database.vector_store_writer:Upserting 30 documents\n",
      "INFO:core.vector_database.vector_store_writer:âœ… Documents upserted successfully into ChromaDB.\n",
      "INFO:core.vector_database.vector_store_writer:âœ… Documents upserted successfully into ChromaDB.\n"
     ]
    }
   ],
   "source": [
    "writer = VectorStoreWriter(collection_name=\"my_collection\", verbose=False)\n",
    "writer.upsert_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "449b0963-690c-49e3-985e-8c4657487c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents in the collection: 30\n"
     ]
    }
   ],
   "source": [
    "collection = writer.collection\n",
    "document_count = writer.collection.count()\n",
    "print(f\"Total documents in the collection: {document_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0d2ac5-1ad2-4125-ad23-03708db54075",
   "metadata": {},
   "source": [
    "## Step 5: Code Generation Chain\n",
    "\n",
    "In this step, we build a LangChain pipeline to generate Python code from natural language questions using context retrieved from ChromaDB.\n",
    "\n",
    "### What Happens:\n",
    "\n",
    "- **Context Window Management**  \n",
    "  We use `get_context_window` to determine the model's token limit, which helps optimize retrieval and formatting.\n",
    "\n",
    "- **Smart Document Retrieval**  \n",
    "  The system now automatically adapts the number of documents retrieved based on the context window size:\n",
    "  - Small contexts (â‰¤4096 tokens): 3 documents\n",
    "  - Medium contexts (â‰¤8192 tokens): 5 documents  \n",
    "  - Large contexts (>8192 tokens): 8 documents\n",
    "\n",
    "- **Accurate Token Estimation**  \n",
    "  The system uses improved token counting that:\n",
    "  - Attempts to use the actual model tokenizer when possible\n",
    "  - Falls back to content-aware estimation (3.2-4.0 chars/token based on content type)\n",
    "  - Provides better accuracy than the previous fixed 4 chars/token ratio\n",
    "\n",
    "- **Multi-Layer Context Protection**  \n",
    "  Context overflow is prevented at multiple levels:\n",
    "  1. **Smart retrieval**: Fewer docs for smaller context windows\n",
    "  2. **Intelligent formatting**: The `format_multi_doc_context` uses 75% of context window (vs previous 70%)\n",
    "  3. **Emergency truncation**: Final safety check with smart break-point detection\n",
    "  4. **Document structure preservation**: Tries to break at section/paragraph boundaries\n",
    "\n",
    "- **Build the Chain**  \n",
    "  The chain takes two inputs: a question and the context retrieved from the vector store.  \n",
    "  The prompt is passed through the model, and the output is parsed into clean Python code using `StrOutputParser`.\n",
    "\n",
    "- **Execute and Print Output**  \n",
    "  The function `clean_and_print_code(result)` cleans up any formatting markers from the model's output and prints the final code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c912eb01-2743-424c-a916-1ec944cad7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the code description prompt template\n",
    "code_description_prompt = get_code_description_prompt(model_source)\n",
    "\n",
    "# Get the code generation prompt template\n",
    "code_generation_prompt = get_code_generation_prompt(model_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0655c86-9e32-41c6-b0f5-c288fb5af188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model context window: 4096 tokens\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = llm\n",
    "\n",
    "# Get the context window size of the model for use in retrieval and document formatting\n",
    "context_window = get_context_window(model)\n",
    "print(f\"Model context window: {context_window} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea3f4a5b-4534-462e-a38d-8ce5ba4f3437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract code information from retrieved documents\n",
    "def extract_code_info_from_docs(inputs):\n",
    "    # Get retrieval query - standardize on \"question\" for clarity\n",
    "    query = inputs.get(\"question\", \"\")\n",
    "    if not query:\n",
    "        query = inputs.get(\"query\", \"\")\n",
    "    \n",
    "    # Add debugging information\n",
    "    print(f\"Searching repository with query: '{query}'\")\n",
    "    \n",
    "    # Process the repository question with enhanced retrieval and formatting\n",
    "    # The process_repository_question now has smarter document count selection\n",
    "    result = process_repository_question(\n",
    "        query=query,\n",
    "        collection=collection,\n",
    "        context_window=context_window,\n",
    "        top_n=None\n",
    "    )\n",
    "    \n",
    "    print(f\"Retrieved {result['document_count']} relevant documents\")\n",
    "    \n",
    "    if result['document_count'] > 0:\n",
    "        # Get specialized prompt based on detected question types\n",
    "        question_types = result.get(\"question_types\", [])\n",
    "        specialized_prompt = get_specialized_prompt(question_types, model_source)\n",
    "        \n",
    "        # The context has already been properly managed by the improved format_multi_doc_context\n",
    "        context = result[\"context\"]\n",
    "        \n",
    "        # Final safety check using the new accurate token estimation\n",
    "        from src.utils import estimate_tokens_accurate, check_context_fits\n",
    "        \n",
    "        fits, estimated_tokens = check_context_fits(\n",
    "            text=context, \n",
    "            context_window=context_window, \n",
    "            model=model,\n",
    "            reserve_tokens=800  # Reserve for prompt template and response\n",
    "        )\n",
    "        \n",
    "        if not fits:\n",
    "            print(f\"Context still too large after processing: {estimated_tokens} tokens (limit: {context_window - 800})\")\n",
    "            # Emergency truncation with better accuracy\n",
    "            max_chars = int((context_window - 800) * 3.5)  # More accurate estimation\n",
    "            \n",
    "            # Smart truncation - try to preserve document structure\n",
    "            truncation_point = max_chars\n",
    "            # Look for section breaks first, then paragraph breaks, then sentences\n",
    "            for break_pattern in ['\\n## ', '\\n### ', '\\n\\n', '. ', '\\n']:\n",
    "                last_break = context[:truncation_point].rfind(break_pattern)\n",
    "                if last_break > truncation_point * 0.8:  # Must retain at least 80% of content\n",
    "                    truncation_point = last_break + len(break_pattern)\n",
    "                    break\n",
    "            \n",
    "            context = context[:truncation_point] + \"\\n\\n... (truncated to fit context window)\"\n",
    "            \n",
    "            # Verify the truncation worked\n",
    "            final_tokens = estimate_tokens_accurate(context, model)\n",
    "            print(f\"Truncated to {final_tokens} tokens\")\n",
    "        else:\n",
    "            print(f\"Context size OK: {estimated_tokens} tokens\")\n",
    "        \n",
    "        # Return the processed result with specialized prompt\n",
    "        print(f\"âœ… Found relevant files with question types: {', '.join(result['question_types'])}\")\n",
    "        return {\n",
    "            \"question\": query,\n",
    "            \"code\": result.get(\"primary_code\", \"\"),\n",
    "            \"filename\": result.get(\"primary_filename\", \"\"),\n",
    "            \"context\": context,\n",
    "            \"question_types\": question_types,\n",
    "            \"specialized_prompt\": specialized_prompt\n",
    "        }\n",
    "    else:\n",
    "        # If no documents found, return empty values\n",
    "        print(\"âŒ No relevant documents found in the repository\")\n",
    "        return {\n",
    "            \"question\": query,\n",
    "            \"code\": \"No code found\",\n",
    "            \"filename\": \"No filename found\",\n",
    "            \"context\": \"No relevant documents retrieved\"\n",
    "        }\n",
    "\n",
    "# Function to apply specialized prompt template\n",
    "def apply_specialized_prompt(inputs):\n",
    "    specialized_prompt = inputs.get(\"specialized_prompt\")\n",
    "    if specialized_prompt:\n",
    "        # Use the specialized prompt if available\n",
    "        return specialized_prompt.format(\n",
    "            question=inputs[\"question\"],\n",
    "            context=inputs[\"context\"]\n",
    "        )\n",
    "    else:\n",
    "        # Fall back to default prompt\n",
    "        return code_description_prompt.format(\n",
    "            question=inputs[\"question\"],\n",
    "            context=inputs[\"context\"]\n",
    "        )\n",
    "\n",
    "# Create the code description chain with dynamic prompt selection\n",
    "code_description_chain = extract_code_info_from_docs | code_description_prompt | model | StrOutputParser()\n",
    "\n",
    "# Create the code generation chain - doesn't need context from repository\n",
    "code_generation_chain = {\n",
    "    \"question\": lambda x: x[\"question\"],\n",
    "    \"context\": lambda x: \"\" \n",
    "} | code_generation_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9790269-881b-4dab-9d8d-0e18a4fa2326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_print_code(result: str):\n",
    "    cleaned = clean_code(result)\n",
    "    print(cleaned)\n",
    "    \n",
    "def print_description(result: str):\n",
    "    print(\"Code Description:\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d3701d9-a07b-4146-8afb-728209bca51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:core.extract_text.rag_utils:Question types: ['concept']\n",
      "INFO:core.extract_text.rag_utils:Expanded query: 'What is the repository about? '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching repository with query: 'What is the repository about?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79.3M/79.3M [00:02<00:00, 28.4MiB/s]\n",
      "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n",
      "INFO:core.extract_text.rag_utils:Retrieved 3 documents after filtering and re-ranking\n",
      "INFO:core.extract_text.rag_utils:Context budget: 3072 tokens (10752.0 chars)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 3 relevant documents\n",
      "Context size OK: 3282 tokens\n",
      "âœ… Found relevant files with question types: concept\n",
      "Code Description:\n",
      "Based on the code context provided, I can see that there are several projects and technologies listed, but I don't have enough information to determine which one is the main subject of the repository.\n",
      "\n",
      "Could you please provide more context or clarify which specific project or technology you would like me to focus on?\n",
      "\n",
      "# Webpage Image URL Scraping Code:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.parse\n",
      "class WebScraper:\n",
      "    def __init__(self, url):\n",
      "        self.url = url\n",
      "        self.image_urls = []\n",
      "    def send_request(self):\n",
      "        try:\n",
      "            response = requests.get(self.url)\n",
      "            response.raise_for_status()\n",
      "            return response.text\n",
      "        except requests.RequestException as e:\n",
      "            print(f\"Request failed: {e}\")\n",
      "            return None\n",
      "    def parse_html(self, html):\n",
      "        soup = BeautifulSoup(html, 'html.parser')\n",
      "        for img in soup.find_all('img'):\n",
      "            url = img.get('src')\n",
      "            if url and not urllib.parse.urlparse(url).netloc:\n",
      "                self.image_urls.append(url)\n",
      "        return self.image_urls\n",
      "def main():\n",
      "    scraper = WebScraper(\"https://www.example.com\")\n",
      "    html = scraper.send_request()\n",
      "    image_urls = scraper.parse_html(html)\n",
      "    print(image_urls)\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the code generation chains\n",
    "\n",
    "# 1. Set logging level to reduce HTTP request logs\n",
    "import logging\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "\n",
    "# 2. Define a code description example with a query to retrieve relevant code\n",
    "code_description_input = {\n",
    "    \"question\": \"What is the repository about?\"\n",
    "}\n",
    "\n",
    "# 3. Run the code description chain\n",
    "description_result = code_description_chain.invoke(code_description_input)\n",
    "\n",
    "# 4. Print the code description result\n",
    "print_description(description_result)\n",
    "\n",
    "# 5. Define a code generation example (no context needed)\n",
    "code_gen_input = {\n",
    "    \"question\": \"Write Python code to extract all image URLs from a webpage using BeautifulSoup\"\n",
    "}\n",
    "\n",
    "# 6. Use the generate_code_with_retries function with our specialized chain\n",
    "_, clean_code_output = generate_code_with_retries(\n",
    "    chain=code_generation_chain,\n",
    "    example_input=code_gen_input,\n",
    "    max_attempts=3,\n",
    "    min_code_length=10\n",
    ")\n",
    "\n",
    "# 7. Print only the final cleaned code result \n",
    "print(\"\\n# Webpage Image URL Scraping Code:\")\n",
    "print(clean_code_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32a53a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 17:44:40 - INFO - â±ï¸ Total execution time: 4m 27.12s\n",
      "2025-08-06 17:44:40 - INFO - âœ… Notebook execution completed successfully.\n"
     ]
    }
   ],
   "source": [
    "end_time: float = time.time()\n",
    "elapsed_time: float = end_time - start_time\n",
    "elapsed_minutes: int = int(elapsed_time // 60)\n",
    "elapsed_seconds: float = elapsed_time % 60\n",
    "\n",
    "logger.info(f\"â±ï¸ Total execution time: {elapsed_minutes}m {elapsed_seconds:.2f}s\")\n",
    "logger.info(\"âœ… Notebook execution completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a79aa0f",
   "metadata": {},
   "source": [
    "Built with â¤ï¸ using [**HP AI Studio**](https://hp.com/ai-studio)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
