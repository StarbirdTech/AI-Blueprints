{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a1d3d1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from core.markdown_correction_service import MarkdownCorrectionService\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src directory to system path\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "from utils import (\n",
    "    load_config_and_secrets,\n",
    ")\n",
    "\n",
    "# Create Logger\n",
    "logger = logging.getLogger(\"english-correction-notebook\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", \n",
    "                             datefmt=\"%Y-%m-%d %H:%M:%S\") \n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.propagate = False\n",
    "\n",
    "CONFIG_PATH = \"../configs/configs.yaml\"\n",
    "SECRETS_PATH = \"../configs/secrets.yaml\"\n",
    "LOCAL_MODEL_PATH = \"/home/jovyan/datafabric/llama3.1-8b-instruct/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\"\n",
    "\n",
    "config, secrets = load_config_and_secrets(CONFIG_PATH, SECRETS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49769695",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models import evaluate\n",
    "\n",
    "mlflow.set_experiment(\"markdown-correction-experiment\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"markdown-correction-run\") as run:\n",
    "    MarkdownCorrectionService.log_model(\n",
    "        llm_artifact=LOCAL_MODEL_PATH,\n",
    "        config_yaml=CONFIG_PATH,\n",
    "        secrets_yaml=SECRETS_PATH,\n",
    "    )\n",
    "\n",
    "    model_uri = f\"runs:/{run.info.run_id}/markdown_corrector\"\n",
    "    mlflow.register_model(model_uri, \"MarkdownCorrector\")\n",
    "\n",
    "    logger.info(f\"Model registered: MarkdownCorrector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96297375",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from mlflow.metrics import (\n",
    "    ari_grade_level,\n",
    "    flesch_kincaid_grade_level,\n",
    "    exact_match,\n",
    "    rouge1,\n",
    "    rougeL\n",
    ")\n",
    "from core.llm_metrics import (\n",
    "    semantic_similarity_metric,\n",
    "    grammar_error_count_metric,\n",
    "    grammar_error_rate_metric,\n",
    "    grammar_improvement_metric,\n",
    "    grammar_score_metric,\n",
    "    readability_improvement_metric,\n",
    "    llm_judge_metric,\n",
    "    llm_judge_metric_local,\n",
    "    generate_gpt_gold_standards\n",
    ")\n",
    "\n",
    "# Generate GPT gold standards\n",
    "print(\"Generating GPT gold standards...\")\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"results.json\", \"r\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "original_texts = [item[\"original\"] for item in results]\n",
    "\n",
    "# Pass API key to the function\n",
    "api_key = secrets.get(\"OPEN_AI_API_KEY\") if secrets else None\n",
    "gpt_gold_standards = generate_gpt_gold_standards(original_texts, api_key)\n",
    "\n",
    "# Create evaluation DataFrame\n",
    "eval_df = pd.DataFrame([\n",
    "    {\n",
    "        \"markdown\": original,\n",
    "        \"gpt_corrected\": gpt_gold  # GPT's correction as gold standard\n",
    "    }\n",
    "    for original, gpt_gold in zip(original_texts, gpt_gold_standards)\n",
    "])\n",
    "\n",
    "# Run evaluation\n",
    "results = mlflow.evaluate(\n",
    "    model=model_uri,\n",
    "    data=eval_df,\n",
    "    targets=\"gpt_corrected\",\n",
    "    feature_names=[\"markdown\"],\n",
    "    extra_metrics=[\n",
    "        ari_grade_level(),\n",
    "        flesch_kincaid_grade_level(),\n",
    "        exact_match(),\n",
    "        rouge1(),\n",
    "        rougeL(),\n",
    "        semantic_similarity_metric,\n",
    "        grammar_error_count_metric,\n",
    "        grammar_error_rate_metric,\n",
    "        grammar_improvement_metric,\n",
    "        grammar_score_metric,\n",
    "        readability_improvement_metric,\n",
    "        #llm_judge_metric,\n",
    "        llm_judge_metric_local\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger.info(\"Evaluation results:\")\n",
    "logger.info(results.metrics)\n",
    "mlflow.log_metrics(results.metrics)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
