{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9ff4be-1fee-47eb-8d09-611c29a7a83f",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-size: 50px;\">Vanilla RAG Chatbot with Langchain and Opik Evaluation</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbb3e26-125f-4151-9ed3-c84a3fd9081d",
   "metadata": {},
   "source": [
    "Retrieval-Augmented Generation (RAG) is an architectural approach that can enhance the effectiveness of large language model (LLM) applications using customized data. In this example, we use LangChain, an orchestrator for language pipelines, to build an assistant capable of loading information from a web page and use it for answering user questions. We'll also use the Opik platform to evaluate and observe the LLM responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a55c55-a1dc-4697-b920-2a81f1bb0f74",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "- Imports\n",
    "- Configurations\n",
    "- Verify Assets\n",
    "- Data Loading\n",
    "- Creation of Chunks\n",
    "- Retrieval\n",
    "- Model Setup\n",
    "- Chain Creation\n",
    "- Model Service "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f78b36-56e2-4596-8325-c75901f30c76",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7156d10f-d930-4be7-a9e8-15606d466460",
   "metadata": {},
   "source": [
    "By using our Local GenAI workspace image, many of the necessary libraries to work with RAG already come pre-installed - in our case, we just need to add the connector to work with PDF documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e700f51-9011-401c-90d3-a07ea8238955",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77853772-0239-40d0-94be-7a62fcb465c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/aistudio/lib/python3.12/site-packages/mlflow/pyfunc/utils/data_validation.py:168: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
      "  color_warning(\n",
      "/opt/conda/envs/aistudio/lib/python3.12/site-packages/mlflow/pyfunc/utils/data_validation.py:168: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
      "  color_warning(\n",
      "2025-07-02 05:26:33,514 - WARNING - USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# === Standard Library Imports ===\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# === MLflow integration ===\n",
    "import mlflow\n",
    "\n",
    "# Define the relative path to the 'core' directory (one level up from current working directory)\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "# === Import ChatbotService from project core ===\n",
    "from core.chatbot_service.chatbot_service import ChatbotService\n",
    "\n",
    "# === Third-Party Imports ===\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.schema.document import Document\n",
    "from langchain.document_loaders import WebBaseLoader, JSONLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import promptquality as pq\n",
    "import torch\n",
    "\n",
    "import opik\n",
    "from opik import Opik\n",
    "from opik.evaluation import evaluate_prompt\n",
    "from opik.evaluation.metrics import (\n",
    "    Hallucination,\n",
    "    AnswerRelevance,\n",
    "    ContextPrecision,\n",
    "    ContextRecall,\n",
    "    GEval,\n",
    ")\n",
    "\n",
    "# Define the relative path to the 'src' directory (one level up from current working directory)\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "# === Project-Specific Imports (from src) ===\n",
    "from src.local_judge import LangChainJudge\n",
    "from src.utils import (\n",
    "    load_config_and_secrets,\n",
    "    configure_proxy,\n",
    "    initialize_llm,\n",
    "    configure_hf_cache,\n",
    "    setup_opik_environment,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c43204-6cb2-48f3-ac72-f821f12585b8",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd3b8d2c-d0e1-4409-86dc-470c8454fff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "022490c4-8d8a-4c53-92ac-c62ea70a8209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logger\n",
    "logger = logging.getLogger(\"vanilla_rag_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", \n",
    "                              datefmt=\"%Y-%m-%d %H:%M:%S\") \n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75f2111d-5508-41ba-967c-256b40a9fb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"../configs/config.yaml\"\n",
    "SECRETS_PATH = \"../configs/secrets.yaml\"\n",
    "DATA_PATH = \"../data\"\n",
    "MLFLOW_EXPERIMENT_NAME = \"Wiki-Chatbot-Experiment\"\n",
    "MLFLOW_RUN_NAME = \"Wiki-Chatbot-Run\"\n",
    "LOCAL_MODEL_PATH = \"/home/jovyan/datafabric/llama3.1-8b-instruct/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\"\n",
    "LOCAL_MODEL_PATHV2 = \"/home/jovyan/datafabric/Qwen3-8B-Q4_K_M/Qwen3-8B-Q4_K_M.gguf\"\n",
    "LOCAL_MODEL_PATHV3 = \"/home/jovyan/datafabric/meta-llama-3.1-8b-instruct-q4_k_m/meta-llama-3.1-8b-instruct-q4_k_m.gguf\"\n",
    "DEMO_FOLDER = \"../demo\"\n",
    "MLFLOW_MODEL_NAME = \"Wiki-Chatbot-Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7384967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f912d15a-7aab-4620-8c89-33c8f07104f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 05:26:34 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "logger.info('Notebook execution started.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ca6361-dd3c-4778-9a9d-b11da7b20151",
   "metadata": {},
   "source": [
    "## Configuration of HuggingFace caches\n",
    "\n",
    "In the next cell, we configure HuggingFace cache, so that all the models downloaded from them are persisted locally, even after the workspace is closed. This is a future desired feature for AI Studio and the GenAI addon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "965e4ea4-084a-4f32-9f51-9ead89dd16fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure HuggingFace cache\n",
    "configure_hf_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41d47161-f197-409b-9a43-a3f59f1f695c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 05:26:34,664 - INFO - PyTorch version 2.7.1 available.\n",
      "2025-07-02 05:26:34,752 - INFO - Use pytorch device_name: cuda\n",
      "2025-07-02 05:26:34,753 - INFO - Load pretrained SentenceTransformer: intfloat/e5-large-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d86860217dfb4e0094e19c91c6643e59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b05805b9bd346ac9186543ae91749ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3730026f8f5445e38e687476f7cb30d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61da437d9df74d45ba1061e0bdf86298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e29b4009244f5992dacd0fdd534c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ee42a9d3d040fb87ffc668fae5475e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04585dc39234fab9278e30b596a682d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dbfbcf1b2db44738397c8ef2b462b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d9f0bce0b614dbf8d1273f1fbe25001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af5bf05946bb4948ab0ef42f16b3a283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/201 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize HuggingFace Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/e5-large-v2\",\n",
    "    cache_folder=\"/tmp/hf_cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7062ee0",
   "metadata": {},
   "source": [
    "## Configuration and Secrets Loading\n",
    "\n",
    "In this section, we load configuration parameters and API keys from separate YAML files. This separation helps maintain security by keeping sensitive information (API keys) separate from configuration settings.\n",
    "\n",
    "- **config.yaml**: Contains non-sensitive configuration parameters like model sources and URLs\n",
    "- **secrets.yaml**: Contains sensitive API keys for services like Galileo and HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e0b7464-ad76-47f2-94f0-e70c4d4cc380",
   "metadata": {},
   "outputs": [],
   "source": [
    "config, secrets = load_config_and_secrets(CONFIG_PATH, SECRETS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d80521-523e-4255-abe5-b4b75acdad80",
   "metadata": {},
   "source": [
    "## Opik Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f531346-8ea2-457a-b84d-ab8427d14090",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 05:27:57,764 - INFO - HTTP Request: GET https://www.comet.com/api/rest/v2/account-details \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to use \"c-jimmy1\" workspace? (Y/n) Y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Configuration saved to file: /home/jovyan/.opik.config\n"
     ]
    }
   ],
   "source": [
    "setup_opik_environment(config, secrets)\n",
    "\n",
    "opik.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e80960-54f9-4ec2-b062-6d49526494d1",
   "metadata": {},
   "source": [
    "# Verify Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4417e48c-2b51-4e61-b74d-8230caf2f7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 05:28:04 - INFO - Config is properly configured. \n",
      "2025-07-02 05:28:04 - INFO - Secrets is properly configured. \n",
      "2025-07-02 05:28:04 - INFO - Local Llama model is properly configured. \n"
     ]
    }
   ],
   "source": [
    "def log_asset_status(asset_path: str, asset_name: str, success_message: str, failure_message: str) -> None:\n",
    "    \"\"\"\n",
    "    Logs the status of a given asset based on its existence.\n",
    "\n",
    "    Parameters:\n",
    "        asset_path (str): File or directory path to check.\n",
    "        asset_name (str): Name of the asset for logging context.\n",
    "        success_message (str): Message to log if asset exists.\n",
    "        failure_message (str): Message to log if asset does not exist.\n",
    "    \"\"\"\n",
    "    if Path(asset_path).exists():\n",
    "        logger.info(f\"{asset_name} is properly configured. {success_message}\")\n",
    "    else:\n",
    "        logger.info(f\"{asset_name} is not properly configured. {failure_message}\")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=CONFIG_PATH,\n",
    "    asset_name=\"Config\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if the configs.yaml was propely connfigured in your project on AI Studio.\"\n",
    ")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=SECRETS_PATH,\n",
    "    asset_name=\"Secrets\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if the secrets.yaml was propely connfigured in your project on AI Studio.\"\n",
    ")\n",
    "log_asset_status(\n",
    "    asset_path=LOCAL_MODEL_PATH,\n",
    "    asset_name=\"Local Llama model\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please create and download the required assets in your project on AI Studio if you want to use local model.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d92659e-23ea-4965-bb27-58cd1dd0f3ec",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "\n",
    "In this step, we will use the Langchain framework to  extract the content from a local PDF file with the product documentation. Also, we have commented some example on how to use Web Loaders to load data from pages on the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb4c1589-285f-4e72-ab8e-5ae9fbd4043d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Started logging traces to the \"RAG_DS_DEMO\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=0197c99b-4157-73b2-98b4-e13129ed9285&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n"
     ]
    }
   ],
   "source": [
    "# === Verify existence of the data directory ===\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    raise FileNotFoundError(f\"'data' folder not found at path: {os.path.abspath(DATA_PATH)}\")\n",
    "\n",
    "@opik.track\n",
    "def load_wiki_data(DATA_PATH):\n",
    "    # === Wiki JSON with JSONLoader ===\n",
    "    wiki_loader = JSONLoader(\n",
    "        file_path=os.path.join(DATA_PATH, \"wiki_flat_structure.json\"),\n",
    "        jq_schema=\"to_entries[] | {source: .key, text: .value.content}\",  # adapt to your schema\n",
    "        text_content=False  # keeps original formatting; set True if you want only strings\n",
    "    )\n",
    "\n",
    "    docs = wiki_loader.load()\n",
    "\n",
    "    return docs\n",
    "\n",
    "docs = load_wiki_data(DATA_PATH)\n",
    "# === Optional: Load additional web-based documents ===\n",
    "# To use a different knowledge base, just change the URLs below\n",
    "\n",
    "# loader1 = WebBaseLoader(\"https://www.hp.com/us-en/workstations/ai-studio.html\")\n",
    "# data1 = loader1.load()\n",
    "\n",
    "# loader2 = WebBaseLoader(\"https://zdocs.datascience.hp.com/docs/aistudio\")\n",
    "# data2 = loader2.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc63ed05-192e-4f59-856c-cf9bff160fc8",
   "metadata": {},
   "source": [
    "# Creation of Chunks\n",
    "Here, we split the loaded documents into chunks, so we have smaller and more specific texts to add to our vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3dbd381-58b8-4569-aa75-b26d07556b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 05:28:04 - INFO - Chunks created: 3633 docs, avg_tokens=499\n"
     ]
    }
   ],
   "source": [
    "# === Initialize text splitter ===\n",
    "# - chunk_size: Maximum number of characters per text chunk.\n",
    "# - chunk_overlap: Number of overlapping characters between chunks.\n",
    "\n",
    "@opik.track\n",
    "def chunk_documents(docs, chunk_size=600, overlap=100):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "        separators=[\"\\n## \", \"\\n# \", \"\\n\\n\", \".\", \"!\", \"?\"]\n",
    "    )\n",
    "    splits = splitter.split_documents(docs)\n",
    "    return splits\n",
    "\n",
    "splits = chunk_documents(docs)\n",
    "def log_stage(name: str, docs: List[Document]):\n",
    "    logger.info(f\"{name}: {len(docs)} docs, avg_tokens={sum(len(d.page_content) for d in docs)/len(docs):.0f}\")\n",
    "# e.g. after splits\n",
    "log_stage(\"Chunks created\", splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7999ff1f-1f6a-452c-848e-e3c67adf7766",
   "metadata": {},
   "source": [
    "# Retrieval\n",
    "\n",
    "We transform the texts into embeddings and store them in a vector database. This allows us to perform similarity search, and proper retrieval of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e43ae88-252a-4cbe-96e4-081daa3dc25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 05:28:04,992 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2025-07-02 05:28:08,270 - INFO - HTTP Request: POST https://www.comet.com/opik/api/v1/private/spans/batch \"HTTP/1.1 204 No Content\"\n",
      "2025-07-02 05:28:08,632 - INFO - HTTP Request: POST https://www.comet.com/opik/api/v1/private/traces/batch \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50.5 s, sys: 1.61 s, total: 52.2 s\n",
      "Wall time: 47.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# === Create a vector database from document chunks ===\n",
    "vectordb = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "# === Configure the vector database as a retriever for querying ===\n",
    "retriever = vectordb.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"fetch_k\": 20, \"k\": 8}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58329084-640e-4b25-be70-ac1ec90467e9",
   "metadata": {},
   "source": [
    "# Model Setup\n",
    "\n",
    "In this notebook, we provide three different options for loading the model:\n",
    " * **local**: by loading the llama3.1-8b-instruct-Q8_0 model from the asset downloaded on the project\n",
    " * **hugging-face-local** by downloading a DeepSeek model from Hugging Face and running locally\n",
    " * **hugging-face-cloud** by accessing the Mistral model through Hugging Face cloud API (requires HuggingFace API key saved on secrets.yaml)\n",
    "\n",
    "This choice can be set in the config.yaml file. The model deployed on the bottom cells of this notebook will load the choice from the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65b1fa71-43ae-49e1-9f2f-ad730a00ca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_source = config[\"model_source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "774e18a8-bef4-4b67-9972-ceda2af6538d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.76 s, sys: 5.11 s, total: 6.88 s\n",
      "Wall time: 44.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "llm = initialize_llm(model_source, secrets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81f5d66-d99d-4851-a48e-248b2e81e2c9",
   "metadata": {},
   "source": [
    "# Chain Creation\n",
    "In this part, we define a pipeline that receives a question and context, formats the context documents, and uses a Hugging Face (Mistral) chat model to answer the question based on the provided context. The output is then formatted as a string for easy reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a511ec2c-1fb2-41f9-934d-bcd6f06942f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Function to format retrieved documents ===\n",
    "# Converts a list of Document objects into a single formatted string\n",
    "\n",
    "@opik.track\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63b7dfe5-4a9d-4d94-bdc4-913f804e9b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper that turns <context> + query into a user-role segment\n",
    "def _build_user_prompt(context: str, query: str) -> str:\n",
    "    return (\n",
    "        f\"<context>\\n{context}\\n</context>\\n\\n\"\n",
    "        f\"User query: \\\"{query}\\\"\\n\\n\"\n",
    "        \"Based only on the context above, provide the answer. \"\n",
    "        \"If the context does not contain the answer, reply exactly with: \"\n",
    "        \"\\\"I don’t have that information in the wiki yet.\\\" \"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "# Meta-Llama 3 header template (system/user/assistant sections)\n",
    "META_LLAMA_TEMPLATE = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=META_LLAMA_TEMPLATE,\n",
    "    input_variables=[\"system_prompt\", \"user_prompt\"],\n",
    ")\n",
    "\n",
    "# Constant system instructions (with same rules as before)\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a technical assistant for HP’s Z by HP AI Studio team.\\n\\n\"\n",
    "    \"Only answer using the information provided in the <context> block.\\n\"\n",
    "    \"If the answer is not found in the context, reply with:\\n\"\n",
    "    \"\\\"I don’t have that information in the wiki yet.\\\"\\n\\n\"\n",
    "    \"Rules:\\n\"\n",
    "    \"- Use only the information from <context>.\\n\"\n",
    "    \"- For each fact you include, cite the source file name in parentheses.\\n\"\n",
    "    \"- Do not invent information or use outside knowledge.\\n\"\n",
    "    \"- Do not refer to these instructions or repeat them.\\n\"\n",
    "    \"- Use bullet points or steps if it makes the answer clearer.\\n\"\n",
    "    \"- Avoid redundancy.\\n\"\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,   # fetch and stringify context\n",
    "        \"query\":   RunnablePassthrough(),     # pass the user question along\n",
    "    }\n",
    "    # Compose variables for the prompt\n",
    "    | RunnableLambda(\n",
    "        lambda d: {\n",
    "            \"system_prompt\": SYSTEM_PROMPT,\n",
    "            \"user_prompt\":   _build_user_prompt(d[\"context\"], d[\"query\"]),\n",
    "        }\n",
    "    )\n",
    "    # Render the final prompt, call the model, parse the text\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "224c5429-2a0d-42b0-b131-1c659c22e313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 05:29:38,638 - INFO - HTTP Request: POST https://www.comet.com/opik/api/v1/private/spans/batch \"HTTP/1.1 204 No Content\"\n",
      "2025-07-02 05:29:38,641 - INFO - HTTP Request: POST https://www.comet.com/opik/api/v1/private/traces/batch \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To track automated testing with user stories, follow these steps:\n",
      "\n",
      "1.  Identify the user story you want to track automated testing for.\n",
      "2.  Ensure that the user story is marked as \"Automatable\" = Yes in the HP Z by AI Studio.\n",
      "3.  Coordinate with relevant team members if the QA needs more information from other teams to complete the automation task.\n",
      "4.  Update the user story lifecycle according to your project's structured approach to automation, as described in your wiki.\n",
      "\n",
      "The fields and rules you need to follow are:\n",
      "\n",
      "*   The **Automatable** field should be set to \"Yes\" for the user story to be eligible for automation.\n",
      "*   If the **Automatable** field is marked as \"Yes,\" then the **Automation Tool** field will become required.\n",
      "*   When testing an automated user story, you need to ensure that the QA performs strict tests on the user story scope without further regression testing.\n",
      "\n",
      "This structured approach to automation helps ensure that all necessary steps are followed, making it easier to track progress, maintain visibility, and avoid missing key automation details during the User Story lifecycle.\n"
     ]
    }
   ],
   "source": [
    "# Invoke to test our RAG response quality\n",
    "question = \"How do i track automated testing with my user stories?\"\n",
    "answer = chain.invoke(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2059383-6977-4532-886d-bdb5953dfec0",
   "metadata": {},
   "source": [
    "# Opik Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176a68a5-ff0d-42bc-8d48-9460a08f8941",
   "metadata": {},
   "source": [
    "## Evaluate Hallucination, Answer Relevance, Context Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a383bd",
   "metadata": {},
   "source": [
    "Evaluation is a crucial step in ensuring the quality and reliability of the model's responses. In this section, we use the Opik platform to evaluate the model's performance on various metrics, including hallucination, answer relevance, context precision, and recall. We have two options for evaluation:\n",
    "- **Local Evaluation**: This option allows you to run the evaluation locally on your machine. Note that small models might not be able to handle the evaluation of large datasets, due to hallucination issues.\n",
    "- **API Evaluation**: This option allows you to use OPENAI's API to perform the evaluation, which is useful for larger datasets or more complex evaluations. To use this option, add a \"OPENAI_API_KEY\" field to your secrets.yaml file with your OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1356c93-e7e7-49c1-9963-cf26a0b4b3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional: Use Opik with OpenAI Models for evaluation. Setup API keys in secrets.yaml\n",
    "# metrics = [\n",
    "#     Hallucination(model=\"gpt-4o-mini\"),\n",
    "#     AnswerRelevance(model=\"gpt-4o-mini\"),\n",
    "#     ContextPrecision(model=\"gpt-4o-mini\"),\n",
    "#     ContextRecall(model=\"gpt-4o-mini\"),\n",
    "# ]\n",
    "\n",
    "# Setup for Opik Evaluation using Local Model as Judge. Note: Small models might not work well due to hallucination issues.\n",
    "# If you want to use a different model, change the LOCAL_MODEL_PATH variable above.\n",
    "hallucination_judge = LangChainJudge(LOCAL_MODEL_PATH, \"hallucination\")\n",
    "answer_relevance_judge = LangChainJudge(LOCAL_MODEL_PATH, \"answer_relevance\")\n",
    "context_precision_judge = LangChainJudge(LOCAL_MODEL_PATH, \"context_precision\")\n",
    "context_recall_judge = LangChainJudge(LOCAL_MODEL_PATH, \"context_recall\")\n",
    "\n",
    "metrics = [\n",
    "    Hallucination(model=hallucination_judge),\n",
    "    AnswerRelevance(model=answer_relevance_judge),\n",
    "    ContextPrecision(model=context_precision_judge),\n",
    "    ContextRecall(model=context_recall_judge),\n",
    "]\n",
    "\n",
    "def rag_with_full_eval(\n",
    "    question: str,\n",
    "    expected_answer: str | None = None\n",
    ") -> dict:\n",
    "    answer       = chain.invoke(question)\n",
    "    context_docs = [\n",
    "        d.page_content for d in retriever.get_relevant_documents(question)\n",
    "    ]\n",
    "\n",
    "    scores = {}\n",
    "    for m in metrics:\n",
    "        if isinstance(m, (ContextPrecision, ContextRecall)):\n",
    "            if expected_answer is None:\n",
    "                continue\n",
    "            result = m.score(\n",
    "                input=question,\n",
    "                output=answer,\n",
    "                expected_output=expected_answer,\n",
    "                context=context_docs,\n",
    "            )\n",
    "        else:\n",
    "            result = m.score(\n",
    "                input=question,\n",
    "                output=answer,\n",
    "                context=context_docs,\n",
    "            )\n",
    "        scores[m.__class__.__name__] = (\n",
    "            round(result.value, 3),\n",
    "            result.reason,\n",
    "        )\n",
    "\n",
    "    return {\"answer\": answer, \"scores\": scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84e7adb4-8794-46d0-81a6-ca9dd55e7a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 05:04:11,447 - INFO - HTTP Request: POST https://www.comet.com/opik/api/v1/private/spans/batch \"HTTP/1.1 204 No Content\"\n",
      "2025-07-02 05:04:11,448 - INFO - HTTP Request: POST https://www.comet.com/opik/api/v1/private/traces/batch \"HTTP/1.1 204 No Content\"\n",
      "2025-07-02 05:06:26,870 - DEBUG - Raw model output:\n",
      "Please provide your answer in the following format:\n",
      "{\n",
      "    \"score\": <your score between 0.0 and 1.0>,\n",
      "    \"reason\": [\"reason 1\", \"reason 2\"]\n",
      "}\n",
      "If you need to add more reasons, just append them to the list.\n",
      "For example:\n",
      "{\n",
      "    \"score\": 0.5,\n",
      "    \"reason\": [\n",
      "        \"The output does not introduce new information beyond what's provided in the context.\",\n",
      "        \"However, it also does not contradict any information given in the context.\"\n",
      "    ]\n",
      "}\n",
      "Please provide your answer in the same format.\n",
      "\n",
      "Here is my analysis of the OUTPUT:\n",
      "\n",
      "The OUTPUT contains a step-by-step guide on how to test AI Studio AI Blueprints. The guide includes creating a project in AI Studio and writing test files.\n",
      "\n",
      "However, upon closer inspection, I noticed that the OUTPUT does not mention anything about ensuring that the local YAML file is not present or about logging out and logging back into AI Studio as mentioned in the CONTEXT.\n",
      "\n",
      "Therefore, I would assign a hallucination score of 0.5 to the OUTPUT.\n",
      "\n",
      "Here is my answer in the required JSON format:\n",
      "\n",
      "{\n",
      "    \"score\": 0.5,\n",
      "    \"reason\": [\n",
      "        \"The output does not introduce new information beyond what's provided in the context.\",\n",
      "        \"However, it also does not contradict any information given in the context.\"\n",
      "    ]\n",
      "}\n",
      "2025-07-02 05:06:26,871 - DEBUG - Raw Score:\n",
      "0.5\n",
      "2025-07-02 05:06:27,742 - INFO - HTTP Request: POST https://www.comet.com/opik/api/v1/private/spans/batch \"HTTP/1.1 204 No Content\"\n",
      "2025-07-02 05:06:27,747 - INFO - HTTP Request: POST https://www.comet.com/opik/api/v1/private/traces/batch \"HTTP/1.1 204 No Content\"\n",
      "2025-07-02 05:09:30,437 - DEBUG - Raw model output:\n",
      " ***\n",
      "\n",
      "        ###OUTPUT FORMAT###\n",
      "        ***\n",
      "        Output:\n",
      "        {\n",
      "          \"answer_relevance_score\": 0.8,\n",
      "          \"reason\": \"The answer provides a step-by-step guide on how to test the AI Studio AI Blueprints, including creating a project in AI Studio, writing test files, and ensuring that the passkey is properly configured. However, it does not explicitly mention the importance of testing the blueprint projects in AI Studio, which slightly reduces its relevance.\"\n",
      "        }\n",
      "        ***\n",
      "     ***\n",
      "\n",
      "        ###FINAL ANSWER###\n",
      "        The final answer is $\\boxed{0.8}$. This score reflects the answer's alignment with the user's query and context, as well as its overall relevance to the topic at hand.\n",
      "\n",
      "        ###REASONING AND EXPLANATION###\n",
      "        The answer provided a step-by-step guide on how to test the AI Studio AI Blueprints, including creating a project in AI Studio, writing test files, and ensuring that the passkey is properly configured. However, it did not explicitly mention the importance of testing the blueprint projects in AI Studio, which slightly reduced its relevance.\n",
      "\n",
      "        The answer's alignment with the user's query and context was also taken into account when determining the final score. While the answer provided a comprehensive guide on how to test the AI Studio AI Blueprints, it could have been more directly relevant to the user's specific question if it had explicitly mentioned the importance of testing the blueprint projects in AI Studio.\n",
      "\n",
      "        Overall, while the answer provided a useful and informative response to the user's query, its relevance was slightly reduced due to its failure to explicitly mention the importance of testing the blueprint projects in AI Studio. Therefore, the final score assigned to this answer is 0.8.\n",
      "2025-07-02 05:09:30,438 - DEBUG - Raw Score:\n",
      "0.8\n",
      "2025-07-02 05:09:32,191 - INFO - HTTP Request: POST https://www.comet.com/opik/api/v1/private/spans/batch \"HTTP/1.1 204 No Content\"\n",
      "2025-07-02 05:09:37,602 - INFO - HTTP Request: POST https://www.comet.com/opik/api/v1/private/traces/batch \"HTTP/1.1 204 No Content\"\n",
      "2025-07-02 05:13:01,247 - DEBUG - Raw model output:\n",
      " - **Input:** \"How do test the AI Studio AI Blueprints to ensure that they work?\"\n",
      "     - **Output:** \"To test the AI Studio AI Blueprints, follow these steps: 1. Create a Project in AI Studio: If the blueprint is published, create a new project in AI Studio using the blueprint directly. 2. Write the Test Files: Each of the test files can be written as traditional Python unittest files (maybe, it is possible to also use other frameworks). However, if any notebook is referred by the code, a header written in YAML format should be added in the beginning.\"\n",
      "     - **Expected Output:** \"Create the project in AI Studio, finish setup, run all notebook cells, register/deploy the model, test the UIs, push the executed notebook and interface PDFs with test results, then open a pull request\"\n",
      "     - **Context:** [\"{\"source\": 36, \"text\": \"\\ud83e\\uddea Blueprint Testing Guide\\n====\\n\\nThis document outlines the **standard and comprehensive steps** for testing blueprint projects in the [AI-Blueprints GitHub repository](https://github.com/HPInc/AI-Blueprints).\\n\\n* * *\\n\\n\\u2705 Testing Workflow\\n------------------\\n\\nPlease follow the steps below when testing any blueprint project:\\n\\n### 1. Create a Project in AI Studio\\n\\n*   If the blueprint is published, create a new project in AI Studio using the blueprint directly\", '{\"source\": 502, \"text\": \"In this article you are going to learn how to create, train, and deploy machine learning models with AI Studio.\\n\\n# **Requirements:**\\nTo go through any of the guidelines you must have those requirements on your machine, however, each experiment may have additional requirements, so pay attention.\\n- AI Studio (v1.10.7+)\\\\n- Experiments Repository [Clone](https://github.com/passarel/aistudio-ds-experiments) This is the url for github not the clone url. \\\\nFor cloning down below use this url (and put into AI Studio): https://github.azc.ext.hp', '.\\\\n9. Save file and now license-checker should work. \\\\n\\\\n\\\\n\\\\nInstall the license-checker node package with:\\\\n\\\\n```\\\\nnpm install -g license-checker\\\\n```\\\\n\\\\nAnd then, after cloning a repo locally, `cd`\n",
      "2025-07-02 05:13:01,248 - DEBUG - Raw Score:\n",
      "-1.0\n",
      "OPIK: Failed to parse model output: Context precision score must be between 0.0 and 1.0, got -1.0\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/aistudio/lib/python3.12/site-packages/opik/evaluation/metrics/llm_judges/context_precision/parser.py\", line 15, in parse_model_output\n",
      "    raise exceptions.MetricComputationError(\n",
      "opik.exceptions.MetricComputationError: Context precision score must be between 0.0 and 1.0, got -1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Failed to calculate context precision score\n"
     ]
    }
   ],
   "source": [
    "demo_q = \"How do test the AI Studio AI Blueprints to ensure that they work?\"\n",
    "try:\n",
    "    expected_answer = \"Create the project in AI Studio, finish setup, run all notebook cells, register/deploy the model, test the UIs, push the executed notebook and interface PDFs with test results, then open a pull request\"\n",
    "    res = rag_with_full_eval(demo_q, expected_answer)\n",
    "    print(\"ASSISTANT:\", res[\"answer\"])\n",
    "    print(\"--- METRICS ---\")\n",
    "    for name, (val, why) in res[\"scores\"].items():\n",
    "        print(f\"{name:16} | {val} | {why}\")\n",
    "except Exception as e:\n",
    "    print(\"ERROR:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc6884d",
   "metadata": {},
   "source": [
    "# Model Service \n",
    "\n",
    "In this section, we demonstrate how to deploy a RAG-based chatbot service. This service provides a REST API endpoint that allows users to query the knowledge base with natural language questions, upload new documents to the knowledge base, and manage conversation history, all with built-in safeguards against sensitive information and toxicity. This service encapsulates all the functionality we developed in this notebook, including the document retrieval system, RAG-based question answering capabilities, and Galileo integration for protection, observation and evaluation. It demonstrates how to use our ChatbotService from the src/service directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "867f62bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/02 05:29:54 INFO mlflow.tracking.fluent: Experiment with name 'Wiki-Chatbot-Experiment' does not exist. Creating a new experiment.\n",
      "2025-07-02 05:29:54,305 - INFO - Use pytorch device_name: cuda\n",
      "2025-07-02 05:29:54,305 - INFO - Load pretrained SentenceTransformer: intfloat/e5-large-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da55f8386be46e7b3bd0eeb6cd48140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d92134710f4142ba0170a06edfb15b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfeae234627348efb16bcb1acbddff8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94afa24523464f869c55dd774132e1f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "018bb29bf23340fc829346867e9e7f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 05:31:29,130 - INFO - Model and artifacts successfully registered in MLflow.\n",
      "Successfully registered model 'Wiki-Chatbot-Model'.\n",
      "Created version '1' of model 'Wiki-Chatbot-Model'.\n",
      "2025-07-02 05:31:29 - INFO - ✅ Model registered successfully with run ID: 7f916757a3f641eebf5128757435adb2\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_tracking_uri('/phoenix/mlflow')\n",
    "# === Set MLflow experiment context ===\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\n",
    "\n",
    "# === Validate local model file path ===\n",
    "if not os.path.exists(LOCAL_MODEL_PATH):\n",
    "    logger.info(f\"⚠️ Warning: Model file not found at {LOCAL_MODEL_PATH}. Please verify the path.\")\n",
    "\n",
    "# === Log and register model to MLflow ===\n",
    "with mlflow.start_run(run_name=MLFLOW_RUN_NAME) as run:\n",
    "    \n",
    "    # Log model artifacts using custom ChatbotService\n",
    "    ChatbotService.log_model(\n",
    "        artifact_path=MLFLOW_MODEL_NAME,\n",
    "        config_path=CONFIG_PATH,\n",
    "        secrets_path=SECRETS_PATH,\n",
    "        docs_path=DATA_PATH,\n",
    "        model_path=LOCAL_MODEL_PATH,\n",
    "        demo_folder=DEMO_FOLDER\n",
    "    )\n",
    "\n",
    "    # Construct the URI for the logged model\n",
    "    model_uri = f\"runs:/{run.info.run_id}/{MLFLOW_MODEL_NAME}\"\n",
    "\n",
    "    # Register the model into MLflow Model Registry\n",
    "    mlflow.register_model(\n",
    "        model_uri=model_uri,\n",
    "        name=MLFLOW_MODEL_NAME\n",
    "    )\n",
    "\n",
    "    logger.info(f\"✅ Model registered successfully with run ID: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "519b97dc-ba54-4256-99fd-4639c505d185",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 05:31:29 - INFO - Notebook execution completed.\n"
     ]
    }
   ],
   "source": [
    "logger.info('Notebook execution completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8147f34-6e3b-47f2-9a68-7e8077f71618",
   "metadata": {},
   "source": [
    "Built with ❤️ using Z by HP AI Studio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aistudio]",
   "language": "python",
   "name": "conda-env-aistudio-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
