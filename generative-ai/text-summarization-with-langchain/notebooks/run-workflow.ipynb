{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f16ef5da",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<h1 style=\"text-align: center; font-size: 50px;\">Text Summarization with LangChain</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7156d10f-d930-4be7-a9e8-15606d466460",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to build a semantic chunking and summarization pipeline for texts using LangChain, sentence transformers for semantic chunking, and LLMs for generating summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db657805",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "- Start Execution\n",
    "- Install and Import Libraries\n",
    "- Configure Settings\n",
    "- Verify Assets\n",
    "- Data Loading\n",
    "- Semantic Chunking\n",
    "- Model Setup\n",
    "- Summarization Chain Creation\n",
    "- Model Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae08a185",
   "metadata": {},
   "source": [
    "# Start Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83818c3c-0efd-49af-a2fc-d4579e4daf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "# Configure logger\n",
    "logger: logging.Logger = logging.getLogger(\"run_workflow_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.propagate = False  # Prevent duplicate logs from parent loggers\n",
    "\n",
    "# Set formatter\n",
    "formatter: logging.Formatter = logging.Formatter(\n",
    "    fmt=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "# Configure and attach stream handler\n",
    "stream_handler: logging.StreamHandler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "207d0596-9e36-4956-96eb-79311c1fa63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-16 15:40:50 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()  \n",
    "\n",
    "logger.info(\"Notebook execution started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061633c9",
   "metadata": {},
   "source": [
    "# Install and Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bae027",
   "metadata": {},
   "source": [
    "Most of the libraries that are necessary for the development of this example are built-in on the GenAI workspace, available in AI Studio. More specific libraries to handle the type of input will be added here. In this case, we are giving support to texts in the webvtt format, used to store texts, which require the webvtt-py library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "023fbdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 23.2 ms, sys: 18.6 ms, total: 41.9 ms\n",
      "Wall time: 1.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "%pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "902fbba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# Define the relative path to the 'src' directory (two levels up from current working directory)\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "# Add 'src' directory to system path for module imports (e.g., utils)\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "# === Standard Library Imports ===\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# === Third-Party Imports ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import webvtt\n",
    "import mlflow\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import cosine\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from operator import itemgetter\n",
    "\n",
    "# === Project-Specific Imports (from src.utils) ===\n",
    "from src.utils import (\n",
    "    load_config,\n",
    "    load_secrets,\n",
    "    load_secrets_to_env,\n",
    "    configure_proxy,\n",
    "    initialize_llm,\n",
    "    configure_hf_cache\n",
    ")\n",
    "from src.prompt_templates import format_chunk_summarization_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9fbb14",
   "metadata": {},
   "source": [
    "# Configure Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "426355b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b055f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Constants ===\n",
    "# Model and experiment configuration\n",
    "SENTENCE_TRANSFORMER_MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "RUN_NAME = \"Text_Summarization_Service\"\n",
    "PROJECT_NAME = \"AIStudio_template_code_summarization\"\n",
    "EVALUATION_RUN_NAME = \"textrization_evaluation\"\n",
    "\n",
    "# Path configuration\n",
    "CONFIG_PATH = \"../configs/config.yaml\"\n",
    "SECRETS_PATH = \"../configs/secrets.yaml\"\n",
    "DATA_PATH = \"../data\"\n",
    "MODEL_PATH = \"/home/jovyan/datafabric/meta-llama3.1-8b-Q8/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\"\n",
    "\n",
    "# Text processing configuration\n",
    "CHUNK_SEPARATOR = \"\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9ff4be-1fee-47eb-8d09-611c29a7a83f",
   "metadata": {},
   "source": [
    "# Summarization of Texts with Langchain\n",
    "\n",
    "In this example, we intend to create a summarizer for long texts. The main goal is to break the original text into different chunks based on context - i.e. using an unsupervised approach to identify the different topics throughout the text (somehow similarly to Topic Modelling) - and summarize each of these chunks. in the end, the different summaries are returned to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ff65e3-dbdf-457a-8746-66c453182f26",
   "metadata": {},
   "source": [
    "### Configuration of Hugging face caches\n",
    "\n",
    "In the next cell, we configure HuggingFace cache, so that all the models downloaded from them are persisted locally, even after the workspace is closed. This is a future desired feature for AI Studio and the GenAI addon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d9e832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the src directory to the path to import utils\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../..\")))\n",
    "\n",
    "# Configure HuggingFace cache\n",
    "configure_hf_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3ad0fd",
   "metadata": {},
   "source": [
    "## Configuration and Secrets Loading\n",
    "\n",
    "In this section, we load configuration parameters and API keys from separate YAML files. This separation helps maintain security by keeping sensitive information (API keys) separate from configuration settings.\n",
    "\n",
    "- **config.yaml**: Contains non-sensitive configuration parameters like model sources and URLs\n",
    "- **secrets.yaml**: Contains sensitive API keys for services like HuggingFace\n",
    "- *(Optional for Premium users)* Secrets such as API keys for services like HuggingFace can be stored as environment variables for the project and loaded into the notebook (see the project's README file for steps on how to save secrets in Secrets Manager)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "713a24f4-01f4-4a33-8124-7d7601ced6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No secrets file found at ../configs/secrets.yaml; relying on preexisting environment\n",
      "✅ Configuration loaded successfully\n",
      "✅ Secrets loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load secrets from secrets.yaml file (if it exists) into environment\n",
    "if Path(SECRETS_PATH).exists():\n",
    "    load_secrets_to_env(SECRETS_PATH)\n",
    "else:\n",
    "    print(f\"No secrets file found at {SECRETS_PATH}; relying on preexisting environment\")\n",
    "\n",
    "# Retrieve secrets from environment\n",
    "try:\n",
    "    secrets = load_secrets()\n",
    "except ValueError:\n",
    "    secrets = {}\n",
    "\n",
    "# Load configuration and secrets\n",
    "config = load_config(CONFIG_PATH)\n",
    "\n",
    "print(\"✅ Configuration loaded successfully\")\n",
    "print(\"✅ Secrets loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95de597-657b-496e-a776-8d6e0f6be190",
   "metadata": {},
   "source": [
    "### Proxy Configuration\n",
    "\n",
    "For certain enterprise networks, accessing external services might require an explicit setup of the proxy configuration. If this is your case, set up the \"proxy\" field on your config.yaml and the following cell will configure the necessary environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ab31867-255e-489c-810d-42786bde5a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "configure_proxy(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4be835",
   "metadata": {},
   "source": [
    "# Verify Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54b28d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-16 15:40:58 - INFO - Local Llama model is properly configured. \n"
     ]
    }
   ],
   "source": [
    "def log_asset_status(asset_path: str, asset_name: str, success_message: str, failure_message: str) -> None:\n",
    "    \"\"\"\n",
    "    Logs the status of a given asset based on its existence.\n",
    "\n",
    "    Parameters:\n",
    "        asset_path (str): File or directory path to check.\n",
    "        asset_name (str): Name of the asset for logging context.\n",
    "        success_message (str): Message to log if asset exists.\n",
    "        failure_message (str): Message to log if asset does not exist.\n",
    "    \"\"\"\n",
    "    if Path(asset_path).exists():\n",
    "        logger.info(f\"{asset_name} is properly configured. {success_message}\")\n",
    "    else:\n",
    "        logger.info(f\"{asset_name} is not properly configured. {failure_message}\")\n",
    "\n",
    "\n",
    "# Check and log status for BERT model, embeddings file, and tokenizer\n",
    "log_asset_status(\n",
    "    asset_path=MODEL_PATH,\n",
    "    asset_name=\"Local Llama model\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please create and download the required assets in your project on AI Studio if you want to use local model.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d92659e-23ea-4965-bb27-58cd1dd0f3ec",
   "metadata": {},
   "source": [
    "## Step 1: Loading the data from the text\n",
    "\n",
    "At first, we need to read the data from the text. As our text is in the .vtt format, we use a library called webvtt-py to read the content. As the text is a trancript of audio/video, it is organized in small chunks of conversation, each containing a sequential id, the time of the start and end of the chunk, and the text content (often in the form speaker:content).\n",
    "\n",
    "From this data, we expect to extract the actual content,  while keeping reference to the other metadata - for this reason, we are loading all the data into a Pandas dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc16a213-9f92-4c75-93ff-66adc3133cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>speaker</th>\n",
       "      <th>content</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>I am happy to join with you today</td>\n",
       "      <td>00:00:00.880</td>\n",
       "      <td>00:00:03.920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>in what will go down in history</td>\n",
       "      <td>00:00:06.500</td>\n",
       "      <td>00:00:09.360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>as the greatest demonstration for freedom in t...</td>\n",
       "      <td>00:00:11.720</td>\n",
       "      <td>00:00:16.460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>nation.</td>\n",
       "      <td>00:00:16.460</td>\n",
       "      <td>00:00:17.293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td>Five score years ago,</td>\n",
       "      <td>00:00:26.410</td>\n",
       "      <td>00:00:28.740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id speaker                                            content         start  \\\n",
       "0  1                          I am happy to join with you today  00:00:00.880   \n",
       "1  2                            in what will go down in history  00:00:06.500   \n",
       "2  3          as the greatest demonstration for freedom in t...  00:00:11.720   \n",
       "3  4                                                    nation.  00:00:16.460   \n",
       "4  5                                      Five score years ago,  00:00:26.410   \n",
       "\n",
       "            end  \n",
       "0  00:00:03.920  \n",
       "1  00:00:09.360  \n",
       "2  00:00:16.460  \n",
       "3  00:00:17.293  \n",
       "4  00:00:28.740  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists(DATA_PATH):\n",
    "    raise FileNotFoundError(f\"'data' folder not found in path: {os.path.abspath(DATA_PATH)}\")\n",
    "\n",
    "file_path = os.path.join(DATA_PATH, \"I_have_a_dream.vtt\")\n",
    "\n",
    "data = {\n",
    "    \"id\": [],\n",
    "    \"speaker\": [],\n",
    "    \"content\": [],\n",
    "    \"start\": [],\n",
    "    \"end\": []\n",
    "}\n",
    "\n",
    "for caption in webvtt.read(file_path):\n",
    "    line = caption.text.split(\":\")\n",
    "    while len(line) < 2:\n",
    "        line = [''] + line\n",
    "    data[\"id\"].append(caption.identifier)\n",
    "    data[\"speaker\"].append(line[0].strip())\n",
    "    data[\"content\"].append(line[1].strip())\n",
    "    data[\"start\"].append(caption.start)\n",
    "    data[\"end\"].append(caption.end)\n",
    "    \n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbb6763-3eb0-41fb-900f-67778c3d5caf",
   "metadata": {},
   "source": [
    "As a second option, we provide here a code to load the same structure from a plain text document, which only contains the actual content of the speech/conversation, without extra metadata. For the sake of simplicity and reuse of code, we keep the same Data Frame structure as the previous version, by filling the remaining fields with empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1b33cbb-1c2b-404e-ad65-b243c6702308",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>speaker</th>\n",
       "      <th>content</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>﻿WEBVTT</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>00:00:00.880 --&gt; 00:00:03.920</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>&lt;v 0&gt;I am happy to join with you today&lt;/v&gt;</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id speaker                                     content start end\n",
       "0                                                ﻿WEBVTT          \n",
       "1                                                      1          \n",
       "2                          00:00:00.880 --> 00:00:03.920          \n",
       "3             <v 0>I am happy to join with you today</v>          \n",
       "4                                                      2          "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(file_path) as file:\n",
    "    lines = file.read()\n",
    "\n",
    "data = {\n",
    "    \"id\": [],\n",
    "    \"speaker\": [],\n",
    "    \"content\": [],\n",
    "    \"start\": [],\n",
    "    \"end\": []\n",
    "}\n",
    "\n",
    "for line in lines.split(\"\\n\"):\n",
    "    if line.strip() != \"\":\n",
    "        data[\"id\"].append(\"\")\n",
    "        data[\"speaker\"].append(\"\")\n",
    "        data[\"content\"].append(line.strip())\n",
    "        data[\"start\"].append(\"\")\n",
    "        data[\"end\"].append(\"\")        \n",
    "        \n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416e122c-5a54-4285-8788-be12fc86e278",
   "metadata": {},
   "source": [
    "## Step 2: Semantic chunking of the text\n",
    "Having the information content loaded according to the text format - with the text split into audio blocks, or into paragraphs, we now want to group these small blocks into relevant topics - so we can summarize each topic individually. Here, we are using a very simple approach for that, by using a semantic embedding of each sentence (using an embedding model from Hugging Face Sentence Transformers), and identifying the \"breaks\" among chunks as the ones with higher semantic distance. Notice that this method can be parameterized, to inform the number of topics or the best method to identify the breaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62c67feb-11f7-47ad-bdec-3ec252e51797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde4140bdb0a403c80389d29af5302a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94b1c4c8b984f4480ed6b31172c5508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a6bb58c8f94bb0ba3b823072e3679d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d2e36dd46db4f9ca507b73c2b15d95d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cdc2a83f3cc41cf89c5066c84298683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "713f0649432243ba95ff1de48c420d6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8bcea7a1864445cb5703a0802c5bef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3593fa3ad09e4b1a8bec226dec6036f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d44efb470bc467b93da44349a7e5e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec92fec29e3432085ee9931a084dba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f9babdd59c4ca3882efde2622e4142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_model = SentenceTransformer(SENTENCE_TRANSFORMER_MODEL_NAME)\n",
    "embeddings = embedding_model.encode(df.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5538aee-0233-4b58-a574-879dfa64a792",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSplitter():\n",
    "    \"\"\"\n",
    "    A class for semantically splitting text into coherent chunks based on embeddings.\n",
    "    This class uses embedding-based distance metrics to identify topic transitions in text.\n",
    "    \"\"\"\n",
    "    def __init__(self, content, embedding_model, method=\"number\", partition_count=10, quantile=0.9, clustering_method=None, n_clusters=None):\n",
    "        \"\"\"\n",
    "        Initialize the SemanticSplitter.\n",
    "        \n",
    "        Args:\n",
    "            content: List of text segments to process and split\n",
    "            embedding_model: Model to use for generating text embeddings\n",
    "            method: Chunking method - 'number' (fixed number of breaks), 'quantiles' (threshold-based), or 'clustering'\n",
    "            partition_count: Number of breaks to create when using 'number' method\n",
    "            quantile: Threshold quantile to use when using 'quantiles' method\n",
    "            clustering_method: Which clustering algorithm to use ('kmeans', 'hierarchical', None)\n",
    "            n_clusters: Number of clusters to create when using clustering method\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.content = content\n",
    "            self.embedding_model = embedding_model\n",
    "            self.partition_count = partition_count\n",
    "            self.quantile = quantile\n",
    "            self.clustering_method = clustering_method\n",
    "            self.n_clusters = n_clusters if n_clusters is not None else partition_count\n",
    "            \n",
    "            logger.info(f\"Encoding {len(content)} content items with embedding model\")\n",
    "            self.embeddings = embedding_model.encode(content)\n",
    "            logger.info(f\"Generated embeddings with shape: {self.embeddings.shape}\")\n",
    "            \n",
    "            # Calculate distances between consecutive embeddings\n",
    "            self.distances = [cosine(self.embeddings[i - 1], self.embeddings[i]) for i in range(1, len(self.embeddings))]\n",
    "            self.breaks = []\n",
    "            self.centroids = []\n",
    "            \n",
    "            # Load break points using the specified method\n",
    "            self.load_breaks(method=method)\n",
    "            logger.info(f\"Created {len(self.breaks)} breaks using method '{method}'\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing SemanticSplitter: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def centroid_distance(self, embedding_id, centroid_id):\n",
    "        \"\"\"\n",
    "        Calculate cosine distance between an embedding and a centroid.\n",
    "        \n",
    "        Args:\n",
    "            embedding_id: Index of the embedding to compare\n",
    "            centroid_id: Index of the centroid to compare\n",
    "            \n",
    "        Returns:\n",
    "            Cosine distance between the embedding and centroid\n",
    "        \"\"\"\n",
    "        if not self.centroids:\n",
    "            logger.warning(\"Centroids haven't been loaded. Call load_centroids() first.\")\n",
    "            return 1.0  # Return max distance if no centroids\n",
    "            \n",
    "        try:\n",
    "            return cosine(self.embeddings[embedding_id], self.centroids[centroid_id])\n",
    "        except IndexError as e:\n",
    "            logger.error(f\"Invalid index in centroid_distance: {str(e)}\")\n",
    "            return 1.0  # Return max distance on error\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in centroid_distance: {str(e)}\")\n",
    "            return 1.0  # Return max distance on error\n",
    "\n",
    "    def adjust_neighbors(self, window_size=3, distance_threshold=0.7):\n",
    "        \"\"\"\n",
    "        Adjust break points by examining neighboring segments to improve coherence.\n",
    "        This helps avoid breaking semantic units that should stay together.\n",
    "        \n",
    "        Args:\n",
    "            window_size: Number of neighboring segments to consider\n",
    "            distance_threshold: Threshold for merging nearby segments\n",
    "        \"\"\"\n",
    "        if not self.breaks:\n",
    "            logger.info(\"No breaks to adjust\")\n",
    "            return\n",
    "            \n",
    "        logger.info(f\"Adjusting {len(self.breaks)} breaks with window size {window_size}\")\n",
    "        \n",
    "        try:\n",
    "            adjusted_breaks = []\n",
    "            # Sort breaks to process them in order\n",
    "            sorted_breaks = sorted(self.breaks)\n",
    "            \n",
    "            for i, break_pos in enumerate(sorted_breaks):\n",
    "                # Skip if this break is too close to the previous adjusted break\n",
    "                if adjusted_breaks and break_pos - adjusted_breaks[-1] < window_size:\n",
    "                    continue\n",
    "                    \n",
    "                # Check surrounding context for better break point\n",
    "                best_pos = break_pos\n",
    "                best_dist = self.distances[break_pos]\n",
    "                \n",
    "                # Look at nearby positions for potentially better break points\n",
    "                start = max(0, break_pos - window_size)\n",
    "                end = min(len(self.distances) - 1, break_pos + window_size)\n",
    "                \n",
    "                for j in range(start, end + 1):\n",
    "                    if j != break_pos and self.distances[j] > best_dist:\n",
    "                        best_pos = j\n",
    "                        best_dist = self.distances[j]\n",
    "                        \n",
    "                # Add the optimized break position\n",
    "                adjusted_breaks.append(best_pos)\n",
    "                \n",
    "            self.breaks = sorted(list(set(adjusted_breaks)))\n",
    "            logger.info(f\"Adjusted breaks count: {len(self.breaks)}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error adjusting neighbors: {str(e)}\")\n",
    "            # Keep original breaks on error\n",
    "\n",
    "    def load_breaks(self, method='number'):\n",
    "        \"\"\"\n",
    "        Load break points based on the specified method.\n",
    "        \n",
    "        Args:\n",
    "            method: Method to determine breaks - 'number', 'quantiles', or 'clustering'\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if method == 'number':\n",
    "                # Ensure we don't request more breaks than possible\n",
    "                if self.partition_count > len(self.distances):\n",
    "                    logger.warning(f\"Requested {self.partition_count} breaks but only {len(self.distances)} positions available.\")\n",
    "                    self.partition_count = len(self.distances)\n",
    "                    \n",
    "                # Find the partition_count highest distance positions\n",
    "                self.breaks = np.sort(np.argpartition(self.distances, len(self.distances) - self.partition_count)[-self.partition_count:])\n",
    "                logger.info(f\"Created {len(self.breaks)} breaks using fixed number method\")\n",
    "                \n",
    "            elif method == 'quantiles':\n",
    "                # Find positions with distance above the quantile threshold\n",
    "                threshold = np.quantile(self.distances, self.quantile)\n",
    "                self.breaks = [i for i, v in enumerate(self.distances) if v >= threshold]\n",
    "                logger.info(f\"Created {len(self.breaks)} breaks using quantile method with threshold {threshold:.4f}\")\n",
    "                \n",
    "            elif method == 'clustering':\n",
    "                # Use clustering algorithms to group similar segments\n",
    "                self._cluster_embeddings()\n",
    "                logger.info(f\"Created {len(self.breaks)} breaks using clustering method\")\n",
    "                \n",
    "            else:\n",
    "                logger.warning(f\"Unknown method: {method}. No breaks created.\")\n",
    "                self.breaks = []\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading breaks with method '{method}': {str(e)}\")\n",
    "            self.breaks = []\n",
    "\n",
    "    def _cluster_embeddings(self):\n",
    "        \"\"\"\n",
    "        Cluster embeddings using the specified clustering method.\n",
    "        Sets breaks at the boundaries between clusters.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "            \n",
    "            if len(self.embeddings) < self.n_clusters:\n",
    "                logger.warning(f\"Not enough samples ({len(self.embeddings)}) for {self.n_clusters} clusters.\")\n",
    "                self.n_clusters = max(2, len(self.embeddings) // 2)\n",
    "            \n",
    "            # Choose clustering algorithm based on configuration\n",
    "            if self.clustering_method == 'kmeans':\n",
    "                logger.info(f\"Performing KMeans clustering with {self.n_clusters} clusters\")\n",
    "                clustering = KMeans(n_clusters=self.n_clusters, random_state=42, n_init=10)\n",
    "            else:  # Default to hierarchical clustering\n",
    "                logger.info(f\"Performing Hierarchical clustering with {self.n_clusters} clusters\")\n",
    "                clustering = AgglomerativeClustering(n_clusters=self.n_clusters)\n",
    "            \n",
    "            # Fit and predict cluster labels\n",
    "            labels = clustering.fit_predict(self.embeddings)\n",
    "            logger.info(f\"Clustering complete, found {len(set(labels))} clusters\")\n",
    "            \n",
    "            # Find transitions between clusters\n",
    "            all_transitions = []\n",
    "            for i in range(1, len(labels)):\n",
    "                if labels[i] != labels[i-1]:\n",
    "                    all_transitions.append((i-1, self.distances[i-1] if i-1 < len(self.distances) else 0))\n",
    "            \n",
    "            logger.info(f\"Found {len(all_transitions)} raw transitions between clusters\")\n",
    "            \n",
    "            # Filter transitions to avoid excessive fragmentation\n",
    "            \n",
    "            # Apply minimum chunk size (at least 3 segments per chunk)\n",
    "            min_chunk_size = 3\n",
    "            valid_transitions = []\n",
    "            last_break = -1\n",
    "            \n",
    "            for idx, (break_pos, _) in enumerate(all_transitions):\n",
    "                if break_pos - last_break >= min_chunk_size:\n",
    "                    valid_transitions.append((break_pos, all_transitions[idx][1]))\n",
    "                    last_break = break_pos\n",
    "            \n",
    "            # Sort by significance (distance) and limit to maximum number of breaks\n",
    "            max_breaks = min(self.partition_count, len(valid_transitions))\n",
    "            valid_transitions.sort(key=lambda x: x[1], reverse=True)\n",
    "            significant_breaks = [x[0] for x in valid_transitions[:max_breaks]]\n",
    "            \n",
    "            # Sort breaks in sequential order\n",
    "            self.breaks = sorted(significant_breaks)\n",
    "            logger.info(f\"After filtering: using {len(self.breaks)} breaks between clusters\")\n",
    "            \n",
    "        except ImportError:\n",
    "            logger.error(\"scikit-learn not available. Install it for clustering support.\")\n",
    "            self.breaks = []\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in clustering: {str(e)}\")\n",
    "            self.breaks = []\n",
    "\n",
    "    def get_centroid(self, beginning, end):\n",
    "        \"\"\"\n",
    "        Calculate centroid embedding for a range of content.\n",
    "        \n",
    "        Args:\n",
    "            beginning: Start index (inclusive)\n",
    "            end: End index (exclusive)\n",
    "            \n",
    "        Returns:\n",
    "            Centroid embedding for the specified content range\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if beginning >= end or beginning < 0 or end > len(self.content):\n",
    "                logger.warning(f\"Invalid range: {beginning}-{end}\")\n",
    "                return np.zeros(self.embeddings[0].shape)\n",
    "                \n",
    "            text = '\\n'.join(self.content[beginning:end])\n",
    "            return self.embedding_model.encode(text)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating centroid: {str(e)}\")\n",
    "            if len(self.embeddings) > 0:\n",
    "                return np.zeros(self.embeddings[0].shape)\n",
    "            return np.zeros(384)  # Default embedding size if unknown\n",
    "    \n",
    "    def load_centroids(self):\n",
    "        \"\"\"\n",
    "        Load centroids for each chunk after breaks have been calculated.\n",
    "        \"\"\"\n",
    "        logger.info(\"Loading centroids for chunks\")\n",
    "        try:\n",
    "            if len(self.breaks) == 0:\n",
    "                self.centroids = [self.get_centroid(0, len(self.content))]\n",
    "                logger.info(\"Created 1 centroid for the entire content\")\n",
    "            else:\n",
    "                self.centroids = []\n",
    "                beginning = 0\n",
    "                for break_position in sorted(self.breaks):\n",
    "                    self.centroids.append(self.get_centroid(beginning, break_position + 1))\n",
    "                    beginning = break_position + 1\n",
    "                self.centroids.append(self.get_centroid(beginning, len(self.content)))\n",
    "                logger.info(f\"Created {len(self.centroids)} centroids\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading centroids: {str(e)}\")\n",
    "            self.centroids = []\n",
    "\n",
    "    def get_chunk(self, beginning, end):\n",
    "        \"\"\"\n",
    "        Get content chunk between specified indices.\n",
    "        \n",
    "        Args:\n",
    "            beginning: Start index (inclusive)\n",
    "            end: End index (exclusive)\n",
    "            \n",
    "        Returns:\n",
    "            Chunk of content as a single string\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if beginning >= end or beginning < 0 or end > len(self.content):\n",
    "                logger.warning(f\"Invalid chunk range: {beginning}-{end}\")\n",
    "                return \"\"\n",
    "            return '\\n'.join(self.content[beginning:end])\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting chunk: {str(e)}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def get_chunks(self):\n",
    "        \"\"\"\n",
    "        Get all content chunks based on calculated breaks.\n",
    "        \n",
    "        Returns:\n",
    "            List of content chunks\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if len(self.breaks) == 0:\n",
    "                logger.info(\"No breaks found, returning entire content as single chunk\")\n",
    "                return [self.get_chunk(0, len(self.content))]\n",
    "            else:\n",
    "                chunks = []\n",
    "                beginning = 0\n",
    "                sorted_breaks = sorted(self.breaks)\n",
    "                for break_position in sorted_breaks:\n",
    "                    chunk = self.get_chunk(beginning, break_position + 1)\n",
    "                    chunks.append(chunk)\n",
    "                    beginning = break_position + 1\n",
    "                # Add the last chunk after the final break\n",
    "                chunks.append(self.get_chunk(beginning, len(self.content)))\n",
    "                logger.info(f\"Generated {len(chunks)} chunks from content\")\n",
    "                return chunks\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting chunks: {str(e)}\")\n",
    "            return [self.get_chunk(0, len(self.content))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff40c88",
   "metadata": {},
   "source": [
    "## Topic Segmentation with Clustering\n",
    "\n",
    "While the basic chunking method using cosine distances can be effective, it may produce noisy results for complex documents. To improve topic identification, we can use clustering algorithms like KMeans and Hierarchical Clustering to group semantically related content.\n",
    "\n",
    "The implementation offers two main clustering approaches:\n",
    "\n",
    "1. **KMeans clustering** - Groups embeddings into k clusters based on vector similarity\n",
    "2. **Hierarchical clustering** - Creates a tree of clusters by progressively merging similar groups\n",
    "\n",
    "These methods identify natural topic boundaries in the text by finding transitions between semantic clusters, which often produces more coherent topical chunks than simple distance-based approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a25ffc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-16 15:41:06 - INFO - Encoding 250 content items with embedding model\n",
      "2025-09-16 15:41:06 - INFO - Generated embeddings with shape: (250, 384)\n",
      "2025-09-16 15:41:06 - INFO - Performing KMeans clustering with 6 clusters\n",
      "2025-09-16 15:41:08 - INFO - Clustering complete, found 6 clusters\n",
      "2025-09-16 15:41:08 - INFO - Found 248 raw transitions between clusters\n",
      "2025-09-16 15:41:08 - INFO - After filtering: using 6 breaks between clusters\n",
      "2025-09-16 15:41:08 - INFO - Created 6 breaks using clustering method\n",
      "2025-09-16 15:41:08 - INFO - Created 6 breaks using method 'clustering'\n",
      "2025-09-16 15:41:08 - INFO - Generated 7 chunks from content\n"
     ]
    }
   ],
   "source": [
    "# Create a new instance of the SemanticSplitter with cosine distance method\n",
    "# splitter = SemanticSplitter(df.content, embedding_model, method=\"number\", partition_count=6)\n",
    "\n",
    "# Create a new instance of the SemanticSplitter with KMeans clustering\n",
    "splitter = SemanticSplitter(\n",
    "    content=df.content, \n",
    "    embedding_model=embedding_model, \n",
    "    method=\"clustering\",\n",
    "    clustering_method=\"kmeans\", \n",
    "    n_clusters=6,\n",
    "    partition_count=6\n",
    ")\n",
    "\n",
    "# Get chunks using KMeans clustering\n",
    "chunks = splitter.get_chunks()\n",
    "text = CHUNK_SEPARATOR.join(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc63ed05-192e-4f59-856c-cf9bff160fc8",
   "metadata": {},
   "source": [
    "## Step 3: Using a LLM model to Summarize each chunk\n",
    "In our example, we are going to summarize each individual chunk separately. This solution might be advantageous in different situations:\n",
    " * When the original text is too big , or the loaded model works with a context that is too small. In this scenario, breaking information into chunks are necessary to allow the model to be applied\n",
    " * When the user wants to make sure that all the separate topics of a conversation are covered into the summarized version. An extra step could be added to allow some verification or manual configuration of the chunks to allow the user to customize the output\n",
    "\n",
    "In this notebook, we provide three different options for loading the model:\n",
    " * **local**: by loading the Meta Llama 3.1 model with 8B parameters from the asset downloaded on the project\n",
    " * **hugging-face-local** by downloading a DeepSeek model from Hugging Face and running locally\n",
    " * **hugging-face-cloud** by accessing the Mistral model through Hugging Face cloud API (requires HuggingFace API key saved on secrets.yaml)\n",
    "\n",
    "This choice can be set in the config.yaml file. The model deployed on the bottom cells of this notebook will load the choice from the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3137ca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_source = config[\"model_source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "539fba44-6a64-40a1-88e6-d5cf1f5cc4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.14 s, sys: 2.27 s, total: 3.41 s\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "llm = initialize_llm(model_source, secrets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "043cdb8f-a70a-499a-a2d6-56c14d965169",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = format_chunk_summarization_prompt(model_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41a586d-fbf5-4551-b022-d50da386e74c",
   "metadata": {},
   "source": [
    "## Step 4: Create parallel chain to summarize the text\n",
    "\n",
    "In the following cell, we create a chain that will receive a single string with multiple chunks (separated by the declared separator), than:\n",
    "  * Break the input into separated chains - using the break_chunks function embedded in a RunnableLambda to be used in LangChain\n",
    "  * Run a Parallel Chain with the following elements for each chunk:\n",
    "    * Get an individual element\n",
    "    * Personalize the prompt template to create an individual prompt for each chunk\n",
    "    * Use the LLM inference to summarize the chunk\n",
    "  * Merge the individual summaries into a single one\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40e5cde3-b064-4280-8ada-8df68820a2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts prompt_template to LangChain object\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "def break_chunks(text):\n",
    "    \"\"\"\n",
    "    Split text into chunks using the predefined separator.\n",
    "    \"\"\"\n",
    "    return text.split(CHUNK_SEPARATOR)\n",
    "\n",
    "def process_chunk(chunk_text):\n",
    "    # Create a proper runnable chain for each chunk\n",
    "    chunk_chain = (\n",
    "        RunnablePassthrough.assign(context=lambda _: chunk_text)\n",
    "        | prompt \n",
    "        | llm\n",
    "    )\n",
    "    return chunk_chain.invoke({})\n",
    "\n",
    "def process_chunks(text):\n",
    "    chunks_list = break_chunks(text)\n",
    "    results = []\n",
    "    \n",
    "    logger.info(f\"Processing {len(chunks_list)} chunks\")\n",
    "    \n",
    "    for i, chunk in enumerate(chunks_list):\n",
    "        try:\n",
    "            result = process_chunk(chunk)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error processing chunk {i+1}: {str(e)}\"\n",
    "            logger.error(error_msg)\n",
    "            logger.error(f\"Exception type: {type(e).__name__}\")\n",
    "            import traceback\n",
    "            logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "            results.append(f\"Error: {str(e)}\")\n",
    "            \n",
    "    return \"\\n\\n\".join(results)\n",
    "\n",
    "lambda_break = RunnableLambda(break_chunks)\n",
    "\n",
    "def join_summaries(summaries_dict):\n",
    "    # Extract values from the dictionary and join them\n",
    "    joined_summaries = \"\\n\\n\".join([str(v) for v in summaries_dict.values()])\n",
    "    logger.info(f\"Joined {len(summaries_dict)} summaries\")\n",
    "    return joined_summaries\n",
    "\n",
    "lambda_join = RunnableLambda(join_summaries)\n",
    "\n",
    "# Create the complete chain\n",
    "chain = RunnableLambda(process_chunks) | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10d47fe-41b9-4f19-9b11-4ef2d3f2740f",
   "metadata": {},
   "source": [
    "## Step 5: Run the chain and evaluate quality metrics\n",
    "\n",
    "In this section, we call the created chain and implement local quality metrics evaluation. We create a local metric evaluation using HuggingFace's implementation of ROUGE (using the evaluate library) to measure the quality of the summarization.\n",
    "\n",
    "This approach provides:\n",
    "- **Local evaluation**: No external service dependencies required\n",
    "- **ROUGE metrics**: Industry-standard evaluation for summarization tasks\n",
    "- **Performance tracking**: Execution time monitoring for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c27bb40e-7823-490a-af94-0d8aae5e5886",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-16 15:42:19 - INFO - Starting text summarization and evaluation...\n",
      "2025-09-16 15:42:19 - INFO - Processing 7 chunks\n",
      "2025-09-16 15:42:29 - INFO - ✅ Summarization completed in 10.46 seconds\n",
      "2025-09-16 15:42:29 - ERROR - Error calculating ROUGE metrics: name 'evaluate' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "SUMMARIZATION RESULTS\n",
      "==================================================\n",
      "Original text length: 6560 characters\n",
      "Summary length: 1715 characters\n",
      "Compression ratio: 26.14%\n",
      "Processing time: 10.46 seconds\n",
      "\n",
      "ROUGE Scores:\n",
      "  ROUGE1: 0.0000\n",
      "  ROUGE2: 0.0000\n",
      "  ROUGEL: 0.0000\n",
      "==================================================\n",
      "\n",
      "Generated Summary:\n",
      "--------------------------------------------------\n",
      "The excerpt appears to be a speech, likely the \"I Have a Dream\" speech by Martin Luther King Jr. The speaker reflects on the progress made since the Emancipation Proclamation 100 years prior, but notes that despite this progress, African Americans still face significant challenges and inequalities in society.\n",
      "\n",
      "The excerpt is a poetic passage that describes the state of Mississippi as being sweltering with the heat of injustice.\n",
      "\n",
      "The excerpt is a passage from Martin Luther King Jr.'s famous \"I Have a Dream\" speech. In the passage, King expresses his vision of a future where people are judged not by the color of their skin but by the content of their character. He envisions a world where children of all colors can join hands and sing together in harmony.\n",
      "\n",
      "The excerpt is a quote from Martin Luther King Jr. It emphasizes the importance of freedom and equality in America, stating that for the country to be great, these values must become true.\n",
      "\n",
      "The excerpt is a poetic passage that calls for freedom to ring from prominent locations. Specifically, it mentions the \"prestigious hilltops of New Hampshire\" and the \"mighty mountains of New York\".\n",
      "\n",
      "The excerpt appears to be a poetic passage, likely from Martin Luther King Jr.'s famous \"I Have a Dream\" speech. The passage describes the call for freedom echoing from various geographical locations in the United States, including the mountains of Pennsylvania and Colorado.\n",
      "\n",
      "The excerpt appears to be a passage from Martin Luther King Jr.'s \"I Have a Dream\" speech. The text describes the vision of freedom and equality, where people of all backgrounds - black and white, Jew and Gentile, Protestant and Catholic - can join hands and sing together in harmony.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def evaluate_rouge_local(reference_text: str, prediction_text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate ROUGE metrics locally using HuggingFace evaluate library.\n",
    "    \n",
    "    Args:\n",
    "        reference_text: Original input text (reference)\n",
    "        prediction_text: Generated summary (prediction)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing ROUGE scores\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not reference_text or not prediction_text:\n",
    "            logger.warning(\"Empty reference or prediction text\")\n",
    "            return {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0}\n",
    "\n",
    "        # Calculate ROUGE metrics\n",
    "        rouge = evaluate.load(\"rouge\")\n",
    "        rouge_values = rouge.compute(\n",
    "            predictions=[prediction_text], \n",
    "            references=[reference_text]\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"ROUGE Scores - ROUGE-1: {rouge_values.get('rouge1', 0.0):.4f}, \"\n",
    "                   f\"ROUGE-2: {rouge_values.get('rouge2', 0.0):.4f}, \"\n",
    "                   f\"ROUGE-L: {rouge_values.get('rougeL', 0.0):.4f}\")\n",
    "        \n",
    "        return rouge_values\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calculating ROUGE metrics: {e}\")\n",
    "        return {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0}\n",
    "\n",
    "# Execute the summarization chain and measure performance\n",
    "logger.info(\"Starting text summarization and evaluation...\")\n",
    "\n",
    "# Measure execution time\n",
    "start_time = time.time()\n",
    "response = chain.invoke(text)\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "logger.info(f\"✅ Summarization completed in {total_time:.2f} seconds\")\n",
    "\n",
    "# Evaluate the summarization quality using ROUGE metrics\n",
    "rouge_scores = evaluate_rouge_local(text, response)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SUMMARIZATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Original text length: {len(text)} characters\")\n",
    "print(f\"Summary length: {len(response)} characters\")\n",
    "print(f\"Compression ratio: {len(response)/len(text):.2%}\")\n",
    "print(f\"Processing time: {total_time:.2f} seconds\")\n",
    "print(\"\\nROUGE Scores:\")\n",
    "for metric, score in rouge_scores.items():\n",
    "    print(f\"  {metric.upper()}: {score:.4f}\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nGenerated Summary:\")\n",
    "print(\"-\"*50)\n",
    "print(response)\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60bd4708-d7b4-4928-a9be-48679ef8748d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-16 15:42:29 - INFO - ⏱️ Total execution time: 0m 10.46s\n",
      "2025-09-16 15:42:29 - INFO - ✅ Notebook execution completed successfully.\n"
     ]
    }
   ],
   "source": [
    "end_time: float = time.time()\n",
    "elapsed_time: float = end_time - start_time\n",
    "elapsed_minutes: int = int(elapsed_time // 60)\n",
    "elapsed_seconds: float = elapsed_time % 60\n",
    "\n",
    "logger.info(f\"⏱️ Total execution time: {elapsed_minutes}m {elapsed_seconds:.2f}s\")\n",
    "logger.info(\"✅ Notebook execution completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57114bea",
   "metadata": {},
   "source": [
    "Built with ❤️ using [**HP AI Studio**](https://hp.com/ai-studio)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
