{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f16ef5da",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "<h1 style=\"text-align: center; font-size: 50px;\">Text Summarization with LangChain and Galileo</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7156d10f-d930-4be7-a9e8-15606d466460",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to build a semantic chunking and summarization pipeline for texts using LangChain, sentence transformers for semantic chunking, and LLMs for generating summaries. It also integrates with Galileo for evaluation, protection, and monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db657805",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "- Imports\n",
    "- Configurations\n",
    "- Verify Assets\n",
    "- Data Loading\n",
    "- Semantic Chunking\n",
    "- Model Setup\n",
    "- Summarization Chain Creation\n",
    "- Galileo Evaluate\n",
    "- Galileo Protect\n",
    "- Galileo Observe\n",
    "- Model Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061633c9",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bae027",
   "metadata": {},
   "source": [
    "Most of the libraries that are necessary for the development of this example are built-in on the GenAI workspace, available in AI Studio. More specific libraries to handle the type of input will be added here. In this case, we are giving support to texts in the webvtt format, used to store texts, which require the webvtt-py library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "023fbdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "902fbba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# Define the relative path to the 'src' directory (two levels up from current working directory)\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "# Add 'src' directory to system path for module imports (e.g., utils)\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "# === Standard Library Imports ===\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# === Third-Party Imports ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import webvtt\n",
    "import mlflow\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import cosine\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from operator import itemgetter\n",
    "import promptquality as pq\n",
    "import galileo_protect as gp\n",
    "from galileo_protect import ProtectTool, ProtectParser, Ruleset\n",
    "\n",
    "# === Project-Specific Imports (from src.utils) ===\n",
    "from src.utils import (\n",
    "    load_config_and_secrets,\n",
    "    configure_proxy,\n",
    "    initialize_llm,\n",
    "    setup_galileo_environment,\n",
    "    initialize_galileo_evaluator,\n",
    "    initialize_galileo_protect,\n",
    "    initialize_galileo_observer,\n",
    "    configure_hf_cache\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9fbb14",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "426355b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9ab1c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Create logger ===\n",
    "logger = logging.getLogger(\"summarization-notebook\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", \n",
    "                             datefmt=\"%Y-%m-%d %H:%M:%S\") \n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b055f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Constants ===\n",
    "# Model and experiment configuration\n",
    "SENTENCE_TRANSFORMER_MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "RUN_NAME = \"Text_Summarization_Service\"\n",
    "PROJECT_NAME = \"AIStudio_template_code_summarization\"\n",
    "EVALUATION_RUN_NAME = \"textrization_evaluation\"\n",
    "GALILEO_PROTECT_PROJECT_NAME = \"AIStudio-Summarizer-ProtectProject\"\n",
    "GALILEO_OBSERVE_PROJECT_NAME = \"AIStudio-Summarizer-ObserveProject\"\n",
    "\n",
    "# Path configuration\n",
    "CONFIG_PATH = \"../configs/config.yaml\"\n",
    "SECRETS_PATH = \"../configs/secrets.yaml\"\n",
    "DATA_PATH = \"../data\"\n",
    "MODEL_PATH = \"/home/jovyan/datafabric/llama2-7b/ggml-model-f16-Q5_K_M.gguf\"\n",
    "\n",
    "# Text processing configuration\n",
    "CHUNK_SEPARATOR = \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ae64372",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 18:04:12 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "logger.info('Notebook execution started.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9ff4be-1fee-47eb-8d09-611c29a7a83f",
   "metadata": {},
   "source": [
    "# Summarization of texts with Langchain\n",
    "\n",
    "In this example, we intend to create a summarizer for long texts. The main goal is to break the original text into different chunks based on context - i.e. using an unsupervised approach to identify the different topics throughout the text (somehow similarly to Topic Modelling) - and summarize each of these chunks. in the end, the different summaries are returned to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ff65e3-dbdf-457a-8746-66c453182f26",
   "metadata": {},
   "source": [
    "### Configuration of Hugging face caches\n",
    "\n",
    "In the next cell, we configure HuggingFace cache, so that all the models downloaded from them are persisted locally, even after the workspace is closed. This is a future desired feature for AI Studio and the GenAI addon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d9e832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the src directory to the path to import utils\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../..\")))\n",
    "\n",
    "# Configure HuggingFace cache\n",
    "configure_hf_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3ad0fd",
   "metadata": {},
   "source": [
    "### Configuration and Secrets Loading\n",
    "\n",
    "In this section, we load configuration parameters and API keys from separate YAML files. This separation helps maintain security by keeping sensitive information (API keys) separate from configuration settings.\n",
    "\n",
    "- **config.yaml**: Contains non-sensitive configuration parameters like model sources and URLs\n",
    "- **secrets.yaml**: Contains sensitive API keys for services like Galileo and HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "713a24f4-01f4-4a33-8124-7d7601ced6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "config, secrets = load_config_and_secrets(CONFIG_PATH, SECRETS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95de597-657b-496e-a776-8d6e0f6be190",
   "metadata": {},
   "source": [
    "### Proxy Configuration\n",
    "\n",
    "In order to connect to Galileo service, a SSH connection needs to be established. For certain enterprise networks, this might require an explicit setup of the proxy configuration. If this is your case, set up the \"proxy\" field on your config.yaml and the following cell will configure the necessary environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ab31867-255e-489c-810d-42786bde5a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "configure_proxy(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4be835",
   "metadata": {},
   "source": [
    "# Verify Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54b28d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 18:04:12 - INFO - Local Llama model is properly configured. \n"
     ]
    }
   ],
   "source": [
    "def log_asset_status(asset_path: str, asset_name: str, success_message: str, failure_message: str) -> None:\n",
    "    \"\"\"\n",
    "    Logs the status of a given asset based on its existence.\n",
    "\n",
    "    Parameters:\n",
    "        asset_path (str): File or directory path to check.\n",
    "        asset_name (str): Name of the asset for logging context.\n",
    "        success_message (str): Message to log if asset exists.\n",
    "        failure_message (str): Message to log if asset does not exist.\n",
    "    \"\"\"\n",
    "    if Path(asset_path).exists():\n",
    "        logger.info(f\"{asset_name} is properly configured. {success_message}\")\n",
    "    else:\n",
    "        logger.info(f\"{asset_name} is not properly configured. {failure_message}\")\n",
    "\n",
    "\n",
    "# Check and log status for BERT model, embeddings file, and tokenizer\n",
    "log_asset_status(\n",
    "    asset_path=MODEL_PATH,\n",
    "    asset_name=\"Local Llama model\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please create and download the required assets in your project on AI Studio if you want to use local model.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d92659e-23ea-4965-bb27-58cd1dd0f3ec",
   "metadata": {},
   "source": [
    "## Step 1: Loading the data from the text\n",
    "\n",
    "At first, we need to read the data from the text. As our text is in the .vtt format, we use a library called webvtt-py to read the content. As the text is a trancript of audio/video, it is organized in small chunks of conversation, each containing a sequential id, the time of the start and end of the chunk, and the text content (often in the form speaker:content).\n",
    "\n",
    "From this data, we expect to extract the actual content,  while keeping reference to the other metadata - for this reason, we are loading all the data into a Pandas dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc16a213-9f92-4c75-93ff-66adc3133cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>speaker</th>\n",
       "      <th>content</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>I am happy to join with you today</td>\n",
       "      <td>00:00:00.880</td>\n",
       "      <td>00:00:03.920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>in what will go down in history</td>\n",
       "      <td>00:00:06.500</td>\n",
       "      <td>00:00:09.360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>as the greatest demonstration for freedom in t...</td>\n",
       "      <td>00:00:11.720</td>\n",
       "      <td>00:00:16.460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>nation.</td>\n",
       "      <td>00:00:16.460</td>\n",
       "      <td>00:00:17.293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td>Five score years ago,</td>\n",
       "      <td>00:00:26.410</td>\n",
       "      <td>00:00:28.740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id speaker                                            content         start  \\\n",
       "0  1                          I am happy to join with you today  00:00:00.880   \n",
       "1  2                            in what will go down in history  00:00:06.500   \n",
       "2  3          as the greatest demonstration for freedom in t...  00:00:11.720   \n",
       "3  4                                                    nation.  00:00:16.460   \n",
       "4  5                                      Five score years ago,  00:00:26.410   \n",
       "\n",
       "            end  \n",
       "0  00:00:03.920  \n",
       "1  00:00:09.360  \n",
       "2  00:00:16.460  \n",
       "3  00:00:17.293  \n",
       "4  00:00:28.740  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists(DATA_PATH):\n",
    "    raise FileNotFoundError(f\"'data' folder not found in path: {os.path.abspath(DATA_PATH)}\")\n",
    "\n",
    "file_path = os.path.join(DATA_PATH, \"I_have_a_dream.vtt\")\n",
    "\n",
    "data = {\n",
    "    \"id\": [],\n",
    "    \"speaker\": [],\n",
    "    \"content\": [],\n",
    "    \"start\": [],\n",
    "    \"end\": []\n",
    "}\n",
    "\n",
    "for caption in webvtt.read(file_path):\n",
    "    line = caption.text.split(\":\")\n",
    "    while len(line) < 2:\n",
    "        line = [''] + line\n",
    "    data[\"id\"].append(caption.identifier)\n",
    "    data[\"speaker\"].append(line[0].strip())\n",
    "    data[\"content\"].append(line[1].strip())\n",
    "    data[\"start\"].append(caption.start)\n",
    "    data[\"end\"].append(caption.end)\n",
    "    \n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbb6763-3eb0-41fb-900f-67778c3d5caf",
   "metadata": {},
   "source": [
    "As a second option, we provide here a code to load the same structure from a plain text document, which only contains the actual content of the speech/conversation, without extra metadata. For the sake of simplicity and reuse of code, we keep the same Data Frame structure as the previous version, by filling the remaining fields with empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1b33cbb-1c2b-404e-ad65-b243c6702308",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>speaker</th>\n",
       "      <th>content</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>﻿WEBVTT</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>00:00:00.880 --&gt; 00:00:03.920</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>&lt;v 0&gt;I am happy to join with you today&lt;/v&gt;</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id speaker                                     content start end\n",
       "0                                                ﻿WEBVTT          \n",
       "1                                                      1          \n",
       "2                          00:00:00.880 --> 00:00:03.920          \n",
       "3             <v 0>I am happy to join with you today</v>          \n",
       "4                                                      2          "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(file_path) as file:\n",
    "    lines = file.read()\n",
    "\n",
    "data = {\n",
    "    \"id\": [],\n",
    "    \"speaker\": [],\n",
    "    \"content\": [],\n",
    "    \"start\": [],\n",
    "    \"end\": []\n",
    "}\n",
    "\n",
    "for line in lines.split(\"\\n\"):\n",
    "    if line.strip() != \"\":\n",
    "        data[\"id\"].append(\"\")\n",
    "        data[\"speaker\"].append(\"\")\n",
    "        data[\"content\"].append(line.strip())\n",
    "        data[\"start\"].append(\"\")\n",
    "        data[\"end\"].append(\"\")        \n",
    "        \n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416e122c-5a54-4285-8788-be12fc86e278",
   "metadata": {},
   "source": [
    "## Step 2: Semantic chunking of the text\n",
    "Having the information content loaded according to the text format - with the text split into audio blocks, or into paragraphs, we now want to group these small blocks into relevant topics - so we can summarize each topic individually. Here, we are using a very simple approach for that, by using a semantic embedding of each sentence (using an embedding model from Hugging Face Sentence Transformers), and identifying the \"breaks\" among chunks as the ones with higher semantic distance. Notice that this method can be parameterized, to inform the number of topics or the best method to identify the breaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62c67feb-11f7-47ad-bdec-3ec252e51797",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(SENTENCE_TRANSFORMER_MODEL_NAME)\n",
    "embeddings = embedding_model.encode(df.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5538aee-0233-4b58-a574-879dfa64a792",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSplitter():\n",
    "    \"\"\"\n",
    "    A class for semantically splitting text into coherent chunks based on embeddings.\n",
    "    This class uses embedding-based distance metrics to identify topic transitions in text.\n",
    "    \"\"\"\n",
    "    def __init__(self, content, embedding_model, method=\"number\", partition_count=10, quantile=0.9, clustering_method=None, n_clusters=None):\n",
    "        \"\"\"\n",
    "        Initialize the SemanticSplitter.\n",
    "        \n",
    "        Args:\n",
    "            content: List of text segments to process and split\n",
    "            embedding_model: Model to use for generating text embeddings\n",
    "            method: Chunking method - 'number' (fixed number of breaks), 'quantiles' (threshold-based), or 'clustering'\n",
    "            partition_count: Number of breaks to create when using 'number' method\n",
    "            quantile: Threshold quantile to use when using 'quantiles' method\n",
    "            clustering_method: Which clustering algorithm to use ('kmeans', 'hierarchical', None)\n",
    "            n_clusters: Number of clusters to create when using clustering method\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.content = content\n",
    "            self.embedding_model = embedding_model\n",
    "            self.partition_count = partition_count\n",
    "            self.quantile = quantile\n",
    "            self.clustering_method = clustering_method\n",
    "            self.n_clusters = n_clusters if n_clusters is not None else partition_count\n",
    "            \n",
    "            logger.info(f\"Encoding {len(content)} content items with embedding model\")\n",
    "            self.embeddings = embedding_model.encode(content)\n",
    "            logger.info(f\"Generated embeddings with shape: {self.embeddings.shape}\")\n",
    "            \n",
    "            # Calculate distances between consecutive embeddings\n",
    "            self.distances = [cosine(self.embeddings[i - 1], self.embeddings[i]) for i in range(1, len(self.embeddings))]\n",
    "            self.breaks = []\n",
    "            self.centroids = []\n",
    "            \n",
    "            # Load break points using the specified method\n",
    "            self.load_breaks(method=method)\n",
    "            logger.info(f\"Created {len(self.breaks)} breaks using method '{method}'\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing SemanticSplitter: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def centroid_distance(self, embedding_id, centroid_id):\n",
    "        \"\"\"\n",
    "        Calculate cosine distance between an embedding and a centroid.\n",
    "        \n",
    "        Args:\n",
    "            embedding_id: Index of the embedding to compare\n",
    "            centroid_id: Index of the centroid to compare\n",
    "            \n",
    "        Returns:\n",
    "            Cosine distance between the embedding and centroid\n",
    "        \"\"\"\n",
    "        if not self.centroids:\n",
    "            logger.warning(\"Centroids haven't been loaded. Call load_centroids() first.\")\n",
    "            return 1.0  # Return max distance if no centroids\n",
    "            \n",
    "        try:\n",
    "            return cosine(self.embeddings[embedding_id], self.centroids[centroid_id])\n",
    "        except IndexError as e:\n",
    "            logger.error(f\"Invalid index in centroid_distance: {str(e)}\")\n",
    "            return 1.0  # Return max distance on error\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in centroid_distance: {str(e)}\")\n",
    "            return 1.0  # Return max distance on error\n",
    "\n",
    "    def adjust_neighbors(self, window_size=3, distance_threshold=0.7):\n",
    "        \"\"\"\n",
    "        Adjust break points by examining neighboring segments to improve coherence.\n",
    "        This helps avoid breaking semantic units that should stay together.\n",
    "        \n",
    "        Args:\n",
    "            window_size: Number of neighboring segments to consider\n",
    "            distance_threshold: Threshold for merging nearby segments\n",
    "        \"\"\"\n",
    "        if not self.breaks:\n",
    "            logger.info(\"No breaks to adjust\")\n",
    "            return\n",
    "            \n",
    "        logger.info(f\"Adjusting {len(self.breaks)} breaks with window size {window_size}\")\n",
    "        \n",
    "        try:\n",
    "            adjusted_breaks = []\n",
    "            # Sort breaks to process them in order\n",
    "            sorted_breaks = sorted(self.breaks)\n",
    "            \n",
    "            for i, break_pos in enumerate(sorted_breaks):\n",
    "                # Skip if this break is too close to the previous adjusted break\n",
    "                if adjusted_breaks and break_pos - adjusted_breaks[-1] < window_size:\n",
    "                    continue\n",
    "                    \n",
    "                # Check surrounding context for better break point\n",
    "                best_pos = break_pos\n",
    "                best_dist = self.distances[break_pos]\n",
    "                \n",
    "                # Look at nearby positions for potentially better break points\n",
    "                start = max(0, break_pos - window_size)\n",
    "                end = min(len(self.distances) - 1, break_pos + window_size)\n",
    "                \n",
    "                for j in range(start, end + 1):\n",
    "                    if j != break_pos and self.distances[j] > best_dist:\n",
    "                        best_pos = j\n",
    "                        best_dist = self.distances[j]\n",
    "                        \n",
    "                # Add the optimized break position\n",
    "                adjusted_breaks.append(best_pos)\n",
    "                \n",
    "            self.breaks = sorted(list(set(adjusted_breaks)))\n",
    "            logger.info(f\"Adjusted breaks count: {len(self.breaks)}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error adjusting neighbors: {str(e)}\")\n",
    "            # Keep original breaks on error\n",
    "\n",
    "    def load_breaks(self, method='number'):\n",
    "        \"\"\"\n",
    "        Load break points based on the specified method.\n",
    "        \n",
    "        Args:\n",
    "            method: Method to determine breaks - 'number', 'quantiles', or 'clustering'\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if method == 'number':\n",
    "                # Ensure we don't request more breaks than possible\n",
    "                if self.partition_count > len(self.distances):\n",
    "                    logger.warning(f\"Requested {self.partition_count} breaks but only {len(self.distances)} positions available.\")\n",
    "                    self.partition_count = len(self.distances)\n",
    "                    \n",
    "                # Find the partition_count highest distance positions\n",
    "                self.breaks = np.sort(np.argpartition(self.distances, len(self.distances) - self.partition_count)[-self.partition_count:])\n",
    "                logger.info(f\"Created {len(self.breaks)} breaks using fixed number method\")\n",
    "                \n",
    "            elif method == 'quantiles':\n",
    "                # Find positions with distance above the quantile threshold\n",
    "                threshold = np.quantile(self.distances, self.quantile)\n",
    "                self.breaks = [i for i, v in enumerate(self.distances) if v >= threshold]\n",
    "                logger.info(f\"Created {len(self.breaks)} breaks using quantile method with threshold {threshold:.4f}\")\n",
    "                \n",
    "            elif method == 'clustering':\n",
    "                # Use clustering algorithms to group similar segments\n",
    "                self._cluster_embeddings()\n",
    "                logger.info(f\"Created {len(self.breaks)} breaks using clustering method\")\n",
    "                \n",
    "            else:\n",
    "                logger.warning(f\"Unknown method: {method}. No breaks created.\")\n",
    "                self.breaks = []\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading breaks with method '{method}': {str(e)}\")\n",
    "            self.breaks = []\n",
    "\n",
    "    def _cluster_embeddings(self):\n",
    "        \"\"\"\n",
    "        Cluster embeddings using the specified clustering method.\n",
    "        Sets breaks at the boundaries between clusters.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "            \n",
    "            if len(self.embeddings) < self.n_clusters:\n",
    "                logger.warning(f\"Not enough samples ({len(self.embeddings)}) for {self.n_clusters} clusters.\")\n",
    "                self.n_clusters = max(2, len(self.embeddings) // 2)\n",
    "            \n",
    "            # Choose clustering algorithm based on configuration\n",
    "            if self.clustering_method == 'kmeans':\n",
    "                logger.info(f\"Performing KMeans clustering with {self.n_clusters} clusters\")\n",
    "                clustering = KMeans(n_clusters=self.n_clusters, random_state=42, n_init=10)\n",
    "            else:  # Default to hierarchical clustering\n",
    "                logger.info(f\"Performing Hierarchical clustering with {self.n_clusters} clusters\")\n",
    "                clustering = AgglomerativeClustering(n_clusters=self.n_clusters)\n",
    "            \n",
    "            # Fit and predict cluster labels\n",
    "            labels = clustering.fit_predict(self.embeddings)\n",
    "            logger.info(f\"Clustering complete, found {len(set(labels))} clusters\")\n",
    "            \n",
    "            # Find transitions between clusters\n",
    "            all_transitions = []\n",
    "            for i in range(1, len(labels)):\n",
    "                if labels[i] != labels[i-1]:\n",
    "                    all_transitions.append((i-1, self.distances[i-1] if i-1 < len(self.distances) else 0))\n",
    "            \n",
    "            logger.info(f\"Found {len(all_transitions)} raw transitions between clusters\")\n",
    "            \n",
    "            # Filter transitions to avoid excessive fragmentation\n",
    "            \n",
    "            # Apply minimum chunk size (at least 3 segments per chunk)\n",
    "            min_chunk_size = 3\n",
    "            valid_transitions = []\n",
    "            last_break = -1\n",
    "            \n",
    "            for idx, (break_pos, _) in enumerate(all_transitions):\n",
    "                if break_pos - last_break >= min_chunk_size:\n",
    "                    valid_transitions.append((break_pos, all_transitions[idx][1]))\n",
    "                    last_break = break_pos\n",
    "            \n",
    "            # Sort by significance (distance) and limit to maximum number of breaks\n",
    "            max_breaks = min(self.partition_count, len(valid_transitions))\n",
    "            valid_transitions.sort(key=lambda x: x[1], reverse=True)\n",
    "            significant_breaks = [x[0] for x in valid_transitions[:max_breaks]]\n",
    "            \n",
    "            # Sort breaks in sequential order\n",
    "            self.breaks = sorted(significant_breaks)\n",
    "            logger.info(f\"After filtering: using {len(self.breaks)} breaks between clusters\")\n",
    "            \n",
    "        except ImportError:\n",
    "            logger.error(\"scikit-learn not available. Install it for clustering support.\")\n",
    "            self.breaks = []\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in clustering: {str(e)}\")\n",
    "            self.breaks = []\n",
    "\n",
    "    def get_centroid(self, beginning, end):\n",
    "        \"\"\"\n",
    "        Calculate centroid embedding for a range of content.\n",
    "        \n",
    "        Args:\n",
    "            beginning: Start index (inclusive)\n",
    "            end: End index (exclusive)\n",
    "            \n",
    "        Returns:\n",
    "            Centroid embedding for the specified content range\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if beginning >= end or beginning < 0 or end > len(self.content):\n",
    "                logger.warning(f\"Invalid range: {beginning}-{end}\")\n",
    "                return np.zeros(self.embeddings[0].shape)\n",
    "                \n",
    "            text = '\\n'.join(self.content[beginning:end])\n",
    "            return self.embedding_model.encode(text)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating centroid: {str(e)}\")\n",
    "            if len(self.embeddings) > 0:\n",
    "                return np.zeros(self.embeddings[0].shape)\n",
    "            return np.zeros(384)  # Default embedding size if unknown\n",
    "    \n",
    "    def load_centroids(self):\n",
    "        \"\"\"\n",
    "        Load centroids for each chunk after breaks have been calculated.\n",
    "        \"\"\"\n",
    "        logger.info(\"Loading centroids for chunks\")\n",
    "        try:\n",
    "            if len(self.breaks) == 0:\n",
    "                self.centroids = [self.get_centroid(0, len(self.content))]\n",
    "                logger.info(\"Created 1 centroid for the entire content\")\n",
    "            else:\n",
    "                self.centroids = []\n",
    "                beginning = 0\n",
    "                for break_position in sorted(self.breaks):\n",
    "                    self.centroids.append(self.get_centroid(beginning, break_position + 1))\n",
    "                    beginning = break_position + 1\n",
    "                self.centroids.append(self.get_centroid(beginning, len(self.content)))\n",
    "                logger.info(f\"Created {len(self.centroids)} centroids\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading centroids: {str(e)}\")\n",
    "            self.centroids = []\n",
    "\n",
    "    def get_chunk(self, beginning, end):\n",
    "        \"\"\"\n",
    "        Get content chunk between specified indices.\n",
    "        \n",
    "        Args:\n",
    "            beginning: Start index (inclusive)\n",
    "            end: End index (exclusive)\n",
    "            \n",
    "        Returns:\n",
    "            Chunk of content as a single string\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if beginning >= end or beginning < 0 or end > len(self.content):\n",
    "                logger.warning(f\"Invalid chunk range: {beginning}-{end}\")\n",
    "                return \"\"\n",
    "            return '\\n'.join(self.content[beginning:end])\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting chunk: {str(e)}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def get_chunks(self):\n",
    "        \"\"\"\n",
    "        Get all content chunks based on calculated breaks.\n",
    "        \n",
    "        Returns:\n",
    "            List of content chunks\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if len(self.breaks) == 0:\n",
    "                logger.info(\"No breaks found, returning entire content as single chunk\")\n",
    "                return [self.get_chunk(0, len(self.content))]\n",
    "            else:\n",
    "                chunks = []\n",
    "                beginning = 0\n",
    "                sorted_breaks = sorted(self.breaks)\n",
    "                for break_position in sorted_breaks:\n",
    "                    chunk = self.get_chunk(beginning, break_position + 1)\n",
    "                    chunks.append(chunk)\n",
    "                    beginning = break_position + 1\n",
    "                # Add the last chunk after the final break\n",
    "                chunks.append(self.get_chunk(beginning, len(self.content)))\n",
    "                logger.info(f\"Generated {len(chunks)} chunks from content\")\n",
    "                return chunks\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting chunks: {str(e)}\")\n",
    "            return [self.get_chunk(0, len(self.content))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff40c88",
   "metadata": {},
   "source": [
    "## Topic Segmentation with Clustering\n",
    "\n",
    "While the basic chunking method using cosine distances can be effective, it may produce noisy results for complex documents. To improve topic identification, we can use clustering algorithms like KMeans and Hierarchical Clustering to group semantically related content.\n",
    "\n",
    "The implementation offers two main clustering approaches:\n",
    "\n",
    "1. **KMeans clustering** - Groups embeddings into k clusters based on vector similarity\n",
    "2. **Hierarchical clustering** - Creates a tree of clusters by progressively merging similar groups\n",
    "\n",
    "These methods identify natural topic boundaries in the text by finding transitions between semantic clusters, which often produces more coherent topical chunks than simple distance-based approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a25ffc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 18:04:15 - INFO - Encoding 250 content items with embedding model\n",
      "2025-06-11 18:04:15 - INFO - Generated embeddings with shape: (250, 384)\n",
      "2025-06-11 18:04:15 - INFO - Performing KMeans clustering with 6 clusters\n",
      "2025-06-11 18:04:16 - INFO - Clustering complete, found 6 clusters\n",
      "2025-06-11 18:04:16 - INFO - Found 248 raw transitions between clusters\n",
      "2025-06-11 18:04:16 - INFO - After filtering: using 6 breaks between clusters\n",
      "2025-06-11 18:04:16 - INFO - Created 6 breaks using clustering method\n",
      "2025-06-11 18:04:16 - INFO - Created 6 breaks using method 'clustering'\n",
      "2025-06-11 18:04:16 - INFO - Generated 7 chunks from content\n"
     ]
    }
   ],
   "source": [
    "# Create a new instance of the SemanticSplitter with cosine distance method\n",
    "# splitter = SemanticSplitter(df.content, embedding_model, method=\"number\", partition_count=6)\n",
    "\n",
    "# Create a new instance of the SemanticSplitter with KMeans clustering\n",
    "splitter = SemanticSplitter(\n",
    "    content=df.content, \n",
    "    embedding_model=embedding_model, \n",
    "    method=\"clustering\",\n",
    "    clustering_method=\"kmeans\", \n",
    "    n_clusters=6,\n",
    "    partition_count=6\n",
    ")\n",
    "\n",
    "# Get chunks using KMeans clustering\n",
    "chunks = splitter.get_chunks()\n",
    "text = CHUNK_SEPARATOR.join(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc63ed05-192e-4f59-856c-cf9bff160fc8",
   "metadata": {},
   "source": [
    "## Step 3: Using a LLM model to Summarize each chunk\n",
    "In our example, we are going to summarize each individual chunk separately. This solution might be advantageous in different situations:\n",
    " * When the original text is too big , or the loaded model works with a context that is too small. In this scenario, breaking information into chunks are necessary to allow the model to be applied\n",
    " * When the user wants to make sure that all the separate topics of a conversation are covered into the summarized version. An extra step could be added to allow some verification or manual configuration of the chunks to allow the user to customize the output\n",
    "\n",
    "In this notebook, we provide three different options for loading the model:\n",
    " * **local**: by loading the llama2-7b model from the asset downloaded on the project\n",
    " * **hugging-face-local** by downloading a DeepSeek model from Hugging Face and running locally\n",
    " * **hugging-face-cloud** by accessing the Mistral model through Hugging Face cloud API (requires HuggingFace API key saved on secrets.yaml)\n",
    "\n",
    "This choice can be set in the config.yaml file. The model deployed on the bottom cells of this notebook will load the choice from the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3137ca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_source = config[\"model_source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "539fba44-6a64-40a1-88e6-d5cf1f5cc4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 877 ms, sys: 3.76 s, total: 4.64 s\n",
      "Wall time: 35.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "llm = initialize_llm(model_source, secrets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043cdb8f-a70a-499a-a2d6-56c14d965169",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = '''\n",
    "The following text is an excerpt of a text\n",
    "\n",
    "### \n",
    "{context} \n",
    "###\n",
    "\n",
    "Please, produce a single paragraph summarizing the given excerpt.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41a586d-fbf5-4551-b022-d50da386e74c",
   "metadata": {},
   "source": [
    "## Step 4: Create parallel chain to summarize the text\n",
    "\n",
    "In the following cell, we create a chain that will receive a single string with multiple chunks (separated by the declared separator), than:\n",
    "  * Break the input into separated chains - using the break_chunks function embedded in a RunnableLambda to be used in LangChain\n",
    "  * Run a Parallel Chain with the following elements for each chunk:\n",
    "    * Get an individual element\n",
    "    * Personalize the prompt template to create an individual prompt for each chunk\n",
    "    * Use the LLM inference to summarize the chunk\n",
    "  * Merge the individual summaries into a single one\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40e5cde3-b064-4280-8ada-8df68820a2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts prompt_template to LangChain object\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "def break_chunks(text):\n",
    "    \"\"\"\n",
    "    Split text into chunks using the predefined separator.\n",
    "    \"\"\"\n",
    "    return text.split(CHUNK_SEPARATOR)\n",
    "\n",
    "def process_chunk(chunk_text):\n",
    "    # Create a proper runnable chain for each chunk\n",
    "    chunk_chain = (\n",
    "        RunnablePassthrough.assign(context=lambda _: chunk_text)\n",
    "        | prompt \n",
    "        | llm\n",
    "    )\n",
    "    return chunk_chain.invoke({})\n",
    "\n",
    "def process_chunks(text):\n",
    "    chunks_list = break_chunks(text)\n",
    "    results = []\n",
    "    \n",
    "    logger.info(f\"Processing {len(chunks_list)} chunks\")\n",
    "    \n",
    "    for i, chunk in enumerate(chunks_list):\n",
    "        try:\n",
    "            result = process_chunk(chunk)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error processing chunk {i+1}: {str(e)}\"\n",
    "            logger.error(error_msg)\n",
    "            logger.error(f\"Exception type: {type(e).__name__}\")\n",
    "            import traceback\n",
    "            logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "            results.append(f\"Error: {str(e)}\")\n",
    "            \n",
    "    return \"\\n\\n\".join(results)\n",
    "\n",
    "lambda_break = RunnableLambda(break_chunks)\n",
    "\n",
    "def join_summaries(summaries_dict):\n",
    "    # Extract values from the dictionary and join them\n",
    "    joined_summaries = \"\\n\\n\".join([str(v) for v in summaries_dict.values()])\n",
    "    logger.info(f\"Joined {len(summaries_dict)} summaries\")\n",
    "    return joined_summaries\n",
    "\n",
    "lambda_join = RunnableLambda(join_summaries)\n",
    "\n",
    "# Create the complete chain\n",
    "chain = RunnableLambda(process_chunks) | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7d2a5d-35e3-46ed-a53e-ef49fc1c11a4",
   "metadata": {},
   "source": [
    "## Step 5: Connect to Galileo\n",
    "Through the Galileo library called Prompt Quality, we connect our API generated in the Galileo console to log in. To get your ApiKey, use this link: https://console.hp.galileocloud.io/api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7e666a9-311c-42d4-bc34-260333184ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://console.hp.galileocloud.io/\n",
      "👋 You have logged into 🔭 Galileo (https://console.hp.galileocloud.io/) as muhammed.turhan@hp.com.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Config(console_url=HttpUrl('https://console.hp.galileocloud.io/'), username=None, password=None, api_key=SecretStr('**********'), token=SecretStr('**********'), current_user='muhammed.turhan@hp.com', current_project_id=None, current_project_name=None, current_run_id=None, current_run_name=None, current_run_url=None, current_run_task_type=None, current_template_id=None, current_template_name=None, current_template_version_id=None, current_template_version=None, current_template=None, current_dataset_id=None, current_job_id=None, current_prompt_optimization_job_id=None, api_url=HttpUrl('https://api.hp.galileocloud.io/'))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#########################################\n",
    "# In order to connect to Galileo, create a secrets.yaml file in the same folder as this notebook\n",
    "# This file should be an entry called Galileo, with the your personal Galileo API Key\n",
    "# Galileo API keys can be created on https://console.hp.galileocloud.io/settings/api-keys\n",
    "#########################################\n",
    "\n",
    "setup_galileo_environment(secrets)\n",
    "print(os.environ['GALILEO_CONSOLE_URL'])\n",
    "pq.login(os.environ['GALILEO_CONSOLE_URL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10d47fe-41b9-4f19-9b11-4ef2d3f2740f",
   "metadata": {},
   "source": [
    "## Step 6: Run the chain and connect the metrics to Galileo\n",
    "\n",
    "In this session, we call the created chain and create the mechanisms to ingest the quality metrics into Galileo. For this example, we create a personalized metric (scorer), that will be running locally to measure the quality of the summarization. For this reason, we use HuggingFace implementation of ROUGE (using evaluate library), and implement into a CustomScorer from Galileo (next cell).\n",
    "\n",
    "Below, we illustrate two alternative ways to connect to Galileo:\n",
    "  * Using a customized run, which calculates the scores and logs into Galileo\n",
    "  * Using the langchain callback (currently unavailable due to compatibility issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c27bb40e-7823-490a-af94-0d8aae5e5886",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 18:04:53 - INFO - Processing 7 chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4a3784a3bb421f85db6163c64109ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing chain run...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial job complete, executing scorers asynchronously. Current status:\n",
      "cost: Done ✅\n",
      "toxicity: Done ✅\n",
      "sexist: Done ✅\n",
      "pii: Done ✅\n",
      "protect_status: Done ✅\n",
      "latency: Done ✅\n",
      "🔭 View your prompt run on the Galileo console at: https://console.hp.galileocloud.io/prompt/chains/10e816d1-b451-4bef-84a5-165b2b11f65e/1183a2ba-8b27-4ca2-866e-464ca15a97d2?taskType=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 18:05:13 - INFO - Calculated ROUGE-L score: 0.0969\n",
      "2025-06-11 18:05:13 - INFO - Calculated ROUGE-L score: 0.0969\n"
     ]
    }
   ],
   "source": [
    "def rouge_executor(row) -> float:\n",
    "    try:\n",
    "        logger.debug(f\"Processing ROUGE score for input/output pair\")\n",
    "        \n",
    "        # Try to decode node_input as JSON\n",
    "        try:\n",
    "            node_input = json.loads(row.node_input)\n",
    "            reference = node_input.get(\"content\", \"\").strip()\n",
    "        except json.JSONDecodeError:\n",
    "            # If not JSON, use the raw input text directly\n",
    "            reference = row.node_input.strip() \n",
    "            \n",
    "        # Handle node_output\n",
    "        try:\n",
    "            node_output = json.loads(row.node_output)\n",
    "            prediction = node_output.get(\"content\", \"\").strip()\n",
    "        except json.JSONDecodeError:\n",
    "            # If not JSON, use the raw output text directly\n",
    "            prediction = row.node_output.strip()\n",
    "\n",
    "        if not reference or not prediction:\n",
    "            logger.warning(\"Empty reference or prediction text\")\n",
    "            return 0.0\n",
    "\n",
    "        # Calculates ROUGE-L\n",
    "        rouge = evaluate.load(\"rouge\")\n",
    "        rouge_values = rouge.compute(predictions=[prediction], references=[reference])\n",
    "        \n",
    "        logger.info(f\"Calculated ROUGE-L score: {rouge_values.get('rougeL', 0.0):.4f}\")\n",
    "        return rouge_values.get(\"rougeL\", 0.0)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error in rouge_executor: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def rouge_aggregator(scores, indices) -> dict:\n",
    "    if len(scores) == 0:\n",
    "        return {'Average RougeL': 0.0}\n",
    "    else:\n",
    "        return {'Average RougeL': sum(scores) / len(scores)}\n",
    "\n",
    "# Define CustomScorer with corrected functions\n",
    "rouge_scorer = pq.CustomScorer(name='RougeL', executor=rouge_executor, aggregator=rouge_aggregator)\n",
    "\n",
    "# Configures the assessment execution\n",
    "partitioned_run = pq.EvaluateRun(\n",
    "    project_name=PROJECT_NAME,\n",
    "    run_name=RUN_NAME + \"_\" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    scorers=[pq.Scorers.toxicity, pq.Scorers.sexist, rouge_scorer]\n",
    ")\n",
    "# Measures execution time\n",
    "start_time = time.time()\n",
    "response = chain.invoke(text)\n",
    "total_time = int((time.time() - start_time) * 1000000)\n",
    "\n",
    "partitioned_run.add_workflow(input=text, output=response, duration_ns=total_time) \n",
    "partitioned_run.add_llm_step(input=text, output=response, duration_ns=total_time, model='local')\n",
    "\n",
    "# Finalizes the execution of the assessment\n",
    "partitioned_run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02e043a-4d8a-47ba-83b2-d99c0d803575",
   "metadata": {},
   "source": [
    "## Galileo Protect\n",
    "\n",
    "Galileo Protect serves as a powerful tool for safeguarding AI model outputs by detecting and preventing the release of sensitive information like personal addresses or other PII. By integrating Galileo Protect into your AI pipelines, you can ensure that model responses comply with privacy and security guidelines in real-time.\n",
    "\n",
    "Galileo functions as an API that provides support for protection verification of your chain/LLM. To log into the Galileo console, it is necessary to integrate it with another service, such as Galileo Evaluate or Galileo Observe.\n",
    "\n",
    "**Attention**: an integrated API within the Galileo console is required to perform this verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0a39a4c-6a04-4510-9c10-4b281566d6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👋 You have logged into 🔭 Galileo (https://console.hp.galileocloud.io/) as muhammed.turhan@hp.com.\n"
     ]
    }
   ],
   "source": [
    "# Create a project and stage for Galileo Protect\n",
    "project, project_id, stage_id = initialize_galileo_protect(GALILEO_PROTECT_PROJECT_NAME + datetime.now().strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc23d0eb-806a-4a47-b051-f45ba2466801",
   "metadata": {},
   "source": [
    "Galileo Protect works by creating rules that identify conditions such as Personally Identifiable Information (PII) and toxicity. It ensures that the prompt will not receive or respond to sensitive questions. In this example, we create a set of rules (ruleset) and a set of actions that return a pre-programmed response if a rule is triggered. Galileo Protect also offers a variety of other metrics to suit different protection needs. You can learn more about the available metrics here: [Supported Metrics and Operators](https://docs.rungalileo.io/galileo/gen-ai-studio-products/galileo-protect/how-to/supported-metrics-and-operators).\n",
    "\n",
    "Additionally, it is possible to import rulesets directly from Galileo through stages. Learn more about this feature here: [Invoking Rulesets](https://docs.rungalileo.io/galileo/gen-ai-studio-products/galileo-protect/how-to/invoking-rulesets).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23894954-93b0-4c06-9a2c-e3323fcaa3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking the chain with PII protection...\n",
      "Protected chain response:\n",
      "Personal Identifiable Information detected. Sorry, I cannot provide the response.\n"
     ]
    }
   ],
   "source": [
    "# Define a ruleset for PII detection (specifically SSN)\n",
    "pii_ruleset = Ruleset(\n",
    "    # Define the rules to check for potential issues\n",
    "    rules=[\n",
    "        {\n",
    "            \"metric\": \"pii\",  # Using Personal Identifiable Information metric\n",
    "            \"operator\": \"contains\",  # Check if PII contains specific type\n",
    "            \"target_value\": \"ssn\",  # Looking for Social Security Numbers\n",
    "        },\n",
    "    ],\n",
    "    # Define the action to take when rules are triggered\n",
    "    action={\n",
    "        \"type\": \"OVERRIDE\",  # Override the model response\n",
    "        \"choices\": [\n",
    "            \"Personal Identifiable Information detected. Sorry, I cannot provide the response.\"\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Initialize ProtectTool with the configured stage_id and ruleset\n",
    "protect_tool = ProtectTool(stage_id=stage_id, prioritized_rulesets=[pii_ruleset], timeout=10)\n",
    "\n",
    "# Use existing chain and combine with ProtectTool\n",
    "protect_parser = ProtectParser(chain=chain)  # 'chain' has already been defined previously\n",
    "protected_chain = protect_tool | protect_parser.parser\n",
    "\n",
    "# Example of using the protected chain with input and output\n",
    "input_data = {\n",
    "    \"input\": \"John Doe's social security number is 123-45-6789.\",\n",
    "    \"output\": \"John Doe's social security number is 123-45-6789.\"\n",
    "}\n",
    "\n",
    "# Invoke the protected chain\n",
    "print(\"Invoking the chain with PII protection...\")\n",
    "response = protected_chain.invoke(input_data)\n",
    "print(\"Protected chain response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6563e41c-c1a9-423e-a11f-5e7c6ac53770",
   "metadata": {},
   "source": [
    "## Galileo Observe\n",
    "\n",
    "Galileo Observe helps you monitor your generative AI applications in production. With Observe you will understand how your users are using your application and identify where things are going wrong. Keep tabs on your production system, instantly receive alerts when bad things happen, and perform deep root cause analysis though the Observe dashboard.\n",
    "\n",
    "You can connect Galileo Observe to your Langchain chain to monitor metrics such as cost and guardrail indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "418cbf0e-6c9d-4120-b9e3-17b376b3b08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Creating new project... project AIStudio-Summarizer-ObserveProject2025-06-11 18:05:18 created!\n",
      "Invoking the chain with Galileo Observe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 18:05:23 - INFO - Joined 1 summaries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed chain output:\n",
      "\n",
      "The excerpt describes the impact of technology and innovation on society, particularly in the fields of artificial intelligence (AI) and renewable energy. The author argues that AI is shaping the future by automating jobs, changing industries, and improving people's lives. Additionally, the excerpt highlights the growing importance of renewable energy sources, such as solar and wind power, which are becoming increasingly cost-competitive with fossil fuels. The author notes that the widespread adoption of renewable energy is transforming societies and economies worldwide, leading to a more sustainable and environmentally friendly future.\n"
     ]
    }
   ],
   "source": [
    "example_query = \"\"\"Tell me a story about technology and innovation. \n",
    "Explain how artificial intelligence is shaping the future. \n",
    "Summarize the impact of renewable energy on society.\"\"\"\n",
    "\n",
    "result_break = lambda_break.invoke(example_query)\n",
    "\n",
    "chain = lambda_break | {\n",
    "    f\"summary_{i}\": itemgetter(i) | prompt | llm\n",
    "    for i in range(len(result_break))\n",
    "} | lambda_join | StrOutputParser()\n",
    "\n",
    "monitor_handler = initialize_galileo_observer(GALILEO_OBSERVE_PROJECT_NAME + datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "print(\"Invoking the chain with Galileo Observe...\")\n",
    "try:\n",
    "    output = chain.invoke(\n",
    "        example_query,\n",
    "        config={\"callbacks\": [monitor_handler]}\n",
    "    )\n",
    "    print(\"Observed chain output:\")\n",
    "    print(output)\n",
    "except Exception as e:\n",
    "    print(f\"Error during chain execution: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2910bddc-a194-4011-b82d-3a8cd6ea762d",
   "metadata": {},
   "source": [
    "## Model service Galileo Protect + Observe\n",
    "\n",
    "In this example, we illustrate a different approach to create a text summarizer. Instead of splitting the text into topics and summarize the topics individually, this model service provides a REST API endpoint to allow summarization of an entire text, in a single call to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c1ce88",
   "metadata": {},
   "source": [
    "## Text Summarization Service\n",
    "\n",
    "This section demonstrates how to use our TextSummarizationService from the src/service directory. This approach improves code organization by separating the service implementation from the notebook, making it easier to maintain and update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90608265-b1d2-497d-b9f7-cbb1aef796bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/mlflow/pyfunc/utils/data_validation.py:168: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
      "  color_warning(\n",
      "/opt/conda/lib/python3.12/site-packages/mlflow/pyfunc/utils/data_validation.py:168: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
      "  color_warning(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23b1fecb2a354383a13efd8208cefe9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c268712f60994d3e8fd04447e7c9ec4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db69dd2be0a747899d2607c862e30db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1546117e432407d9d5dd1c463ae1191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'Text_Summarization_Service'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model registered successfully with run ID: 8e8ce255a31a48f88f0933e9623de958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '1' of model 'Text_Summarization_Service'.\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "# In case you just want to run this cell without the rest of the notebook \n",
    "# (you still need to install the requirements and run the import block), run the following block:\n",
    "# CONFIG_PATH = \"../configs/config.yaml\"\n",
    "# SECRETS_PATH = \"../configs/secrets.yaml\"\n",
    "# MODEL_PATH = \"/home/jovyan/datafabric/llama2-7b/ggml-model-f16-Q5_K_M.gguf\"\n",
    "# config, secrets = load_config_and_secrets(CONFIG_PATH, SECRETS_PATH)\n",
    "\n",
    "# Import the TextSummarizationService class\n",
    "from core.service.text_summarization_service import TextSummarizationService\n",
    "\n",
    "mlflow.set_tracking_uri('/phoenix/mlflow')\n",
    "# Set up the MLflow experiment\n",
    "mlflow.set_experiment(\"Summarization_Service\")\n",
    "\n",
    "# Define path to the model\n",
    "\n",
    "# Check if the model file exists\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    print(f\"Warning: Model file not found at {MODEL_PATH}. You may need to update the path.\")\n",
    "\n",
    "# Define demo folder path\n",
    "DEMO_FOLDER = \"../demo\"\n",
    "\n",
    "#Only logs the model path in the case where it is local\n",
    "if config[\"model_source\"] == \"local\":\n",
    "    model_path = MODEL_PATH\n",
    "else:\n",
    "    model_path = None\n",
    "\n",
    "\n",
    "# Use the TextSummarizationService's log_model method to register the model in MLflow\n",
    "with mlflow.start_run(run_name=\"Text_Summarization_Service\") as run:\n",
    "    # Log and register the model using the service's classmethod\n",
    "    TextSummarizationService.log_model(\n",
    "        artifact_path=\"text_summarization_service\",\n",
    "        secrets_path=SECRETS_PATH,\n",
    "        config_path=CONFIG_PATH,\n",
    "        model_path=model_path,\n",
    "        demo_folder=DEMO_FOLDER\n",
    "    )\n",
    "    \n",
    "    # Register the model in MLflow Model Registry\n",
    "    model_uri = f\"runs:/{run.info.run_id}/text_summarization_service\"\n",
    "    mlflow.register_model(model_uri=model_uri, name=\"Text_Summarization_Service\")\n",
    "    print(f\"Model registered successfully with run ID: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57114bea",
   "metadata": {},
   "source": [
    "Built with ❤️ using Z by HP AI Studio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
