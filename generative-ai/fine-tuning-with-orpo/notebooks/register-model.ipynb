{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c599d9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "# Configure logger\n",
    "logger: logging.Logger = logging.getLogger(\"register_model_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.propagate = False  # Prevent duplicate logs from parent loggers\n",
    "\n",
    "# Set formatter\n",
    "formatter: logging.Formatter = logging.Formatter(\n",
    "    fmt=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "# Configure and attach stream handler\n",
    "stream_handler: logging.StreamHandler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c9dc43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 23:37:29 - INFO - Model registration notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()  \n",
    "\n",
    "logger.info(\"Model registration notebook execution started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9162d3a3",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-size: 50px;\"> Fine-Tuned Model Registration Service </h1>\n",
    "\n",
    "This notebook demonstrates how to register a fine-tuned LLM comparison service that allows switching between base and fine-tuned models through a single MLflow endpoint. This follows the same pattern used across all AI-Blueprints for consistent model deployment and serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07d2e80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5add58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 23:38:00.643878: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-21 23:38:00.663848: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753141080.687826    7848 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753141080.694770    7848 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753141080.711440    7848 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753141080.711460    7848 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753141080.711462    7848 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753141080.711463    7848 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-21 23:38:00.717267: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-21 23:38:10,549] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/opt/conda/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/opt/conda/lib/python3.12/site-packages/mlflow/pyfunc/utils/data_validation.py:137: FutureWarning: Model's `predict` method contains invalid parameters: {'X'}. Only the following parameter names are allowed: context, model_input, and params. Note that invalid parameters will no longer be permitted in future versions.\n",
      "  param_names = _check_func_signature(func, \"predict\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import warnings\n",
    "import mlflow\n",
    "\n",
    "# Add the core directory to the path to import utils\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "# ===============================\n",
    "# üß† Model Selection & Loading\n",
    "# ===============================\n",
    "from core.selection.model_selection import ModelSelector\n",
    "\n",
    "# ===============================\n",
    "# üöÄ Deployment & Registration\n",
    "# ===============================\n",
    "from core.deploy.deploy_fine_tuning import register_llm_comparison_model\n",
    "\n",
    "# ===============================\n",
    "# ‚öôÔ∏è Utility Functions\n",
    "# ===============================\n",
    "from src.utils import (\n",
    "    load_config_and_secrets,\n",
    "    configure_proxy,\n",
    "    login_huggingface,\n",
    "    get_configs_dir,\n",
    "    get_fine_tuned_models_dir,\n",
    "    get_models_dir,\n",
    "    format_model_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c53dba",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47c2bb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress Python warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdcc8396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 23:38:15 - INFO - üìã Model Registration Configuration:\n",
      "2025-07-21 23:38:15 - INFO -    ‚Ä¢ Base model (HF): TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "2025-07-21 23:38:15 - INFO -    ‚Ä¢ Fine-tuned model: Orpo-TinyLlama-1.1B-Chat-v1.0-FT\n",
      "2025-07-21 23:38:15 - INFO -    ‚Ä¢ MLflow experiment: AIStudio-Fine-Tuning-Experiment\n",
      "2025-07-21 23:38:15 - INFO -    ‚Ä¢ Service name: AIStudio-Fine-Tuning-Model\n"
     ]
    }
   ],
   "source": [
    "# Configuration paths and parameters\n",
    "CONFIG_PATH = str(get_configs_dir() / \"config.yaml\")\n",
    "SECRETS_PATH = str(get_configs_dir() / \"secrets.yaml\")\n",
    "MLFLOW_EXPERIMENT_NAME = \"AIStudio-Fine-Tuning-Experiment\"\n",
    "MODEL_SERVICE_RUN_NAME = \"AIStudio-Fine-Tuning-Service-Run\"\n",
    "MODEL_SERVICE_NAME = \"AIStudio-Fine-Tuning-Model\"\n",
    "\n",
    "# Model configuration - update these based on your training\n",
    "BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Update to match your base model\n",
    "FINE_TUNED_MODEL_NAME = \"Orpo-TinyLlama-1.1B-Chat-v1.0-FT\"  # Update to match your fine-tuned model\n",
    "\n",
    "logger.info(\"üìã Model Registration Configuration:\")\n",
    "logger.info(f\"   ‚Ä¢ Base model (HF): {BASE_MODEL}\")\n",
    "logger.info(f\"   ‚Ä¢ Fine-tuned model: {FINE_TUNED_MODEL_NAME}\")\n",
    "logger.info(f\"   ‚Ä¢ MLflow experiment: {MLFLOW_EXPERIMENT_NAME}\")\n",
    "logger.info(f\"   ‚Ä¢ Service name: {MODEL_SERVICE_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fd99766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 23:38:15 - INFO - ‚úÖ Configuration loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load configuration and configure proxy if needed\n",
    "config, secrets = load_config_and_secrets(CONFIG_PATH, SECRETS_PATH)\n",
    "configure_proxy(config)\n",
    "\n",
    "logger.info(\"‚úÖ Configuration loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc60c9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 23:38:16 - INFO - ‚úÖ Hugging Face authentication successful\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Logged into Hugging Face successfully.\n"
     ]
    }
   ],
   "source": [
    "# Login to Hugging Face (required for downloading gated models)\n",
    "try:\n",
    "    login_huggingface(secrets)\n",
    "    logger.info(\"‚úÖ Hugging Face authentication successful\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"‚ö†Ô∏è Hugging Face authentication failed: {e}\")\n",
    "    logger.info(\"Some models may not be accessible if they require authentication\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9191436c",
   "metadata": {},
   "source": [
    "## Verify and Prepare Model Assets\n",
    "\n",
    "Before registering the models, let's verify that both the base model and fine-tuned model are accessible. If the base model hasn't been downloaded locally yet, we'll download it using the same approach as the training workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c18944ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 23:38:16 - INFO - ‚úÖ Fine-tuned model found: /home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/output/fine_tuned_models/Orpo-TinyLlama-1.1B-Chat-v1.0-FT\n",
      "2025-07-21 23:38:16 - INFO - üîç Checking base model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "2025-07-21 23:38:16,169 ‚Äî INFO ‚Äî [ModelSelector] Selected model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "2025-07-21 23:38:16,174 ‚Äî INFO ‚Äî [ModelSelector] Downloading model snapshot to: /home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/models/TinyLlama__TinyLlama-1.1B-Chat-v1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6989acd1887e488ab5f34065750e47c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 23:38:16,428 ‚Äî INFO ‚Äî [ModelSelector] ‚úÖ Model downloaded successfully to: /home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/models/TinyLlama__TinyLlama-1.1B-Chat-v1.0\n",
      "2025-07-21 23:38:16,430 ‚Äî INFO ‚Äî [ModelSelector] Loading model and tokenizer from: /home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/models/TinyLlama__TinyLlama-1.1B-Chat-v1.0\n",
      "2025-07-21 23:39:04,484 ‚Äî INFO ‚Äî [ModelSelector] Checking model for ORPO compatibility...\n",
      "2025-07-21 23:39:04,488 ‚Äî INFO ‚Äî [ModelSelector] ‚úÖ Model 'TinyLlama/TinyLlama-1.1B-Chat-v1.0' is ORPO-compatible.\n",
      "2025-07-21 23:39:04 - INFO - ‚úÖ Base model prepared locally: /home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/models/TinyLlama__TinyLlama-1.1B-Chat-v1.0\n"
     ]
    }
   ],
   "source": [
    "def verify_and_prepare_model_assets():\n",
    "    \"\"\"Verify and prepare both base and fine-tuned model assets.\"\"\"\n",
    "    \n",
    "    # Check fine-tuned model directory\n",
    "    fine_tuned_dir = get_fine_tuned_models_dir()\n",
    "    fine_tuned_path = fine_tuned_dir / FINE_TUNED_MODEL_NAME\n",
    "    \n",
    "    if fine_tuned_path.exists():\n",
    "        logger.info(f\"‚úÖ Fine-tuned model found: {fine_tuned_path}\")\n",
    "        fine_tuned_available = True\n",
    "    else:\n",
    "        logger.warning(f\"‚ö†Ô∏è Fine-tuned model not found: {fine_tuned_path}\")\n",
    "        logger.info(\"Please run the run-workflow.ipynb notebook first to create the fine-tuned model\")\n",
    "        fine_tuned_available = False\n",
    "    \n",
    "    # Handle base model - download locally if needed using ModelSelector\n",
    "    logger.info(f\"üîç Checking base model: {BASE_MODEL}\")\n",
    "    \n",
    "    try:\n",
    "        # Use ModelSelector to handle model downloading and verification\n",
    "        selector = ModelSelector()\n",
    "        selector.select_model(BASE_MODEL)\n",
    "        \n",
    "        # Get the local model path\n",
    "        base_model_local_path = selector.format_model_path(BASE_MODEL)\n",
    "        \n",
    "        logger.info(f\"‚úÖ Base model prepared locally: {base_model_local_path}\")\n",
    "        \n",
    "        return fine_tuned_available, base_model_local_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Failed to prepare base model: {str(e)}\")\n",
    "        return False, None\n",
    "\n",
    "# Verify and prepare assets\n",
    "assets_verified, base_model_path = verify_and_prepare_model_assets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df91317",
   "metadata": {},
   "source": [
    "## Adaptive Model Registration Service\n",
    "\n",
    "This section demonstrates how to register the **adaptive** LLM comparison model that automatically adjusts to different hardware and memory constraints. The model provides a single API endpoint that works efficiently across various deployment environments.\n",
    "\n",
    "### Key Adaptive Features:\n",
    "- **Automatic Device Selection**: Intelligently chooses between CPU and GPU based on availability\n",
    "- **Dynamic Memory Management**: Adapts memory usage patterns based on available resources  \n",
    "- **Smart Device Mapping**: Uses transformers' auto device mapping for optimal model distribution\n",
    "- **Precision Optimization**: Automatically selects FP16 on GPU, FP32 on CPU for best performance\n",
    "- **Robust Error Handling**: Graceful fallbacks when advanced features aren't available\n",
    "- **Universal Compatibility**: Works in both memory-constrained and resource-rich environments\n",
    "\n",
    "The service provides:\n",
    "\n",
    "- **Base Model Inference**: Access to the original pre-trained model\n",
    "- **Fine-Tuned Model Inference**: Access to the ORPO fine-tuned model  \n",
    "- **Comparison Mode**: Switch between models using the `use_finetuning` parameter\n",
    "- **Adaptive Performance**: Automatically optimizes for the deployment environment\n",
    "- **Flexible Input**: Support for custom prompts and generation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6afec12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 23:39:04 - INFO - üìù Registering comparison model with:\n",
      "2025-07-21 23:39:04 - INFO -    ‚Ä¢ Base model path: /home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/models/TinyLlama__TinyLlama-1.1B-Chat-v1.0\n",
      "2025-07-21 23:39:04 - INFO -    ‚Ä¢ Fine-tuned model: Orpo-TinyLlama-1.1B-Chat-v1.0-FT\n",
      "2025-07-21 23:39:04,694 ‚Äî INFO ‚Äî Resolved base model path: /home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/models/TinyLlama__TinyLlama-1.1B-Chat-v1.0\n",
      "2025-07-21 23:39:04,696 ‚Äî INFO ‚Äî Resolved fine-tuned model path: /home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/output/fine_tuned_models/Orpo-TinyLlama-1.1B-Chat-v1.0-FT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb292c5ac92845168559c890653f4008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b60cb4b85116496d90b7089a3bab7bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99f5846586f0433bbe5dd38b0e71892c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'AIStudio-Fine-Tuning-Model'.\n",
      "Created version '1' of model 'AIStudio-Fine-Tuning-Model'.\n",
      "2025-07-21 23:43:18,760 ‚Äî INFO ‚Äî ‚úÖ Registered as `AIStudio-Fine-Tuning-Model` (run df609175f14643ca86e353fcb4c712e4)\n",
      "2025-07-21 23:43:18 - INFO - ‚úÖ LLM comparison model registered successfully!\n",
      "2025-07-21 23:43:18 - INFO - Model name: AIStudio-Fine-Tuning-Model\n",
      "2025-07-21 23:43:18 - INFO - Experiment: AIStudio-Fine-Tuning-Experiment\n"
     ]
    }
   ],
   "source": [
    "# Set MLflow tracking URI and experiment\n",
    "mlflow.set_tracking_uri('/phoenix/mlflow')\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\n",
    "\n",
    "if assets_verified and base_model_path:\n",
    "    try:\n",
    "        logger.info(f\"üìù Registering comparison model with:\")\n",
    "        logger.info(f\"   ‚Ä¢ Base model path: {base_model_path}\")\n",
    "        logger.info(f\"   ‚Ä¢ Fine-tuned model: {FINE_TUNED_MODEL_NAME}\")\n",
    "        \n",
    "        # Register the adaptive LLM comparison model\n",
    "        register_llm_comparison_model(\n",
    "            model_base_path=base_model_path,\n",
    "            model_finetuned_path=FINE_TUNED_MODEL_NAME,\n",
    "            experiment=MLFLOW_EXPERIMENT_NAME,\n",
    "            run_name=MODEL_SERVICE_RUN_NAME,\n",
    "            registry_name=MODEL_SERVICE_NAME,\n",
    "            config_path=CONFIG_PATH\n",
    "        )\n",
    "        \n",
    "        logger.info(\"‚úÖ Adaptive LLM comparison model registered successfully!\")\n",
    "        logger.info(f\"Model name: {MODEL_SERVICE_NAME}\")\n",
    "        logger.info(f\"Experiment: {MLFLOW_EXPERIMENT_NAME}\")\n",
    "        logger.info(\"This model automatically adapts to memory constraints and available hardware.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Failed to register comparison model: {str(e)}\")\n",
    "        logger.info(\"Please check the error details above and ensure all dependencies are installed\")\n",
    "        \n",
    "else:\n",
    "    logger.error(\"‚ùå Cannot register model - required assets not found or not prepared\")\n",
    "    if not assets_verified:\n",
    "        logger.info(\"Please run the run-workflow.ipynb notebook first to create the fine-tuned model\")\n",
    "    if not base_model_path:\n",
    "        logger.info(\"Base model could not be downloaded or prepared locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0418bd15",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "Once the adaptive model is registered, you can use it through the MLflow model serving interface. The adaptive version automatically optimizes performance and memory usage based on your environment.\n",
    "\n",
    "### Input Format\n",
    "The model expects a pandas DataFrame with the following columns:\n",
    "- `prompt` (string): The text prompt to generate from\n",
    "- `use_finetuning` (boolean): Whether to use the fine-tuned model (True) or base model (False)\n",
    "- `max_tokens` (integer, optional): Maximum number of tokens to generate (default: 128)\n",
    "\n",
    "### Example Usage\n",
    "```python\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "\n",
    "# Load the registered adaptive model\n",
    "model = mlflow.pyfunc.load_model(f\"models:/{MODEL_SERVICE_NAME}/latest\")\n",
    "\n",
    "# Create input data\n",
    "input_data = pd.DataFrame({\n",
    "    \"prompt\": [\"Explain the importance of sustainable agriculture.\"],\n",
    "    \"use_finetuning\": [True],  # Use fine-tuned model\n",
    "    \"max_tokens\": [200]\n",
    "})\n",
    "\n",
    "# Generate response (automatically optimized)\n",
    "response = model.predict(input_data)\n",
    "print(response[\"response\"].iloc[0])\n",
    "```\n",
    "\n",
    "### Adaptive Comparison Mode\n",
    "The adaptive version efficiently handles model switching with automatic optimization:\n",
    "\n",
    "```python\n",
    "# Compare base vs fine-tuned (adaptive optimization)\n",
    "prompts = [\"Your test prompt here\"]\n",
    "\n",
    "for use_ft in [False, True]:\n",
    "    input_data = pd.DataFrame({\n",
    "        \"prompt\": prompts,\n",
    "        \"use_finetuning\": [use_ft],\n",
    "        \"max_tokens\": [150]\n",
    "    })\n",
    "    response = model.predict(input_data)\n",
    "    model_type = \"Fine-tuned\" if use_ft else \"Base\"\n",
    "    print(f\"{model_type} Model: {response['response'].iloc[0]}\")\n",
    "    # Model automatically handles device placement and memory management\n",
    "```\n",
    "\n",
    "### Adaptive Benefits\n",
    "- **Environment Detection**: Automatically detects available hardware and memory\n",
    "- **Performance Optimization**: Uses best settings for each deployment environment\n",
    "- **Memory Safety**: Prevents OOM errors through intelligent memory management\n",
    "- **Hardware Efficiency**: Leverages GPU acceleration when available, graceful CPU fallback\n",
    "- **Robust Operation**: Handles various deployment scenarios without configuration changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db5b29ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 23:43:18 - INFO - ‚è±Ô∏è Total execution time: 5m 49.18s\n",
      "2025-07-21 23:43:18 - INFO - ‚úÖ Model registration notebook execution completed successfully.\n"
     ]
    }
   ],
   "source": [
    "end_time: float = time.time()\n",
    "elapsed_time: float = end_time - start_time\n",
    "elapsed_minutes: int = int(elapsed_time // 60)\n",
    "elapsed_seconds: float = elapsed_time % 60\n",
    "\n",
    "logger.info(f\"‚è±Ô∏è Total execution time: {elapsed_minutes}m {elapsed_seconds:.2f}s\")\n",
    "logger.info(\"‚úÖ Model registration notebook execution completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4483b4c0",
   "metadata": {},
   "source": [
    "Built with ‚ù§Ô∏è using [**HP AI Studio**](https://hp.com/ai-studio)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
