{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77d494ea",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-size: 50px;\"> Fine-Tuned Model Registration Service </h1>\n",
    "\n",
    "This notebook demonstrates how to register a fine-tuned LLM comparison service that allows switching between base and fine-tuned models through a single MLflow endpoint. This follows the same pattern used across all AI-Blueprints for consistent model deployment and serving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e589ed3",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "\n",
    "- Start Execution\n",
    "- Install and Import Libraries\n",
    "- Configure Settings\n",
    "- Verify and Prepare Model Assets\n",
    "- Model Service Registration to MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5706d1f",
   "metadata": {},
   "source": [
    " # Start Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba200196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "# Configure logger\n",
    "logger: logging.Logger = logging.getLogger(\"register_model_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.propagate = False  # Prevent duplicate logs from parent loggers\n",
    "\n",
    "# Set formatter\n",
    "formatter: logging.Formatter = logging.Formatter(\n",
    "    fmt=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "# Configure and attach stream handler\n",
    "stream_handler: logging.StreamHandler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c9dc43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 13:48:42 - INFO - Model registration notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()  \n",
    "\n",
    "logger.info(\"Model registration notebook execution started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd52f5e",
   "metadata": {},
   "source": [
    "# Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07d2e80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 57.3 ms, sys: 42.2 ms, total: 99.4 ms\n",
      "Wall time: 2.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "%pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb5add58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 13:48:57.235874: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-21 13:48:57.248843: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755784137.265235    1561 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755784137.270416    1561 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1755784137.283734    1561 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755784137.283748    1561 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755784137.283749    1561 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755784137.283750    1561 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-21 13:48:57.288816: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/conda/lib/python3.12/site-packages/mlflow/pyfunc/utils/data_validation.py:168: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
      "  color_warning(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import mlflow\n",
    "from typing import Dict, Any, Optional, Union, List, Tuple\n",
    "\n",
    "\n",
    "# Add the core directory to the path to import utils\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "# ===============================\n",
    "# üß† Model Selection & Loading\n",
    "# ===============================\n",
    "from core.selection.model_selection import ModelSelector\n",
    "\n",
    "# ===============================\n",
    "# üöÄ Deployment & Registration\n",
    "# ===============================\n",
    "from core.deploy.deploy_fine_tuning import register_llm_comparison_model\n",
    "\n",
    "# ===============================\n",
    "# ‚öôÔ∏è Utility Functions\n",
    "# ===============================\n",
    "from src.utils import (\n",
    "    load_configuration,\n",
    "    load_secrets,\n",
    "    load_secrets_to_env,\n",
    "    configure_proxy,\n",
    "    login_huggingface,\n",
    "    get_configs_dir,\n",
    "    get_fine_tuned_models_dir,\n",
    "    get_models_dir,\n",
    "    format_model_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c53dba",
   "metadata": {},
   "source": [
    "# Configure Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47c2bb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress Python warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdcc8396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 13:49:01 - INFO - üìã Model Registration Configuration:\n",
      "2025-08-21 13:49:01 - INFO -    ‚Ä¢ Base model (HF): TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "2025-08-21 13:49:01 - INFO -    ‚Ä¢ Fine-tuned model: Orpo-TinyLlama-1.1B-Chat-v1.0-FT\n",
      "2025-08-21 13:49:01 - INFO -    ‚Ä¢ MLflow experiment: AIStudio-Fine-Tuning-Experiment\n",
      "2025-08-21 13:49:01 - INFO -    ‚Ä¢ Service name: AIStudio-Fine-Tuning-Model\n"
     ]
    }
   ],
   "source": [
    "# Configuration paths and parameters\n",
    "DEMO_FOLDER = Path(\"../demo\")\n",
    "CONFIG_PATH = str(get_configs_dir() / \"config.yaml\")\n",
    "SECRETS_PATH = str(get_configs_dir() / \"secrets.yaml\")\n",
    "MLFLOW_EXPERIMENT_NAME = \"AIStudio-Fine-Tuning-Experiment\"\n",
    "MODEL_SERVICE_RUN_NAME = \"AIStudio-Fine-Tuning-Service-Run\"\n",
    "MODEL_SERVICE_NAME = \"AIStudio-Fine-Tuning-Model\"\n",
    "\n",
    "# Model configuration - update these based on your training\n",
    "BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Update to match your base model\n",
    "FINE_TUNED_MODEL_NAME = \"Orpo-TinyLlama-1.1B-Chat-v1.0-FT\"  # Update to match your fine-tuned model\n",
    "\n",
    "logger.info(\"üìã Model Registration Configuration:\")\n",
    "logger.info(f\"   ‚Ä¢ Base model (HF): {BASE_MODEL}\")\n",
    "logger.info(f\"   ‚Ä¢ Fine-tuned model: {FINE_TUNED_MODEL_NAME}\")\n",
    "logger.info(f\"   ‚Ä¢ MLflow experiment: {MLFLOW_EXPERIMENT_NAME}\")\n",
    "logger.info(f\"   ‚Ä¢ Service name: {MODEL_SERVICE_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a128f15",
   "metadata": {},
   "source": [
    "### Configuration and Secrets Loading\n",
    "\n",
    "In this section, we load configuration parameters and API keys from separate YAML files. This separation helps maintain security by keeping sensitive information (API keys) separate from configuration settings.\n",
    "\n",
    "- **config.yaml**: Contains non-sensitive configuration parameters like model sources and URLs\n",
    "- **secrets.yaml**: Contains sensitive API keys for services like HuggingFace\n",
    "- *(Optional for Premium users)* Secrets such as API keys for services like HuggingFace can be stored as environment variables for the project and loaded into the notebook (see the project's README file for steps on how to save secrets in Secrets Manager)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "020d3a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 2 secrets into environment variables.\n",
      "‚úÖ Configuration loaded successfully\n",
      "‚úÖ Secrets loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load secrets from secrets.yaml file (if it exists) into environment\n",
    "if Path(SECRETS_PATH).exists():\n",
    "    load_secrets_to_env(SECRETS_PATH)\n",
    "else:\n",
    "    print(f\"No secrets file found at {SECRETS_PATH}; relying on preexisting environment\")\n",
    "\n",
    "# Retrieve secrets from environment\n",
    "try:\n",
    "    secrets = load_secrets()\n",
    "except ValueError:\n",
    "    secrets = {}\n",
    "\n",
    "# Load configuration and secrets\n",
    "config = load_configuration(CONFIG_PATH)\n",
    "\n",
    "print(\"‚úÖ Configuration loaded successfully\")\n",
    "print(\"‚úÖ Secrets loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fd99766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure proxy if needed\n",
    "configure_proxy(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7b6e09",
   "metadata": {},
   "source": [
    "### üîê Login to Hugging Face\n",
    "\n",
    "To access gated models (e.g., LLaMA, Mistral, or Gemma), you must authenticate using your Hugging Face token.\n",
    "\n",
    "Make sure your `secrets.yaml` file (or AIS Secrets Manager for Premium Users) contains the following key:\n",
    "\n",
    "```yaml\n",
    "AIS_HUGGINGFACE_API_KEY: your_huggingface_token\n",
    "```\n",
    "\n",
    "**Note**: Please refer to this project's README for detailed instuctions on how to configure secrets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc60c9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 13:49:01 - INFO - ‚úÖ Hugging Face authentication successful\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Logged into Hugging Face successfully.\n"
     ]
    }
   ],
   "source": [
    "# Login to Hugging Face (required for downloading gated models)\n",
    "try:\n",
    "    login_huggingface(secrets)\n",
    "    logger.info(\"‚úÖ Hugging Face authentication successful\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"‚ö†Ô∏è Hugging Face authentication failed: {e}\")\n",
    "    logger.info(\"Some models may not be accessible if they require authentication\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9191436c",
   "metadata": {},
   "source": [
    "# Verify and Prepare Model Assets\n",
    "\n",
    "Before registering the models, let's verify that both the base model and fine-tuned model are accessible. If the base model hasn't been downloaded locally yet, we'll download it using the same approach as the training workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c18944ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 13:49:01 - INFO - ‚úÖ Fine-tuned model found: /home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/output/fine_tuned_models/Orpo-TinyLlama-1.1B-Chat-v1.0-FT\n",
      "2025-08-21 13:49:01 - INFO - üîç Checking base model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "2025-08-21 13:49:01,778 ‚Äî INFO ‚Äî [ModelSelector] Selected model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "2025-08-21 13:49:01,781 ‚Äî INFO ‚Äî [ModelSelector] Downloading model snapshot to: /home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/models/TinyLlama__TinyLlama-1.1B-Chat-v1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8917d03254a64fb0bf54a9715d8494d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 13:49:01,979 ‚Äî INFO ‚Äî [ModelSelector] ‚úÖ Model downloaded successfully to: /home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/models/TinyLlama__TinyLlama-1.1B-Chat-v1.0\n",
      "2025-08-21 13:49:01,980 ‚Äî INFO ‚Äî [ModelSelector] Loading model and tokenizer from: /home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/models/TinyLlama__TinyLlama-1.1B-Chat-v1.0\n",
      "2025-08-21 13:49:41,961 ‚Äî INFO ‚Äî [ModelSelector] Checking model for ORPO compatibility...\n",
      "2025-08-21 13:49:41,963 ‚Äî INFO ‚Äî [ModelSelector] ‚úÖ Model 'TinyLlama/TinyLlama-1.1B-Chat-v1.0' is ORPO-compatible.\n",
      "2025-08-21 13:49:41 - INFO - ‚úÖ Base model prepared locally: /home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/models/TinyLlama__TinyLlama-1.1B-Chat-v1.0\n"
     ]
    }
   ],
   "source": [
    "def verify_and_prepare_model_assets():\n",
    "    \"\"\"Verify and prepare both base and fine-tuned model assets.\"\"\"\n",
    "    \n",
    "    # Check fine-tuned model directory\n",
    "    fine_tuned_dir = get_fine_tuned_models_dir()\n",
    "    fine_tuned_path = fine_tuned_dir / FINE_TUNED_MODEL_NAME\n",
    "    \n",
    "    if fine_tuned_path.exists():\n",
    "        logger.info(f\"‚úÖ Fine-tuned model found: {fine_tuned_path}\")\n",
    "        fine_tuned_available = True\n",
    "    else:\n",
    "        logger.warning(f\"‚ö†Ô∏è Fine-tuned model not found: {fine_tuned_path}\")\n",
    "        logger.info(\"Please run the run-workflow.ipynb notebook first to create the fine-tuned model\")\n",
    "        fine_tuned_available = False\n",
    "    \n",
    "    # Handle base model - download locally if needed using ModelSelector\n",
    "    logger.info(f\"üîç Checking base model: {BASE_MODEL}\")\n",
    "    \n",
    "    try:\n",
    "        # Use ModelSelector to handle model downloading and verification\n",
    "        selector = ModelSelector()\n",
    "        selector.select_model(BASE_MODEL)\n",
    "        \n",
    "        # Get the local model path\n",
    "        base_model_local_path = selector.format_model_path(BASE_MODEL)\n",
    "        \n",
    "        logger.info(f\"‚úÖ Base model prepared locally: {base_model_local_path}\")\n",
    "        \n",
    "        return fine_tuned_available, base_model_local_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Failed to prepare base model: {str(e)}\")\n",
    "        return False, None\n",
    "\n",
    "# Verify and prepare assets\n",
    "assets_verified, base_model_path = verify_and_prepare_model_assets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df91317",
   "metadata": {},
   "source": [
    "## Adaptive Model Registration Service\n",
    "\n",
    "This section demonstrates how to register the **adaptive** LLM comparison model that automatically adjusts to different hardware and memory constraints. The model provides a single API endpoint that works efficiently across various deployment environments.\n",
    "\n",
    "### Key Adaptive Features:\n",
    "- **Automatic Device Selection**: Intelligently chooses between CPU and GPU based on availability\n",
    "- **Dynamic Memory Management**: Adapts memory usage patterns based on available resources  \n",
    "- **Smart Device Mapping**: Uses transformers' auto device mapping for optimal model distribution\n",
    "- **Precision Optimization**: Automatically selects FP16 on GPU, FP32 on CPU for best performance\n",
    "- **Robust Error Handling**: Graceful fallbacks when advanced features aren't available\n",
    "- **Universal Compatibility**: Works in both memory-constrained and resource-rich environments\n",
    "\n",
    "The service provides:\n",
    "\n",
    "- **Base Model Inference**: Access to the original pre-trained model\n",
    "- **Fine-Tuned Model Inference**: Access to the ORPO fine-tuned model  \n",
    "- **Comparison Mode**: Switch between models using the `use_finetuning` parameter\n",
    "- **Adaptive Performance**: Automatically optimizes for the deployment environment\n",
    "- **Flexible Input**: Support for custom prompts and generation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6afec12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 13:49:42 - INFO - üìù Registering comparison model with:\n",
      "2025-08-21 13:49:42 - INFO -    ‚Ä¢ Base model path: /home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/models/TinyLlama__TinyLlama-1.1B-Chat-v1.0\n",
      "2025-08-21 13:49:42 - INFO -    ‚Ä¢ Fine-tuned model: Orpo-TinyLlama-1.1B-Chat-v1.0-FT\n",
      "2025-08-21 13:49:42,085 ‚Äî INFO ‚Äî Resolved base model path: /home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/models/TinyLlama__TinyLlama-1.1B-Chat-v1.0\n",
      "2025-08-21 13:49:42,086 ‚Äî INFO ‚Äî Resolved fine-tuned model path: /home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/output/fine_tuned_models/Orpo-TinyLlama-1.1B-Chat-v1.0-FT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d7f4d25922640189abce50fbc9e75b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a5a80f303c4ae19c8a48b5bfdc4364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd1c8125aebc4ea6bd084de8eaa5c26d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0705f332c5453aae8a7f5b8e160a62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'AIStudio-Fine-Tuning-Model'.\n",
      "Created version '1' of model 'AIStudio-Fine-Tuning-Model'.\n",
      "2025-08-21 13:52:12,895 ‚Äî INFO ‚Äî ‚úÖ Adaptive LLM comparison model registered as `AIStudio-Fine-Tuning-Model` (run b5871a06b8c546419197166f3e5f634b)\n",
      "2025-08-21 13:52:12 - INFO - ‚úÖ Adaptive LLM comparison model registered successfully!\n",
      "2025-08-21 13:52:12 - INFO - Model name: AIStudio-Fine-Tuning-Model\n",
      "2025-08-21 13:52:12 - INFO - Experiment: AIStudio-Fine-Tuning-Experiment\n",
      "2025-08-21 13:52:12 - INFO - This model automatically adapts to memory constraints and available hardware.\n"
     ]
    }
   ],
   "source": [
    "# Set MLflow tracking URI and experiment\n",
    "mlflow.set_tracking_uri('/phoenix/mlflow')\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\n",
    "\n",
    "if assets_verified and base_model_path:\n",
    "    try:\n",
    "        logger.info(f\"üìù Registering comparison model with:\")\n",
    "        logger.info(f\"   ‚Ä¢ Base model path: {base_model_path}\")\n",
    "        logger.info(f\"   ‚Ä¢ Fine-tuned model: {FINE_TUNED_MODEL_NAME}\")\n",
    "        \n",
    "        # Register the adaptive LLM comparison model\n",
    "        register_llm_comparison_model(\n",
    "            model_base_path=base_model_path,\n",
    "            model_finetuned_path=FINE_TUNED_MODEL_NAME,\n",
    "            experiment=MLFLOW_EXPERIMENT_NAME,\n",
    "            run_name=MODEL_SERVICE_RUN_NAME,\n",
    "            registry_name=MODEL_SERVICE_NAME,\n",
    "            config_path=CONFIG_PATH,\n",
    "            demo_folder=DEMO_FOLDER\n",
    "        )\n",
    "        \n",
    "        logger.info(\"‚úÖ Adaptive LLM comparison model registered successfully!\")\n",
    "        logger.info(f\"Model name: {MODEL_SERVICE_NAME}\")\n",
    "        logger.info(f\"Experiment: {MLFLOW_EXPERIMENT_NAME}\")\n",
    "        logger.info(\"This model automatically adapts to memory constraints and available hardware.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Failed to register comparison model: {str(e)}\")\n",
    "        logger.info(\"Please check the error details above and ensure all dependencies are installed\")\n",
    "        \n",
    "else:\n",
    "    logger.error(\"‚ùå Cannot register model - required assets not found or not prepared\")\n",
    "    if not assets_verified:\n",
    "        logger.info(\"Please run the run-workflow.ipynb notebook first to create the fine-tuned model\")\n",
    "    if not base_model_path:\n",
    "        logger.info(\"Base model could not be downloaded or prepared locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0418bd15",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "Once the adaptive model is registered, you can use it through the MLflow model serving interface. The adaptive version automatically optimizes performance and memory usage based on your environment.\n",
    "\n",
    "### Input Format\n",
    "The model expects a pandas DataFrame with the following columns:\n",
    "- `prompt` (string): The text prompt to generate from\n",
    "- `use_finetuning` (boolean): Whether to use the fine-tuned model (True) or base model (False)\n",
    "- `max_tokens` (integer, optional): Maximum number of tokens to generate (default: 128)\n",
    "\n",
    "### Example Usage\n",
    "```python\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "\n",
    "# Load the registered adaptive model\n",
    "model = mlflow.pyfunc.load_model(f\"models:/{MODEL_SERVICE_NAME}/latest\")\n",
    "\n",
    "# Create input data\n",
    "input_data = pd.DataFrame({\n",
    "    \"prompt\": [\"Explain the importance of sustainable agriculture.\"],\n",
    "    \"use_finetuning\": [True],  # Use fine-tuned model\n",
    "    \"max_tokens\": [200]\n",
    "})\n",
    "\n",
    "# Generate response (automatically optimized)\n",
    "response = model.predict(input_data)\n",
    "print(response[\"response\"].iloc[0])\n",
    "```\n",
    "\n",
    "### Adaptive Comparison Mode\n",
    "The adaptive version efficiently handles model switching with automatic optimization:\n",
    "\n",
    "```python\n",
    "# Compare base vs fine-tuned (adaptive optimization)\n",
    "prompts = [\"Your test prompt here\"]\n",
    "\n",
    "for use_ft in [False, True]:\n",
    "    input_data = pd.DataFrame({\n",
    "        \"prompt\": prompts,\n",
    "        \"use_finetuning\": [use_ft],\n",
    "        \"max_tokens\": [150]\n",
    "    })\n",
    "    response = model.predict(input_data)\n",
    "    model_type = \"Fine-tuned\" if use_ft else \"Base\"\n",
    "    print(f\"{model_type} Model: {response['response'].iloc[0]}\")\n",
    "    # Model automatically handles device placement and memory management\n",
    "```\n",
    "\n",
    "### Adaptive Benefits\n",
    "- **Environment Detection**: Automatically detects available hardware and memory\n",
    "- **Performance Optimization**: Uses best settings for each deployment environment\n",
    "- **Memory Safety**: Prevents OOM errors through intelligent memory management\n",
    "- **Hardware Efficiency**: Leverages GPU acceleration when available, graceful CPU fallback\n",
    "- **Robust Operation**: Handles various deployment scenarios without configuration changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db5b29ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 13:52:12 - INFO - ‚è±Ô∏è Total execution time: 3m 30.36s\n",
      "2025-08-21 13:52:12 - INFO - ‚úÖ Model registration notebook execution completed successfully.\n"
     ]
    }
   ],
   "source": [
    "end_time: float = time.time()\n",
    "elapsed_time: float = end_time - start_time\n",
    "elapsed_minutes: int = int(elapsed_time // 60)\n",
    "elapsed_seconds: float = elapsed_time % 60\n",
    "\n",
    "logger.info(f\"‚è±Ô∏è Total execution time: {elapsed_minutes}m {elapsed_seconds:.2f}s\")\n",
    "logger.info(\"‚úÖ Model registration notebook execution completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4483b4c0",
   "metadata": {},
   "source": [
    "Built with ‚ù§Ô∏è using [**HP AI Studio**](https://hp.com/ai-studio)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
