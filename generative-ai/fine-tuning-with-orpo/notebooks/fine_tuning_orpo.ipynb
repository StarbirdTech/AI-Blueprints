{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec84ac00-1a02-4fe0-af00-31c2232ba831",
   "metadata": {},
   "source": [
    "<h1 style=\\\"text-align: center; font-size: 50px;\\\"> Interactive ORPO Fine-Tuning & Inference Hub for Open LLMs </h1>\n",
    "\n",
    "This experiment provides an interactive and modular interface for selecting, downloading, fine-tuning, and evaluating large language models using ORPO (Optimal Reward Preferring Optimization).\n",
    "The user can choose between state-of-the-art open LLMs like Mistral, LLaMA 2/3, and Gemma. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c54ddb0",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "- Imports\n",
    "- Configurations\n",
    "- Verify Assets\n",
    "- Model Loader\n",
    "- Inference with Default Model\n",
    "- Creating the Fine-Tuned Model Name (ORPO)\n",
    "- Dataset Loader\n",
    "- ORPO Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54051b27-5b97-47cd-89b2-4f2861711f14",
   "metadata": {},
   "source": [
    "## üì¶ Imports\n",
    "\n",
    "By using our Local GenAI workspace image, most of the necessary libraries to work with ORPO-based fine-tuning and evaluation already come pre-installed. In this notebook, we only need to import components for model loading, quantization, inference, and feedback visualization to run the complete ORPO workflow locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be25e3e1-7f9c-4536-8304-c9f1762a8f63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380f85cd-8e76-4f3e-8022-c8ad210a72a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b15f11-b47a-4735-ab36-1063e88489f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# üß† Core Libraries\n",
    "# ===============================\n",
    "import torch\n",
    "import multiprocessing\n",
    "import mlflow\n",
    "from datasets import load_dataset\n",
    "\n",
    "# ===============================\n",
    "# üß™ Hugging Face & Transformers\n",
    "# ===============================\n",
    "from huggingface_hub import login\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# üß© Fine-tuning (ORPO + PEFT)\n",
    "# ===============================\n",
    "from trl import ORPOConfig, ORPOTrainer, setup_chat_format\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "\n",
    "# ===============================\n",
    "# üß∞ Project Modules: Core Pipeline\n",
    "# ===============================\n",
    "# Add the core directory to the path to import utils\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from core.selection.model_selection import ModelSelector\n",
    "from core.local_inference.inference import InferenceRunner\n",
    "from core.target_mapper.lora_target_mapper import LoRATargetMapper\n",
    "from core.data_visualizer.feedback_visualizer import UltraFeedbackVisualizer\n",
    "from core.finetuning_inference.inference_runner import AcceleratedInferenceRunner\n",
    "from core.merge_model.merge_lora import merge_lora_and_save\n",
    "from core.quantization.quantization_config import QuantizationSelector\n",
    "from core.comparer.model_comparer import ModelComparer\n",
    "\n",
    "# ===============================\n",
    "# üöÄ Deployment & Evaluation\n",
    "# ===============================\n",
    "from core.deploy.deploy_fine_tuning import register_llm_comparison_model\n",
    "\n",
    "# ===============================\n",
    "# ‚öôÔ∏è Utility Functions\n",
    "# ===============================\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from src.utils import (\n",
    "    load_config_and_secrets,\n",
    "    configure_proxy,\n",
    "    login_huggingface,\n",
    "    get_project_root,\n",
    "    get_config_dir,\n",
    "    get_configs_dir,\n",
    "    get_output_dir,\n",
    "    get_models_dir,\n",
    "    get_fine_tuned_models_dir,\n",
    "    get_model_cache_dir,\n",
    "    format_model_path,\n",
    "    setup_model_environment\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625a13c8-7f4d-4942-8edd-888695bba2a4",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac369051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress Python warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a97b90b-ed6f-4546-86d5-59279ed7ebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = str(get_configs_dir() / \"config.yaml\")\n",
    "SECRETS_PATH = str(get_configs_dir() / \"secrets.yaml\")\n",
    "MLFLOW_EXPERIMENT_NAME = \"AIStudio-Fine-Tuning-Experiment\"\n",
    "MLFLOW_RUN_NAME = \"AIStudio-Fine-Tuning-Run\"\n",
    "MLFLOW_MODEL_NAME = \"AIStudio-Fine-Tuning-Model\"\n",
    "MODEL_SERVICE_RUN_NAME=\"AIStudio-Fine-Tuning-Service-Run\"\n",
    "MODEL_SERVICE_NAME=\"AIStudio-Fine-Tuning-Model\"\n",
    "MODEL_SERVICE_EXPERIMENT_NAME=\"AIStudio-Fine-Tuning-Experiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dc3f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"fine_tuning_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57ce8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c805cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Notebook execution started.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50143f8-ea8e-4e16-a33d-2a4b5f397222",
   "metadata": {},
   "source": [
    "### Proxy Configuration\n",
    "For certain enterprise networks, a proxy configuration might be required for external service connections. If this is your case, set up the \"proxy\" field in your config.yaml and the following cell will configure the necessary environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd88721-583c-47d9-9d50-fa2f232ab176",
   "metadata": {},
   "outputs": [],
   "source": [
    "config, secrets = load_config_and_secrets(CONFIG_PATH, SECRETS_PATH)\n",
    "\n",
    "# Configure proxy using the loaded config\n",
    "configure_proxy(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585a4dd8-5f6d-4158-bbe6-6500d3e1ccd4",
   "metadata": {},
   "source": [
    "### üîç Model Selector\n",
    "\n",
    "Below are the available models for fine-tuning with ORPO.  \n",
    "> ‚ö†Ô∏è **Note:** Make sure your Hugging Face account has access permissions for the selected model (some require manual approval).\n",
    "\n",
    "| Model ID | Hugging Face Link |\n",
    "|----------|-------------------|\n",
    "| `mistralai/Mistral-7B-Instruct-v0.1` | [üîó View on Hugging Face](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) |\n",
    "| `meta-llama/Llama-2-7b-chat-hf` | [üîó View on Hugging Face](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) |\n",
    "| `meta-llama/Meta-Llama-3-8B-Instruct` | [üîó View on Hugging Face](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) |\n",
    "| `google/gemma-7b-it` | [üîó View on Hugging Face](https://huggingface.co/google/gemma-7b-it) |\n",
    "| `google/gemma-3-1b-it` | [üîó View on Hugging Face](https://huggingface.co/google/gemma-3-1b-it) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b8e20b-84b3-4837-9727-fd959e980354",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL =  \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6cfd57-da28-49dd-b6b6-50ca4189c766",
   "metadata": {},
   "source": [
    "### üîê Login to Hugging Face\n",
    "\n",
    "To access gated models (e.g., LLaMA, Mistral, or Gemma), you must authenticate using your Hugging Face token.\n",
    "\n",
    "Make sure your `secrets.yaml` file contains the following key:\n",
    "\n",
    "```yaml\n",
    "HUGGINGFACE_API_KEY: your_huggingface_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c965d03b-bf88-4caf-a736-e6c52aefc277",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "login_huggingface(secrets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6825c882-1649-4f5c-94de-5b405a5af1c0",
   "metadata": {},
   "source": [
    "### Attention Optimization Config\n",
    "Automatically selects the most efficient attention implementation and data type (dtype) based on the GPU‚Äôs compute capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cace749-80b9-4e5a-8a45-c40df3521dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    !pip install -qqq flash-attn\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91c97e2",
   "metadata": {},
   "source": [
    "## Verify Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b64cdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_asset_status(asset_path: str, asset_name: str, success_message: str, failure_message: str) -> None:\n",
    "    \"\"\"\n",
    "    Logs the status of a given asset based on its existence.\n",
    "\n",
    "    Parameters:\n",
    "        asset_path (str): File or directory path to check.\n",
    "        asset_name (str): Name of the asset for logging context.\n",
    "        success_message (str): Message to log if asset exists.\n",
    "        failure_message (str): Message to log if asset does not exist.\n",
    "    \"\"\"\n",
    "    if Path(asset_path).exists():\n",
    "        logger.info(f\"{asset_name} is properly configured. {success_message}\")\n",
    "    else:\n",
    "        logger.info(f\"{asset_name} is not properly configured. {failure_message}\")\n",
    "\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=CONFIG_PATH,\n",
    "    asset_name=\"configs.yaml\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if the configs.yaml was propely connfigured in your project on AI Studio.\"\n",
    ")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=SECRETS_PATH,\n",
    "    asset_name=\"secrets.yaml\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if the secrets.yaml was propely connfigured in your project on AI Studio.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c92c03-006e-4316-a411-2f8a3960590c",
   "metadata": {},
   "source": [
    "## Model Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4caff0c-9f4a-40cb-b0d1-49a1afccbd6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selector = ModelSelector()\n",
    "selector.select_model(MODEL)\n",
    "\n",
    "model = selector.get_model()\n",
    "tokenizer = selector.get_tokenizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f763541-d805-4110-bec6-cb2c94c72e14",
   "metadata": {},
   "source": [
    "## ü§ñ Inference with Default Model\n",
    "\n",
    "The following cell runs inference using the base (non fine-tuned) model you selected earlier.\n",
    "\n",
    "We've prepared a few prompts to test different types of reasoning and writing skills.  \n",
    "You can later compare these outputs with the results generated by the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eee80d8-b12d-46a7-b057-718cf8dc5079",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# üìã Custom prompts for evaluation\n",
    "prompts = [\n",
    "    \"I need to write some nodejs code that publishes a message to a Telegram group.\",\n",
    "    \"What advice would you give to a frontend developer?\",\n",
    "    \"Propose a solution that could reduce the rate of deforestation.\",\n",
    "    \"Write a eulogy for a public figure who inspired you.\"\n",
    "]\n",
    "\n",
    "# ‚öôÔ∏è Run inference with the selected model\n",
    "runner = InferenceRunner(selector)\n",
    "\n",
    "for idx, prompt in enumerate(prompts, 1):\n",
    "    response = runner.infer(prompt)\n",
    "    print(f\"\\nüü¢ Prompt {idx}: {prompt}\\nüîΩ Model Response:\\n{response}\\n{'-'*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8185a001-2ed0-481f-8b3c-b7651ff8523d",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Creating the Fine-Tuned Model Name (ORPO)\n",
    "\n",
    "We define a clean and consistent name for the fine-tuned version of the selected base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08c6b04-0354-4fe6-aa6e-98fd08ca08ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = selector.model_id\n",
    "model_path = selector.format_model_path(base_model)\n",
    "new_model = f\"Orpo-{base_model.split('/')[-1]}-FT\"\n",
    "fine_tuned_name = f\"Orpo-{base_model.split('/')[-1]}-FT\"\n",
    "\n",
    "fine_tuned_dir = get_fine_tuned_models_dir()\n",
    "fine_tuned_dir.mkdir(parents=True, exist_ok=True)\n",
    "fine_tuned_path = str(fine_tuned_dir / fine_tuned_name)\n",
    "\n",
    "print(f\"Fine-tuned model will be saved to: {fine_tuned_path}\")\n",
    "print(f\"Directory exists: {Path(fine_tuned_path).parent.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c562b4c-3e2b-4ce0-b9dd-020ec89d45d6",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è  Automatic Quantization Configuration\n",
    "\n",
    "We use an intelligent selector to automatically choose the optimal quantization strategy for the hardware environment.\n",
    "\n",
    "- `QuantizationSelector()` analyzes the number of available GPUs and their memory.\n",
    "- If multiple GPUs with sufficient VRAM are detected, it applies 8-bit quantization for faster performance.\n",
    "- Otherwise, it falls back to `4-bit QLoRA` using `nf4` and double quantization to reduce memory usage.\n",
    "\n",
    "This adaptive configuration ensures efficient fine-tuning of large language models by balancing performance and hardware constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311e4393-0a58-403a-8137-6fd75fd4371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization = QuantizationSelector()\n",
    "bnb_config = quantization.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08163f23-e86a-4873-98a9-1b405c7d2065",
   "metadata": {},
   "source": [
    "### üß© PEFT Configuration (LoRA)\n",
    "\n",
    "We define the LoRA configuration using the `LoraConfig` from PEFT (Parameter-Efficient Fine-Tuning).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eabcbe2-f5eb-48f5-a40e-1ab4b8f2e15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=LoRATargetMapper.get_target_modules(base_model)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e0f756-0e6b-406d-8a4e-ce139522b0f6",
   "metadata": {},
   "source": [
    "### üß† Load and Prepare Base Model for Training\n",
    "\n",
    "In this step, we load the base model and tokenizer from the local path, apply the quantization configuration (`bnb_config`), prepare it for tra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1ea031-aed8-4ec3-a5d3-63f1217f0ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vocab_size = AutoModelForCausalLM.from_pretrained(model_path).config.vocab_size\n",
    "tokenizer_vocab_size = len(tokenizer)\n",
    "\n",
    "if tokenizer_vocab_size != model_vocab_size:\n",
    "    print(f\"‚ö†Ô∏è Adjusting vocabulary ({tokenizer_vocab_size}) ‚â† Model ({model_vocab_size})\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token  \n",
    "    tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43be1d90-7d0e-42f0-8b9b-e03fd2ee99db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c40902b-9ff1-4fb0-b04f-f3387784a6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safely apply chat format only if tokenizer doesn't already have a chat_template\n",
    "if tokenizer.chat_template is None:\n",
    "    model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Tokenizer already has a chat_template. Skipping setup_chat_format to avoid overwriting.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c845a74e-764d-4324-a64a-74c0e385ee0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a99c22c-d97a-4f9d-a396-db890c1f1e50",
   "metadata": {},
   "source": [
    "## üìö Dataset Loader\n",
    "\n",
    "We use the [UltraFeedback Binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized) dataset provided by Hugging Face.\n",
    "\n",
    "This dataset contains prompts along with two model-generated responses:\n",
    "- **chosen**: the response preferred by human annotators\n",
    "- **rejected**: the less preferred one\n",
    "\n",
    "For this experiment, we load a subset of the data to speed up training and evaluation.  \n",
    "A fixed seed ensures reproducibility when shuffling the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812d6d5f-0877-4850-acda-f2a612ea261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"HuggingFaceH4/ultrafeedback_binarized\", split=[\"train_prefs\", \"test_prefs\"])\n",
    "\n",
    "# üìä Define sample sizes for a lightweight experiment\n",
    "train_samples = 5000                         # Subset size for training\n",
    "original_train_samples = 61135              # Total training examples in the original dataset\n",
    "test_samples = int((2000 / original_train_samples) * train_samples)  # Proportional test size\n",
    "\n",
    "# üîÄ Shuffle and sample subsets from both splits\n",
    "train_subset = dataset[0].shuffle(seed=42).select(range(train_samples))\n",
    "test_subset = dataset[1].shuffle(seed=42).select(range(test_samples))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ae9ddd-b459-487d-b9fa-a2ca42d8eaf2",
   "metadata": {},
   "source": [
    "### üìä Dataset Visualization\n",
    "\n",
    "To help understand how the dataset works, we use the `UltraFeedbackVisualizer`.\n",
    "\n",
    "This tool logs examples from the dataset into **TensorBoard**, including:\n",
    "- The **original prompt** given to the model\n",
    "- The two possible answers: one **preferred by humans** and one that was **rejected**\n",
    "- A simple comparison showing which response was rated better\n",
    "\n",
    "Each example is displayed with clear labels and scores to help illustrate the kinds of outputs humans value more ‚Äî **before we do any fine-tuning**.\n",
    "\n",
    "> This is useful to explore what ‚Äúgood answers‚Äù look like, based on real human feedback.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a8b0bd-b153-4771-a196-fd0f6b80e761",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualizer = UltraFeedbackVisualizer(train_subset, test_subset,max_samples=20)\n",
    "visualizer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e3ec8a-1c9b-4d79-8960-3fdbeae10449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(row):\n",
    "    \"\"\"\n",
    "    Specifies how to convert row into a tokenizable string in the expected model format\n",
    "    \"\"\"\n",
    "    row[\"chosen\"] = tokenizer.apply_chat_template(row[\"chosen\"], tokenize=False)\n",
    "    row[\"rejected\"] = tokenizer.apply_chat_template(row[\"rejected\"], tokenize=False)\n",
    "    return row\n",
    "\n",
    "dataset[0] = train_subset.map(\n",
    "    process,\n",
    "    num_proc= multiprocessing.cpu_count(),\n",
    "    load_from_cache_file=False,\n",
    ")\n",
    "\n",
    "dataset[1] = test_subset.map(\n",
    "    process,\n",
    "    num_proc= multiprocessing.cpu_count(),\n",
    "    load_from_cache_file=False,\n",
    ")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ee1788-fc5a-4afd-bb22-d5b812b41635",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è ORPO Configuration\n",
    "\n",
    "We define the training configuration using the `ORPOConfig` class from TRL (Transformers Reinforcement Learning).\n",
    "\n",
    "This configuration controls how the model will be fine-tuned using ORPO (Offline Reinforcement Preference Optimization), a technique that aligns model outputs with human preferences.\n",
    "\n",
    "Key parameters include:\n",
    "- `learning_rate`: sets how fast the model updates (8e-6 is typical for PEFT)\n",
    "- `beta`: the strength of the ORPO loss term\n",
    "- `optim`: uses 8-bit optimizer for memory efficiency (paged_adamw_8bit)\n",
    "- `max_steps`: controls how long training will run (e.g., 1000 steps)\n",
    "- `eval_strategy` and `eval_steps`: defines how and when to evaluate during training\n",
    "- `output_dir`: directory to save the trained model\n",
    "\n",
    "> This configuration is compatible with all the selected models (e.g., Mistral, LLaMA, Gemma) and optimized for QLoRA fine-tuning on consumer or research-grade GPUs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dfc31b-313d-46c7-ac16-80534efb8169",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri('/phoenix/mlflow')\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\n",
    "\n",
    "# Ensure training output directory exists\n",
    "training_output_dir = get_output_dir() / \"training_results\"\n",
    "training_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "orpo_args = ORPOConfig(\n",
    "    learning_rate=8e-6,\n",
    "    beta=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    max_length=1024,\n",
    "    max_prompt_length=512,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    max_steps=1000,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    report_to=[\"mlflow\",\"tensorboard\"],\n",
    "    output_dir=str(training_output_dir),\n",
    ")\n",
    "\n",
    "print(f\"Training output directory: {training_output_dir}\")\n",
    "print(f\"Directory exists: {training_output_dir.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d084b1-62da-4ed7-8d49-729db9bb4e3d",
   "metadata": {},
   "source": [
    "### üöÄ ORPO Trainer\n",
    "\n",
    "We now initialize the `ORPOTrainer`, which orchestrates the fine-tuning process using the Offline Reinforcement Preference Optimization (ORPO) strategy.\n",
    "\n",
    "It takes as input:\n",
    "- The **base model**, already prepared with QLoRA and chat formatting\n",
    "- The **ORPO configuration** (`orpo_args`) containing all training hyperparameters\n",
    "- The **training and evaluation datasets**\n",
    "- The **LoRA configuration** (`peft_config`) for parameter-efficient fine-tuning\n",
    "- The **tokenizer**, passed as a `processing_class`, to apply proper formatting and padding\n",
    "\n",
    "Once initialized, the trainer will be ready to start training with `trainer.train()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ed75d4-44c8-4284-9f12-3da8f4cce525",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ORPOTrainer(\n",
    "    model=model,\n",
    "    args=orpo_args,\n",
    "    train_dataset=dataset[0],\n",
    "    eval_dataset=dataset[1],\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca638984-d847-41fd-b918-78517ba82817",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "# Copy the final model to our desired fine_tuned_path location\n",
    "import shutil\n",
    "if Path(orpo_args.output_dir).exists():\n",
    "    # Remove existing fine_tuned_path if it exists\n",
    "    if Path(fine_tuned_path).exists():\n",
    "        shutil.rmtree(fine_tuned_path)\n",
    "    \n",
    "    # Copy the trained model to our desired location\n",
    "    shutil.copytree(orpo_args.output_dir, fine_tuned_path)\n",
    "    print(f\"Model copied to: {fine_tuned_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bad850f-43fc-4d88-a276-18fa70726863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the LoRA adapters in the training output directory\n",
    "training_output_dir = Path(orpo_args.output_dir)\n",
    "\n",
    "# Look for adapter_config.json in checkpoint subdirectories\n",
    "adapter_configs = list(training_output_dir.rglob(\"adapter_config.json\"))\n",
    "if adapter_configs:\n",
    "    # Use the directory containing adapter_config.json (typically checkpoint-X)\n",
    "    lora_adapter_path = str(adapter_configs[0].parent)\n",
    "    print(f\"Found LoRA adapters at: {lora_adapter_path}\")\n",
    "    \n",
    "    # Merge LoRA adapters with base model\n",
    "    merge_lora_and_save(\n",
    "        base_model_id=MODEL,\n",
    "        finetuned_lora_path=lora_adapter_path\n",
    "    )\n",
    "else:\n",
    "    print(\"‚ùå No LoRA adapters found in training output directory!\")\n",
    "    print(\"This might indicate an issue with the training process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4242ea0-b894-4852-9263-2e48eedaf7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the merged fine-tuned model for inference\n",
    "final_model_path = str(get_fine_tuned_models_dir() / fine_tuned_name)\n",
    "\n",
    "print(f\"Loading fine-tuned model from: {final_model_path}\")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(final_model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(final_model_path, torch_dtype=torch.float16).cuda().eval()\n",
    "\n",
    "# Test the fine-tuned model with a sample prompt\n",
    "prompt = \"Propose a solution that could reduce the rate of deforestation\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=500)\n",
    "\n",
    "print(\"\\nFine-tuned Model Response:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5201cd97",
   "metadata": {},
   "source": [
    "## üîç Model Evaluation and Comparison\n",
    "\n",
    "After completing the ORPO fine-tuning process, we can evaluate the performance improvements by comparing responses from the base model and our fine-tuned model.\n",
    "\n",
    "This comparison helps us understand:\n",
    "- **Quality Improvements**: How the fine-tuned model generates more helpful and aligned responses\n",
    "- **Training Effectiveness**: Whether the ORPO training successfully improved the model's preference alignment\n",
    "- **Response Consistency**: How well the model maintains coherent and relevant outputs\n",
    "\n",
    "The comparison uses the same test prompts to ensure fair evaluation between the base and fine-tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf71f8e-0e1a-4732-8f6f-1c62d128d4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare base model vs fine-tuned model using ModelComparer\n",
    "final_model_path = str(get_fine_tuned_models_dir() / fine_tuned_name)\n",
    "\n",
    "print(\"üîç MODEL COMPARISON RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize the ModelComparer\n",
    "comparer = ModelComparer()\n",
    "\n",
    "# Load both models\n",
    "base_model_selector = ModelSelector()\n",
    "print(f\"üìä Base model: {base_model_selector.model_id}\")\n",
    "print(f\"üìä Fine-tuned model: {final_model_path}\")\n",
    "\n",
    "# Define test prompts for comparison\n",
    "test_prompts = [\n",
    "    \"Explain the importance of sustainable agriculture.\",\n",
    "    \"Write a Python function to check for palindromes.\",\n",
    "    \"Describe the benefits of renewable energy sources.\",\n",
    "    \"What are the key principles of machine learning?\"\n",
    "]\n",
    "\n",
    "# Run comparison using ModelComparer\n",
    "print(\"\\nüöÄ Running model comparison...\")\n",
    "comparison_results = comparer.compare_models(\n",
    "    base_model_path=base_model_selector.format_model_path(base_model_selector.model_id),\n",
    "    finetuned_model_path=final_model_path,\n",
    "    prompts=test_prompts,\n",
    "    max_new_tokens=150\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nüìù COMPARISON RESULTS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, result in enumerate(comparison_results, 1):\n",
    "    print(f\"\\n--- Test Prompt {i} ---\")\n",
    "    print(f\"Prompt: {result['prompt']}\")\n",
    "    print(f\"\\nüî∏ Base Model Response:\")\n",
    "    print(result['base_response'])\n",
    "    print(f\"\\nüîπ Fine-tuned Model Response:\")\n",
    "    print(result['finetuned_response'])\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "print(\"\\n‚úÖ Model comparison completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4987f4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Notebook execution completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e391ccbf-ca08-4299-93c0-f67fb2551a59",
   "metadata": {},
   "source": [
    "Built with ‚ù§Ô∏è using Z by HP AI Studio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
