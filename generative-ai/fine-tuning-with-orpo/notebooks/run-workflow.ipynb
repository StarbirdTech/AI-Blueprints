{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec84ac00-1a02-4fe0-af00-31c2232ba831",
   "metadata": {},
   "source": [
    "<h1 style=\\\"text-align: center; font-size: 50px;\\\"> Interactive ORPO Fine-Tuning & Inference Hub for Open LLMs </h1>\n",
    "\n",
    "This experiment provides an interactive and modular interface for selecting, downloading, fine-tuning, and evaluating large language models using ORPO (Optimal Reward Preferring Optimization).\n",
    "The user can choose between state-of-the-art open LLMs like Mistral, LLaMA 2/3, and Gemma. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c54ddb0",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "- Start Execution\n",
    "- Install and Import Libraries\n",
    "- Configure Settings\n",
    "- Verify Assets\n",
    "- Model Loader\n",
    "- Inference with Default Model\n",
    "- Creating the Fine-Tuned Model Name (ORPO)\n",
    "- Dataset Loader\n",
    "- ORPO Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66af6b10",
   "metadata": {},
   "source": [
    "# Start Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dc1ccf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "# Configure logger\n",
    "logger: logging.Logger = logging.getLogger(\"run_workflow_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.propagate = False  # Prevent duplicate logs from parent loggers\n",
    "\n",
    "# Set formatter\n",
    "formatter: logging.Formatter = logging.Formatter(\n",
    "    fmt=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "# Configure and attach stream handler\n",
    "stream_handler: logging.StreamHandler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a99fc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 16:20:55 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()  \n",
    "\n",
    "logger.info(\"Notebook execution started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54051b27-5b97-47cd-89b2-4f2861711f14",
   "metadata": {},
   "source": [
    "## üì¶ Install and Import Libraries\n",
    "\n",
    "By using our Local GenAI workspace image, most of the necessary libraries to work with ORPO-based fine-tuning and evaluation already come pre-installed. In this notebook, we only need to import components for model loading, quantization, inference, and feedback visualization to run the complete ORPO workflow locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be25e3e1-7f9c-4536-8304-c9f1762a8f63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 36 ms, sys: 12 ms, total: 48 ms\n",
      "Wall time: 1.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "%pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6b15f11-b47a-4735-ab36-1063e88489f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 16:21:05.588911: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-06 16:21:05.741003: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754497265.808964    1630 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754497265.830784    1630 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754497265.977542    1630 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754497265.977587    1630 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754497265.977588    1630 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754497265.977590    1630 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-06 16:21:05.994594: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-06 16:21:11,422] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/opt/conda/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# üß† Core Libraries\n",
    "# ===============================\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import torch\n",
    "import multiprocessing\n",
    "import mlflow\n",
    "from datasets import load_dataset\n",
    "from typing import Dict, Any, Optional, Union, List, Tuple\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# üß™ Hugging Face & Transformers\n",
    "# ===============================\n",
    "from huggingface_hub import login\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# üß© Fine-tuning (ORPO + PEFT)\n",
    "# ===============================\n",
    "from trl import ORPOConfig, ORPOTrainer, setup_chat_format\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "\n",
    "# ===============================\n",
    "# üß∞ Project Modules: Core Pipeline\n",
    "# ===============================\n",
    "# Add the core directory to the path to import utils\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from core.selection.model_selection import ModelSelector\n",
    "from core.local_inference.inference import InferenceRunner\n",
    "from core.target_mapper.lora_target_mapper import LoRATargetMapper\n",
    "from core.data_visualizer.feedback_visualizer import UltraFeedbackVisualizer\n",
    "from core.finetuning_inference.inference_runner import AcceleratedInferenceRunner\n",
    "from core.merge_model.merge_lora import merge_lora_and_save\n",
    "from core.quantization.quantization_config import QuantizationSelector\n",
    "from core.comparer.model_comparer import ModelComparer\n",
    "\n",
    "# ===============================\n",
    "# ‚öôÔ∏è Utility Functions\n",
    "# ===============================\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from src.utils import (\n",
    "    load_configuration,\n",
    "    load_secrets,\n",
    "    load_secrets_to_env,\n",
    "    configure_proxy,\n",
    "    login_huggingface,\n",
    "    get_project_root,\n",
    "    get_configs_dir,\n",
    "    get_output_dir,\n",
    "    get_models_dir,\n",
    "    get_fine_tuned_models_dir,\n",
    "    get_model_cache_dir,\n",
    "    format_model_path,\n",
    "    setup_model_environment\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625a13c8-7f4d-4942-8edd-888695bba2a4",
   "metadata": {},
   "source": [
    "# Configure Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac369051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress Python warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a97b90b-ed6f-4546-86d5-59279ed7ebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = str(get_configs_dir() / \"config.yaml\")\n",
    "SECRETS_PATH = str(get_configs_dir() / \"secrets.yaml\")\n",
    "MLFLOW_EXPERIMENT_NAME = \"AIStudio-Fine-Tuning-Experiment\"\n",
    "MLFLOW_RUN_NAME = \"AIStudio-Fine-Tuning-Run\"\n",
    "MLFLOW_MODEL_NAME = \"AIStudio-Fine-Tuning-Model\"\n",
    "MODEL_SERVICE_RUN_NAME=\"AIStudio-Fine-Tuning-Service-Run\"\n",
    "MODEL_SERVICE_NAME=\"AIStudio-Fine-Tuning-Model\"\n",
    "MODEL_SERVICE_EXPERIMENT_NAME=\"AIStudio-Fine-Tuning-Experiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a57ce8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054ba56a",
   "metadata": {},
   "source": [
    "### Configuration and Secrets Loading\n",
    "\n",
    "In this section, we load configuration parameters and API keys from separate YAML files. This separation helps maintain security by keeping sensitive information (API keys) separate from configuration settings.\n",
    "\n",
    "- **config.yaml**: Contains non-sensitive configuration parameters like model sources and URLs\n",
    "- **secrets.yaml**: Contains sensitive API keys for services like HuggingFace\n",
    "- *(Optional for Premium users)* Secrets such as API keys for services like HuggingFace can be stored as environment variables for the project and loaded into the notebook (see the project's README file for steps on how to save secrets in Secrets Manager)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c26e81cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No secrets file found at /home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/configs/secrets.yaml; relying on preexisting environment\n",
      "‚úÖ Configuration loaded successfully\n",
      "‚úÖ Secrets loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load secrets from secrets.yaml file (if it exists) into environment\n",
    "if Path(SECRETS_PATH).exists():\n",
    "    load_secrets_to_env(SECRETS_PATH)\n",
    "else:\n",
    "    print(f\"No secrets file found at {SECRETS_PATH}; relying on preexisting environment\")\n",
    "\n",
    "# Retrieve secrets from environment\n",
    "try:\n",
    "    secrets = load_secrets()\n",
    "except ValueError:\n",
    "    secrets = {}\n",
    "\n",
    "# Load configuration and secrets\n",
    "config = load_configuration(CONFIG_PATH)\n",
    "\n",
    "print(\"‚úÖ Configuration loaded successfully\")\n",
    "print(\"‚úÖ Secrets loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50143f8-ea8e-4e16-a33d-2a4b5f397222",
   "metadata": {},
   "source": [
    "### Proxy Configuration\n",
    "For certain enterprise networks, a proxy configuration might be required for external service connections. If this is your case, set up the \"proxy\" field in your config.yaml and the following cell will configure the necessary environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbd88721-583c-47d9-9d50-fa2f232ab176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure proxy using the loaded config\n",
    "configure_proxy(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585a4dd8-5f6d-4158-bbe6-6500d3e1ccd4",
   "metadata": {},
   "source": [
    "### üîç Model Selector\n",
    "\n",
    "Below are the available models for fine-tuning with ORPO.  \n",
    "> ‚ö†Ô∏è **Note:** Make sure your Hugging Face account has access permissions for the selected model (some require manual approval).\n",
    "\n",
    "| Model ID | Hugging Face Link |\n",
    "|----------|-------------------|\n",
    "| `mistralai/Mistral-7B-Instruct-v0.1` | [üîó View on Hugging Face](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) |\n",
    "| `meta-llama/Llama-2-7b-chat-hf` | [üîó View on Hugging Face](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) |\n",
    "| `meta-llama/Meta-Llama-3-8B-Instruct` | [üîó View on Hugging Face](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) |\n",
    "| `google/gemma-7b-it` | [üîó View on Hugging Face](https://huggingface.co/google/gemma-7b-it) |\n",
    "| `google/gemma-3-1b-it` | [üîó View on Hugging Face](https://huggingface.co/google/gemma-3-1b-it) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14b8e20b-84b3-4837-9727-fd959e980354",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL =  \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6cfd57-da28-49dd-b6b6-50ca4189c766",
   "metadata": {},
   "source": [
    "### üîê Login to Hugging Face\n",
    "\n",
    "To access gated models (e.g., LLaMA, Mistral, or Gemma), you must authenticate using your Hugging Face token.\n",
    "\n",
    "Make sure your `secrets.yaml` file (or AIS Secrets Manager for Premium Users) contains the following key:\n",
    "\n",
    "```yaml\n",
    "AIS_HUGGINGFACE_API_KEY: your_huggingface_token\n",
    "```\n",
    "\n",
    "**Note**: Please refer to this project's README for detailed instuctions on how to configure secrets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c965d03b-bf88-4caf-a736-e6c52aefc277",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 16:21:13 - WARNING - ‚ö†Ô∏è Hugging Face authentication failed: ‚ùå Hugging Face token not found in secrets.yaml.\n",
      "2025-08-06 16:21:13 - INFO - Some models may not be accessible if they require authentication\n"
     ]
    }
   ],
   "source": [
    "# Login to Hugging Face (required for downloading gated models)\n",
    "try:\n",
    "    login_huggingface(secrets)\n",
    "    logger.info(\"‚úÖ Hugging Face authentication successful\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"‚ö†Ô∏è Hugging Face authentication failed: {e}\")\n",
    "    logger.info(\"Some models may not be accessible if they require authentication\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6825c882-1649-4f5c-94de-5b405a5af1c0",
   "metadata": {},
   "source": [
    "### Attention Optimization Config\n",
    "Automatically selects the most efficient attention implementation and data type (dtype) based on the GPU‚Äôs compute capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cace749-80b9-4e5a-8a45-c40df3521dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    !pip install -qqq flash-attn\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91c97e2",
   "metadata": {},
   "source": [
    "## Verify Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b64cdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 16:21:14 - INFO - Config is properly configured. \n",
      "2025-08-06 16:21:14 - INFO - There are no project secrets found. Please check if the secrets were propely connfigured.\n"
     ]
    }
   ],
   "source": [
    "def log_asset_status(asset_path: str, asset_name: str, success_message: str, failure_message: str) -> None:\n",
    "    \"\"\"\n",
    "    Logs the status of a given asset based on its existence.\n",
    "\n",
    "    Parameters:\n",
    "        asset_path (str): File or directory path to check.\n",
    "        asset_name (str): Name of the asset for logging context.\n",
    "        success_message (str): Message to log if asset exists.\n",
    "        failure_message (str): Message to log if asset does not exist.\n",
    "    \"\"\"\n",
    "    if Path(asset_path).exists():\n",
    "        logger.info(f\"{asset_name} is properly configured. {success_message}\")\n",
    "    else:\n",
    "        logger.info(f\"{asset_name} is not properly configured. {failure_message}\")\n",
    "\n",
    "def log_secrets_status(secrets: Dict[str, Any], success_message: str, failure_message: str) -> None:\n",
    "    \"\"\"\n",
    "    Logs the status of secrets based on their existence.\n",
    "\n",
    "    Parameters:\n",
    "        secrets (Dict[str, Any]): Secrets retrieved to check if they exist.\n",
    "        success_message (str): Message to log if secrets exists.\n",
    "        failure_message (str): Message to log if secrets do not exist.\n",
    "    \"\"\"\n",
    "    if secrets:\n",
    "        logger.info(f\"Project secrets are available. {success_message}\")\n",
    "    else:\n",
    "        logger.info(f\"There are no project secrets found. {failure_message}\")\n",
    "\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=CONFIG_PATH,\n",
    "    asset_name=\"Config\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if the configs.yaml was propely connfigured in your project on AI Studio.\"\n",
    ")\n",
    "\n",
    "log_secrets_status(\n",
    "    secrets=secrets,\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if the secrets were propely connfigured.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c92c03-006e-4316-a411-2f8a3960590c",
   "metadata": {},
   "source": [
    "## Model Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4caff0c-9f4a-40cb-b0d1-49a1afccbd6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ModelSelector:[ModelSelector] Selected model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "INFO:ModelSelector:[ModelSelector] Downloading model snapshot to: /home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/models/TinyLlama__TinyLlama-1.1B-Chat-v1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec09515ee8694c3cb6aa75fefeed8854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ModelSelector:[ModelSelector] ‚úÖ Model downloaded successfully to: /home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/models/TinyLlama__TinyLlama-1.1B-Chat-v1.0\n",
      "INFO:ModelSelector:[ModelSelector] Loading model and tokenizer from: /home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/models/TinyLlama__TinyLlama-1.1B-Chat-v1.0\n",
      "INFO:ModelSelector:[ModelSelector] Checking model for ORPO compatibility...\n",
      "INFO:ModelSelector:[ModelSelector] ‚úÖ Model 'TinyLlama/TinyLlama-1.1B-Chat-v1.0' is ORPO-compatible.\n"
     ]
    }
   ],
   "source": [
    "selector = ModelSelector()\n",
    "selector.select_model(MODEL)\n",
    "\n",
    "model = selector.get_model()\n",
    "tokenizer = selector.get_tokenizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f763541-d805-4110-bec6-cb2c94c72e14",
   "metadata": {},
   "source": [
    "## ü§ñ Inference with Default Model\n",
    "\n",
    "The following cell runs inference using the base (non fine-tuned) model you selected earlier.\n",
    "\n",
    "We've prepared a few prompts to test different types of reasoning and writing skills.  \n",
    "You can later compare these outputs with the results generated by the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0eee80d8-b12d-46a7-b057-718cf8dc5079",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:InferenceRunner:[InferenceRunner] Detected 1 GPU, loading single-GPU config.\n",
      "INFO:InferenceRunner:[InferenceRunner] Loading model and tokenizer from: /home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/models/TinyLlama__TinyLlama-1.1B-Chat-v1.0\n",
      "INFO:InferenceRunner:[InferenceRunner] running: I need to write some nodejs code that publishes a message to a Telegram group....\n",
      "INFO:InferenceRunner:[InferenceRunner] ‚úÖ Inference complete.\n",
      "INFO:InferenceRunner:[InferenceRunner] running: What advice would you give to a frontend developer?...\n",
      "INFO:InferenceRunner:[InferenceRunner] ‚úÖ Inference complete.\n",
      "INFO:InferenceRunner:[InferenceRunner] running: Propose a solution that could reduce the rate of deforestation....\n",
      "INFO:InferenceRunner:[InferenceRunner] ‚úÖ Inference complete.\n",
      "INFO:InferenceRunner:[InferenceRunner] running: Write a eulogy for a public figure who inspired you....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üü¢ Prompt 1: I need to write some nodejs code that publishes a message to a Telegram group.\n",
      "üîΩ Model Response:\n",
      "I need to write some nodejs code that publishes a message to a Telegram group. I followed the steps mentioned in the documentation to set it up, but I'm having trouble testing it. I'm using the `node-telegram-bot-api` library and I've set up a simple server that listens for incoming messages. Here's the code:\n",
      "\n",
      "```javascript\n",
      "const { Client } = require('node-telegram-bot-api');\n",
      "const { token } = require('./config.json');\n",
      "\n",
      "const bot = new\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üü¢ Prompt 2: What advice would you give to a frontend developer?\n",
      "üîΩ Model Response:\n",
      "What advice would you give to a frontend developer?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üü¢ Prompt 3: Propose a solution that could reduce the rate of deforestation.\n",
      "üîΩ Model Response:\n",
      "Propose a solution that could reduce the rate of deforestation.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:InferenceRunner:[InferenceRunner] ‚úÖ Inference complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üü¢ Prompt 4: Write a eulogy for a public figure who inspired you.\n",
      "üîΩ Model Response:\n",
      "Write a eulogy for a public figure who inspired you. Make sure to provide an overview of their life story and how they impacted society. Use specific examples and anecdotes to showcase their contributions and impact. Consider incorporating themes such as leadership, vision, and inspiration, and use poetic language and symbolism to convey your message. Your eulogy should conclude with a reflection on the legacy left behind by the public figure and a call to action for future generations to emulate their values and impact.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# üìã Custom prompts for evaluation\n",
    "prompts = [\n",
    "    \"I need to write some nodejs code that publishes a message to a Telegram group.\",\n",
    "    \"What advice would you give to a frontend developer?\",\n",
    "    \"Propose a solution that could reduce the rate of deforestation.\",\n",
    "    \"Write a eulogy for a public figure who inspired you.\"\n",
    "]\n",
    "\n",
    "# ‚öôÔ∏è Run inference with the selected model\n",
    "runner = InferenceRunner(selector)\n",
    "\n",
    "for idx, prompt in enumerate(prompts, 1):\n",
    "    response = runner.infer(prompt)\n",
    "    print(f\"\\nüü¢ Prompt {idx}: {prompt}\\nüîΩ Model Response:\\n{response}\\n{'-'*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8185a001-2ed0-481f-8b3c-b7651ff8523d",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Creating the Fine-Tuned Model Name (ORPO)\n",
    "\n",
    "We define a clean and consistent name for the fine-tuned version of the selected base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a08c6b04-0354-4fe6-aa6e-98fd08ca08ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model will be saved to: /home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/output/fine_tuned_models/Orpo-TinyLlama-1.1B-Chat-v1.0-FT\n",
      "Directory exists: True\n"
     ]
    }
   ],
   "source": [
    "base_model = selector.model_id\n",
    "model_path = selector.format_model_path(base_model)\n",
    "new_model = f\"Orpo-{base_model.split('/')[-1]}-FT\"\n",
    "fine_tuned_name = f\"Orpo-{base_model.split('/')[-1]}-FT\"\n",
    "\n",
    "fine_tuned_dir = get_fine_tuned_models_dir()\n",
    "fine_tuned_dir.mkdir(parents=True, exist_ok=True)\n",
    "fine_tuned_path = str(fine_tuned_dir / fine_tuned_name)\n",
    "\n",
    "print(f\"Fine-tuned model will be saved to: {fine_tuned_path}\")\n",
    "print(f\"Directory exists: {Path(fine_tuned_path).parent.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c562b4c-3e2b-4ce0-b9dd-020ec89d45d6",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è  Automatic Quantization Configuration\n",
    "\n",
    "We use an intelligent selector to automatically choose the optimal quantization strategy for the hardware environment.\n",
    "\n",
    "- `QuantizationSelector()` analyzes the number of available GPUs and their memory.\n",
    "- If multiple GPUs with sufficient VRAM are detected, it applies 8-bit quantization for faster performance.\n",
    "- Otherwise, it falls back to `4-bit QLoRA` using `nf4` and double quantization to reduce memory usage.\n",
    "\n",
    "This adaptive configuration ensures efficient fine-tuning of large language models by balancing performance and hardware constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "311e4393-0a58-403a-8137-6fd75fd4371e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Using 4-bit quantization (fallback due to lower resources).\n"
     ]
    }
   ],
   "source": [
    "quantization = QuantizationSelector()\n",
    "bnb_config = quantization.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08163f23-e86a-4873-98a9-1b405c7d2065",
   "metadata": {},
   "source": [
    "### üß© PEFT Configuration (LoRA)\n",
    "\n",
    "We define the LoRA configuration using the `LoraConfig` from PEFT (Parameter-Efficient Fine-Tuning).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3eabcbe2-f5eb-48f5-a40e-1ab4b8f2e15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:LoRATargetMapper:‚úÖ Matched model 'TinyLlama/TinyLlama-1.1B-Chat-v1.0' to LoRA target modules: llama\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=LoRATargetMapper.get_target_modules(base_model)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e0f756-0e6b-406d-8a4e-ce139522b0f6",
   "metadata": {},
   "source": [
    "### üß† Load and Prepare Base Model for Training\n",
    "\n",
    "In this step, we load the base model and tokenizer from the local path, apply the quantization configuration (`bnb_config`), prepare it for tra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a1ea031-aed8-4ec3-a5d3-63f1217f0ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vocab_size = AutoModelForCausalLM.from_pretrained(model_path).config.vocab_size\n",
    "tokenizer_vocab_size = len(tokenizer)\n",
    "\n",
    "if tokenizer_vocab_size != model_vocab_size:\n",
    "    print(f\"‚ö†Ô∏è Adjusting vocabulary ({tokenizer_vocab_size}) ‚â† Model ({model_vocab_size})\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token  \n",
    "    tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43be1d90-7d0e-42f0-8b9b-e03fd2ee99db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c40902b-9ff1-4fb0-b04f-f3387784a6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Tokenizer already has a chat_template. Skipping setup_chat_format to avoid overwriting.\n"
     ]
    }
   ],
   "source": [
    "# Safely apply chat format only if tokenizer doesn't already have a chat_template\n",
    "if tokenizer.chat_template is None:\n",
    "    model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Tokenizer already has a chat_template. Skipping setup_chat_format to avoid overwriting.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c845a74e-764d-4324-a64a-74c0e385ee0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a99c22c-d97a-4f9d-a396-db890c1f1e50",
   "metadata": {},
   "source": [
    "## üìö Dataset Loader\n",
    "\n",
    "We use the [UltraFeedback Binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized) dataset provided by Hugging Face.\n",
    "\n",
    "This dataset contains prompts along with two model-generated responses:\n",
    "- **chosen**: the response preferred by human annotators\n",
    "- **rejected**: the less preferred one\n",
    "\n",
    "For this experiment, we load a subset of the data to speed up training and evaluation.  \n",
    "A fixed seed ensures reproducibility when shuffling the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "812d6d5f-0877-4850-acda-f2a612ea261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"HuggingFaceH4/ultrafeedback_binarized\", split=[\"train_prefs\", \"test_prefs\"])\n",
    "\n",
    "# üìä Define sample sizes for a lightweight experiment\n",
    "train_samples = 5000                         # Subset size for training\n",
    "original_train_samples = 61135              # Total training examples in the original dataset\n",
    "test_samples = int((2000 / original_train_samples) * train_samples)  # Proportional test size\n",
    "\n",
    "# üîÄ Shuffle and sample subsets from both splits\n",
    "train_subset = dataset[0].shuffle(seed=42).select(range(train_samples))\n",
    "test_subset = dataset[1].shuffle(seed=42).select(range(test_samples))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ae9ddd-b459-487d-b9fa-a2ca42d8eaf2",
   "metadata": {},
   "source": [
    "### üìä Dataset Visualization\n",
    "\n",
    "To help understand how the dataset works, we use the `UltraFeedbackVisualizer`.\n",
    "\n",
    "This tool logs examples from the dataset into **TensorBoard**, including:\n",
    "- The **original prompt** given to the model\n",
    "- The two possible answers: one **preferred by humans** and one that was **rejected**\n",
    "- A simple comparison showing which response was rated better\n",
    "\n",
    "Each example is displayed with clear labels and scores to help illustrate the kinds of outputs humans value more ‚Äî **before we do any fine-tuning**.\n",
    "\n",
    "> This is useful to explore what ‚Äúgood answers‚Äù look like, based on real human feedback.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40a8b0bd-b153-4771-a196-fd0f6b80e761",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:UltraFeedbackVisualizer:üìä Logging training samples (human feedback)...\n",
      "INFO:UltraFeedbackVisualizer:[Example 0] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 1] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 2] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 3] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 4] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 5] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 6] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 7] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 8] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 9] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 10] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 11] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 12] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 13] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 14] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 15] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 16] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 17] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 18] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 19] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:üìä Logging test samples (human feedback)...\n",
      "INFO:UltraFeedbackVisualizer:[Example 0] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 1] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 2] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 3] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 4] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 5] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 6] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 7] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 8] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 9] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 10] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 11] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 12] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 13] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 14] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 15] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 16] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 17] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 18] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:[Example 19] ‚úÖ Logged successfully.\n",
      "INFO:UltraFeedbackVisualizer:‚úÖ Human feedback logging complete!\n",
      "To visualize, run:\n",
      "tensorboard --logdir=/home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/output/tensorboard/tensorlogs --port 6006\n"
     ]
    }
   ],
   "source": [
    "visualizer = UltraFeedbackVisualizer(train_subset, test_subset,max_samples=20)\n",
    "visualizer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39e3ec8a-1c9b-4d79-8960-3fdbeae10449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561d1d7802b043c8a2305411845c0f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55dc7a1329a64c57b78087fa7f043382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/163 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset({\n",
      "    features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'score_chosen', 'score_rejected'],\n",
      "    num_rows: 5000\n",
      "}), Dataset({\n",
      "    features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'score_chosen', 'score_rejected'],\n",
      "    num_rows: 163\n",
      "})]\n"
     ]
    }
   ],
   "source": [
    "def process(row):\n",
    "    \"\"\"\n",
    "    Specifies how to convert row into a tokenizable string in the expected model format\n",
    "    \"\"\"\n",
    "    # Remove the 'messages' key to avoid conflicts with chosen/rejected format\n",
    "    if 'messages' in row:\n",
    "        del row['messages']\n",
    "    \n",
    "    # Apply chat template to chosen and rejected responses\n",
    "    row[\"chosen\"] = tokenizer.apply_chat_template(row[\"chosen\"], tokenize=False)\n",
    "    row[\"rejected\"] = tokenizer.apply_chat_template(row[\"rejected\"], tokenize=False)\n",
    "    return row\n",
    "\n",
    "dataset[0] = train_subset.map(\n",
    "    process,\n",
    "    num_proc= multiprocessing.cpu_count(),\n",
    "    load_from_cache_file=False,\n",
    ")\n",
    "\n",
    "dataset[1] = test_subset.map(\n",
    "    process,\n",
    "    num_proc= multiprocessing.cpu_count(),\n",
    "    load_from_cache_file=False,\n",
    ")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ee1788-fc5a-4afd-bb22-d5b812b41635",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è ORPO Configuration\n",
    "\n",
    "We define the training configuration using the `ORPOConfig` class from TRL (Transformers Reinforcement Learning).\n",
    "\n",
    "This configuration controls how the model will be fine-tuned using ORPO (Offline Reinforcement Preference Optimization), a technique that aligns model outputs with human preferences.\n",
    "\n",
    "Key parameters include:\n",
    "- `learning_rate`: sets how fast the model updates (8e-6 is typical for PEFT)\n",
    "- `beta`: the strength of the ORPO loss term\n",
    "- `optim`: uses 8-bit optimizer for memory efficiency (paged_adamw_8bit)\n",
    "- `max_steps`: controls how long training will run (e.g., 1000 steps)\n",
    "- `eval_strategy` and `eval_steps`: defines how and when to evaluate during training\n",
    "- `output_dir`: directory to save the trained model\n",
    "\n",
    "> This configuration is compatible with all the selected models (e.g., Mistral, LLaMA, Gemma) and optimized for QLoRA fine-tuning on consumer or research-grade GPUs.\n",
    "\n",
    "> **‚è±Ô∏è Training time notice**  \n",
    "> The duration of ORPO fine-tuning depends heavily on the model‚Äôs hyper-parameters (e.g., number of epochs, learning-rate, batch size, resolution). Reducing these values can speed up training, but expect a possible drop in quality. Tune them according to your time / quality trade-off.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11dfc31b-313d-46c7-ac16-80534efb8169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training output directory: /home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/output/training_results\n",
      "Directory exists: True\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_tracking_uri('/phoenix/mlflow')\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\n",
    "\n",
    "# Ensure training output directory exists\n",
    "training_output_dir = get_output_dir() / \"training_results\"\n",
    "training_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "orpo_args = ORPOConfig(\n",
    "    learning_rate=8e-6,\n",
    "    beta=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    max_length=1024,\n",
    "    max_prompt_length=512,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    max_steps=2,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=1,\n",
    "    report_to=[\"mlflow\",\"tensorboard\"],\n",
    "    output_dir=str(training_output_dir),\n",
    ")\n",
    "\n",
    "print(f\"Training output directory: {training_output_dir}\")\n",
    "print(f\"Directory exists: {training_output_dir.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d084b1-62da-4ed7-8d49-729db9bb4e3d",
   "metadata": {},
   "source": [
    "### üöÄ ORPO Trainer\n",
    "\n",
    "We now initialize the `ORPOTrainer`, which orchestrates the fine-tuning process using the Offline Reinforcement Preference Optimization (ORPO) strategy.\n",
    "\n",
    "It takes as input:\n",
    "- The **base model**, already prepared with QLoRA and chat formatting\n",
    "- The **ORPO configuration** (`orpo_args`) containing all training hyperparameters\n",
    "- The **training and evaluation datasets**\n",
    "- The **LoRA configuration** (`peft_config`) for parameter-efficient fine-tuning\n",
    "- The **tokenizer**, passed as a `processing_class`, to apply proper formatting and padding\n",
    "\n",
    "Once initialized, the trainer will be ready to start training with `trainer.train()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45ed75d4-44c8-4284-9f12-3da8f4cce525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61af8e4b7671473782d6f607794cc099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/163 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2135dc98fd524a0b933238d86b6ec69e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/163 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2784 > 2048). Running this sequence through the model will result in indexing errors\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = ORPOTrainer(\n",
    "    model=model,\n",
    "    args=orpo_args,\n",
    "    train_dataset=dataset[0],\n",
    "    eval_dataset=dataset[1],\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca638984-d847-41fd-b918-78517ba82817",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 03:02, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "      <th>Steps Per Second</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "      <th>Nll Loss</th>\n",
       "      <th>Log Odds Ratio</th>\n",
       "      <th>Log Odds Chosen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.925600</td>\n",
       "      <td>1.391509</td>\n",
       "      <td>81.528200</td>\n",
       "      <td>1.999000</td>\n",
       "      <td>1.006000</td>\n",
       "      <td>-0.092238</td>\n",
       "      <td>-0.095709</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.003472</td>\n",
       "      <td>-0.957094</td>\n",
       "      <td>-0.922378</td>\n",
       "      <td>-2.804329</td>\n",
       "      <td>-2.889130</td>\n",
       "      <td>1.316913</td>\n",
       "      <td>-0.699385</td>\n",
       "      <td>0.041224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.810100</td>\n",
       "      <td>1.390907</td>\n",
       "      <td>84.512500</td>\n",
       "      <td>1.929000</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>-0.092148</td>\n",
       "      <td>-0.095610</td>\n",
       "      <td>0.469512</td>\n",
       "      <td>0.003462</td>\n",
       "      <td>-0.956098</td>\n",
       "      <td>-0.921475</td>\n",
       "      <td>-2.804189</td>\n",
       "      <td>-2.889081</td>\n",
       "      <td>1.316299</td>\n",
       "      <td>-0.699535</td>\n",
       "      <td>0.041081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Output saved to: /home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/output/training_results\n",
      "LoRA adapters will be merged and saved to: /home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/output/fine_tuned_models/Orpo-TinyLlama-1.1B-Chat-v1.0-FT\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "# The training output will contain LoRA adapters in checkpoint directories\n",
    "# We'll locate these adapters and merge them in the next step\n",
    "print(f\"Training completed. Output saved to: {orpo_args.output_dir}\")\n",
    "print(f\"LoRA adapters will be merged and saved to: {fine_tuned_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8bad850f-43fc-4d88-a276-18fa70726863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found LoRA adapters at: /home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/output/training_results/checkpoint-100\n",
      "Removed existing directory: /home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/output/fine_tuned_models/Orpo-TinyLlama-1.1B-Chat-v1.0-FT\n",
      "üßπ Cleaning up memory...\n",
      "üîÑ Loading base tokenizer and model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Tokenizer already contains a chat_template. Skipping setup.\n",
      "üîó Loading LoRA fine-tuned weights from: /home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/output/training_results/checkpoint-100\n",
      "üß† Merging LoRA weights into the base model...\n",
      "üíæ Saving merged model to: /home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/output/fine_tuned_models/Orpo-TinyLlama-1.1B-Chat-v1.0-FT\n",
      "‚úÖ Merge complete! Model successfully saved locally.\n",
      "‚úÖ Merged model saved to: /home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/output/fine_tuned_models/Orpo-TinyLlama-1.1B-Chat-v1.0-FT\n"
     ]
    }
   ],
   "source": [
    "# Find the LoRA adapters in the training output directory\n",
    "training_output_dir = Path(orpo_args.output_dir)\n",
    "\n",
    "# Look for adapter_config.json in checkpoint subdirectories\n",
    "adapter_configs = list(training_output_dir.rglob(\"adapter_config.json\"))\n",
    "if adapter_configs:\n",
    "    # Use the directory containing adapter_config.json (typically checkpoint-X)\n",
    "    lora_adapter_path = str(adapter_configs[0].parent)\n",
    "    print(f\"Found LoRA adapters at: {lora_adapter_path}\")\n",
    "    \n",
    "    # Remove existing fine_tuned_path if it exists to avoid conflicts\n",
    "    if Path(fine_tuned_path).exists():\n",
    "        import shutil\n",
    "        shutil.rmtree(fine_tuned_path)\n",
    "        print(f\"Removed existing directory: {fine_tuned_path}\")\n",
    "    \n",
    "    # Merge LoRA adapters with base model and save to fine_tuned_path\n",
    "    merge_lora_and_save(\n",
    "        base_model_id=MODEL,\n",
    "        finetuned_lora_path=lora_adapter_path\n",
    "    )\n",
    "    print(f\"‚úÖ Merged model saved to: {fine_tuned_path}\")\n",
    "else:\n",
    "    print(\"‚ùå No LoRA adapters found in training output directory!\")\n",
    "    print(\"This might indicate an issue with the training process.\")\n",
    "    print(\"Available files in training output:\")\n",
    "    for file_path in training_output_dir.rglob(\"*\"):\n",
    "        if file_path.is_file():\n",
    "            print(f\"  - {file_path.relative_to(training_output_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4242ea0-b894-4852-9263-2e48eedaf7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned model from: /home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/output/fine_tuned_models/Orpo-TinyLlama-1.1B-Chat-v1.0-FT\n",
      "Directory exists: True\n",
      "\n",
      "Contents of fine-tuned model directory:\n",
      "  üìÑ chat_template.jinja\n",
      "  üìÑ config.json\n",
      "  üìÑ generation_config.json\n",
      "  üìÑ model.safetensors\n",
      "  üìÑ special_tokens_map.json\n",
      "  üìÑ tokenizer.json\n",
      "  üìÑ tokenizer_config.json\n",
      "\n",
      "Model type analysis:\n",
      "  ‚Ä¢ Has adapter_config.json: False\n",
      "  ‚Ä¢ Has model weights: True\n",
      "  ‚Ä¢ Has config.json: True\n",
      "  ‚Ä¢ Detected type: Complete model\n",
      "\n",
      "‚úÖ Fine-tuned Model Response:\n",
      "Propose a solution that could reduce the rate of deforestation in the Amazon rainforest.\n"
     ]
    }
   ],
   "source": [
    "# Verify and load the merged fine-tuned model for inference\n",
    "final_model_path = str(get_fine_tuned_models_dir() / fine_tuned_name)\n",
    "\n",
    "print(f\"Loading fine-tuned model from: {final_model_path}\")\n",
    "print(f\"Directory exists: {Path(final_model_path).exists()}\")\n",
    "\n",
    "# Show contents of the model directory for debugging\n",
    "if Path(final_model_path).exists():\n",
    "    print(\"\\nContents of fine-tuned model directory:\")\n",
    "    for item in Path(final_model_path).iterdir():\n",
    "        if item.is_file():\n",
    "            print(f\"  üìÑ {item.name}\")\n",
    "        elif item.is_dir():\n",
    "            print(f\"  üìÅ {item.name}/\")\n",
    "    \n",
    "    # Check if it's a complete model or LoRA adapters\n",
    "    has_adapter_config = (Path(final_model_path) / \"adapter_config.json\").exists()\n",
    "    has_model_weights = any(f.suffix in ['.bin', '.safetensors'] for f in Path(final_model_path).iterdir() if f.is_file())\n",
    "    has_config = (Path(final_model_path) / \"config.json\").exists()\n",
    "    \n",
    "    print(f\"\\nModel type analysis:\")\n",
    "    print(f\"  ‚Ä¢ Has adapter_config.json: {has_adapter_config}\")\n",
    "    print(f\"  ‚Ä¢ Has model weights: {has_model_weights}\")\n",
    "    print(f\"  ‚Ä¢ Has config.json: {has_config}\")\n",
    "    print(f\"  ‚Ä¢ Detected type: {'LoRA adapters' if has_adapter_config else 'Complete model' if has_model_weights else 'Unknown'}\")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(final_model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(final_model_path, torch_dtype=torch.float16).cuda().eval()\n",
    "    \n",
    "    # Test the fine-tuned model with a sample prompt\n",
    "    prompt = \"Propose a solution that could reduce the rate of deforestation\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=500)\n",
    "\n",
    "    print(\"\\n‚úÖ Fine-tuned Model Response:\")\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading or testing model: {e}\")\n",
    "    print(\"This indicates an issue with the model merging or saving process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5201cd97",
   "metadata": {},
   "source": [
    "## üîç Model Evaluation and Comparison\n",
    "\n",
    "After completing the ORPO fine-tuning process, we can evaluate the performance improvements by comparing responses from the base model and our fine-tuned model.\n",
    "\n",
    "This comparison helps us understand:\n",
    "- **Quality Improvements**: How the fine-tuned model generates more helpful and aligned responses\n",
    "- **Training Effectiveness**: Whether the ORPO training successfully improved the model's preference alignment\n",
    "- **Response Consistency**: How well the model maintains coherent and relevant outputs\n",
    "\n",
    "The comparison uses the same test prompts to ensure fair evaluation between the base and fine-tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6bf71f8e-0e1a-4732-8f6f-1c62d128d4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ModelSelector:[ModelSelector] Selected model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "INFO:ModelSelector:[ModelSelector] Downloading model snapshot to: /home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/models/TinyLlama__TinyLlama-1.1B-Chat-v1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç MODEL COMPARISON RESULTS\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34fb16be5ecb4b0b989408774d7042da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ModelSelector:[ModelSelector] ‚úÖ Model downloaded successfully to: /home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/models/TinyLlama__TinyLlama-1.1B-Chat-v1.0\n",
      "INFO:ModelSelector:[ModelSelector] Loading model and tokenizer from: /home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/models/TinyLlama__TinyLlama-1.1B-Chat-v1.0\n",
      "INFO:ModelSelector:[ModelSelector] Checking model for ORPO compatibility...\n",
      "INFO:ModelSelector:[ModelSelector] ‚úÖ Model 'TinyLlama/TinyLlama-1.1B-Chat-v1.0' is ORPO-compatible.\n",
      "INFO:AcceleratedInferenceRunner:üîÑ Loading tokenizer and base model from ModelSelector...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Base model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "üìä Fine-tuned model: /home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/output/fine_tuned_models/Orpo-TinyLlama-1.1B-Chat-v1.0-FT\n",
      "\n",
      "üöÄ Running model comparison...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:AcceleratedInferenceRunner:‚úÖ Model loaded and ready for inference.\n",
      "INFO:AcceleratedInferenceRunner:üîÑ Loading tokenizer and base model from ModelSelector...\n",
      "WARNING:AcceleratedInferenceRunner:‚ö†Ô∏è No adapter_config.json found at /home/jovyan/AI-Blueprints/generative-ai/fine-tuning-with-orpo/output/fine_tuned_models/Orpo-TinyLlama-1.1B-Chat-v1.0-FT. Skipping LoRA application.\n",
      "INFO:AcceleratedInferenceRunner:‚úÖ Model loaded and ready for inference.\n",
      "INFO:AcceleratedInferenceRunner:üîç Running inference for prompt (truncated): Explain the importance of sustainable agriculture....\n",
      "INFO:AcceleratedInferenceRunner:‚úÖ Inference complete.\n",
      "INFO:AcceleratedInferenceRunner:üîç Running inference for prompt (truncated): Explain the importance of sustainable agriculture....\n",
      "INFO:AcceleratedInferenceRunner:‚úÖ Inference complete.\n",
      "INFO:AcceleratedInferenceRunner:üîç Running inference for prompt (truncated): Write a Python function to check for palindromes....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Processing prompt 1/4\n",
      "\n",
      "================================================================================\n",
      "üìù COMPARISON 1: Explain the importance of sustainable agriculture.\n",
      "================================================================================\n",
      "\n",
      "üî∏ BASE MODEL (50 chars):\n",
      "----------------------------------------\n",
      "Explain the importance of sustainable agriculture.\n",
      "\n",
      "üîπ FINE-TUNED MODEL (50 chars):\n",
      "----------------------------------------\n",
      "Explain the importance of sustainable agriculture.\n",
      "\n",
      "üìä METRICS:\n",
      "   ‚Ä¢ Length difference: +0 chars\n",
      "   ‚Ä¢ Length change: +0.0%\n",
      "‚öôÔ∏è Processing prompt 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:AcceleratedInferenceRunner:‚úÖ Inference complete.\n",
      "INFO:AcceleratedInferenceRunner:üîç Running inference for prompt (truncated): Write a Python function to check for palindromes....\n",
      "INFO:AcceleratedInferenceRunner:‚úÖ Inference complete.\n",
      "INFO:AcceleratedInferenceRunner:üîç Running inference for prompt (truncated): Describe the benefits of renewable energy sources....\n",
      "INFO:AcceleratedInferenceRunner:‚úÖ Inference complete.\n",
      "INFO:AcceleratedInferenceRunner:üîç Running inference for prompt (truncated): Describe the benefits of renewable energy sources....\n",
      "INFO:AcceleratedInferenceRunner:‚úÖ Inference complete.\n",
      "INFO:AcceleratedInferenceRunner:üîç Running inference for prompt (truncated): What are the key principles of machine learning?...\n",
      "INFO:AcceleratedInferenceRunner:‚úÖ Inference complete.\n",
      "INFO:AcceleratedInferenceRunner:üîç Running inference for prompt (truncated): What are the key principles of machine learning?...\n",
      "INFO:AcceleratedInferenceRunner:‚úÖ Inference complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìù COMPARISON 2: Write a Python function to check for palindromes.\n",
      "================================================================================\n",
      "\n",
      "üî∏ BASE MODEL (471 chars):\n",
      "----------------------------------------\n",
      "Write a Python function to check for palindromes. Your function should take a string as input and return True if the string is a palindrome and False otherwise. The function should ignore whitespace and punctuation when comparing the characters. Use appropriate variable names and comments to explain the logic and functionality of the function. Bonus points for adding error handling to handle cases where the input string is empty or contains non-alphabetic characters.\n",
      "\n",
      "üîπ FINE-TUNED MODEL (420 chars):\n",
      "----------------------------------------\n",
      "Write a Python function to check for palindromes. Your function should take a string as input and return `True` if the input string is a palindrome; otherwise, return `False`. Your function should be well-documented, clearly explain its purpose and implementation, and include appropriate error handling for invalid input. Use appropriate variable names and comment your code to make it easily understandable for others.\n",
      "\n",
      "üìä METRICS:\n",
      "   ‚Ä¢ Length difference: -51 chars\n",
      "   ‚Ä¢ Length change: -10.8%\n",
      "‚öôÔ∏è Processing prompt 3/4\n",
      "\n",
      "================================================================================\n",
      "üìù COMPARISON 3: Describe the benefits of renewable energy sources.\n",
      "================================================================================\n",
      "\n",
      "üî∏ BASE MODEL (50 chars):\n",
      "----------------------------------------\n",
      "Describe the benefits of renewable energy sources.\n",
      "\n",
      "üîπ FINE-TUNED MODEL (50 chars):\n",
      "----------------------------------------\n",
      "Describe the benefits of renewable energy sources.\n",
      "\n",
      "üìä METRICS:\n",
      "   ‚Ä¢ Length difference: +0 chars\n",
      "   ‚Ä¢ Length change: +0.0%\n",
      "‚öôÔ∏è Processing prompt 4/4\n",
      "\n",
      "================================================================================\n",
      "üìù COMPARISON 4: What are the key principles of machine learning?\n",
      "================================================================================\n",
      "\n",
      "üî∏ BASE MODEL (48 chars):\n",
      "----------------------------------------\n",
      "What are the key principles of machine learning?\n",
      "\n",
      "üîπ FINE-TUNED MODEL (48 chars):\n",
      "----------------------------------------\n",
      "What are the key principles of machine learning?\n",
      "\n",
      "üìä METRICS:\n",
      "   ‚Ä¢ Length difference: +0 chars\n",
      "   ‚Ä¢ Length change: +0.0%\n",
      "\n",
      "================================================================================\n",
      "üìà SUMMARY STATISTICS\n",
      "================================================================================\n",
      "üìä Total prompts evaluated: 4\n",
      "üìä Average base model response length: 154.8 chars\n",
      "üìä Average fine-tuned model response length: 142.0 chars\n",
      "üìä Overall length difference: -12.8 chars\n",
      "üìä Overall length change: -8.2%\n",
      "\n",
      "‚úÖ Model comparison completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Compare base model vs fine-tuned model using a custom comparison approach\n",
    "final_model_path = str(get_fine_tuned_models_dir() / fine_tuned_name)\n",
    "\n",
    "print(\"üîç MODEL COMPARISON RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load both models for comparison\n",
    "base_model_selector = ModelSelector()\n",
    "base_model_selector.select_model(MODEL)\n",
    "print(f\"üìä Base model: {base_model_selector.model_id}\")\n",
    "print(f\"üìä Fine-tuned model: {final_model_path}\")\n",
    "\n",
    "# Define test prompts for comparison\n",
    "test_prompts = [\n",
    "    \"Explain the importance of sustainable agriculture.\",\n",
    "    \"Write a Python function to check for palindromes.\",\n",
    "    \"Describe the benefits of renewable energy sources.\",\n",
    "    \"What are the key principles of machine learning?\"\n",
    "]\n",
    "\n",
    "print(\"\\nüöÄ Running model comparison...\")\n",
    "\n",
    "# Initialize inference runners for both models\n",
    "base_runner = AcceleratedInferenceRunner(\n",
    "    model_selector=base_model_selector,\n",
    "    dtype=torch.float16\n",
    ")\n",
    "\n",
    "ft_runner = AcceleratedInferenceRunner(\n",
    "    model_selector=base_model_selector,\n",
    "    finetuned_path=final_model_path,\n",
    "    dtype=torch.float16\n",
    ")\n",
    "\n",
    "base_runner.load_model()\n",
    "ft_runner.load_model()\n",
    "\n",
    "# Run comparison and display results in a readable format\n",
    "comparison_results = []\n",
    "\n",
    "for idx, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"‚öôÔ∏è Processing prompt {idx}/{len(test_prompts)}\")\n",
    "    \n",
    "    # Get responses from both models\n",
    "    base_response = base_runner.infer(prompt)\n",
    "    ft_response = ft_runner.infer(prompt)\n",
    "    \n",
    "    # Store results\n",
    "    result = {\n",
    "        \"prompt\": prompt,\n",
    "        \"base_response\": base_response,\n",
    "        \"ft_response\": ft_response,\n",
    "        \"base_length\": len(base_response),\n",
    "        \"ft_length\": len(ft_response)\n",
    "    }\n",
    "    comparison_results.append(result)\n",
    "    \n",
    "    # Display each comparison in a structured format\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üìù COMPARISON {idx}: {prompt}\")\n",
    "    print('='*80)\n",
    "    \n",
    "    print(f\"\\nüî∏ BASE MODEL ({result['base_length']} chars):\")\n",
    "    print(\"-\" * 40)\n",
    "    print(base_response)\n",
    "    \n",
    "    print(f\"\\nüîπ FINE-TUNED MODEL ({result['ft_length']} chars):\")\n",
    "    print(\"-\" * 40)\n",
    "    print(ft_response)\n",
    "    \n",
    "    print(f\"\\nüìä METRICS:\")\n",
    "    print(f\"   ‚Ä¢ Length difference: {result['ft_length'] - result['base_length']:+d} chars\")\n",
    "    if result['base_length'] > 0:\n",
    "        length_change = ((result['ft_length'] - result['base_length']) / result['base_length']) * 100\n",
    "        print(f\"   ‚Ä¢ Length change: {length_change:+.1f}%\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"üìà SUMMARY STATISTICS\")\n",
    "print('='*80)\n",
    "\n",
    "# Calculate summary statistics\n",
    "total_base_length = sum(r['base_length'] for r in comparison_results)\n",
    "total_ft_length = sum(r['ft_length'] for r in comparison_results)\n",
    "avg_base_length = total_base_length / len(comparison_results)\n",
    "avg_ft_length = total_ft_length / len(comparison_results)\n",
    "\n",
    "print(f\"üìä Total prompts evaluated: {len(comparison_results)}\")\n",
    "print(f\"üìä Average base model response length: {avg_base_length:.1f} chars\")\n",
    "print(f\"üìä Average fine-tuned model response length: {avg_ft_length:.1f} chars\")\n",
    "print(f\"üìä Overall length difference: {avg_ft_length - avg_base_length:+.1f} chars\")\n",
    "\n",
    "if avg_base_length > 0:\n",
    "    overall_change = ((avg_ft_length - avg_base_length) / avg_base_length) * 100\n",
    "    print(f\"üìä Overall length change: {overall_change:+.1f}%\")\n",
    "\n",
    "print(\"\\n‚úÖ Model comparison completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4987f4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 16:29:24 - INFO - ‚è±Ô∏è Total execution time: 8m 28.30s\n",
      "2025-08-06 16:29:24 - INFO - ‚úÖ Notebook execution completed successfully.\n"
     ]
    }
   ],
   "source": [
    "end_time: float = time.time()\n",
    "elapsed_time: float = end_time - start_time\n",
    "elapsed_minutes: int = int(elapsed_time // 60)\n",
    "elapsed_seconds: float = elapsed_time % 60\n",
    "\n",
    "logger.info(f\"‚è±Ô∏è Total execution time: {elapsed_minutes}m {elapsed_seconds:.2f}s\")\n",
    "logger.info(\"‚úÖ Notebook execution completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e391ccbf-ca08-4299-93c0-f67fb2551a59",
   "metadata": {},
   "source": [
    "Built with ‚ù§Ô∏è using Z by HP AI Studio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
