{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5f8d3ca-e260-4d56-967d-dc4feb92e2e4",
   "metadata": {},
   "source": [
    "<h1 style=\\\"text-align: center; font-size: 50px;\\\"> <h1 style=\\\"text-align: center; font-size: 50px;\\\"> 📦 Register Model </h1> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74217537-b174-49e0-b746-f3c53b5b0043",
   "metadata": {},
   "source": [
    "📘 Project Overview: \n",
    " This notebook demonstrates a modular architecture for answering natural language questions \n",
    " over one or more feedback documents using only local and open-source models (e.g., LLaMA.cpp).\n",
    " The system processes long documents chunk-by-chunk and synthesizes a final answer using a multi-step LLM workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c461da6-1862-4941-bb13-49f7e3b0aaf9",
   "metadata": {},
   "source": [
    "## 🧠 System Architecture\"\n",
    "\n",
    "                 \"```mermaid\\n\"\n",
    "                 \"graph TD\\n\"\n",
    "                 \"    A[Original Question] --> B[LLM generates Prompts]\\n\"\n",
    "                 \"    B --> C[Split Documents into Chunks]\\n\"\n",
    "                 \"    C --> D[LLM answers Question per Chunk]\\n\"\n",
    "                 \"    D --> E[Collect All Chunk-Level Answers]\\n\"\n",
    "                 \"    E --> F[LLM Synthesizes Final Answer]\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f858ca",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "\n",
    "- Start Execution\n",
    "- Define User Constants\n",
    "- Install and Import Libraries\n",
    "- Configure Settings\n",
    "- Verify Assets\n",
    "- KV Memory\n",
    "- LLM Setup\n",
    "- State Model\n",
    "- Node Functions\n",
    "- Graph Definition\n",
    "- Graph Visualization\n",
    "- Generated Answer\n",
    "- Message History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a533df-f921-4f06-9197-9e33b9e5c799",
   "metadata": {},
   "source": [
    "# Start Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "490becc1-2ff2-4273-a49f-0fdacf30c2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os \n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from src.utils import display_image, get_response_from_llm, json_schema_from_type, log_timing, logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16d77c68-e160-4c24-9844-de484272733e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 05:10:37 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()  \n",
    "logger.info(\"Notebook execution started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84444adc-85c6-45cd-8769-7159e29fbace",
   "metadata": {},
   "source": [
    "# Define User Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a21d3385-704e-4107-ae45-749decdd305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC: str = \"Focus Flow\"  \n",
    "QUESTION: str = \"Which poeple provided the feedback?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b7b5e0-2acd-4729-b8a2-03e81cf0df2d",
   "metadata": {},
   "source": [
    "# Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b876fc6-cbb8-4330-9529-d2936b202d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 93.8 ms, sys: 32.5 ms, total: 126 ms\n",
      "Wall time: 9.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "%pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b95da1c-2715-497b-b1e0-2aaefe09dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "# ─────── Standard Library ───────\n",
    "import base64\n",
    "import functools\n",
    "import json\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from collections import namedtuple\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Literal, Optional, TypedDict\n",
    "import os\n",
    "import sys\n",
    "from IPython.display import display, Markdown\n",
    "import yaml\n",
    "\n",
    "# ─────── Third-Party: Torch & Progress ───────\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ─────── LangChain Core & Community ───────\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_community.document_loaders import (\n",
    "    CSVLoader,\n",
    "    PyPDFLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredExcelLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    ")\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "# ─────── LangGraph & Messaging ───────\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "\n",
    "# ─────── Jupyter Display ───────\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "\n",
    "import mlflow\n",
    "\n",
    "from src.simple_kv_memory import SimpleKVMemory\n",
    "from src.agentic_feedback_model import AgenticFeedbackModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14909251-86d2-4eff-b15e-4a8c4cbddc61",
   "metadata": {},
   "source": [
    "# Configure Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27be6c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress Python warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b49f4d0-7662-43ce-ae95-59c13af024bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH: Path = Path(\"../data/input\")  \n",
    "\n",
    "MEMORY_PATH: Path = Path(\"../data/memory\")   \n",
    "\n",
    "LLM_PATH = \"/home/jovyan/datafabric/meta-llama3.1-8b-Q8/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\"\n",
    "CONTEXT_WINDOW = 8192\n",
    "MAX_TOKENS = CONTEXT_WINDOW // 8\n",
    "CHUNK_SIZE = CONTEXT_WINDOW // 2\n",
    "CHUNK_OVERLAP = CHUNK_SIZE // 8  \n",
    "\n",
    "EXPERIMENT_NAME = \"AIStudio-Customer-Feedback-Analyzer-Experiment\"\n",
    "RUN_NAME = \"AIStudio-Customer-Feedback-Analyzer-Run\"\n",
    "MODEL_NAME = \"AIStudioCustomerFeedbackAnalyzerModel\"\n",
    "MODEL_PATH = \"AIStudio-Customer-Feedback-Analyzer-Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92c8dc01-e50f-4077-9056-b966895ad8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 05:10:50 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "logger.info('Notebook execution started.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dad079",
   "metadata": {},
   "source": [
    "## Verify Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98593ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_asset_status(asset_path: str, asset_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Logs the status of a given asset based on its existence.\n",
    "\n",
    "    Parameters:\n",
    "        asset_path (str): File or directory path to check.\n",
    "        asset_name (str): Name of the asset for logging context.\n",
    "    \"\"\"\n",
    "    if Path(asset_path).exists():\n",
    "        logger.info(f\"{asset_name} is properly configured.\")\n",
    "    else:\n",
    "        logger.info(f\"{asset_name} is not properly configured. Please ensure the required asset is correctly configured in your AI Studio project according to the README file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61dd389c-4a0c-421f-9f29-fe9080aec9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 05:10:50 - INFO - Input Data is properly configured.\n",
      "2025-08-02 05:10:50 - INFO - LLM is not properly configured. Please ensure the required asset is correctly configured in your AI Studio project according to the README file.\n"
     ]
    }
   ],
   "source": [
    "log_asset_status(\n",
    "    asset_path=INPUT_PATH,\n",
    "    asset_name=\"Input Data\",\n",
    ")\n",
    "log_asset_status(\n",
    "    asset_path=MODEL_PATH,\n",
    "    asset_name=\"LLM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5f0b3a-b878-4528-99f9-64678ae273bf",
   "metadata": {},
   "source": [
    "# KV Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa0e3126-b7c4-407f-82ac-73955213f4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory: SimpleKVMemory = SimpleKVMemory(MEMORY_PATH)\n",
    "memory.set('dummy key', 'dummy value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48ec21b-ac6e-44c8-ba14-955edb9ddab3",
   "metadata": {},
   "source": [
    "# Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da62e7ed-cd98-46a3-a954-a7a3cf39d8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 05:10:50 - INFO - 📂 Scanning directory for documents: ../data/input\n",
      "2025-08-02 05:10:50 - INFO - ✅ Loaded 1 docs from csv.csv\n",
      "short text: \"docx test\". Defaulting to English.\n",
      "2025-08-02 05:10:55 - INFO - ✅ Loaded 1 docs from docx.docx\n",
      "short text: \"md test\". Defaulting to English.\n",
      "2025-08-02 05:10:55 - INFO - ✅ Loaded 1 docs from md.md\n",
      "2025-08-02 05:10:55 - INFO - ✅ Loaded 1 docs from pdf.pdf\n",
      "2025-08-02 05:10:55 - INFO - ✅ Loaded 1 docs from sample-product-feedback-doc.md\n",
      "2025-08-02 05:10:55 - INFO - ✅ Loaded 1 docs from txt.txt\n",
      "short text: \"excel test excel test\". Defaulting to English.\n",
      "2025-08-02 05:10:56 - INFO - ✅ Loaded 1 docs from xlsx.xlsx\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"📂 Scanning directory for documents: %s\", INPUT_PATH)\n",
    "\n",
    "supported_extensions = {\n",
    "\".txt\": TextLoader,\n",
    "\".csv\": lambda path: CSVLoader(path, encoding=\"utf-8\", csv_args={\"delimiter\": \",\"}),\n",
    "\".xlsx\": UnstructuredExcelLoader,\n",
    "\".docx\": UnstructuredWordDocumentLoader,\n",
    "\".pdf\": PyPDFLoader,\n",
    "\".md\": UnstructuredMarkdownLoader,\n",
    "}\n",
    "\n",
    "all_docs = []\n",
    "\n",
    "for file_path in Path(INPUT_PATH).rglob(\"*\"):\n",
    "    # Skip hidden/system folders\n",
    "    if any(part.startswith(\".\") and part not in {\".\", \"..\"} for part in file_path.parts):\n",
    "        continue\n",
    "    \n",
    "    ext = file_path.suffix.lower()\n",
    "    loader_class = supported_extensions.get(ext)\n",
    "    \n",
    "    if loader_class:\n",
    "        try:\n",
    "            loader = loader_class(str(file_path)) if callable(loader_class) else loader_class\n",
    "            docs = loader.load()\n",
    "            all_docs.extend(docs)\n",
    "            logger.info(\"✅ Loaded %d docs from %s\", len(docs), file_path.name)\n",
    "        except Exception as e:\n",
    "            logger.warning(\"❌ Failed to load %s: %s\", file_path.name, e)\n",
    "    else:\n",
    "        logger.info(\"⚠️ Unsupported file type: %s\", file_path.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f2df425-9816-4337-b573-e8ecb48a122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_TEXT = '\\n\\n'.join([doc.page_content for doc in all_docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882e9213-a911-400f-a79f-b8fe944b887d",
   "metadata": {},
   "source": [
    "# MLflow Registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "914f9774-40d1-4186-a16a-1b7bc22d7858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 05:10:56 INFO mlflow.tracking.fluent: Experiment with name 'AIStudio-Customer-Feedback-Analyzer-Experiment' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLflow tracking URI: /phoenix/mlflow\n",
      "Experiment: AIStudio-Customer-Feedback-Analyzer-Experiment\n"
     ]
    }
   ],
   "source": [
    "# 1. Set MLflow tracking URI and experiment\n",
    "mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\", \"/phoenix/mlflow\"))\n",
    "mlflow.set_experiment(experiment_name=EXPERIMENT_NAME)\n",
    "print(f\"Using MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"Experiment: {EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1c56599-743c-4ded-a3c9-4f03c0fbb04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 05:10:56 INFO mlflow.models.signature: Inferring model signature from type hints\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Started MLflow run: a7b20165a68840458b267295f8c3d29d\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb612c0dacf4298be035abf34192373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2f569661ca43508e01503826e35122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'AIStudioCustomerFeedbackAnalyzerModel'.\n",
      "Created version '1' of model 'AIStudioCustomerFeedbackAnalyzerModel'.\n",
      "Registered model 'AIStudioCustomerFeedbackAnalyzerModel' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'AIStudioCustomerFeedbackAnalyzerModel'.\n",
      "2025-08-02 05:14:58 - INFO - ✅ Model 'AIStudioCustomerFeedbackAnalyzerModel' successfully logged and registered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 892 ms, sys: 17.1 s, total: 18 s\n",
      "Wall time: 4min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# These should point to the actual files you're using for model and memory\n",
    "MODEL_ARTIFACTS = {\n",
    "    \"model_path\": str(LLM_PATH),\n",
    "    \"memory_path\": str(MEMORY_PATH),\n",
    "}\n",
    " \n",
    "# === Start MLflow run, log, and register ===\n",
    "with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "    print(f\"🚀 Started MLflow run: {run.info.run_id}\")\n",
    "\n",
    "    # Log and register the model using the classmethod\n",
    "    AgenticFeedbackModel.log_model(\n",
    "        model_name=MODEL_NAME,\n",
    "        model_path=MODEL_PATH,\n",
    "        model_artifacts=MODEL_ARTIFACTS\n",
    "    )\n",
    "\n",
    "    # Construct model URI and register it\n",
    "    model_uri = f\"runs:/{run.info.run_id}/{MODEL_PATH}\"\n",
    "    mlflow.register_model(model_uri=model_uri, name=MODEL_NAME)\n",
    "\n",
    "logger.info(f\"✅ Model '{MODEL_NAME}' successfully logged and registered.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "139d19b6-c12f-4cdd-84eb-4ecd2466330a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 05:14:59 - INFO - Latest registered version of 'AIStudioCustomerFeedbackAnalyzerModel': 2\n",
      "2025-08-02 05:14:59 - INFO - Signature: inputs: \n",
      "  [{input_text: string (required), question: string (required), topic: string (required)} (required)]\n",
      "outputs: \n",
      "  [{answer: string (required), messages: string (required)} (required)]\n",
      "params: \n",
      "  None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# 3. Retrieve the latest version from the Model Registry\n",
    "client = MlflowClient()\n",
    "versions = client.get_latest_versions(MODEL_NAME, stages=[\"None\"])\n",
    "\n",
    "if not versions:\n",
    "    raise RuntimeError(f\"No registered versions found for model '{MODEL_NAME}'.\")\n",
    "    \n",
    "latest_version = versions[0].version\n",
    "model_info = mlflow.models.get_model_info(f\"models:/{MODEL_NAME}/{latest_version}\")\n",
    "\n",
    "logger.info(f\"Latest registered version of '{MODEL_NAME}': {latest_version}\")\n",
    "logger.info(f\"Signature: {model_info.signature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "051d1f33-6f06-4c07-a7e3-dd60e478f95a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 05:16:07 - INFO - Successfully loaded model 'AIStudioCustomerFeedbackAnalyzerModel' version 2 for inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.27 s, sys: 2 s, total: 3.27 s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 4. Load the model from the Model Registry\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri=f\"models:/{MODEL_NAME}/{latest_version}\")\n",
    "logger.info(f\"Successfully loaded model '{MODEL_NAME}' version {latest_version} for inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f9435e6-b591-416a-b263-b2432e61f0b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 05:16:07 - INFO - 🗣️ Ingested user question: Which poeple provided the feedback?\n",
      "2025-08-02 05:16:07 - INFO - Function 'ingest_question' took 0.0006 seconds.\n",
      "--------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running Sample Inference ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 05:16:19 - INFO - 🧠 Relevance response: yes → Relevant\n",
      "2025-08-02 05:16:19 - INFO - Function 'check_relevance' took 11.4955 seconds.\n",
      "--------------------------------------------------------------\n",
      "\n",
      "2025-08-02 05:16:19 - INFO - 🧭 Cache miss for question: Which poeple provided the feedback?\n",
      "2025-08-02 05:16:19 - INFO - Function 'check_memory' took 0.0021 seconds.\n",
      "--------------------------------------------------------------\n",
      "\n",
      "2025-08-02 05:16:19 - INFO - ✏️ Rewritten user question:\n",
      "→ Who are the individuals mentioned in the document as providing feedback?\n",
      "2025-08-02 05:16:19 - INFO - Function 'rewrite_question' took 0.5458 seconds.\n",
      "--------------------------------------------------------------\n",
      "\n",
      "2025-08-02 05:16:19 - INFO - 📑 Starting chunking for 1 loaded documents\n",
      "2025-08-02 05:16:19 - INFO - 🧩 Created 2 total chunks (size=4096, overlap=256)\n",
      "2025-08-02 05:16:19 - INFO - Function 'create_chunks' took 0.0016 seconds.\n",
      "--------------------------------------------------------------\n",
      "\n",
      "2025-08-02 05:16:19 - INFO - 🧩 Generating answers for 2 chunks using rewritten question: 'Who are the individuals mentioned in the document as providing feedback?'\n",
      "🔁 Processing each chunk: 100%|██████████| 2/2 [00:01<00:00,  1.95it/s, group=✅ Chunk 2 response length: 28 chars]\n",
      "2025-08-02 05:16:20 - INFO - 🧠 Finished generating 2 chunk-level responses.\n",
      "2025-08-02 05:16:20 - INFO - Function 'generate_answer_per_chunks' took 1.0303 seconds.\n",
      "--------------------------------------------------------------\n",
      "\n",
      "2025-08-02 05:16:20 - INFO - 🧠 Synthesizing across 1 chunk groups\n",
      "2025-08-02 05:16:20 - INFO - 🧠 Synthesizing final answer from 2 chunk responses\n",
      "🔁 Processing each grouped chunk answers: 100%|██████████| 1/1 [00:01<00:00,  1.43s/it, group=🧠 Synthesized partial answer (1/1)]\n",
      "2025-08-02 05:16:22 - INFO - ✅ Synthesized 1 group-level summaries.\n",
      "2025-08-02 05:16:22 - INFO - Function 'generate_synthetized_answer' took 1.4334 seconds.\n",
      "--------------------------------------------------------------\n",
      "\n",
      "2025-08-02 05:16:22 - INFO - 💾 Stored question-answer pair in memory (key: which poeple provided the feedback?)\n",
      "2025-08-02 05:16:22 - INFO - Function 'update_memory' took 0.0249 seconds.\n",
      "--------------------------------------------------------------\n",
      "\n",
      "2025-08-02 05:16:22 - INFO - 📤 Delivered final answer (304 characters)\n",
      "2025-08-02 05:16:22 - INFO - Function 'output_answer' took 0.0005 seconds.\n",
      "--------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔚 === Final Answer ===\n",
      "\n",
      "# 🧠 Synthesized partial answer (1/1)\n",
      "\n",
      "Since the user's question is about individuals mentioned in the document as providing feedback, and neither Chunk 1 nor Chunk 2 mentions any individuals providing feedback, the final answer is:\n",
      "\n",
      "**No individuals are mentioned in the document as providing feedback.**\n",
      "\n",
      "========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Run a sample inference using the loaded model\n",
    "input_payload = [{\"topic\": TOPIC, \"question\": QUESTION, \"input_text\": INPUT_TEXT, }]\n",
    "\n",
    "print(\"\\n=== Running Sample Inference ===\")\n",
    "results = loaded_model.predict(input_payload)\n",
    "result = results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4d05ba-ed68-46a5-971a-491ffb93c17a",
   "metadata": {},
   "source": [
    "# Generated Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3216c60d-5bfe-405f-868c-bf353d3609f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# 🧠 Synthesized partial answer (1/1)\n",
       "\n",
       "Since the user's question is about individuals mentioned in the document as providing feedback, and neither Chunk 1 nor Chunk 2 mentions any individuals providing feedback, the final answer is:\n",
       "\n",
       "**No individuals are mentioned in the document as providing feedback.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(result.answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16159a90-d40d-4d20-b9e6-57cb3bd14cf3",
   "metadata": {},
   "source": [
    "# Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "370f4cac-c92f-4f8b-b2e5-10cd12e08a87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"role\": \"developer\",\n",
      "        \"content\": \"User submitted a question.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Which poeple provided the feedback?\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"developer\",\n",
      "        \"content\": \"\\ud83e\\udde0 Relevance check result:\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"yes\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"developer\",\n",
      "        \"content\": \"\\ud83e\\udded No cached answer found for question: 'Which poeple provided the feedback?'\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"developer\",\n",
      "        \"content\": \"\\u270f\\ufe0f Rewritten user question:\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Who are the individuals mentioned in the document as providing feedback?\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"developer\",\n",
      "        \"content\": \"\\ud83e\\udde9 Chunked 1 documents into 2 chunks (size=4096, overlap=256)\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"developer\",\n",
      "        \"content\": \"\\ud83e\\udde0 Processed 2 chunks for question: 'Who are the individuals mentioned in the document as providing feedback?'\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"developer\",\n",
      "        \"content\": \"\\u2705 Synthesized 1 group-level summaries.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"# \\ud83e\\udde0 Synthesized partial answer (1/1)\\n\\nSince the user's question is about individuals mentioned in the document as providing feedback, and neither Chunk 1 nor Chunk 2 mentions any individuals providing feedback, the final answer is:\\n\\n**No individuals are mentioned in the document as providing feedback.**\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"developer\",\n",
      "        \"content\": \"\\ud83d\\udcbe Stored answer in memory for question key: 'which poeple provided the feedback?'\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"developer\",\n",
      "        \"content\": \"\\ud83d\\udce4 Final answer delivered: # \\ud83e\\udde0 Synthesized partial answer (1/1)\\n\\nSince the user's question is about individuals mentioned in the document as providing feedback, and neither Chunk 1 nor Chunk 2 mentions any individuals providing feedback, the final answer is:\\n\\n**No individuals are mentioned in the document as providing feedback.**\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(result.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1723e941-2523-4db6-ab89-fedffbc0be0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 05:16:22 - INFO - ⏱️ Total execution time: 5m 44.16s\n",
      "2025-08-02 05:16:22 - INFO - ✅ Notebook execution completed successfully.\n"
     ]
    }
   ],
   "source": [
    "end_time: float = time.time()\n",
    "elapsed_time: float = end_time - start_time\n",
    "elapsed_minutes: int = int(elapsed_time // 60)\n",
    "elapsed_seconds: float = elapsed_time % 60\n",
    "\n",
    "logger.info(f\"⏱️ Total execution time: {elapsed_minutes}m {elapsed_seconds:.2f}s\")\n",
    "logger.info(\"✅ Notebook execution completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c45acb",
   "metadata": {},
   "source": [
    "Built with ❤️ using [**HP AI Studio**](https://hp.com/ai-studio)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
