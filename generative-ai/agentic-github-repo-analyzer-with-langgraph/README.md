# ü§ñ Agentic GitHub Repo Analyzer with LangGraph

<div align="center">

![Python](https://img.shields.io/badge/Python-3.11+-blue.svg?logo=python)
![MLflow](https://img.shields.io/badge/MLflow-Model_Deployment-orange.svg?logo=mlflow)
![Streamlit](https://img.shields.io/badge/Streamlit-Frontend_App-ff4b4b.svg?logo=streamlit)
![LangGraph](https://img.shields.io/badge/LangGraph-Agentic_Workflow-blue.svg?logo=langchain)
![LangChain](https://img.shields.io/badge/LangChain-LLM_Orchestration-lightgreen.svg?logo=langchain)

</div>

---

## üìö Contents

* [üß† Overview](#üß†-overview)
* [üìÅ Project Structure](#üìÅ-project-structure)
* [‚öôÔ∏è Setup](#‚öôÔ∏è-setup)
* [üöÄ Usage](#üöÄ-usage)
* [üìû Contact & Support](#üìû-contact--support)

---

## üß† Overview

The **Agentic GitHub Repo Analyzer** is a production-grade LangGraph-powered blueprint designed to help users ingest, reason over, and synthesize insights from a Github repository.

It integrates:

* üìÑ Multi-format document ingestion (TXT, PDF, DOCX, XLSX, CSV, Markdown, PY, JSON, YML, YAML, IPYNB)
* üß† Per-chunk reasoning using LLMs orchestrated via LangGraph
* üß¨ Chunk grouping to respect token limits during synthesis
* üìä Final markdown-formatted, non-redundant answer synthesis
* üß™ MLflow model packaging and deployment
* üåê Streamlit-based UI for interactive inference

---

## üìÅ Project Structure

```bash
agentic-github-repo-analyzer-with-langgraph/
‚îú‚îÄ‚îÄ data/                                # Github repo files (input directory)
‚îÇ   ‚îî‚îÄ‚îÄ input/
‚îú‚îÄ‚îÄ demo/                                # UI frontend code (Streamlit)
‚îÇ   ‚îî‚îÄ‚îÄ streamlit/                       
‚îú‚îÄ‚îÄ docs/                                # Documents for UI Testing
‚îÇ   ‚îú‚îÄ‚îÄ diagram-for-agentic-github-repo-analyzer-with-langgraph.png
‚îÇ   ‚îú‚îÄ‚îÄ Streamlit UI Page - Agentic Github Repo Analyzer.pdf
‚îÇ   ‚îî‚îÄ‚îÄ streamlit-ui-ss-agentic-github-repo-analyzer.png
‚îú‚îÄ‚îÄ notebooks/                           # Workflow and MLflow notebooks
‚îÇ   ‚îú‚îÄ‚îÄ register-model.ipynb             
‚îÇ   ‚îî‚îÄ‚îÄ run-workflow.ipynb
‚îú‚îÄ‚îÄ requirements.txt                     # All required packages
‚îú‚îÄ‚îÄ README.md                            # Project documentation
‚îî‚îÄ‚îÄ src/                                 # Core LangGraph modules
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ agentic_feedback_model.py        # MLflow PyFunc model class
    ‚îú‚îÄ‚îÄ agentic_nodes.py                 # LangGraph nodes
    ‚îú‚îÄ‚îÄ agentic_state.py                 # Shared state schema
    ‚îú‚îÄ‚îÄ agentic_workflow.py              # LangGraph DAG construction
    ‚îú‚îÄ‚îÄ simple_kv_memory.py              # Disk-based memory module
    ‚îî‚îÄ‚îÄ utils.py                         # Helper functions
```

---

## ‚öôÔ∏è Setup

### Step 0: Minimum Hardware Requirements

* ‚úÖ **GPU**: NVIDIA GPU with 12 GB+ VRAM (recommended for LLM acceleration)
* ‚úÖ **RAM**: 32‚Äì64 GB system memory
* ‚úÖ **Disk**: ‚â• 10 GB free space

### Step 1: Create an AI Studio Project

1. Go to [HP AI Studio](https://hp.com/ai-studio) and create a new project.
2. Use the base image: `Local GenAI`

### Step 2: Add Required Assets

- Download the Meta Llama 3.1 model with 8B parameters via Models tab:

  - **Model Name**: `meta-llama3.1-8b-Q8`
  - **Model Source**: `AWS S3`
  - **S3 URI**: `s3://149536453923-hpaistudio-public-assets/Meta-Llama-3.1-8B-Instruct-Q8_0`
  - **Resource Type**: `public`
  - **Bucket Region**: `us-west-2`

- Make sure that the model is in the `datafabric` folder inside your workspace. If the model does not appear after downloading, please restart your workspace.

---

## üöÄ Usage

### üß™ Step 1: Run LangGraph Workflow

Use the provided notebook to run the end-to-end pipeline:

```bash
notebooks/run-workflow.ipynb
```

This notebook will:

* Ingest GitHub repo files
* Rewrite user questions
* Generate per-chunk responses
* Synthesize final answer via chunk-group aggregation

### üß† Step 2: Register Model with MLflow

Log and serve the full pipeline as an MLflow `pyfunc` model:

```bash
notebooks/register-model.ipynb
```

This registers the model so it can be queried over HTTP.

### üåê Step 3: Launch Streamlit UI

This web UI allows:

* Specifying GitHub repo url and folder path to analyze
* Entering user questions
* Connecting to a local MLflow model endpoint
* Viewing markdown-formatted answers


---

## üìû Contact & Support

  - **Troubleshooting:** Refer to the [**Troubleshooting**](https://github.com/HPInc/AI-Blueprints/tree/main?tab=readme-ov-file#troubleshooting) section of the main README in our public AI-Blueprints GitHub repo for solutions to common issues.

  - **Issues & Bugs:** Open a new issue in our [**AI-Blueprints GitHub repo**](https://github.com/HPInc/AI-Blueprints).

  - **Docs:** [**AI Studio Documentation**](https://zdocs.datascience.hp.com/docs/aistudio/overview).

  - **Community:** Join the [**HP AI Creator Community**](https://community.datascience.hp.com/) for questions and help.

---

> Built with ‚ù§Ô∏è using [**HP AI Studio**](https://hp.com/ai-studio)