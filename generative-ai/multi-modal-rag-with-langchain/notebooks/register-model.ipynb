{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff193981-f701-4595-9d5f-a08779c65508",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-size: 50px;\"> ü§ñ MLFlow Registration for Multimodal RAG</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2607392-6bd0-4950-8ac8-84026da9df0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt --quiet "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e7b230-e638-458a-8104-0a30727b2466",
   "metadata": {},
   "source": [
    "# MLFlow Model Service \n",
    "\n",
    "In this section, we demonstrate how to deploy a RAG-based chatbot service. This service provides a REST API endpoint that allows users to query the knowledge base with natural language questions, upload new documents to the knowledge base, and manage conversation history, all with built-in safeguards against sensitive information and toxicity. This service encapsulates all the functionality we developed in this notebook, including the document retrieval system, RAG-based question answering capabilities, and Galileo integration for protection, observation and evaluation. It demonstrates how to use our ChatbotService from the src/service directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032ab59a-a358-4b11-9ee4-ac38101666bf",
   "metadata": {},
   "source": [
    "## Step 0: Imports and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622040db-4d03-482f-958f-45cc87492ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Standard Library Imports ===\n",
    "import gc\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, TypedDict\n",
    "\n",
    "# === Third-Party Library Imports ===\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain.schema.document import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.types import ColSpec, DataType, Schema, TensorSpec\n",
    "from PIL import Image as PILImage\n",
    "from sentence_transformers import CrossEncoder\n",
    "from transformers import AutoImageProcessor, AutoModel, AutoTokenizer, BitsAndBytesConfig, SiglipModel, SiglipProcessor\n",
    "\n",
    "\n",
    "# === Project-Specific Imports ===\n",
    "# Add the project root to the system path to allow importing from 'src'\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from src.utils import (\n",
    "    configure_hf_cache,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5a1883-f877-414a-9ff9-b876a2ba6531",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"multimodal_rag_register_notebook\")\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.handlers:\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "    stream_handler = logging.StreamHandler(sys.stdout)\n",
    "    stream_handler.setFormatter(formatter)\n",
    "    logger.addHandler(stream_handler)\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192457a9-952b-4b3f-9640-6875692f5f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38660c87-34b3-40f2-be21-16c78fb12598",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Notebook execution started.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4576dc32-7e04-48aa-b19d-da1adc92693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "configure_hf_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769f37b3-a909-4cae-8278-015adcdcf69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalRagModel(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"\n",
    "    An MLflow PythonModel that encapsulates the entire Multimodal RAG pipeline.\n",
    "\n",
    "    This class faithfully reproduces the workflow from the `run-notebook.ipynb`, including\n",
    "    data loading, multi-stage retrieval (vector search + reranking), and multimodal\n",
    "    generation with the InternVL model.\n",
    "\n",
    "    Expected Artifacts during logging/loading:\n",
    "      - \"chroma_dir\": Path to the persisted Chroma vectorstore directory (containing both text and image collections).\n",
    "      - \"context_dir\": Path to the root data directory, which must contain the `images` subdirectory.\n",
    "      - \"cache_dir\": Path to the directory for the semantic cache.\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Helper Classes (Encapsulated from the original notebook)\n",
    "    # --------------------------------------------------------------------------\n",
    "\n",
    "    class SemanticCache:\n",
    "        \"\"\"\n",
    "        A semantic cache using a Chroma vector store to find similar queries.\n",
    "        \"\"\"\n",
    "        def __init__(self, persist_directory: Path, embedding_function: Embeddings, collection_name: str = \"multimodal_cache\"):\n",
    "            self.embedding_function = embedding_function\n",
    "            self._vectorstore = Chroma(\n",
    "                collection_name=collection_name,\n",
    "                persist_directory=str(persist_directory),\n",
    "                embedding_function=self.embedding_function\n",
    "            )\n",
    "\n",
    "        def get(self, query: str, threshold: float = 0.90) -> Optional[Dict[str, Any]]:\n",
    "            \"\"\"\n",
    "            Searches for a semantically similar query in the cache.\n",
    "            \"\"\"\n",
    "            if self._vectorstore._collection.count() == 0:\n",
    "                return None # Cache is empty\n",
    "\n",
    "            results = self._vectorstore.similarity_search_with_score(query, k=1)\n",
    "            if not results:\n",
    "                return None\n",
    "\n",
    "            most_similar_doc, score = results[0]\n",
    "            similarity = 1.0 - score\n",
    "\n",
    "            logger.info(f\"Most similar cached query: '{most_similar_doc.page_content}' (Similarity: {similarity:.4f})\")\n",
    "\n",
    "            if similarity >= threshold:\n",
    "                cached_result = most_similar_doc.metadata\n",
    "                cached_result['used_images'] = json.loads(cached_result.get('used_images', '[]'))\n",
    "                return cached_result\n",
    "\n",
    "            return None\n",
    "\n",
    "        def set(self, query: str, result_dict: Dict[str, Any]) -> None:\n",
    "            \"\"\"\n",
    "            Adds a new query and its result to the cache.\n",
    "            \"\"\"\n",
    "            metadata_to_store = {\n",
    "                'reply': result_dict.get('reply', ''),\n",
    "                'used_images': json.dumps(result_dict.get('used_images', []))\n",
    "            }\n",
    "\n",
    "            doc = Document(page_content=query, metadata=metadata_to_store)\n",
    "            self._vectorstore.add_documents([doc])\n",
    "            logger.info(f\"Added query to semantic cache: '{query}'\")\n",
    "\n",
    "        def delete(self, query: str) -> None:\n",
    "            \"\"\"\n",
    "            Finds and deletes a query and its cached response from the vector store.\n",
    "            \"\"\"\n",
    "            results = self._vectorstore.get(where={\"page_content\": query})\n",
    "            if results and 'ids' in results and results['ids']:\n",
    "                doc_id_to_delete = results['ids'][0]\n",
    "                self._vectorstore._collection.delete(ids=[doc_id_to_delete])\n",
    "                logger.info(f\"Cleared old cache for query: '{query}'\")\n",
    "\n",
    "    class SiglipEmbeddings(Embeddings):\n",
    "        \"\"\"LangChain compatible wrapper for SigLIP image/text embeddings.\"\"\"\n",
    "        def __init__(self, model_id: str, device: str):\n",
    "            self.device = device\n",
    "            self.model = SiglipModel.from_pretrained(model_id).to(self.device)\n",
    "            self.processor = SiglipProcessor.from_pretrained(model_id)\n",
    "\n",
    "        def _embed_text(self, txts: List[str]) -> np.ndarray:\n",
    "            inp = self.processor(text=txts, return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                return self.model.get_text_features(**inp).cpu().numpy()\n",
    "\n",
    "        def _embed_imgs(self, paths: List[str]) -> np.ndarray:\n",
    "            imgs = [PILImage.open(p).convert(\"RGB\") for p in paths]\n",
    "            inp = self.processor(images=imgs, return_tensors=\"pt\").to(self.device)\n",
    "            with torch.no_grad():\n",
    "                return self.model.get_image_features(**inp).cpu().numpy()\n",
    "\n",
    "        def embed_documents(self, docs: List[str]) -> List[List[float]]:\n",
    "            return self._embed_imgs(docs).tolist()\n",
    "\n",
    "        def embed_query(self, txt: str) -> List[float]:\n",
    "            return self._embed_text([txt])[0].tolist()\n",
    "\n",
    "    class InternVLMM:\n",
    "        \"\"\"Minimal, self-contained multimodal QA wrapper around InternVL3-8B-Instruct.\"\"\"\n",
    "        MODEL_NAME = \"OpenGVLab/InternVL3-8B-Instruct\"\n",
    "\n",
    "        def __init__(self, device: str, cache: Any):\n",
    "            self.device = device\n",
    "            self.model = None\n",
    "            self.tok = None\n",
    "            self.image_processor = None\n",
    "            self.cache = cache\n",
    "            self._load()\n",
    "\n",
    "        def generate(self, query: str, context: Dict[str, Any], force_regenerate: bool = False) -> Dict[str, Any]:\n",
    "            \"\"\"Run retrieval, prompt assembly, and model generation.\"\"\"\n",
    "            if not force_regenerate:\n",
    "                cached_result = self.cache.get(query, threshold=0.98)\n",
    "                if cached_result:\n",
    "                    logger.info(f\"SEMANTIC CACHE HIT for query: '{query}'\")\n",
    "                    return cached_result\n",
    "\n",
    "            if force_regenerate:\n",
    "                logger.info(f\"Forced regeneration for query: '{query}'. Clearing old cache entry.\")\n",
    "                self.cache.delete(query)\n",
    "\n",
    "            logger.info(f\"CACHE MISS for query: '{query}'. Running full pipeline.\")\n",
    "            if self.model is None or self.tok is None:\n",
    "                return {\"reply\": \"Error: model not initialised.\", \"used_images\": []}\n",
    "\n",
    "            hits = self._retrieve_mm(query, **context)\n",
    "            docs, images = hits[\"docs\"], hits[\"images\"]\n",
    "\n",
    "            if not docs and not images:\n",
    "                return {\"reply\": \"I don't know based on the provided context.\", \"used_images\": []}\n",
    "\n",
    "            # Build prompt\n",
    "            context_str = \"\\n\\n\".join(\n",
    "                f\"<source_document name=\\\"{d.metadata.get('source', 'unknown')}\\\">\\n{d.page_content}\\n</source_document>\"\n",
    "                for d in docs\n",
    "            )\n",
    "\n",
    "            user_content = f\"\"\"\n",
    "                <task_instructions>\n",
    "                Your response must follow this exact structure:\n",
    "\n",
    "                ##**1. Visual Analysis**\n",
    "                First, provide a detailed description of what is shown in the provided image(s), based only on what you can see.\n",
    "\n",
    "                ##**2. Synthesized Answer**\n",
    "                Next, answer the user's original query. Your answer must synthesize information STRICTLY from your **Visual Analysis** and the provided text `<context>`.\n",
    "\n",
    "                ##**3. Source Documents**\n",
    "                At the very end of your response, cite the source from the context in brackets and backticks, like this: [`source-file-name.md`].\n",
    "                </task_instructions>\n",
    "\n",
    "                <context>\n",
    "                {context_str}\n",
    "                </context>\n",
    "\n",
    "                <user_query>\n",
    "                {query}\n",
    "                </user_query>\n",
    "\n",
    "                Now, generate the response following all instructions.\n",
    "                \"\"\"\n",
    "            SYSTEM_PROMPT = \"\"\"\n",
    "You are AI Studio DevOps Assistant. Your function is to analyze images and text, then answer questions based ONLY on the provided materials.\n",
    "\n",
    "**PERMANENT INSTRUCTIONS:**\n",
    "1.  **Analyze and Answer from Context**: Your entire response MUST be derived exclusively from the provided `<context>` block and the user's image(s).\n",
    "2.  **No External Knowledge**: You MUST NOT use any of your pre-trained knowledge or information outside the provided materials.\n",
    "3.  **No Hallucination**: Do not invent or assume any details. If information is not present, it does not exist.\n",
    "4.  **Handle Missing Information**: If the provided context and image(s) do not contain the answer, your ONLY response will be: \"Based on the provided context, I cannot answer this question.\" Do not add any other words or explanation.\n",
    "\"\"\"\n",
    "            conversation = [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": user_content}\n",
    "            ]\n",
    "\n",
    "            prompt = self.tok.apply_chat_template(\n",
    "                conversation,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "\n",
    "            if images:\n",
    "                prompt += f\"\\n\\n[{len(images)} image(s) are provided for analysis]\"\n",
    "\n",
    "            # Generate\n",
    "            try:\n",
    "                self._clear_cuda()\n",
    "                pixel_values = self._process_images(images) if images else None\n",
    "                reply = self.model.chat(\n",
    "                    self.tok, pixel_values, prompt,\n",
    "                    generation_config=dict(max_new_tokens=8916, pad_token_id=self.tok.pad_token_id, eos_token_id=self.tok.eos_token_id),\n",
    "                )\n",
    "                self._clear_cuda()\n",
    "                result_dict = {\"reply\": reply, \"used_images\": images}\n",
    "\n",
    "                self.cache.set(query, result_dict)\n",
    "\n",
    "                return result_dict\n",
    "            except RuntimeError as e:\n",
    "                logger.error(\"InternVL generation failed: %s\", e)\n",
    "                return {\"reply\": f\"Error during generation: {e}\", \"used_images\": images}\n",
    "\n",
    "        def _retrieve_mm(self, query: str, text_db: Chroma, image_db: Chroma, siglip_embeds: Any, cross_encoder: Any, k_txt: int = 8, k_img: int = 8, fetch_k: int = 20) -> Dict[str, Any]:\n",
    "            \"\"\"Performs hybrid retrieval for text and associated images.\"\"\"\n",
    "            # 1. Coarse recall (text)\n",
    "            docs_and_init = text_db.similarity_search_with_score(query, k=fetch_k)\n",
    "            if not docs_and_init:\n",
    "                return {\"docs\": [], \"images\": []}\n",
    "            docs, init_scores = zip(*docs_and_init)\n",
    "\n",
    "            # 2. Rerank (text)\n",
    "            rerank_scores = cross_encoder.predict([(query, d.page_content) for d in docs])\n",
    "\n",
    "            # 3. Hybrid scoring\n",
    "            hybrid_scores = [0.6 * init + 0.4 * rerank for init, rerank in zip(init_scores, rerank_scores)]\n",
    "\n",
    "            # 4. Select top-k text\n",
    "            scored_docs = sorted(zip(docs, hybrid_scores), key=lambda x: x[1], reverse=True)\n",
    "            selected_docs = [doc for doc, score in scored_docs[:k_txt]]\n",
    "\n",
    "            # 5. Image retrieval\n",
    "            sources = [d.metadata[\"source\"] for d in selected_docs]\n",
    "            q_emb = siglip_embeds.embed_query(query)\n",
    "            img_hits = image_db.similarity_search_by_vector(q_emb, k=k_img * 2, filter={\"source\": {\"$in\": sources}})\n",
    "            images = [img.page_content for img in img_hits[:k_img]]\n",
    "\n",
    "            return {\"docs\": selected_docs, \"images\": images}\n",
    "\n",
    "        def _load(self):\n",
    "            \"\"\"Load tokenizer, image_processor, & model.\"\"\"\n",
    "            logger.info(\"Loading %s...\", self.MODEL_NAME)\n",
    "            self._clear_cuda()\n",
    "\n",
    "            self.tok = AutoTokenizer.from_pretrained(self.MODEL_NAME, trust_remote_code=True)\n",
    "            if self.tok.pad_token is None: self.tok.pad_token = self.tok.eos_token\n",
    "\n",
    "            self.image_processor = AutoImageProcessor.from_pretrained(self.MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "            q_cfg = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "            )\n",
    "\n",
    "            self.model = AutoModel.from_pretrained(\n",
    "                self.MODEL_NAME,\n",
    "                quantization_config=q_cfg,\n",
    "                torch_dtype=(torch.bfloat16 if self.device == \"cuda\" else torch.float32),\n",
    "                low_cpu_mem_usage=True,\n",
    "                trust_remote_code=True,\n",
    "                device_map=\"auto\" if self.device == \"cuda\" else None,\n",
    "            ).eval()\n",
    "            logger.info(\"Model loaded on %s.\", self.device)\n",
    "\n",
    "        def _process_images(self, image_paths: List[str]):\n",
    "            \"\"\"Convert a list of image filepaths to a single batched tensor.\"\"\"\n",
    "            if not image_paths: return None\n",
    "            try:\n",
    "                pil_images = [PILImage.open(p).convert(\"RGB\") for p in image_paths]\n",
    "                processed_data = self.image_processor(images=pil_images, return_tensors=\"pt\")\n",
    "                pixel_values = processed_data['pixel_values'].to(device=self.device, dtype=next(self.model.parameters()).dtype)\n",
    "                return pixel_values\n",
    "            except Exception as e:\n",
    "                logger.error(\"Image processing failed: %s\", e)\n",
    "                return None\n",
    "\n",
    "        def _clear_cuda(self):\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # MLflow pyfunc Methods\n",
    "    # --------------------------------------------------------------------------\n",
    "\n",
    "    def load_context(self, context: mlflow.pyfunc.PythonModelContext) -> None:\n",
    "        \"\"\"\n",
    "        This method is called when loading an MLflow model. It initializes all\n",
    "        necessary components using the artifacts logged with the model.\n",
    "        \"\"\"\n",
    "        logger.info(\"--- Initializing MultimodalRagModel context ---\")\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # 1. Define paths from artifacts\n",
    "        self.chroma_dir = Path(context.artifacts[\"chroma_dir\"])\n",
    "        self.context_dir = Path(context.artifacts[\"context_dir\"])\n",
    "        self.image_dir = self.context_dir / \"images\"\n",
    "        self.cache_dir = Path(context.artifacts[\"cache_dir\"])\n",
    "\n",
    "        logger.info(f\"Artifacts loaded: chroma_dir='{self.chroma_dir}', context_dir='{self.context_dir}', cache_dir='{self.cache_dir}'\")\n",
    "\n",
    "        # 2. Initialize embedding models and reranker\n",
    "        logger.info(\"Loading embedding models and cross-encoder...\")\n",
    "        self.text_embed_model = HuggingFaceEmbeddings(model_name=\"intfloat/e5-large-v2\")\n",
    "        self.siglip_embed_model = self.SiglipEmbeddings(model_id=\"google/siglip2-base-patch16-224\", device=self.device)\n",
    "        self.cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "        # 3. Load persisted Chroma vector stores\n",
    "        logger.info(\"Loading ChromaDB vector stores...\")\n",
    "        self.text_db = Chroma(\n",
    "            collection_name=\"mm_text\",\n",
    "            persist_directory=str(self.chroma_dir),\n",
    "            embedding_function=self.text_embed_model,\n",
    "        )\n",
    "        self.image_db = Chroma(\n",
    "            collection_name=\"mm_image\",\n",
    "            persist_directory=str(self.chroma_dir),\n",
    "            embedding_function=self.siglip_embed_model,\n",
    "        )\n",
    "        logger.info(f\"Text DB count: {self.text_db._collection.count()}, Image DB count: {self.image_db._collection.count()}\")\n",
    "\n",
    "        # 4. Initialize semantic cache\n",
    "        self.cache = self.SemanticCache(persist_directory=self.cache_dir, embedding_function=self.text_embed_model)\n",
    "\n",
    "        # 5. Load the main multimodal LLM\n",
    "        self.mm_llm = self.InternVLMM(device=self.device, cache=self.cache)\n",
    "        logger.info(\"--- Context initialization complete ---\")\n",
    "\n",
    "\n",
    "    def predict(self, context: mlflow.pyfunc.PythonModelContext, model_input: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        MLflow inference entrypoint.\n",
    "        Expects a pandas DataFrame with a \"query\" column.\n",
    "        Returns a DataFrame with \"reply\" and \"used_images\" columns.\n",
    "        \"\"\"\n",
    "        logger.info(\"Received prediction request.\")\n",
    "        queries = model_input[\"query\"].tolist()\n",
    "        results = []\n",
    "\n",
    "        # The context passed to the generate function needs all the retrieved components\n",
    "        retrieval_context = {\n",
    "            \"text_db\": self.text_db,\n",
    "            \"image_db\": self.image_db,\n",
    "            \"siglip_embeds\": self.siglip_embed_model,\n",
    "            \"cross_encoder\": self.cross_encoder\n",
    "        }\n",
    "\n",
    "        for query in queries:\n",
    "            logger.info(f\"Processing query: '{query}'\")\n",
    "            response_dict = self.mm_llm.generate(query, retrieval_context)\n",
    "            results.append(response_dict)\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    @classmethod\n",
    "    def log_model(cls, model_name: str, local_model_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Helper class method to log the MultimodalRagModel to MLflow.\n",
    "        \"\"\"\n",
    "        logger.info(f\"--- Logging '{model_name}' to MLflow ---\")\n",
    "\n",
    "        # 1. Define local artifact paths, including the local model directory\n",
    "        project_root = Path.cwd().parent.resolve()\n",
    "        artifacts = {\n",
    "            \"local_model_dir\": local_model_path,\n",
    "            \"chroma_dir\": str(project_root / \"data\" / \"chroma_store\"),\n",
    "            \"context_dir\": str(project_root / \"data\" / \"context\"),\n",
    "            \"cache_dir\": str(project_root / \"data\" / \"chroma_store\" / \"semantic_cache\"),\n",
    "        }\n",
    "\n",
    "        # 2. Validate artifact paths\n",
    "        for key, path in artifacts.items():\n",
    "            if not Path(path).exists():\n",
    "                raise FileNotFoundError(f\"Required artifact not found: {key} at '{path}'\")\n",
    "\n",
    "        # 3. Define model signature\n",
    "        input_schema = Schema([ColSpec(DataType.string, \"query\")])\n",
    "        output_schema = Schema([\n",
    "            ColSpec(DataType.string, \"reply\"),\n",
    "            ColSpec(DataType.string, \"used_images\"),\n",
    "        ])\n",
    "        signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "\n",
    "        # 4. Log the model\n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=model_name,\n",
    "            python_model=cls(),\n",
    "            artifacts=artifacts,\n",
    "            signature=signature,\n",
    "            pip_requirements=\"../requirements.txt\",\n",
    "            code_paths=[\"../src\"],\n",
    "        )\n",
    "        logger.info(f\"‚úÖ Successfully logged '{model_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d6964a-0768-4fa0-9da4-e8ecbc5487db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MLflow Configuration ---\n",
    "MODEL_NAME = \"AIStudio-Multimodal-Chatbot-Model\"\n",
    "RUN_NAME = f\"Register_{MODEL_NAME}\"\n",
    "EXPERIMENT_NAME = \"AIStudio-Multimodal-Chatbot-Experiment\"\n",
    "\n",
    "LOCAL_MODEL_PATH = \"/home/jovyan/datafabric/InternVL3-8B-Instruct-1\"\n",
    "# Set MLflow tracking URI and experiment\n",
    "# This should be configured for your environment, e.g., a remote server or local file path\n",
    "mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\", \"/phoenix/mlflow\"))\n",
    "mlflow.set_experiment(experiment_name=EXPERIMENT_NAME)\n",
    "\n",
    "logger.info(f\"Using MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "logger.info(f\"Using MLflow experiment: '{EXPERIMENT_NAME}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c7b785-0628-4358-8e8d-9d49a0d6f1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# --- Start MLflow Run and Log the Model ---\n",
    "try:\n",
    "    with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "        run_id = run.info.run_id\n",
    "        logger.info(f\"Started MLflow run: {run_id}\")\n",
    "\n",
    "        # Use the class method to log the model and its artifacts\n",
    "        MultimodalRagModel.log_model(model_name=MODEL_NAME, local_model_path=LOCAL_MODEL_PATH)\n",
    "\n",
    "\n",
    "        model_uri = f\"runs:/{run_id}/{MODEL_NAME}\"\n",
    "        logger.info(f\"Registering model from URI: {model_uri}\")\n",
    "        \n",
    "        # Register the model in the MLflow Model Registry\n",
    "        mlflow.register_model(model_uri=model_uri, name=MODEL_NAME)\n",
    "        logger.info(f\"‚úÖ Successfully registered model '{MODEL_NAME}'\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    logger.error(f\"Error: A required file or directory was not found. Please ensure the project structure is correct.\")\n",
    "    logger.error(f\"Details: {e}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred during the MLflow run: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c628e7f-9d9d-4e7a-9790-34ed1b003caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Retrieve the latest version from the Model Registry ---\n",
    "try:\n",
    "    client = MlflowClient()\n",
    "    versions = client.get_latest_versions(MODEL_NAME, stages=[\"None\"])\n",
    "    if not versions:\n",
    "        raise RuntimeError(f\"No registered versions found for model '{MODEL_NAME}'.\")\n",
    "    \n",
    "    latest_version = versions[0]\n",
    "    logger.info(f\"Found latest version '{latest_version.version}' for model '{MODEL_NAME}' in stage '{latest_version.current_stage}'.\")\n",
    "    model_uri_registry = latest_version.source\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to retrieve model from registry: {e}\", exc_info=True)\n",
    "    model_uri_registry = None # Ensure variable exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31856b3-5ff5-48b9-98b7-ddbf7dd16be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_uri_registry:\n",
    "    try:\n",
    "        logger.info(f\"Loading model from: {model_uri_registry}\")\n",
    "        loaded_model = mlflow.pyfunc.load_model(model_uri=model_uri_registry)\n",
    "        logger.info(\"‚úÖ Successfully loaded model from registry.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load model from registry URI: {e}\", exc_info=True)\n",
    "        loaded_model = None\n",
    "else:\n",
    "    logger.warning(\"Skipping model loading due to previous errors.\")\n",
    "    loaded_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279582c0-74e3-4b98-9d1d-6007c98f109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Function to Display Test Results ---\n",
    "def display_results(query: str, result_df: pd.DataFrame):\n",
    "    \"\"\"Helper to neatly print the query, reply, and display images.\"\"\"\n",
    "    if result_df.empty:\n",
    "        print(\"Received an empty result.\")\n",
    "        return\n",
    "\n",
    "    reply = result_df[\"reply\"].iloc[0]\n",
    "    # Images are stored as a string representation of a list, so we need to evaluate it\n",
    "    image_paths_str = result_df[\"used_images\"].iloc[0]\n",
    "    image_paths = eval(image_paths_str) if isinstance(image_paths_str, str) and image_paths_str.startswith('[') else []\n",
    "    \n",
    "    print(\"---\" * 20)\n",
    "    print(f\"‚ùì Query:\\n{query}\\n\")\n",
    "    print(f\"ü§ñ Reply:\\n{reply}\\n\")\n",
    "    \n",
    "    if image_paths:\n",
    "        print(f\"üñºÔ∏è Displaying {len(image_paths)} retrieved image(s):\")\n",
    "        # You can integrate the display_images function from the original notebook here\n",
    "        # For simplicity, we'll just print the paths.\n",
    "        for path in image_paths:\n",
    "            print(f\"  - {path}\")\n",
    "    else:\n",
    "        print(\"‚ñ∂ No images were retrieved for this query.\")\n",
    "    print(\"---\" * 20 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa1c9dd-974c-475d-876c-e4621b03a202",
   "metadata": {},
   "outputs": [],
   "source": [
    "if loaded_model:\n",
    "    logger.info(\"Running sample inference with the loaded model...\")\n",
    "    \n",
    "    sample_queries = [\n",
    "        \"How do i run blueprints locally?\",\n",
    "        \"What are some feature flags that i can enable in AIStudio?\",\n",
    "        \"How do i manually clean my environment without hooh?\",\n",
    "    ]\n",
    "\n",
    "    for query in sample_queries:\n",
    "        try:\n",
    "            input_payload = pd.DataFrame([{\"query\": query}])\n",
    "            result = loaded_model.predict(input_payload)\n",
    "            display_results(query, result)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Prediction failed for query '{query}': {e}\", exc_info=True)\n",
    "\n",
    "else:\n",
    "    logger.warning(\"Skipping sample inference because the model was not loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725e1d0b-333c-43fd-95a7-5ce13fdfe3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"‚úÖ Notebook execution completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e89eee-6171-446a-8da6-9381d3fe10c0",
   "metadata": {},
   "source": [
    "Built with ‚ù§Ô∏è using Z by HP AI Studio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83806df-79a8-4273-ab62-96c4bf1c2ded",
   "metadata": {},
   "source": [
    "# Evaluate Hallucinations & Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79afd929-1a1c-40df-ad6f-2140059ebe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_source = config[\"model_source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65785be-a49d-447d-855d-7a359ead6e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# llm = initialize_llm(model_source, secrets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a04d0f-2827-4f3d-9b2e-88f09f5aeeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def model(batch_df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     preds, contexts = [], []\n",
    "#     for q in batch_df[\"questions\"]:\n",
    "#         answer = mm_chain.invoke(q)\n",
    "#         preds.append(answer)\n",
    "\n",
    "#         docs = retriever.get_relevant_documents(q)\n",
    "#         contexts.append(\" \".join(d.page_content for d in docs))\n",
    "\n",
    "#     # keep the incoming index so every batch‚Äôs rows stay unique\n",
    "#     return pd.DataFrame(\n",
    "#         {\n",
    "#             \"result\": preds,\n",
    "#             \"source_documents\": contexts,\n",
    "#         },\n",
    "#         index=batch_df.index,      #  ‚Üê key line\n",
    "#     )\n",
    "\n",
    "# # --- 3)  Evaluation dataset\n",
    "# eval_df = pd.DataFrame({\"questions\": [\n",
    "#     \"What naming convention should I use for a new blueprint project folder?\",\n",
    "#     \"What is the first step in the standard blueprint testing workflow?\",\n",
    "#     \"How do I fetch logs from a running Kubernetes pod?\",\n",
    "# ]})\n",
    "\n",
    "# judge = LocalGenAIJudge(\n",
    "#     llm=llm\n",
    "# )\n",
    "\n",
    "# faithfulness_metric = judge.to_mlflow_metric(\"faithfulness\")\n",
    "# relevance_metric = judge.to_mlflow_metric(\"relevance\")\n",
    "\n",
    "# results = mlflow.evaluate(\n",
    "#     model,\n",
    "#     eval_df,\n",
    "#     predictions=\"result\",\n",
    "#     evaluators=\"default\",\n",
    "#     extra_metrics=[faithfulness_metric, relevance_metric],\n",
    "#     evaluator_config={\n",
    "#         \"col_mapping\": {\n",
    "#             \"inputs\": \"questions\",\n",
    "#             \"context\": \"source_documents\"\n",
    "#         }\n",
    "#     },\n",
    "# )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
