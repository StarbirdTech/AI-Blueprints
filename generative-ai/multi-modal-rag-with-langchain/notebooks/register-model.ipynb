{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff193981-f701-4595-9d5f-a08779c65508",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-size: 50px;\"> ðŸ¤– MLFlow Registration for Multimodal RAG</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2607392-6bd0-4950-8ac8-84026da9df0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e7b230-e638-458a-8104-0a30727b2466",
   "metadata": {},
   "source": [
    "# MLFlow Model Service \n",
    "\n",
    "In this section, we demonstrate how to deploy a RAG-based chatbot service. This service provides a REST API endpoint that allows users to query the knowledge base with natural language questions, upload new documents to the knowledge base, and manage conversation history, all with built-in safeguards against sensitive information and toxicity. This service encapsulates all the functionality we developed in this notebook, including the document retrieval system, RAG-based question answering capabilities, and Galileo integration for protection, observation and evaluation. It demonstrates how to use our ChatbotService from the src/service directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032ab59a-a358-4b11-9ee4-ac38101666bf",
   "metadata": {},
   "source": [
    "## Step 0: Imports and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "622040db-4d03-482f-958f-45cc87492ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-27 09:28:59.824944: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-27 09:28:59.885032: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753608539.915328   10683 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753608539.927023   10683 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753608539.965360   10683 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753608539.965381   10683 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753608539.965382   10683 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753608539.965383   10683 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-27 09:28:59.975636: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# === Standard Library Imports ===\n",
    "import gc\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tempfile\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, TypedDict\n",
    "\n",
    "# === Third-Party Library Imports ===\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain.schema.document import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.types import ColSpec, DataType, Schema, TensorSpec\n",
    "from PIL import Image as PILImage\n",
    "from sentence_transformers import CrossEncoder, SentenceTransformer\n",
    "from transformers import pipeline, AutoImageProcessor, AutoModel, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, SiglipModel, SiglipProcessor\n",
    "\n",
    "# === Project-Specific Imports ===\n",
    "# Add the project root to the system path to allow importing from 'src'\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from src.components import SemanticCache, SiglipEmbeddings\n",
    "from src.local_genai_judge import LocalGenAIJudge\n",
    "from src.utils import (\n",
    "    configure_hf_cache,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e5a1883-f877-414a-9ff9-b876a2ba6531",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"multimodal_rag_register_notebook\")\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.handlers:\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "    stream_handler = logging.StreamHandler(sys.stdout)\n",
    "    stream_handler.setFormatter(formatter)\n",
    "    logger.addHandler(stream_handler)\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cb6032-d809-4bf9-b9b8-87ef1f3c47ed",
   "metadata": {},
   "source": [
    "## Step 1: Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82d6964a-0768-4fa0-9da4-e8ecbc5487db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '925116260695393880'. Detailed error Yaml file '/phoenix/mlflow/925116260695393880/meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/mlflow/store/tracking/file_store.py\", line 329, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/mlflow/store/tracking/file_store.py\", line 427, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/mlflow/store/tracking/file_store.py\", line 1373, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/mlflow/store/tracking/file_store.py\", line 1366, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/mlflow/utils/file_utils.py\", line 310, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/phoenix/mlflow/925116260695393880/meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-27 09:29:02 - INFO - Using MLflow tracking URI: /phoenix/mlflow\n",
      "2025-07-27 09:29:02 - INFO - Using MLflow experiment: 'AIStudio-Multimodal-Chatbot-Experiment'\n"
     ]
    }
   ],
   "source": [
    "# --- MLflow Configuration ---\n",
    "MODEL_NAME = \"AIStudio-Multimodal-Chatbot-Model\"\n",
    "RUN_NAME = f\"Register_{MODEL_NAME}\"\n",
    "EXPERIMENT_NAME = \"AIStudio-Multimodal-Chatbot-Experiment\"\n",
    "\n",
    "LOCAL_MODEL_PATH = \"/home/jovyan/datafabric/InternVL3-8B-Instruct\"\n",
    "# Set MLflow tracking URI and experiment\n",
    "# This should be configured for your environment, e.g., a remote server or local file path\n",
    "mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\", \"/phoenix/mlflow\"))\n",
    "mlflow.set_experiment(experiment_name=EXPERIMENT_NAME)\n",
    "\n",
    "logger.info(f\"Using MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "logger.info(f\"Using MLflow experiment: '{EXPERIMENT_NAME}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "192457a9-952b-4b3f-9640-6875692f5f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38660c87-34b3-40f2-be21-16c78fb12598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-27 09:29:02 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Notebook execution started.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4576dc32-7e04-48aa-b19d-da1adc92693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "configure_hf_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1cb8d4-d58a-48bc-96e9-9bd63d43f80f",
   "metadata": {},
   "source": [
    "## Step 2: MLflow Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "769f37b3-a909-4cae-8278-015adcdcf69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalRagModel(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"\n",
    "    An MLflow PythonModel that encapsulates the entire Multimodal RAG pipeline.\n",
    "\n",
    "    This class faithfully reproduces the workflow from the `run-notebook.ipynb`, including\n",
    "    data loading, multi-stage retrieval (vector search + reranking), and multimodal\n",
    "    generation with the InternVL model.\n",
    "\n",
    "    Expected Artifacts during logging/loading:\n",
    "      - \"chroma_dir\": Path to the persisted Chroma vectorstore directory.\n",
    "      - \"context_dir\": Path to the root data directory, containing the `images` subdirectory.\n",
    "      - \"cache_dir\": Path to the directory for the semantic cache.\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Helper Classes (Encapsulated from the original notebook)\n",
    "    # --------------------------------------------------------------------------\n",
    "\n",
    "    class InternVLMM:\n",
    "        \"\"\"Minimal, self-contained multimodal QA wrapper around InternVL-Chat-V1-5.\"\"\"\n",
    "        def __init__(self, model_path: str, device: str, cache: Any):\n",
    "            self.device = device\n",
    "            self.model = None\n",
    "            self.model_path = model_path\n",
    "            self.tok = None\n",
    "            self.image_processor = None\n",
    "            self.cache = cache\n",
    "            self._load()\n",
    "\n",
    "        def generate(self, query: str, context: Dict[str, Any], force_regenerate: bool = False) -> Dict[str, Any]:\n",
    "            if not force_regenerate:\n",
    "                cached_result = self.cache.get(query, threshold=0.92)\n",
    "                if cached_result:\n",
    "                    logger.info(f\"SEMANTIC CACHE HIT for query: '{query}'\")\n",
    "                    return cached_result\n",
    "\n",
    "            if force_regenerate:\n",
    "                logger.info(f\"Forced regeneration for query: '{query}'. Clearing old cache entry.\")\n",
    "                self.cache.delete(query)\n",
    "\n",
    "            logger.info(f\"CACHE MISS for query: '{query}'. Running full pipeline.\")\n",
    "            if self.model is None or self.tok is None:\n",
    "                return {\"reply\": \"Error: model not initialised.\", \"used_images\": []}\n",
    "\n",
    "            hits = self._retrieve_mm(query, **context)\n",
    "            docs, images = hits[\"docs\"], hits[\"images\"]\n",
    "\n",
    "            if not docs and not images:\n",
    "                return {\"reply\": \"I don't know based on the provided context.\", \"used_images\": []}\n",
    "\n",
    "            # Build prompt\n",
    "            context_str = \"\\n\\n\".join(\n",
    "                f\"<source_document name=\\\"{d.metadata.get('source', 'unknown')}\\\">\\n{d.page_content}\\n</source_document>\"\n",
    "                for d in docs\n",
    "            )\n",
    "\n",
    "            visual_analysis_prompt = \"\"\n",
    "            if images:\n",
    "                visual_analysis_prompt = \"\"\"\n",
    "                ## **Visual Analysis**\n",
    "                #### Answer Here\n",
    "                First, provide a detailed description of what is shown in the provided image(s), based only on what you can see.\n",
    "                \"\"\"\n",
    "            \n",
    "            # Construct the final prompt\n",
    "            user_content = f\"\"\"\n",
    "                <task_instructions>\n",
    "                Your response must follow this exact structure:\n",
    "                {visual_analysis_prompt}\n",
    "                ## **Synthesized Answer**\n",
    "                #### Answer Here\n",
    "                Next, answer the user's original query. Your answer must be synthesized from the provided text `<context>`. Use the visual analysis only if it is relevant. If the image is not relevant, rely solely on the text context to formulate your answer.\n",
    "        \n",
    "                ## **Source Documents**\n",
    "                #### Answer Here\n",
    "                At the very end of your response, cite the source from the context in brackets and backticks, like this: [`source-file-name.md`].\n",
    "                </task_instructions>\n",
    "        \n",
    "                <context>\n",
    "                    {context_str}\n",
    "                </context>\n",
    "        \n",
    "                <user_query>\n",
    "                     {query}\n",
    "                </user_query>\n",
    "        \n",
    "                Now, generate the response following all instructions.\n",
    "                \"\"\"\n",
    "            SYSTEM_PROMPT = \"\"\"\n",
    "                You are AI Studio DevOps Assistant. Your function is to analyze images and text, then answer questions based ONLY on the provided materials.\n",
    "                \n",
    "                **PERMANENT INSTRUCTIONS:**\n",
    "                1.  **Analyze and Answer from Context**: Your entire response MUST be derived thoroughly from the provided `<context>` block or the user's image(s).\n",
    "                2.  **Follow Output Structure**: You MUST follow the multi-part response structure outlined in the user's message. Completing all sections is mandatory.\n",
    "                3.  **No External Knowledge**: You MUST NOT use any information outside the provided materials.\n",
    "                4.  **No Hallucination**: Do not invent or assume any details. If information is not present, it does not exist.\n",
    "                5.  **Handle Missing Information**: If the provided context or image(s) do not contain the answer, your ONLY response will be: \"Based on the provided context, I cannot answer this question.\" Do not add any other words or explanation.\n",
    "                \"\"\"\n",
    "            conversation = [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": user_content}\n",
    "            ]\n",
    "\n",
    "            prompt = self.tok.apply_chat_template(\n",
    "                conversation,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "\n",
    "            # Generate\n",
    "            try:\n",
    "                self._clear_cuda()\n",
    "                pixel_values = self._process_images(images) if images else None\n",
    "                reply = self.model.chat(\n",
    "                    self.tok, pixel_values, prompt,\n",
    "                    generation_config=dict(\n",
    "                        max_new_tokens=16384, \n",
    "                        pad_token_id=self.tok.pad_token_id, \n",
    "                        eos_token_id=self.tok.eos_token_id,\n",
    "                        repetition_penalty=1.10,\n",
    "                        # --- Add these lines for creative sampling ---\n",
    "                        do_sample=True,         # This is required to enable sampling\n",
    "                        temperature=0.3,        # Controls randomness. Lower is more predictable, higher is more creative.\n",
    "                        top_p=0.9,              # Nucleus sampling: considers the smallest set of tokens whose cumulative probability exceeds top_p.\n",
    "                    ),\n",
    "                )\n",
    "                self._clear_cuda()\n",
    "                result_dict = {\"reply\": reply, \"used_images\": images}\n",
    "\n",
    "                self.cache.set(query, result_dict)\n",
    "                return result_dict\n",
    "            except RuntimeError as e:\n",
    "                logger.error(\"InternVL generation failed: %s\", e)\n",
    "                return {\"reply\": f\"Error during generation: {e}\", \"used_images\": images}\n",
    "\n",
    "        def _retrieve_mm(self, query: str, text_db: Chroma, image_db: Chroma, siglip_embeds: Any, cross_encoder: Any, k_txt: int = 4, k_img: int = 8, fetch_k: int = 20) -> Dict[str, Any]:\n",
    "            \"\"\"Performs hybrid retrieval for text and associated images.\"\"\"\n",
    "            # 1. Coarse recall (text)\n",
    "            docs_and_init = text_db.similarity_search_with_score(query, k=fetch_k)\n",
    "            if not docs_and_init:\n",
    "                return {\"docs\": [], \"images\": []}\n",
    "            docs, init_scores = zip(*docs_and_init)\n",
    "\n",
    "            # 2. Rerank (text)\n",
    "            rerank_scores = cross_encoder.predict([(query, d.page_content) for d in docs])\n",
    "\n",
    "            # 3. Hybrid scoring\n",
    "            hybrid_scores = [0.4 * init + 0.6 * rerank for init, rerank in zip(init_scores, rerank_scores)]\n",
    "\n",
    "            # 4. Select top-k text\n",
    "            scored_docs = sorted(zip(docs, hybrid_scores), key=lambda x: x[1], reverse=True)\n",
    "            selected_docs = [doc for doc, score in scored_docs[:k_txt]]\n",
    "\n",
    "            # 5. Image retrieval\n",
    "            sources = [d.metadata[\"source\"] for d in selected_docs]\n",
    "            q_emb = siglip_embeds.embed_query(query)\n",
    "            img_hits = image_db.similarity_search_by_vector(q_emb, k=k_img * 2, filter={\"source\": {\"$in\": sources}})\n",
    "            images = [img.page_content for img in img_hits[:k_img]]\n",
    "\n",
    "            return {\"docs\": selected_docs, \"images\": images}\n",
    "\n",
    "        def _load(self):\n",
    "            logger.info(\"Loading %s...\", self.model_path)\n",
    "            self._clear_cuda()\n",
    "\n",
    "            self.tok = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)\n",
    "            if self.tok.pad_token is None: self.tok.pad_token = self.tok.eos_token\n",
    "\n",
    "            self.image_processor = AutoImageProcessor.from_pretrained(self.model_path, trust_remote_code=True, use_fast=True)\n",
    "\n",
    "            q_cfg = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "            ) if self.device == \"cuda\" else None\n",
    "\n",
    "            self.model = AutoModel.from_pretrained(\n",
    "                self.model_path,\n",
    "                quantization_config=q_cfg,\n",
    "                torch_dtype=(torch.bfloat16 if self.device == \"cuda\" else torch.float32),\n",
    "                low_cpu_mem_usage=True,\n",
    "                use_flash_attn=False,\n",
    "                trust_remote_code=True,\n",
    "                device_map=\"auto\" if self.device == \"cuda\" else None,\n",
    "            ).eval()\n",
    "            logger.info(\"Model loaded on %s.\", self.device)\n",
    "\n",
    "        def _process_images(self, image_paths: List[str]):\n",
    "            if not image_paths: return None\n",
    "            try:\n",
    "                pil_images = [PILImage.open(p).convert(\"RGB\") for p in image_paths]\n",
    "                processed_data = self.image_processor(images=pil_images, return_tensors=\"pt\")\n",
    "                # Ensure pixel values are on the same device and dtype as the model\n",
    "                pixel_values = processed_data['pixel_values'].to(device=self.device, dtype=next(self.model.parameters()).dtype)\n",
    "                return pixel_values\n",
    "            except Exception as e:\n",
    "                logger.error(\"Image processing failed: %s\", e)\n",
    "                return None\n",
    "\n",
    "        def _clear_cuda(self):\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # MLflow pyfunc Methods\n",
    "    # --------------------------------------------------------------------------\n",
    "\n",
    "    def load_context(self, context: mlflow.pyfunc.PythonModelContext) -> None:\n",
    "        \"\"\"\n",
    "        This method is called when loading an MLflow model. It initializes all\n",
    "        necessary components using the artifacts logged with the model.\n",
    "        \"\"\"\n",
    "        logger.info(\"--- Initializing MultimodalRagModel context ---\")\n",
    "        \n",
    "        # This part remains the same\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        logger.info(f\"Running on device: {self.device}\")\n",
    "    \n",
    "        # Get the path to the bundled model artifacts that MLflow provides\n",
    "        # The key \"local_model_dir\" must match the key used in the `artifacts` dict during logging\n",
    "        model_artifact_path = Path(context.artifacts[\"local_model_dir\"])\n",
    "    \n",
    "        self.model_path = model_artifact_path.resolve()\n",
    "        logger.info(f\"Resolved local model path to: {self.model_path}\")\n",
    "        \n",
    "        # The rest of the method now uses this resolved path\n",
    "        self.chroma_dir = Path(context.artifacts[\"chroma_dir\"])\n",
    "        self.context_dir = Path(context.artifacts[\"context_dir\"])\n",
    "        self.cache_dir = Path(context.artifacts[\"cache_dir\"])\n",
    "        logger.info(f\"Artifacts loaded: chroma_dir='{self.chroma_dir}', context_dir='{self.context_dir}', cache_dir='{self.cache_dir}'\")\n",
    "    \n",
    "        # This loads from the local artifacts bundled with your MLflow model\n",
    "        logger.info(\"Loading embedding models and cross-encoder from local artifacts...\")\n",
    "        \n",
    "        # 1. Get the local paths from the MLflow context\n",
    "        e5_model_path = context.artifacts[\"e5_model_dir\"]\n",
    "        siglip_model_path = context.artifacts[\"siglip_model_dir\"]\n",
    "        cross_encoder_path = context.artifacts[\"cross_encoder_dir\"]\n",
    "        \n",
    "        # 2. Initialize models using the local paths\n",
    "        self.text_embed_model = HuggingFaceEmbeddings(\n",
    "            model_name=e5_model_path, \n",
    "            model_kwargs={\"device\": self.device}\n",
    "        )\n",
    "        self.siglip_embed_model = SiglipEmbeddings(\n",
    "            model_id=siglip_model_path, \n",
    "            device=self.device\n",
    "        )\n",
    "        self.cross_encoder = CrossEncoder(\n",
    "            cross_encoder_path, \n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        logger.info(\"âœ… Models loaded successfully from artifacts.\")\n",
    "    \n",
    "        logger.info(\"Loading ChromaDB vector stores...\")\n",
    "        self.text_db = Chroma(\n",
    "            collection_name=\"mm_text\",\n",
    "            persist_directory=str(self.chroma_dir),\n",
    "            embedding_function=self.text_embed_model,\n",
    "        )\n",
    "        self.image_db = Chroma(\n",
    "            collection_name=\"mm_image\",\n",
    "            persist_directory=str(self.chroma_dir),\n",
    "            embedding_function=self.siglip_embed_model,\n",
    "        )\n",
    "        logger.info(f\"Text DB count: {self.text_db._collection.count()}, Image DB count: {self.image_db._collection.count()}\")\n",
    "    \n",
    "        self.cache = SemanticCache(persist_directory=self.cache_dir, embedding_function=self.text_embed_model)\n",
    "    \n",
    "        # The InternVLMM will now be initialized with the safe, absolute path\n",
    "        self.mm_llm = self.InternVLMM(model_path=self.model_path, device=self.device, cache=self.cache)\n",
    "\n",
    "        logger.info(\"Initializing evaluation judge by sharing the main model...\")\n",
    "        try:\n",
    "            # Instantiate the MODIFIED judge with the model, not a pipeline\n",
    "            self.judge = LocalGenAIJudge(\n",
    "                model=self.mm_llm.model,      # Pass the model directly\n",
    "                tokenizer=self.mm_llm.tok   # Pass the tokenizer\n",
    "            )\n",
    "            logger.info(\"âœ… Evaluation judge initialized successfully.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize evaluation judge: {e}\")\n",
    "            self.judge = None\n",
    "\n",
    "        logger.info(\"--- Context initialization complete ---\")\n",
    "\n",
    "\n",
    "    def predict(self, context: mlflow.pyfunc.PythonModelContext, model_input: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        MLflow inference entrypoint.\n",
    "        Expects a pandas DataFrame with a \"query\" column.\n",
    "        Returns a DataFrame with \"reply\" and \"used_images\" columns.\n",
    "        \"\"\"\n",
    "        logger.info(\"Received prediction request.\")\n",
    "        queries = model_input[\"query\"].tolist()\n",
    "        force_regenerate = model_input.get(\"force_regenerate\", pd.Series([False] * len(queries))).tolist()\n",
    "        results = []\n",
    "\n",
    "        retrieval_context = {\n",
    "            \"text_db\": self.text_db,\n",
    "            \"image_db\": self.image_db,\n",
    "            \"siglip_embeds\": self.siglip_embed_model,\n",
    "            \"cross_encoder\": self.cross_encoder\n",
    "        }\n",
    "\n",
    "        for i, query in enumerate(queries):\n",
    "            logger.info(f\"Processing query: '{query}'\")\n",
    "            \n",
    "            # 1. Generate the answer\n",
    "            response_dict = self.mm_llm.generate(query, retrieval_context, force_regenerate=force_regenerate[i])\n",
    "\n",
    "            # Re-run retrieval to get the context string for evaluation\n",
    "            retrieved_info = self.mm_llm._retrieve_mm(query, **retrieval_context)\n",
    "            context_str = \"\\n\\n\".join(d.page_content for d in retrieved_info[\"docs\"])\n",
    "\n",
    "            # 2. Run evaluation if the judge was loaded successfully\n",
    "            if self.judge:\n",
    "                # Create a single-row DataFrame for the judge\n",
    "                eval_df = pd.DataFrame([{\n",
    "                    \"questions\": query,\n",
    "                    \"result\": response_dict[\"reply\"],\n",
    "                    \"source_documents\": context_str\n",
    "                }])\n",
    "                \n",
    "                # Get scores and add them to the response dictionary\n",
    "                response_dict[\"faithfulness\"] = self.judge.evaluate_faithfulness(eval_df).iloc[0]\n",
    "                response_dict[\"relevance\"] = self.judge.evaluate_relevance(eval_df).iloc[0]\n",
    "            else:\n",
    "                # Provide default null values if the judge isn't available\n",
    "                response_dict[\"faithfulness\"] = None\n",
    "                response_dict[\"relevance\"] = None\n",
    "            \n",
    "            results.append(response_dict)\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    @classmethod\n",
    "    def log_model(cls, model_name: str, local_model_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Helper class method to log the MultimodalRagModel to MLflow.\n",
    "        This version downloads supporting models to a temporary directory that is\n",
    "        automatically cleaned up after logging.\n",
    "        \"\"\"\n",
    "        logger.info(f\"--- Logging '{model_name}' to MLflow ---\")\n",
    "        \n",
    "        # Use a temporary directory that gets automatically deleted\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            logger.info(f\"Created temporary directory for models: {temp_dir}\")\n",
    "            temp_path = Path(temp_dir)\n",
    "    \n",
    "            # --- 1. Download models into the temporary directory ---\n",
    "            # e5 model\n",
    "            e5_path = temp_path / \"e5-large-v2\"\n",
    "            e5_model = SentenceTransformer(\"intfloat/e5-large-v2\")\n",
    "            e5_model.save(str(e5_path))\n",
    "            logger.info(f\"âœ… Temporarily saved e5-large-v2 to {e5_path}\")\n",
    "    \n",
    "            # SigLIP model\n",
    "            siglip_path = temp_path / \"siglip2-base-patch16-224\"\n",
    "            SiglipModel.from_pretrained(\"google/siglip2-base-patch16-224\").save_pretrained(siglip_path)\n",
    "            SiglipProcessor.from_pretrained(\"google/siglip2-base-patch16-224\").save_pretrained(siglip_path)\n",
    "            logger.info(f\"âœ… Temporarily saved SigLIP to {siglip_path}\")\n",
    "    \n",
    "            # Cross-Encoder model\n",
    "            cross_encoder_path = temp_path / \"ms-marco-MiniLM-L-6-v2\"\n",
    "            CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\").save(str(cross_encoder_path))\n",
    "            logger.info(f\"âœ… Temporarily saved Cross-Encoder to {cross_encoder_path}\")\n",
    "    \n",
    "            # --- 2. Define artifacts using paths from the temporary directory ---\n",
    "            project_root = Path.cwd().parent.resolve()\n",
    "            artifacts = {\n",
    "                \"local_model_dir\": local_model_path,\n",
    "                \"e5_model_dir\": str(e5_path),\n",
    "                \"siglip_model_dir\": str(siglip_path),\n",
    "                \"cross_encoder_dir\": str(cross_encoder_path),\n",
    "                \"chroma_dir\": str(project_root / \"data\" / \"chroma_store\"),\n",
    "                \"context_dir\": str(project_root / \"data\" / \"context\"),\n",
    "                \"cache_dir\": str(project_root / \"data\" / \"chroma_store\" / \"semantic_cache\"),\n",
    "            }\n",
    "    \n",
    "            # --- 3. Log the model (MLflow will copy from the temp dir) ---\n",
    "            input_schema = Schema([\n",
    "                ColSpec(DataType.string, \"query\"),\n",
    "                ColSpec(DataType.boolean, \"force_regenerate\")\n",
    "            ])\n",
    "            output_schema = Schema([\n",
    "                ColSpec(DataType.string, \"reply\"),\n",
    "                ColSpec(DataType.string, \"used_images\"),\n",
    "                ColSpec(DataType.double, \"faithfulness\"),\n",
    "                ColSpec(DataType.double, \"relevance\"),\n",
    "            ])\n",
    "            signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "    \n",
    "            mlflow.pyfunc.log_model(\n",
    "                artifact_path=model_name,\n",
    "                python_model=cls(),\n",
    "                artifacts=artifacts,\n",
    "                pip_requirements=\"../requirements.txt\",\n",
    "                signature=signature,\n",
    "                code_paths=[\"../src\"],\n",
    "            )\n",
    "    \n",
    "        # The temporary directory and all its contents are automatically deleted here\n",
    "        logger.info(f\"âœ… Successfully logged '{model_name}' and cleaned up temporary files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfee18b-a495-42c0-aaf4-a37a4d5009eb",
   "metadata": {},
   "source": [
    "## Step 3: Start Run, Log & Register Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97c7b785-0628-4358-8e8d-9d49a0d6f1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-27 09:29:02 - INFO - Started MLflow run: 7ba5a0f2454a460e85ff2f294ed62d31\n",
      "2025-07-27 09:29:02 - INFO - --- Logging 'AIStudio-Multimodal-Chatbot-Model' to MLflow ---\n",
      "2025-07-27 09:29:02 - INFO - Created temporary directory for models: /tmp/tmp76qsgq6l\n",
      "2025-07-27 09:29:07 - INFO - âœ… Temporarily saved e5-large-v2 to /tmp/tmp76qsgq6l/e5-large-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-27 09:29:14 - INFO - âœ… Temporarily saved SigLIP to /tmp/tmp76qsgq6l/siglip2-base-patch16-224\n",
      "2025-07-27 09:29:15 - INFO - âœ… Temporarily saved Cross-Encoder to /tmp/tmp76qsgq6l/ms-marco-MiniLM-L-6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c0014cedf5f4c65849dda46f80e56c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3792c0b28f434e699d48e3dd4db801b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e50a144a174ea2afbf90fac33f604f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753ec9f9159f4eb897f1ec4d282c924b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c5d25e0bed46e196e8edd799da2156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0306084c2ae94f11976ff8d03f8fc5c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/741 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7987ef5d184b10a23d2dc01b3c29cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-27 09:33:25 - INFO - âœ… Successfully logged 'AIStudio-Multimodal-Chatbot-Model' and cleaned up temporary files.\n",
      "2025-07-27 09:33:25 - INFO - Registering model from URI: runs:/7ba5a0f2454a460e85ff2f294ed62d31/AIStudio-Multimodal-Chatbot-Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'AIStudio-Multimodal-Chatbot-Model' already exists. Creating a new version of this model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-27 09:33:25 - INFO - âœ… Successfully registered model 'AIStudio-Multimodal-Chatbot-Model'\n",
      "CPU times: user 5.2 s, sys: 56.2 s, total: 1min 1s\n",
      "Wall time: 4min 23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '12' of model 'AIStudio-Multimodal-Chatbot-Model'.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# --- Start MLflow Run and Log the Model ---\n",
    "try:\n",
    "    with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "        run_id = run.info.run_id\n",
    "        logger.info(f\"Started MLflow run: {run_id}\")\n",
    "\n",
    "        # Use the class method to log the model and its artifacts\n",
    "        MultimodalRagModel.log_model(model_name=MODEL_NAME, local_model_path=LOCAL_MODEL_PATH)\n",
    "\n",
    "\n",
    "        model_uri = f\"runs:/{run_id}/{MODEL_NAME}\"\n",
    "        logger.info(f\"Registering model from URI: {model_uri}\")\n",
    "        \n",
    "        # R.0egister the model in the MLflow Model Registry\n",
    "        mlflow.register_model(model_uri=model_uri, name=MODEL_NAME)\n",
    "        logger.info(f\"âœ… Successfully registered model '{MODEL_NAME}'\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    logger.error(f\"Error: A required file or directory was not found. Please ensure the project structure is correct.\")\n",
    "    logger.error(f\"Details: {e}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred during the MLflow run: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c628e7f-9d9d-4e7a-9790-34ed1b003caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-27 09:33:26 - INFO - Found latest version '12' for model 'AIStudio-Multimodal-Chatbot-Model' in stage 'None'.\n"
     ]
    }
   ],
   "source": [
    "# --- Retrieve the latest version from the Model Registry ---\n",
    "try:\n",
    "    client = MlflowClient()\n",
    "    versions = client.get_latest_versions(MODEL_NAME, stages=[\"None\"])\n",
    "    if not versions:\n",
    "        raise RuntimeError(f\"No registered versions found for model '{MODEL_NAME}'.\")\n",
    "    \n",
    "    latest_version = versions[0]\n",
    "    logger.info(f\"Found latest version '{latest_version.version}' for model '{MODEL_NAME}' in stage '{latest_version.current_stage}'.\")\n",
    "    model_uri_registry = latest_version.source\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to retrieve model from registry: {e}\", exc_info=True)\n",
    "    model_uri_registry = None # Ensure variable exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c31856b3-5ff5-48b9-98b7-ddbf7dd16be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-27 09:33:26 - INFO - Loading model from: /phoenix/mlflow/745750915506484464/7ba5a0f2454a460e85ff2f294ed62d31/artifacts/AIStudio-Multimodal-Chatbot-Model\n",
      "2025-07-27 09:33:26 - INFO - --- Initializing MultimodalRagModel context ---\n",
      "2025-07-27 09:33:26 - INFO - Running on device: cuda\n",
      "2025-07-27 09:33:26 - INFO - Resolved local model path to: /phoenix/mlflow/745750915506484464/7ba5a0f2454a460e85ff2f294ed62d31/artifacts/AIStudio-Multimodal-Chatbot-Model/artifacts/InternVL3-8B-Instruct\n",
      "2025-07-27 09:33:26 - INFO - Artifacts loaded: chroma_dir='/phoenix/mlflow/745750915506484464/7ba5a0f2454a460e85ff2f294ed62d31/artifacts/AIStudio-Multimodal-Chatbot-Model/artifacts/chroma_store', context_dir='/phoenix/mlflow/745750915506484464/7ba5a0f2454a460e85ff2f294ed62d31/artifacts/AIStudio-Multimodal-Chatbot-Model/artifacts/context', cache_dir='/phoenix/mlflow/745750915506484464/7ba5a0f2454a460e85ff2f294ed62d31/artifacts/AIStudio-Multimodal-Chatbot-Model/artifacts/semantic_cache'\n",
      "2025-07-27 09:33:26 - INFO - Loading embedding models and cross-encoder from local artifacts...\n",
      "2025-07-27 09:33:42 - INFO - âœ… Models loaded successfully from artifacts.\n",
      "2025-07-27 09:33:42 - INFO - Loading ChromaDB vector stores...\n",
      "2025-07-27 09:33:42 - INFO - Text DB count: 2614, Image DB count: 752\n",
      "2025-07-27 09:33:42 - INFO - Loading /phoenix/mlflow/745750915506484464/7ba5a0f2454a460e85ff2f294ed62d31/artifacts/AIStudio-Multimodal-Chatbot-Model/artifacts/InternVL3-8B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6221749e63da4a4faca26f50cd27389f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-27 09:34:54 - INFO - Model loaded on cuda.\n",
      "2025-07-27 09:34:54 - INFO - Initializing evaluation judge by sharing the main model...\n",
      "2025-07-27 09:34:54 - INFO - âœ… Evaluation judge initialized successfully.\n",
      "2025-07-27 09:34:54 - INFO - --- Context initialization complete ---\n",
      "2025-07-27 09:34:54 - INFO - âœ… Successfully loaded model from registry.\n"
     ]
    }
   ],
   "source": [
    "if model_uri_registry:\n",
    "    try:\n",
    "        logger.info(f\"Loading model from: {model_uri_registry}\")\n",
    "        loaded_model = mlflow.pyfunc.load_model(model_uri=model_uri_registry)\n",
    "        logger.info(\"âœ… Successfully loaded model from registry.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load model from registry URI: {e}\", exc_info=True)\n",
    "        loaded_model = None\n",
    "else:\n",
    "    logger.warning(\"Skipping model loading due to previous errors.\")\n",
    "    loaded_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2d9db6-b2dd-44e5-989d-ebb498372229",
   "metadata": {},
   "source": [
    "## Step 4: Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "279582c0-74e3-4b98-9d1d-6007c98f109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Function to Display Test Results ---\n",
    "def display_results(query: str, result_df: pd.DataFrame):\n",
    "    \"\"\"Helper to neatly print the query, reply, and display images.\"\"\"\n",
    "    if result_df.empty:\n",
    "        print(\"Received an empty result.\")\n",
    "        return\n",
    "\n",
    "    reply = result_df[\"reply\"].iloc[0]\n",
    "    # Images are stored as a string representation of a list, so we need to evaluate it\n",
    "    image_paths_str = result_df[\"used_images\"].iloc[0]\n",
    "    image_paths = eval(image_paths_str) if isinstance(image_paths_str, str) and image_paths_str.startswith('[') else []\n",
    "    \n",
    "    print(\"---\" * 20)\n",
    "    print(f\"â“ Query:\\n{query}\\n\")\n",
    "    print(f\"ðŸ¤– Reply:\\n{reply}\\n\")\n",
    "    \n",
    "    if image_paths:\n",
    "        print(f\"ðŸ–¼ï¸ Displaying {len(image_paths)} retrieved image(s):\")\n",
    "        # You can integrate the display_images function from the original notebook here\n",
    "        # For simplicity, we'll just print the paths.\n",
    "        for path in image_paths:\n",
    "            print(f\"  - {path}\")\n",
    "    else:\n",
    "        print(\"â–¶ No images were retrieved for this query.\")\n",
    "    print(\"---\" * 20 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fa1c9dd-974c-475d-876c-e4621b03a202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-27 09:34:54 - INFO - Running sample inference with the loaded model...\n",
      "2025-07-27 09:34:54 - INFO - Received prediction request.\n",
      "2025-07-27 09:34:54 - INFO - Processing query: 'What are the AI Blueprints Repository best practices?'\n",
      "2025-07-27 09:34:54 - INFO - Most similar cached query: 'How do i run blueprints locally?' (Similarity: 0.6612)\n",
      "2025-07-27 09:34:54 - INFO - CACHE MISS for query: 'What are the AI Blueprints Repository best practices?'. Running full pipeline.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-27 09:35:02 - INFO - Added query to semantic cache: 'What are the AI Blueprints Repository best practices?'\n",
      "DEBUG: Judge model raw responses: ['0.9']\n",
      "DEBUG: Judge model raw responses: ['0.7']\n",
      "faithfulness: 0.9 relevance:  0.7\n",
      "------------------------------------------------------------\n",
      "â“ Query:\n",
      "What are the AI Blueprints Repository best practices?\n",
      "\n",
      "ðŸ¤– Reply:\n",
      "## **Synthesized Answer**\n",
      "The AI Blueprints Repository best practices include:\n",
      "\n",
      "1. Using `logging` module for enhanced maintainability instead of `print()`.\n",
      "2 Use the Jupyter notebook if working with larger inputs.\n",
      "* Define constants at the top to avoid hardcoded values.\n",
      "* Utilize `pathlib` library for OS compatibility in file paths.\n",
      "* Implement two main notebooks (`run-workflow.ipynb` and `register-model.ipynb`) per project.\n",
      "\n",
      "## **Source Documents**\n",
      "\n",
      "[`Data-Science-Team/Best-Practices-for-HP-AI-Studio-Blueprints-Repository.md`]\n",
      "\n",
      "â–¶ No images were retrieved for this query.\n",
      "------------------------------------------------------------\n",
      "\n",
      "2025-07-27 09:35:03 - INFO - Received prediction request.\n",
      "2025-07-27 09:35:03 - INFO - Processing query: 'What are some feature flags that i can enable in AIStudio?'\n",
      "2025-07-27 09:35:03 - INFO - Most similar cached query: 'What are some feature flags in AIStudio?' (Similarity: 0.9370)\n",
      "2025-07-27 09:35:03 - INFO - SEMANTIC CACHE HIT for query: 'What are some feature flags that i can enable in AIStudio?'\n",
      "DEBUG: Judge model raw responses: ['0.85']\n",
      "DEBUG: Judge model raw responses: ['0.6']\n",
      "faithfulness: 0.85 relevance:  0.6\n",
      "------------------------------------------------------------\n",
      "â“ Query:\n",
      "What are some feature flags that i can enable in AIStudio?\n",
      "\n",
      "ðŸ¤– Reply:\n",
      "## **1. Visual Analysis**\n",
      "There is no image provided in the context.\n",
      "\n",
      "## **2. Synthesized Answer**\n",
      "The feature flags mentioned in AIStudio include:\n",
      "- `azure`\n",
      "- `gcp`\n",
      "- `ngc`\n",
      "\n",
      "These are used to enable or disable features such as Azure and GCP datasets, and integration with NVIDIA. They can be managed using the command line tool hooh by adding them commands like `$ hooh flags add azure`, `$ hooh flags add gcp`, and `$ hooh flags add ngc`.\n",
      "\n",
      "## **3. Source Documents**\n",
      "[`Feature-Flags.md`](#)\n",
      "\n",
      "â–¶ No images were retrieved for this query.\n",
      "------------------------------------------------------------\n",
      "\n",
      "2025-07-27 09:35:04 - INFO - Received prediction request.\n",
      "2025-07-27 09:35:04 - INFO - Processing query: 'How do i manually clean my environment without hooh?'\n",
      "2025-07-27 09:35:04 - INFO - Most similar cached query: 'How do i manually clean my environment without hooh?' (Similarity: 1.0000)\n",
      "2025-07-27 09:35:04 - INFO - SEMANTIC CACHE HIT for query: 'How do i manually clean my environment without hooh?'\n",
      "DEBUG: Judge model raw responses: ['0.85']\n",
      "DEBUG: Judge model raw responses: ['0.0']\n",
      "faithfulness: 0.85 relevance:  0.0\n",
      "------------------------------------------------------------\n",
      "â“ Query:\n",
      "How do i manually clean my environment without hooh?\n",
      "\n",
      "ðŸ¤– Reply:\n",
      "## **1. Visual Analysis**\n",
      "The image shows the Windows Credential Manager interface, displaying various credentials stored on a computer. The user is instructed to delete specific directories and files in the AIStudio directory as part of manually cleaning their environment without using Hooh.\n",
      "\n",
      "## **2 Synthesized Answer**\n",
      "To manually clean your environment without using Hooh, follow these steps:\n",
      "\n",
      "1. Open File Explorer and navigate to `%localappdata%`.\n",
      "2 2\n",
      "2Access the AIStudio directory by typing `%localappdata%` in your explorer.\n",
      "Next, go to the **HP** directory and then **AiStudio**.\n",
      "In the AIStudio directory, delete the directories that have either an **Account ID**, the **db** and **creds** directories, as circled below.\n",
      "Also delete the `userconfig` file, as it stores environment-specific variables.\n",
      "![image.png](/.attachments/image-fe32abea-eb8c-44a9-a052-061bbc2cd9f9.png)\n",
      "In the start menu, search for **Windows Credentials** and open the first result.\n",
      "Once open, select Windows Credentials and remove both **hp-aistudio-app** and **hp-aistudio-mongodb** as shown below.\n",
      "![image.png](/.attachments/image-cadda90f-682d-4655-a6da-99cef82f80cb.png)\n",
      "\n",
      "## **3. Source Documents**\n",
      "[How-to-manual-clean--your-environment.md]\n",
      "\n",
      "â–¶ No images were retrieved for this query.\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if loaded_model:\n",
    "    logger.info(\"Running sample inference with the loaded model...\")\n",
    "    \n",
    "    sample_queries = [\n",
    "        \"What are the AI Blueprints Repository best practices?\",\n",
    "        \"What are some feature flags that i can enable in AIStudio?\",\n",
    "        \"How do i manually clean my environment without hooh?\",\n",
    "    ]\n",
    "\n",
    "    for query in sample_queries:\n",
    "        try:\n",
    "            # --- MODIFIED LINE ---\n",
    "            # Add 'force_regenerate': False to the dictionary when creating the DataFrame\n",
    "            input_payload = pd.DataFrame([{\"query\": query, \"force_regenerate\": False}])\n",
    "            \n",
    "            result = loaded_model.predict(input_payload)\n",
    "\n",
    "            print(\"faithfulness:\", result[\"faithfulness\"][0], \"relevance: \", result[\"relevance\"][0])\n",
    "            display_results(query, result)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Prediction failed for query '{query}': {e}\", exc_info=True)\n",
    "\n",
    "else:\n",
    "    logger.warning(\"Skipping sample inference because the model was not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e89eee-6171-446a-8da6-9381d3fe10c0",
   "metadata": {},
   "source": [
    "Built with â¤ï¸ using Z by HP AI Studio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83806df-79a8-4273-ab62-96c4bf1c2ded",
   "metadata": {},
   "source": [
    "# Step 5: Evaluate Hallucinations & Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65476b0a-e314-44a6-8e51-856e237b00b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-27 09:35:05 - INFO - --- Reopening original run (7ba5a0f2454a460e85ff2f294ed62d31) to log pre-computed evaluations ---\n",
      "2025-07-27 09:35:05 - INFO - Received prediction request.\n",
      "2025-07-27 09:35:05 - INFO - Processing query: 'What are the AI Blueprints Repository best practices?'\n",
      "2025-07-27 09:35:05 - INFO - Forced regeneration for query: 'What are the AI Blueprints Repository best practices?'. Clearing old cache entry.\n",
      "2025-07-27 09:35:05 - INFO - CACHE MISS for query: 'What are the AI Blueprints Repository best practices?'. Running full pipeline.\n",
      "2025-07-27 09:35:18 - INFO - Added query to semantic cache: 'What are the AI Blueprints Repository best practices?'\n",
      "DEBUG: Judge model raw responses: ['0.9']\n",
      "DEBUG: Judge model raw responses: ['0.8']\n",
      "2025-07-27 09:35:19 - INFO - Processing query: 'What are some feature flags that i can enable in AIStudio?'\n",
      "2025-07-27 09:35:19 - INFO - Forced regeneration for query: 'What are some feature flags that i can enable in AIStudio?'. Clearing old cache entry.\n",
      "2025-07-27 09:35:19 - INFO - CACHE MISS for query: 'What are some feature flags that i can enable in AIStudio?'. Running full pipeline.\n",
      "2025-07-27 09:35:23 - INFO - Added query to semantic cache: 'What are some feature flags that i can enable in AIStudio?'\n",
      "DEBUG: Judge model raw responses: ['0.85']\n",
      "DEBUG: Judge model raw responses: ['0.5']\n",
      "2025-07-27 09:35:24 - INFO - Processing query: 'How do i manually clean my environment without hooh?'\n",
      "2025-07-27 09:35:24 - INFO - Forced regeneration for query: 'How do i manually clean my environment without hooh?'. Clearing old cache entry.\n",
      "2025-07-27 09:35:24 - INFO - CACHE MISS for query: 'How do i manually clean my environment without hooh?'. Running full pipeline.\n",
      "2025-07-27 09:35:38 - INFO - Added query to semantic cache: 'How do i manually clean my environment without hooh?'\n",
      "DEBUG: Judge model raw responses: ['0.85']\n",
      "DEBUG: Judge model raw responses: ['0.0']\n",
      "2025-07-27 09:35:40 - INFO - Successfully reopened existing run. Logging metrics and artifacts...\n",
      "2025-07-27 09:35:40 - INFO - âœ… Successfully logged metrics and artifacts to the original model run.\n"
     ]
    }
   ],
   "source": [
    "# In register_model.ipynb, add this as the final cell\n",
    "\n",
    "# Check if the model was loaded and the original run_id is available\n",
    "if loaded_model and 'run_id' in locals():\n",
    "    logger.info(f\"--- Reopening original run ({run_id}) to log pre-computed evaluations ---\")\n",
    "\n",
    "    # 1. Define your evaluation dataset\n",
    "    evaluation_payload = pd.DataFrame([\n",
    "        {\"query\": \"What are the AI Blueprints Repository best practices?\", \"force_regenerate\": True},\n",
    "        {\"query\": \"What are some feature flags that i can enable in AIStudio?\", \"force_regenerate\": True},\n",
    "        {\"query\": \"How do i manually clean my environment without hooh?\", \"force_regenerate\": True},\n",
    "    ])\n",
    "\n",
    "    # 2. Run predict() to get results with the embedded scores\n",
    "    results_df = loaded_model.predict(evaluation_payload)\n",
    "    \n",
    "    # Add the original query to the results for clarity in the logged table\n",
    "    results_df['query'] = evaluation_payload['query']\n",
    "\n",
    "    # 3. Reopen the existing run using its ID\n",
    "    with mlflow.start_run(run_id=run_id) as run:\n",
    "        logger.info(\"Successfully reopened existing run. Logging metrics and artifacts...\")\n",
    "\n",
    "        # 4. Calculate average scores from the DataFrame\n",
    "        avg_faithfulness = results_df[\"faithfulness\"].mean()\n",
    "        avg_relevance = results_df[\"relevance\"].mean()\n",
    "\n",
    "        # 5. Log the average scores as metrics to the original run\n",
    "        mlflow.log_metrics({\n",
    "            \"avg_faithfulness\": avg_faithfulness,\n",
    "            \"avg_relevance\": avg_relevance\n",
    "        })\n",
    "\n",
    "        # 6. Log the full results DataFrame as a table artifact to the original run\n",
    "        mlflow.log_table(data=results_df, artifact_file=\"inline_evaluation_results.json\")\n",
    "        \n",
    "        logger.info(\"âœ… Successfully logged metrics and artifacts to the original model run.\")\n",
    "\n",
    "else:\n",
    "    logger.warning(\"Skipping logging because the model was not loaded or run_id was not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "725e1d0b-333c-43fd-95a7-5ce13fdfe3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-27 09:35:05 - INFO - âœ… Notebook execution completed.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"âœ… Notebook execution completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
