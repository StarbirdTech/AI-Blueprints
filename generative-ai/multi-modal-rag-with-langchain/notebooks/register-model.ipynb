{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff193981-f701-4595-9d5f-a08779c65508",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-size: 50px;\"> 🤖 MLFlow Registration for Multimodal RAG with Local Chroma Cache</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e7b230-e638-458a-8104-0a30727b2466",
   "metadata": {},
   "source": [
    "# MLFlow Model Service \n",
    "\n",
    "In this section, we demonstrate how to deploy a RAG-based chatbot service. This service provides a REST API endpoint that allows users to query the knowledge base with natural language questions, upload new documents to the knowledge base, and manage conversation history, all with built-in safeguards against sensitive information and toxicity. This service encapsulates all the functionality we developed in this notebook, including the document retrieval system, RAG-based question answering capabilities, and MLFlow integration for observation and evaluation. It demonstrates how to use our ChatbotService from the src/service directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032ab59a-a358-4b11-9ee4-ac38101666bf",
   "metadata": {},
   "source": [
    "## Step 0: Imports and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5a1883-f877-414a-9ff9-b876a2ba6531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os \n",
    "from pathlib import Path\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "# Create logger\n",
    "logger = logging.getLogger(\"multimodal_rag_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.handlers:\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "    stream_handler = logging.StreamHandler(sys.stdout)\n",
    "    stream_handler.setFormatter(formatter)\n",
    "    logger.addHandler(stream_handler)\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38660c87-34b3-40f2-be21-16c78fb12598",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()  \n",
    "\n",
    "logger.info(\"Notebook execution started.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2607392-6bd0-4950-8ac8-84026da9df0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622040db-4d03-482f-958f-45cc87492ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Standard Library Imports ===\n",
    "import gc\n",
    "import json\n",
    "import math\n",
    "import base64\n",
    "import tempfile\n",
    "import shutil\n",
    "import warnings\n",
    "from rank_bm25 import BM25Okapi\n",
    "from typing import Any, Dict, List, Optional, TypedDict\n",
    "from statistics import mean\n",
    "from collections import defaultdict\n",
    "import hashlib\n",
    "from IPython.display import display, Image, Markdown\n",
    "\n",
    "# === Third-Party Library Imports ===\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain.schema.document import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.types import ColSpec, DataType, Schema, TensorSpec\n",
    "from PIL import Image as PILImage\n",
    "from sentence_transformers import CrossEncoder, SentenceTransformer\n",
    "from transformers import pipeline, AutoImageProcessor, AutoModel, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, SiglipModel, SiglipProcessor\n",
    "\n",
    "# === Project-Specific Imports ===\n",
    "# Add the project root to the system path to allow importing from 'src'\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from src.components import SemanticCache, SiglipEmbeddings\n",
    "from src.wiki_pages_clone import orchestrate_wiki_clone\n",
    "from src.local_genai_judge import LocalGenAIJudge\n",
    "from src.utils import (\n",
    "    configure_hf_cache,\n",
    "    multimodal_rag_asset_status,\n",
    "    load_config,\n",
    "    load_secrets,\n",
    "    load_mm_docs_clean,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0def9a7-d827-4158-be16-302c79bfeaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f036c1-4922-4389-94d0-406f7e769612",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cb6032-d809-4bf9-b9b8-87ef1f3c47ed",
   "metadata": {},
   "source": [
    "## Step 1: Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60df3050-7f7a-4735-af13-67cbadcd0b78",
   "metadata": {},
   "source": [
    "### Verify Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869f893e-bddd-4f15-8c76-298091f9daee",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"../configs/config.yaml\"\n",
    "SECRETS_PATH = \"../configs/secrets.yaml\"\n",
    "\n",
    "LOCAL_MODEL = \"/home/jovyan/datafabric/InternVL3-8B-Instruct\"\n",
    "CONTEXT_DIR: Path = Path(\"../data/context\")\n",
    "CHROMA_DIR: Path = Path(\"../data/chroma_store\")     \n",
    "CACHE_DIR: Path = CHROMA_DIR / \"semantic_cache\"\n",
    "MANIFEST_PATH: Path = CHROMA_DIR / \"manifest.json\"\n",
    "\n",
    "IMAGE_DIR = CONTEXT_DIR / \"images\"\n",
    "WIKI_METADATA_DIR = CONTEXT_DIR / \"wiki_flat_structure.json\"\n",
    "\n",
    "CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "multimodal_rag_asset_status(\n",
    "    local_model_path=LOCAL_MODEL,\n",
    "    config_path=CONFIG_PATH,\n",
    "    secrets_path=SECRETS_PATH,\n",
    "    wiki_metadata_dir=WIKI_METADATA_DIR,\n",
    "    context_dir=CONTEXT_DIR,\n",
    "    chroma_dir=CHROMA_DIR,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    manifest_path=MANIFEST_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f98b97-0338-4107-91ea-797ff3a26072",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bfe368-e9c5-4856-8aad-a93026bacd7a",
   "metadata": {},
   "source": [
    "### Config HuggingFace Caches\n",
    "\n",
    "In the next cell, we configure HuggingFace cache, so that all the models downloaded from them are persisted locally, even after the workspace is closed. This is a future desired feature for AI Studio and the GenAI addon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4576dc32-7e04-48aa-b19d-da1adc92693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "configure_hf_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8125515-1c50-4297-8986-63ba9944a958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HuggingFace Embeddings\n",
    "txt_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/e5-large-v2\",\n",
    "    cache_folder=\"/tmp/hf_cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cebbd13-6e1f-46e9-a710-9621b3010bc8",
   "metadata": {},
   "source": [
    "### MLflow Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d6964a-0768-4fa0-9da4-e8ecbc5487db",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"AIStudio-Multimodal-Chatbot-Model\"\n",
    "RUN_NAME = f\"Register_{MODEL_NAME}\"\n",
    "EXPERIMENT_NAME = \"AIStudio-Multimodal-Chatbot-Experiment\"\n",
    "\n",
    "# Set MLflow tracking URI and experiment\n",
    "# This should be configured for your environment, e.g., a remote server or local file path\n",
    "mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\", \"/phoenix/mlflow\"))\n",
    "mlflow.set_experiment(experiment_name=EXPERIMENT_NAME)\n",
    "\n",
    "logger.info(f\"Using MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "logger.info(f\"Using MLflow experiment: '{EXPERIMENT_NAME}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1cb8d4-d58a-48bc-96e9-9bd63d43f80f",
   "metadata": {},
   "source": [
    "## Step 2: MLflow Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec72ef2-03e5-40e4-aa2e-580222f79c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultimodalRagModel(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"\n",
    "    An MLflow PythonModel for a dynamic, updatable Multimodal RAG pipeline.\n",
    "\n",
    "    This model acts as a service with multiple commands:\n",
    "    - 'update_kb': Fetches the latest data from a source (e.g., ADO Wiki),\n",
    "      incrementally updates the vector knowledge base, and clears the cache.\n",
    "    - 'query': Answers a user's question using the RAG pipeline, with\n",
    "      built-in semantic caching and self-evaluation.\n",
    "\n",
    "    The knowledge base (ChromaDB) and semantic cache are managed dynamically\n",
    "    on the server's persistent storage, not as static model artifacts.\n",
    "    \"\"\"\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 1. Inner Class for the RAG Generation Pipeline\n",
    "    # This is your original logic, kept intact. It's the \"engine\" of our service.\n",
    "    # ==========================================================================\n",
    "    class InternVLMM:\n",
    "        \"\"\"Minimal, self-contained multimodal QA wrapper around InternVL3-8B-Instruct.\"\"\"\n",
    "        def __init__(self, model_path: str, device: str, cache: Any, text_db: Chroma, image_db: Chroma, bm25_index: BM25Okapi, doc_map: dict):\n",
    "            self.model_path = str(model_path)\n",
    "            self.device = device\n",
    "            self.model = None\n",
    "            self.tok = None\n",
    "            self.image_processor = None\n",
    "            self.cache = cache\n",
    "            self.text_db = text_db\n",
    "            self.image_db = image_db\n",
    "            self.bm25_index = bm25_index\n",
    "            self.doc_map = doc_map\n",
    "            self._load()\n",
    "\n",
    "        @staticmethod\n",
    "        def _reciprocal_rank_fusion(results: list[list[Document]], k: int = 60) -> list[tuple[Document, float]]:\n",
    "            \"\"\"Performs Reciprocal Rank Fusion on multiple ranked lists of documents.\"\"\"\n",
    "            ranked_lists = [\n",
    "                {doc.page_content: (doc, i + 1) for i, doc in enumerate(res)}\n",
    "                for res in results\n",
    "            ]\n",
    "            rrf_scores = defaultdict(float)\n",
    "            all_docs = {}\n",
    "            # Calculate RRF scores for each content across all ranked lists\n",
    "            for ranked_list in ranked_lists:\n",
    "                for content, (doc, rank) in ranked_list.items():\n",
    "                    rrf_scores[content] += 1 / (k + rank)\n",
    "                    if content not in all_docs:\n",
    "                        all_docs[content] = doc\n",
    "            fused_results = [\n",
    "                (all_docs[content], rrf_scores[content])\n",
    "                for content in sorted(rrf_scores, key=rrf_scores.get, reverse=True)\n",
    "            ]\n",
    "            return fused_results\n",
    "\n",
    "        def _retrieve_mm(self, query: str, k_text: int = 3, k_img: int = 4, recall_k: int = 20) -> dict[str, any]:\n",
    "            \"\"\"Retrieves relevant documents and images based on the query using both dense and sparse retrieval methods.\"\"\"\n",
    "            dense_hits = self.text_db.similarity_search(query, k=recall_k)\n",
    "            tokenized_query = query.lower().split(\" \")\n",
    "            sparse_texts = self.bm25_index.get_top_n(tokenized_query, list(self.doc_map.keys()), n=recall_k)\n",
    "            sparse_hits = [self.doc_map[text] for text in sparse_texts]\n",
    "\n",
    "            if not dense_hits and not sparse_hits:\n",
    "                return {\"docs\": [], \"scores\": [], \"images\": []}\n",
    "            \n",
    "            # Perform Reciprocal Rank Fusion on the hits\n",
    "            fused_results = self._reciprocal_rank_fusion([dense_hits, sparse_hits])\n",
    "            final_docs = [doc for doc, score in fused_results[:k_text]]\n",
    "            final_scores = [score for doc, score in fused_results[:k_text]]\n",
    "\n",
    "            # Retrieve images based on the sources of the final documents\n",
    "            retrieved_images = []\n",
    "            if final_docs:\n",
    "                final_sources = list(set(d.metadata[\"source\"] for d in final_docs))\n",
    "                image_hits = self.image_db.similarity_search(query, k=k_img, filter={\"source\": {\"$in\": final_sources}})\n",
    "                retrieved_images = [img.page_content for img in image_hits]\n",
    "\n",
    "            return {\"docs\": final_docs, \"scores\": final_scores, \"images\": retrieved_images}\n",
    "\n",
    "        def generate(self, query: str, force_regenerate: bool = False, **retrieval_kwargs) -> Dict[str, Any]:\n",
    "            \"\"\"Generates a response to the user's query using the multimodal RAG pipeline.\"\"\"\n",
    "            start_gen_time = time.time()\n",
    "            \n",
    "            # Check if we can use the cache\n",
    "            if not force_regenerate and self.cache:\n",
    "                cached_result = self.cache.get(query, threshold=0.92)\n",
    "                if cached_result:\n",
    "                    logger.info(f\"SEMANTIC CACHE HIT for query: '{query}'\")\n",
    "                    if \"generation_time_seconds\" not in cached_result:\n",
    "                        cached_result[\"generation_time_seconds\"] = 0.0\n",
    "                    return cached_result\n",
    "                \n",
    "            # If cache miss or forced regeneration, clear old cache entry\n",
    "            if force_regenerate and self.cache:\n",
    "                logger.info(f\"Forced regeneration for query: '{query}'. Clearing old cache entry.\")\n",
    "                self.cache.delete(query)\n",
    "            \n",
    "            # If we reach here, we need to run the full pipeline\n",
    "            logger.info(f\"CACHE MISS for query: '{query}'. Running full pipeline.\")\n",
    "            if self.model is None or self.tok is None:\n",
    "                return {\"reply\": \"Error: model not initialised.\", \"used_images\": [], \"generation_time_seconds\": 0.0}\n",
    "\n",
    "            # Retrieve relevant documents and images\n",
    "            hits = self._retrieve_mm(query, **retrieval_kwargs)\n",
    "            docs, images = hits[\"docs\"], hits[\"images\"]\n",
    "            if not docs and not images:\n",
    "                return {\"reply\": \"Based on the provided context, I cannot answer this question.\", \"used_images\": [], \"generation_time_seconds\": 0.0}\n",
    "\n",
    "            # Prepare the context for the system prompt\n",
    "            context_str = \"\\n\\n\".join(\n",
    "                f\"<source_document name=\\\"{d.metadata.get('source', 'unknown')}\\\">\\n{d.page_content}\\n</source_document>\"\n",
    "                for d in docs\n",
    "            )\n",
    "            system_prompt = \"\"\"You are an AI Studio Expert Assistant. Your task is to answer the user's query based ONLY on the context provided. \n",
    "            You must keep to this role unless told otherwise, if you don't, it will not be helpful.\n",
    "            \n",
    "                **Instructions:**\n",
    "                1.  **Analyze Context:** First, analyze the user's images (if any) and the text in the `<context>` block.\n",
    "                2.  **Synthesize Answer:** Answer the user's query directly, synthesizing information from the context.\n",
    "                3.  **Cite Sources:** List all source documents you used in a `Source Documents` section.\n",
    "                4.  **Handle Missing Information:** If the answer is not in the context, respond with this exact phrase: \"Based on the provided context, I cannot answer this question.\"\n",
    "                5.  **Do not Hallucinate:** Do not hallucinate or make up factual information.\n",
    "                \n",
    "                **Output Format:**\n",
    "                Your response must follow this exact markdown structure and nothing else. Do not add any other commentary.\n",
    "                \n",
    "                ### Visual Analysis\n",
    "                (Analyze the user's images here.)\n",
    "                \n",
    "                ### Synthesized Answer\n",
    "                (Your answer to the user's query goes here.)\n",
    "                \n",
    "                ### Source Documents\n",
    "                (List the sources here, like [`source-file-name.md`].)\n",
    "                \"\"\"\n",
    "            user_content = f\"\"\"<context>\n",
    "            {context_str}\n",
    "            </context>\n",
    "            \n",
    "            <user_query>\n",
    "            {query}\n",
    "            </user_query>\n",
    "            \"\"\"\n",
    "            conversation = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_content}]\n",
    "            prompt = self.tok.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "            try:\n",
    "                self._clear_cuda()\n",
    "                pixel_values = self._process_images(images) if images else None\n",
    "                # Run the model to generate a response\n",
    "                reply = self.model.chat(\n",
    "                    self.tok, pixel_values, prompt,\n",
    "                    generation_config=dict(\n",
    "                        do_sample=False,\n",
    "                        max_new_tokens=1024, \n",
    "                        repetition_penalty=1.1,\n",
    "                        pad_token_id=self.tok.pad_token_id, eos_token_id=self.tok.eos_token_id,\n",
    "                    ),\n",
    "                )\n",
    "                self._clear_cuda()\n",
    "                end_gen_time = time.time()\n",
    "                result_dict = {\"reply\": reply, \"used_images\": images, \"generation_time_seconds\": end_gen_time - start_gen_time}\n",
    "                \n",
    "                if self.cache:\n",
    "                    self.cache.set(query, result_dict)\n",
    "                \n",
    "                return result_dict\n",
    "            except RuntimeError as e:\n",
    "                logger.error(\"InternVL generation failed: %s\", e)\n",
    "                return {\"reply\": f\"Error during generation: {e}\", \"used_images\": images, \"generation_time_seconds\": 0.0}\n",
    "\n",
    "        def _load(self):\n",
    "            \"\"\"Loads the model and tokenizer from the specified path.\"\"\"\n",
    "            logger.info(\"Loading %s...\", self.model_path)\n",
    "            self._clear_cuda()\n",
    "            \n",
    "            # Load the tokenizer and image processor\n",
    "            self.tok = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)\n",
    "            if self.tok.pad_token is None: self.tok.pad_token = self.tok.eos_token\n",
    "            self.image_processor = AutoImageProcessor.from_pretrained(self.model_path, trust_remote_code=True, use_fast=True)\n",
    "            \n",
    "            q_cfg = BitsAndBytesConfig(\n",
    "                load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True,\n",
    "            ) if self.device == \"cuda\" else None\n",
    "            \n",
    "            # Load the model with quantization if on GPU, otherwise load normally\n",
    "            self.model = AutoModel.from_pretrained(\n",
    "                self.model_path, quantization_config=q_cfg,\n",
    "                torch_dtype=(torch.bfloat16 if self.device == \"cuda\" else torch.float32),\n",
    "                low_cpu_mem_usage=True, use_flash_attn=False, trust_remote_code=True,\n",
    "                device_map=\"auto\" if self.device == \"cuda\" else None,\n",
    "            ).eval()\n",
    "            logger.info(\"Model loaded on %s.\", self.device)\n",
    "\n",
    "        def _process_images(self, image_paths: List[str]):\n",
    "            \"\"\"Processes a list of image paths into pixel values for the model.\"\"\"\n",
    "            if not image_paths: return None\n",
    "            try:\n",
    "                pil_images = [PILImage.open(p).convert(\"RGB\") for p in image_paths]\n",
    "                processed_data = self.image_processor(images=pil_images, return_tensors=\"pt\")\n",
    "                pixel_values = processed_data['pixel_values'].to(device=self.device, dtype=next(self.model.parameters()).dtype)\n",
    "                return pixel_values\n",
    "            except Exception as e:\n",
    "                logger.error(\"Image processing failed: %s\", e)\n",
    "                return None\n",
    "\n",
    "        def _clear_cuda(self):\n",
    "            \"\"\"Clears CUDA memory to prevent OOM errors during generation.\"\"\"\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 2. MLflow `pyfunc` Life-cycle and Service Methods\n",
    "    # ==========================================================================\n",
    "    def load_context(self, context: mlflow.pyfunc.PythonModelContext) -> None:\n",
    "        \"\"\"Initializes the service context, loading models and setting up storage paths.\"\"\"\n",
    "        logger.info(\"--- Initializing Dynamic MultimodalRAG Service ---\")\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        self.persistent_storage_path = Path(\"/tmp/multimodal_rag_service_data\")\n",
    "        \n",
    "        self.persistent_storage_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.chroma_store_path = self.persistent_storage_path / \"chroma_store\"\n",
    "        self.cache_dir = self.persistent_storage_path / \"semantic_cache\"\n",
    "        self.manifest_path = self.persistent_storage_path / \"manifest.json\"\n",
    "        \n",
    "        logger.info(f\"Service data will be managed at: {self.persistent_storage_path}\")\n",
    "\n",
    "        self.model_path = Path(context.artifacts[\"local_model_dir\"]).resolve()\n",
    "        e5_model_path = context.artifacts[\"e5_model_dir\"]\n",
    "        siglip_model_path = context.artifacts[\"siglip_model_dir\"]\n",
    "        \n",
    "        self.text_embed_model = HuggingFaceEmbeddings(model_name=e5_model_path, model_kwargs={\"device\": self.device})\n",
    "        self.siglip_embed_model = SiglipEmbeddings(model_id=siglip_model_path, device=self.device)\n",
    "\n",
    "        self.text_db = None\n",
    "        self.image_db = None\n",
    "        self.bm25_index = None\n",
    "        self.doc_map = None\n",
    "        self.cache = None\n",
    "        self.mm_llm = None\n",
    "        self.judge = None\n",
    "\n",
    "        logger.info(\"--- Service initialized. Ready to receive commands. ---\")\n",
    "\n",
    "    def predict(self, context: mlflow.pyfunc.PythonModelContext, model_input: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Handles incoming requests to the Multimodal RAG service.\"\"\"\n",
    "        command = model_input[\"command\"].iloc[0]\n",
    "        logger.info(f\"Received command: '{command}'\")\n",
    "        \n",
    "        # Initialize the RAG pipeline if it hasn't been loaded yet\n",
    "        if command == \"update_kb\":\n",
    "            payload = json.loads(model_input[\"payload\"].iloc[0])\n",
    "            result = self.update_knowledge_base(config=payload[\"config\"], secrets=payload[\"secrets\"])\n",
    "            return pd.DataFrame([result])\n",
    "    \n",
    "        # Handle the query command\n",
    "        elif command == \"query\":\n",
    "            if self.mm_llm is None:\n",
    "                logger.info(\"First query received. Initializing RAG pipeline from persistent storage...\")\n",
    "                self._initialize_rag_pipeline()\n",
    "            \n",
    "            query = model_input[\"query\"].iloc[0]\n",
    "            force_regenerate = model_input.get(\"force_regenerate\", pd.Series([False])).iloc[0]\n",
    "            \n",
    "            response_dict = self.mm_llm.generate(query, force_regenerate=force_regenerate)\n",
    "            \n",
    "            # --- Convert image paths to Base64 strings ---\n",
    "            image_paths = response_dict.get(\"used_images\", [])\n",
    "            base64_images = []\n",
    "            if image_paths:\n",
    "                for path in image_paths:\n",
    "                    try:\n",
    "                        with open(path, \"rb\") as img_file:\n",
    "                            encoded_string = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "                            base64_images.append(encoded_string)\n",
    "                    except FileNotFoundError:\n",
    "                        logger.warning(f\"Image file not found at path during encoding: {path}\")\n",
    "            \n",
    "            # Replace the path list with a JSON string of Base64 images\n",
    "            response_dict[\"used_images\"] = json.dumps(base64_images)\n",
    "    \n",
    "            if self.judge:\n",
    "                retrieved_info = self.mm_llm._retrieve_mm(query)\n",
    "                context_str = \"\\n\\n\".join(d.page_content for d in retrieved_info[\"docs\"])\n",
    "                eval_df = pd.DataFrame([{\"questions\": query, \"result\": response_dict[\"reply\"], \"source_documents\": context_str}])\n",
    "                response_dict[\"faithfulness\"] = self.judge.evaluate_faithfulness(eval_df).iloc[0]\n",
    "                response_dict[\"relevance\"] = self.judge.evaluate_relevance(eval_df).iloc[0]\n",
    "            else:\n",
    "                response_dict[\"faithfulness\"], response_dict[\"relevance\"] = None, None\n",
    "            \n",
    "            return pd.DataFrame([response_dict])\n",
    "    \n",
    "        else:\n",
    "            return pd.DataFrame([{\"status\": \"error\", \"message\": f\"Unknown command: {command}\"}])\n",
    "\n",
    "    def update_knowledge_base(self, config: dict, secrets: dict) -> dict:\n",
    "        logger.info(\"Starting knowledge base update...\")\n",
    "        \n",
    "        # --- FIX: Use a permanent directory for processed data, not a temporary one ---\n",
    "        processed_data_dir = self.persistent_storage_path / \"processed_data\"\n",
    "        processed_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "        try:\n",
    "            # Step 1: Fetch latest data directly into the permanent processed data directory\n",
    "            orchestrate_wiki_clone(pat=secrets['AIS_ADO_TOKEN'], config=config, output_dir=processed_data_dir)\n",
    "    \n",
    "            # Step 2: Check for changes using the data in the permanent directory\n",
    "            current_manifest = self._create_json_manifest(processed_data_dir)\n",
    "            old_manifest = json.loads(self.manifest_path.read_text()) if self.manifest_path.exists() else {}\n",
    "    \n",
    "            if not current_manifest:\n",
    "                raise FileNotFoundError(\"Cloning step did not produce a 'wiki_flat_structure.json' manifest.\")\n",
    "            \n",
    "            needs_rebuild = not old_manifest or list(current_manifest.values())[0] != list(old_manifest.values())[0]\n",
    "    \n",
    "            if not needs_rebuild:\n",
    "                logger.info(\"Knowledge base is already up-to-date.\")\n",
    "                # Still initialize the pipeline if it hasn't been loaded yet\n",
    "                if not self.mm_llm:\n",
    "                    self._initialize_rag_pipeline()\n",
    "                return {\"status\": \"success\", \"message\": \"Knowledge base is already up-to-date.\"}\n",
    "    \n",
    "            # Step 3: Perform a full wipe and rebuild\n",
    "            logger.info(\"Knowledge base has changed. Performing a full re-index.\")\n",
    "            if self.chroma_store_path.exists():\n",
    "                shutil.rmtree(self.chroma_store_path)\n",
    "            self.chroma_store_path.mkdir()\n",
    "    \n",
    "            image_dir = processed_data_dir / \"images\"\n",
    "            wiki_metadata_path = processed_data_dir / \"wiki_flat_structure.json\"\n",
    "            \n",
    "            all_raw_docs = load_mm_docs_clean(wiki_metadata_path, image_dir)\n",
    "            all_chunks = self._chunk_docs(all_raw_docs)\n",
    "    \n",
    "            # Index Text Chunks\n",
    "            if all_chunks:\n",
    "                logger.info(f\"Indexing {len(all_chunks)} new text chunks...\")\n",
    "                doc_ids = [f\"{doc.metadata['source']}::{doc.metadata['chunk_id']}\" for doc in all_chunks]\n",
    "                Chroma.from_documents(\n",
    "                    documents=all_chunks, ids=doc_ids,\n",
    "                    embedding=self.text_embed_model,\n",
    "                    persist_directory=str(self.chroma_store_path), collection_name=\"mm_text\"\n",
    "                )\n",
    "            \n",
    "            # Index Images\n",
    "            img_paths, img_ids, img_meta = self._collect_image_vectors(all_raw_docs, image_dir)\n",
    "            if img_paths:\n",
    "                logger.info(f\"Indexing {len(img_paths)} unique images...\")\n",
    "                image_db = Chroma(\n",
    "                    collection_name=\"mm_image\",\n",
    "                    persist_directory=str(self.chroma_store_path),\n",
    "                    embedding_function=self.siglip_embed_model,\n",
    "                )\n",
    "                image_db.add_texts(texts=img_paths, metadatas=img_meta, ids=img_ids)\n",
    "                image_db.persist()\n",
    "    \n",
    "            # Step 4: Finalize\n",
    "            if self.cache_dir.exists(): shutil.rmtree(self.cache_dir)\n",
    "            self.manifest_path.write_text(json.dumps(current_manifest, indent=2))\n",
    "            self._initialize_rag_pipeline()\n",
    "            \n",
    "            return {\"status\": \"success\", \"message\": \"Knowledge base rebuilt successfully.\"}\n",
    "    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Knowledge base update failed: {e}\", exc_info=True)\n",
    "            return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 3. Helper and Class Methods\n",
    "    # ==========================================================================\n",
    "    def _initialize_rag_pipeline(self):\n",
    "        \"\"\"Initializes all RAG components from the persistent storage.\"\"\"\n",
    "        if not self.chroma_store_path.exists():\n",
    "            raise FileNotFoundError(f\"ChromaDB not found at {self.chroma_store_path}. The 'update_kb' command may have failed.\")\n",
    "        \n",
    "        # Load the vector databases\n",
    "        self.text_db = Chroma(collection_name=\"mm_text\", persist_directory=str(self.chroma_store_path), embedding_function=self.text_embed_model)\n",
    "        self.image_db = Chroma(collection_name=\"mm_image\", persist_directory=str(self.chroma_store_path), embedding_function=self.siglip_embed_model)\n",
    "    \n",
    "        # Build BM25 index and doc map\n",
    "        all_docs_data = self.text_db.get(include=[\"documents\", \"metadatas\"])\n",
    "        if not all_docs_data['ids']:\n",
    "             logger.warning(\"Knowledge base is empty. RAG will not have context to answer questions.\")\n",
    "             unique_splits = []\n",
    "        else:\n",
    "            all_docs = [Document(page_content=txt, metadata=meta) for txt, meta in zip(all_docs_data['documents'], all_docs_data['metadatas'])]\n",
    "            unique_splits = list({doc.page_content: doc for doc in all_docs}.values())\n",
    "    \n",
    "        corpus = [doc.page_content for doc in unique_splits]\n",
    "        \n",
    "        # Safety check for BM25\n",
    "        if not corpus:\n",
    "            self.bm25_index = None\n",
    "            logger.warning(\"Corpus is empty, BM25 index will not be created.\")\n",
    "        else:\n",
    "            self.bm25_index = BM25Okapi([doc.split(\" \") for doc in corpus])\n",
    "    \n",
    "        self.doc_map = {doc.page_content: doc for doc in unique_splits}\n",
    "    \n",
    "        # Initialize Cache\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.cache = SemanticCache(persist_directory=str(self.cache_dir), embedding_function=self.text_embed_model)\n",
    "        \n",
    "        # Initialize the main LLM wrapper and evaluation judge\n",
    "        self.mm_llm = self.InternVLMM(\n",
    "            model_path=self.model_path, device=self.device, cache=self.cache,\n",
    "            text_db=self.text_db, image_db=self.image_db,\n",
    "            bm25_index=self.bm25_index, doc_map=self.doc_map\n",
    "        )\n",
    "        self.judge = LocalGenAIJudge(model=self.mm_llm.model, tokenizer=self.mm_llm.tok)\n",
    "        logger.info(\"✅ RAG pipeline fully initialized.\")\n",
    "\n",
    "    # In MultimodalRagModel, REPLACE the old helper methods with these two:\n",
    "\n",
    "    def _create_json_manifest(self, context_dir: Path) -> Dict[str, str]:\n",
    "        \"\"\"Creates a manifest by hashing the single 'wiki_flat_structure.json' file.\"\"\"\n",
    "        manifest = {}\n",
    "        json_file = context_dir / \"wiki_flat_structure.json\"\n",
    "        if json_file.exists():\n",
    "            logger.info(f\"Creating manifest from {json_file}\")\n",
    "            file_bytes = json_file.read_bytes()\n",
    "            file_hash = hashlib.sha256(file_bytes).hexdigest()\n",
    "            manifest[str(json_file.resolve())] = file_hash\n",
    "        else:\n",
    "            logger.warning(f\"Manifest JSON not found at {json_file}\")\n",
    "        return manifest\n",
    "    \n",
    "    def _chunk_docs(self, docs: List[Document]) -> List[Document]:\n",
    "        \"\"\"Takes a list of raw docs and performs chunking with unique IDs per doc.\"\"\"\n",
    "        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[(\"#\", \"title\"), (\"##\", \"section\")])\n",
    "        recursive_splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=200)\n",
    "        all_chunks: list[Document] = []\n",
    "        \n",
    "        for doc in docs:\n",
    "            page_title = Path(doc.metadata[\"source\"]).stem.replace(\"-\", \" \")\n",
    "            section_docs = header_splitter.split_text(doc.page_content)\n",
    "            \n",
    "            # --- Initialize a chunk counter for each document ---\n",
    "            doc_chunk_counter = 0\n",
    "    \n",
    "            for section in section_docs:\n",
    "                tiny_texts = recursive_splitter.split_text(section.page_content)\n",
    "                for tiny in tiny_texts:\n",
    "                    chunk_metadata = {\n",
    "                        \"title\": page_title,\n",
    "                        \"source\": doc.metadata[\"source\"],\n",
    "                        \"section_header\": section.metadata.get(\"header\", \"\"),\n",
    "                        \"chunk_id\": doc_chunk_counter,  # Use the per-document counter\n",
    "                    }\n",
    "                    all_chunks.append(Document(\n",
    "                        page_content=f\"{page_title}\\n\\n{tiny.strip()}\",\n",
    "                        metadata=chunk_metadata\n",
    "                    ))\n",
    "                    # --- Increment the counter after each chunk ---\n",
    "                    doc_chunk_counter += 1\n",
    "        \n",
    "        return all_chunks\n",
    "        \n",
    "    def _collect_image_vectors(self, mm_raw_docs: List[Document], image_dir: Path):\n",
    "        \"\"\"Scans raw docs and returns paths, IDs, and metadata for unique images.\"\"\"\n",
    "        img_paths, img_ids, img_meta = [], [], []\n",
    "        seen = set()\n",
    "        for doc in mm_raw_docs:\n",
    "            src = doc.metadata[\"source\"]\n",
    "            for name in doc.metadata.get(\"images\", []):\n",
    "                img_id = f\"{src}::{name}\"\n",
    "                if img_id in seen:\n",
    "                    continue\n",
    "                seen.add(img_id)\n",
    "                img_paths.append(str(image_dir / name))\n",
    "                img_ids.append(img_id)\n",
    "                img_meta.append({\"source\": src, \"image\": name})\n",
    "        return img_paths, img_ids, img_meta\n",
    "\n",
    "    @classmethod\n",
    "    def log_model(cls, model_name: str, local_model: str) -> None:\n",
    "        logger.info(f\"--- Logging '{model_name}' Service to MLflow ---\")\n",
    "        \n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            temp_path = Path(temp_dir)\n",
    "            e5_path = temp_path / \"e5-large-v2\"\n",
    "            SentenceTransformer(\"intfloat/e5-large-v2\").save(str(e5_path))\n",
    "            \n",
    "            siglip_path = temp_path / \"siglip2-base-patch16-224\"\n",
    "            SiglipModel.from_pretrained(\"google/siglip2-base-patch16-224\").save_pretrained(siglip_path)\n",
    "            SiglipProcessor.from_pretrained(\"google/siglip2-base-patch16-224\").save_pretrained(siglip_path)\n",
    "            \n",
    "            artifacts = {\n",
    "                \"local_model_dir\": local_model,\n",
    "                \"e5_model_dir\": str(e5_path),\n",
    "                \"siglip_model_dir\": str(siglip_path),\n",
    "            }\n",
    "            \n",
    "            input_schema = Schema([\n",
    "                ColSpec(DataType.string, \"command\"),\n",
    "                ColSpec(DataType.string, \"query\", required=False),\n",
    "                ColSpec(DataType.string, \"payload\", required=False),\n",
    "                ColSpec(DataType.boolean, \"force_regenerate\", required=False)\n",
    "            ])\n",
    "            output_schema = Schema([\n",
    "                ColSpec(DataType.string, \"reply\", required=False),\n",
    "                ColSpec(DataType.string, \"used_images\", required=False),\n",
    "                ColSpec(DataType.double, \"generation_time_seconds\", required=False),\n",
    "                ColSpec(DataType.double, \"faithfulness\", required=False),\n",
    "                ColSpec(DataType.double, \"relevance\", required=False),\n",
    "                ColSpec(DataType.string, \"status\", required=False),\n",
    "                ColSpec(DataType.string, \"message\", required=False),\n",
    "            ])\n",
    "            signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "\n",
    "            mlflow.pyfunc.log_model(\n",
    "                artifact_path=model_name,\n",
    "                python_model=cls(),\n",
    "                artifacts=artifacts,\n",
    "                pip_requirements=\"../requirements.txt\",\n",
    "                signature=signature,\n",
    "                code_paths=[\"../src\"],\n",
    "            )\n",
    "        logger.info(f\"✅ Successfully logged '{model_name}' service and cleaned up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfee18b-a495-42c0-aaf4-a37a4d5009eb",
   "metadata": {},
   "source": [
    "## Step 3: Start Run, Log & Register Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c7b785-0628-4358-8e8d-9d49a0d6f1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# --- Start MLflow Run and Log the Model ---\n",
    "try:\n",
    "    with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "        run_id = run.info.run_id\n",
    "        logger.info(f\"Started MLflow run: {run_id}\")\n",
    "\n",
    "        # Use the class method to log the model and its artifacts\n",
    "        MultimodalRagModel.log_model(model_name=MODEL_NAME, local_model=LOCAL_MODEL)\n",
    "\n",
    "        model_uri = f\"runs:/{run_id}/{MODEL_NAME}\"\n",
    "        logger.info(f\"Registering model from URI: {model_uri}\")\n",
    "        \n",
    "        # Register the model in the MLflow Model Registry\n",
    "        mlflow.register_model(model_uri=model_uri, name=MODEL_NAME)\n",
    "        logger.info(f\"✅ Successfully registered model '{MODEL_NAME}'\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    logger.error(f\"Error: A required file or directory was not found. Please ensure the project structure is correct.\")\n",
    "    logger.error(f\"Details: {e}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred during the MLflow run: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c628e7f-9d9d-4e7a-9790-34ed1b003caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Retrieve the latest version from the Model Registry ---\n",
    "try:\n",
    "    client = MlflowClient()\n",
    "    versions = client.get_latest_versions(MODEL_NAME, stages=[\"None\"])\n",
    "    if not versions:\n",
    "        raise RuntimeError(f\"No registered versions found for model '{MODEL_NAME}'.\")\n",
    "    \n",
    "    latest_version = versions[0]\n",
    "    logger.info(f\"Found latest version '{latest_version.version}' for model '{MODEL_NAME}' in stage '{latest_version.current_stage}'.\")\n",
    "    model_uri_registry = latest_version.source\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to retrieve model from registry: {e}\", exc_info=True)\n",
    "    model_uri_registry = None # Ensure variable exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31856b3-5ff5-48b9-98b7-ddbf7dd16be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_uri_registry:\n",
    "    try:\n",
    "        logger.info(f\"Loading model from: {model_uri_registry}\")\n",
    "        loaded_model = mlflow.pyfunc.load_model(model_uri=model_uri_registry)\n",
    "        logger.info(\"✅ Successfully loaded model from registry.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load model from registry URI: {e}\", exc_info=True)\n",
    "        loaded_model = None\n",
    "else:\n",
    "    logger.warning(\"Skipping model loading due to previous errors.\")\n",
    "    loaded_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2d9db6-b2dd-44e5-989d-ebb498372229",
   "metadata": {},
   "source": [
    "## Step 4: Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa1c9dd-974c-475d-876c-e4621b03a202",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if loaded_model:\n",
    "    logger.info(\"--- Step 1: Populating the model's knowledge base ---\")\n",
    "\n",
    "    # Make sure your 'config' and 'secrets' are loaded from your YAML files\n",
    "    # This assumes 'config' and 'ADO_PAT' variables are available like at the start of the notebook.\n",
    "    try:\n",
    "        config = load_config(CONFIG_PATH)\n",
    "        ADO_PAT = os.getenv(\"AIS_ADO_TOKEN\")\n",
    "        if not ADO_PAT:\n",
    "            logger.info(\"Environment variable not found... Secrets Manager not properly set. Falling to secrets.yaml.\")\n",
    "            try:\n",
    "                secrets = load_secrets(SECRETS_PATH)\n",
    "                ADO_PAT = secrets.get('AIS_ADO_TOKEN')\n",
    "            except NameError:\n",
    "                logger.error(\"The 'secrets' object is not defined or available.\")\n",
    "\n",
    "        # 1. Construct the payload with credentials and config\n",
    "        update_payload_dict = {\n",
    "            \"config\": config,\n",
    "            \"secrets\": {\"AIS_ADO_TOKEN\": ADO_PAT}\n",
    "        }\n",
    "        \n",
    "        # 2. Create the DataFrame with the 'update_kb' command\n",
    "        update_df = pd.DataFrame([{\n",
    "            \"command\": \"update_kb\",\n",
    "            \"payload\": json.dumps(update_payload_dict)\n",
    "        }])\n",
    "        \n",
    "        # 3. Send the command to the model\n",
    "        logger.info(\"Sending 'update_kb' command to build the database. This may take a few minutes...\")\n",
    "        update_status = loaded_model.predict(update_df)\n",
    "        \n",
    "        logger.info(f\"Update Status: {update_status['message'].iloc[0]}\")\n",
    "\n",
    "    except NameError:\n",
    "        logger.error(\"Could not find 'config' or 'secrets' objects. Make sure they are loaded.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to update knowledge base: {e}\", exc_info=True)\n",
    "\n",
    "else:\n",
    "    logger.warning(\"Skipping knowledge base update because the model was not loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279582c0-74e3-4b98-9d1d-6007c98f109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(query: str, result_df: pd.DataFrame):\n",
    "    \"\"\"Helper to neatly print the query, reply, and display Base64 images.\"\"\"\n",
    "    if result_df.empty:\n",
    "        print(\"Received an empty result.\")\n",
    "        return\n",
    "\n",
    "    # Extract results from the DataFrame\n",
    "    row = result_df.iloc[0]\n",
    "    reply = row.get(\"reply\", \"No reply generated.\")\n",
    "    # This field now contains a JSON string of a list of Base64 strings\n",
    "    used_images_json = row.get(\"used_images\", \"[]\")\n",
    "    gen_time = row.get(\"generation_time_seconds\", 0)\n",
    "    faithfulness = row.get(\"faithfulness\", 0)\n",
    "    relevance = row.get(\"relevance\", 0)\n",
    "\n",
    "    # Safely parse the JSON string of Base64 images\n",
    "    base64_images = []\n",
    "    try:\n",
    "        # Use json.loads to parse the string into a list\n",
    "        base64_images = json.loads(used_images_json)\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        print(\"Warning: Could not parse image data from the API response.\")\n",
    "\n",
    "    # Display the output\n",
    "    print(\"---\" * 20)\n",
    "    print(f\"❓ Query:\\n{query}\\n\")\n",
    "    print(f\"🤖 Reply:\")\n",
    "    display(Markdown(reply)) # Render markdown for better formatting\n",
    "    \n",
    "    print(f\"\\n📊 Faithfulness: {faithfulness:.4f} | Relevance: {relevance:.4f}\")\n",
    "    print(f\"⏱️ Generation Time: {gen_time:.2f}s\\n\")\n",
    "\n",
    "    if base64_images and isinstance(base64_images, list):\n",
    "        print(f\"🖼️ Displaying {len(base64_images)} retrieved image(s):\")\n",
    "        for b64_string in base64_images:\n",
    "            try:\n",
    "                # Decode the Base64 string into bytes\n",
    "                image_bytes = base64.b64decode(b64_string)\n",
    "                # Display the image directly from the bytes data\n",
    "                display(Image(data=image_bytes, width=400))\n",
    "            except Exception as e:\n",
    "                print(f\"  - Could not decode or display an image: {e}\")\n",
    "    else:\n",
    "        print(\"▶ No images were retrieved for this query.\")\n",
    "    print(\"---\" * 20 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1230ed-5e16-4c18-8115-47dffd40bc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "\n",
    "if loaded_model:\n",
    "    logger.info(\"Running sample inference with the loaded model...\")\n",
    "    \n",
    "    sample_queries = [\n",
    "        \"What are the AI Blueprints Repository best practices?\",\n",
    "        \"What are some feature flags that i can enable in AIStudio?\",\n",
    "        \"How do i manually clean my environment without hooh?\",\n",
    "    ]\n",
    "\n",
    "    for query in sample_queries:\n",
    "        try:\n",
    "            input_payload = pd.DataFrame([{\n",
    "                \"command\": \"query\",\n",
    "                \"query\": query,\n",
    "                \"force_regenerate\": False\n",
    "            }])\n",
    "            \n",
    "            # --- RENAME the variable for clarity ---\n",
    "            result_df = loaded_model.predict(input_payload)\n",
    "\n",
    "            # --- ADD the original query to the result for logging ---\n",
    "            result_df['query'] = query\n",
    "\n",
    "            display_results(query, result_df)\n",
    "\n",
    "            # --- ADD the result to our list ---\n",
    "            all_results.append(result_df)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Prediction failed for query '{query}': {e}\", exc_info=True)\n",
    "\n",
    "    # --- ADD these lines to combine all results into one DataFrame ---\n",
    "    if all_results:\n",
    "        final_results_df = pd.concat(all_results, ignore_index=True)\n",
    "    else:\n",
    "        final_results_df = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    logger.warning(\"Skipping sample inference because the model was not loaded.\")\n",
    "    # --- ADD this line to ensure the variable exists ---\n",
    "    final_results_df = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83806df-79a8-4273-ab62-96c4bf1c2ded",
   "metadata": {},
   "source": [
    "## Step 5: Log Hallucinations & Relevance Evaluations to MlFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65476b0a-e314-44a6-8e51-856e237b00b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model was loaded, run_id exists, AND we have results to log\n",
    "if loaded_model and 'run_id' in locals() and not final_results_df.empty:\n",
    "    logger.info(f\"--- Reopening original run ({run_id}) to log pre-computed evaluations ---\")\n",
    "\n",
    "    # --- USE the DataFrame we already generated ---\n",
    "    results_df = final_results_df\n",
    "    \n",
    "    # Reopen the existing run using its ID\n",
    "    with mlflow.start_run(run_id=run_id) as run:\n",
    "        logger.info(\"Successfully reopened existing run. Logging metrics and artifacts...\")\n",
    "\n",
    "        # Calculate average scores from the DataFrame\n",
    "        avg_faithfulness = results_df[\"faithfulness\"].astype(float).mean()\n",
    "        avg_relevance = results_df[\"relevance\"].astype(float).mean()\n",
    "\n",
    "        # Log the average scores as metrics\n",
    "        mlflow.log_metrics({\n",
    "            \"avg_faithfulness\": avg_faithfulness,\n",
    "            \"avg_relevance\": avg_relevance\n",
    "        })\n",
    "\n",
    "        # Log the detailed results as a table artifact\n",
    "        mlflow.log_table(\n",
    "            data=results_df[['query', 'reply', 'faithfulness', 'relevance']], \n",
    "            artifact_file=\"inline_evaluation_results.json\"\n",
    "        )\n",
    "        \n",
    "        logger.info(\"✅ Successfully logged metrics and artifacts to the original model run.\")\n",
    "\n",
    "else:\n",
    "    logger.warning(\"Skipping evaluation logging because the model was not loaded, run_id was not found, or no results were generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725e1d0b-333c-43fd-95a7-5ce13fdfe3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time: float = time.time()\n",
    "elapsed_time: float = end_time - start_time\n",
    "elapsed_minutes: int = int(elapsed_time // 60)\n",
    "elapsed_seconds: float = elapsed_time % 60\n",
    "\n",
    "logger.info(f\"⏱️ Total execution time: {elapsed_minutes}m {elapsed_seconds:.2f}s\")\n",
    "logger.info(\"✅ Notebook execution completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e89eee-6171-446a-8da6-9381d3fe10c0",
   "metadata": {},
   "source": [
    "Built with ❤️ using Z by HP AI Studio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
