{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff193981-f701-4595-9d5f-a08779c65508",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-size: 50px;\"> 🤖 MLFlow Registration for Multimodal RAG</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e7b230-e638-458a-8104-0a30727b2466",
   "metadata": {},
   "source": [
    "# MLFlow Model Service \n",
    "\n",
    "In this section, we demonstrate how to deploy a RAG-based chatbot service. This service provides a REST API endpoint that allows users to query the knowledge base with natural language questions, upload new documents to the knowledge base, and manage conversation history, all with built-in safeguards against sensitive information and toxicity. This service encapsulates all the functionality we developed in this notebook, including the document retrieval system, RAG-based question answering capabilities, and Galileo integration for protection, observation and evaluation. It demonstrates how to use our ChatbotService from the src/service directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032ab59a-a358-4b11-9ee4-ac38101666bf",
   "metadata": {},
   "source": [
    "## Step 0: Imports and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e5a1883-f877-414a-9ff9-b876a2ba6531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os \n",
    "from pathlib import Path\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "# Create logger\n",
    "logger = logging.getLogger(\"multimodal_rag_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.handlers:\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "    stream_handler = logging.StreamHandler(sys.stdout)\n",
    "    stream_handler.setFormatter(formatter)\n",
    "    logger.addHandler(stream_handler)\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38660c87-34b3-40f2-be21-16c78fb12598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-28 12:41:28 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()  \n",
    "\n",
    "logger.info(\"Notebook execution started.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2607392-6bd0-4950-8ac8-84026da9df0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "galileo-observe 1.13.2 requires galileo-core<3.0.0,>=2.20.0, but you have galileo-core 3.60.0 which is incompatible.\n",
      "galileo-protect 0.15.1 requires galileo-core<3.0.0,>=2.17.0, but you have galileo-core 3.60.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "622040db-4d03-482f-958f-45cc87492ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 12:42:18.214778: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-28 12:42:18.223632: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753706538.233736     358 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753706538.237477     358 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753706538.245976     358 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753706538.245985     358 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753706538.245986     358 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753706538.245987     358 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-28 12:42:18.249169: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# === Standard Library Imports ===\n",
    "import gc\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "import warnings\n",
    "from rank_bm25 import BM25Okapi\n",
    "from typing import Any, Dict, List, Optional, TypedDict\n",
    "from statistics import mean\n",
    "from collections import defaultdict\n",
    "import hashlib\n",
    "\n",
    "# === Third-Party Library Imports ===\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain.schema.document import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.types import ColSpec, DataType, Schema, TensorSpec\n",
    "from PIL import Image as PILImage\n",
    "from sentence_transformers import CrossEncoder, SentenceTransformer\n",
    "from transformers import pipeline, AutoImageProcessor, AutoModel, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, SiglipModel, SiglipProcessor\n",
    "\n",
    "# === Project-Specific Imports ===\n",
    "# Add the project root to the system path to allow importing from 'src'\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from src.components import SemanticCache, SiglipEmbeddings\n",
    "from src.wiki_pages_clone import orchestrate_wiki_clone\n",
    "from src.local_genai_judge import LocalGenAIJudge\n",
    "from src.utils import (\n",
    "    configure_hf_cache,\n",
    "    multimodal_rag_asset_status,\n",
    "    load_config,\n",
    "    load_secrets,\n",
    "    load_mm_docs_clean,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0def9a7-d827-4158-be16-302c79bfeaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84f036c1-4922-4389-94d0-406f7e769612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cb6032-d809-4bf9-b9b8-87ef1f3c47ed",
   "metadata": {},
   "source": [
    "## Step 1: Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60df3050-7f7a-4735-af13-67cbadcd0b78",
   "metadata": {},
   "source": [
    "### Verify Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "869f893e-bddd-4f15-8c76-298091f9daee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-28 12:42:19 - INFO - Local Model is properly configured. \n",
      "2025-07-28 12:42:19 - INFO - Config is properly configured. \n",
      "2025-07-28 12:42:19 - INFO - Secrets is properly configured. \n",
      "2025-07-28 12:42:19 - INFO - wiki_flat_structure.json is properly configured. \n",
      "2025-07-28 12:42:19 - INFO - CONTEXT is properly configured. \n",
      "2025-07-28 12:42:19 - INFO - CHROMA is properly configured. \n",
      "2025-07-28 12:42:19 - INFO - CACHE is properly configured. \n",
      "2025-07-28 12:42:19 - INFO - MANIFEST is properly configured. \n"
     ]
    }
   ],
   "source": [
    "CONFIG_PATH = \"../configs/config.yaml\"\n",
    "SECRETS_PATH = \"../configs/secrets.yaml\"\n",
    "\n",
    "LOCAL_MODEL = \"/home/jovyan/datafabric/InternVL3-8B-Instruct\"\n",
    "CONTEXT_DIR: Path = Path(\"../data/context\")\n",
    "CHROMA_DIR: Path = Path(\"../data/chroma_store\")     \n",
    "CACHE_DIR: Path = CHROMA_DIR / \"semantic_cache\"\n",
    "MANIFEST_PATH: Path = CHROMA_DIR / \"manifest.json\"\n",
    "\n",
    "IMAGE_DIR = CONTEXT_DIR / \"images\"\n",
    "WIKI_METADATA_DIR = CONTEXT_DIR / \"wiki_flat_structure.json\"\n",
    "\n",
    "CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "multimodal_rag_asset_status(\n",
    "    local_model_path=LOCAL_MODEL,\n",
    "    config_path=CONFIG_PATH,\n",
    "    secrets_path=SECRETS_PATH,\n",
    "    wiki_metadata_dir=WIKI_METADATA_DIR,\n",
    "    context_dir=CONTEXT_DIR,\n",
    "    chroma_dir=CHROMA_DIR,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    manifest_path=MANIFEST_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2f98b97-0338-4107-91ea-797ff3a26072",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bfe368-e9c5-4856-8aad-a93026bacd7a",
   "metadata": {},
   "source": [
    "### Config HuggingFace Caches\n",
    "\n",
    "In the next cell, we configure HuggingFace cache, so that all the models downloaded from them are persisted locally, even after the workspace is closed. This is a future desired feature for AI Studio and the GenAI addon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4576dc32-7e04-48aa-b19d-da1adc92693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "configure_hf_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8125515-1c50-4297-8986-63ba9944a958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d76dc59f128542e4a06de17665b990dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c348d68609b44a39be42bdc75972f299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d08fb3e708ba4f36a2f57791ff91c1bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d0c8eb4a3342289c380a4f5a4719a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf6d317f5f147a69286ef5f8d3b29c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c13aa2b6965c4c3c84bc1ecf19178527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e891769f97da4105992857488107bca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01fd36f64cf249d8bf8d9cc2b34b4d7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed66d4da29064d6e9ef209c667baf974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4e8cbb532741688ef87782d027819e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/201 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize HuggingFace Embeddings\n",
    "txt_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/e5-large-v2\",\n",
    "    cache_folder=\"/tmp/hf_cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cebbd13-6e1f-46e9-a710-9621b3010bc8",
   "metadata": {},
   "source": [
    "### MLflow Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82d6964a-0768-4fa0-9da4-e8ecbc5487db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-28 12:42:43 - INFO - Using MLflow tracking URI: /phoenix/mlflow\n",
      "2025-07-28 12:42:43 - INFO - Using MLflow experiment: 'AIStudio-Multimodal-Chatbot-Experiment'\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"AIStudio-Multimodal-Chatbot-Model\"\n",
    "RUN_NAME = f\"Register_{MODEL_NAME}\"\n",
    "EXPERIMENT_NAME = \"AIStudio-Multimodal-Chatbot-Experiment\"\n",
    "\n",
    "# Set MLflow tracking URI and experiment\n",
    "# This should be configured for your environment, e.g., a remote server or local file path\n",
    "mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\", \"/phoenix/mlflow\"))\n",
    "mlflow.set_experiment(experiment_name=EXPERIMENT_NAME)\n",
    "\n",
    "logger.info(f\"Using MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "logger.info(f\"Using MLflow experiment: '{EXPERIMENT_NAME}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21125235-376c-4ed2-8109-5100645c67a0",
   "metadata": {},
   "source": [
    "## Step 2: Data loading, Cleaning, and Setup\n",
    "\n",
    "`wiki_flat_structure.json` is a custom json metadata for ADO Wiki data. It is flatly structured, with keys for filepath, md content, and a list of images. We also have a image folder that contains all the images for every md page. We directly scrape this data from ADO and perform any cleanup if necessary.\n",
    "\n",
    "- **secrets.yaml**: For Freemium users, use secrets.yaml to store your sensitive data like API Keys. If you are a Premium user, you can use secrets manager.\n",
    "- **AIS Secrets Manager**: For Paid users, use the secrets manager in the `Project Setup` tab to configure your API key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8643f2-458e-427d-9037-38b14f7d236d",
   "metadata": {},
   "source": [
    "### Parse from ADO Wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ad2ab18-6a08-4050-84fc-8612a28ae8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-28 12:42:43 - INFO - Starting ADO Wiki clone process...\n",
      "2025-07-28 12:42:43 - INFO - Cloning wiki 'Phoenix-DS-Platform.wiki' to temporary directory: /tmp/tmpdw6k59dx\n",
      "2025-07-28 12:43:10 - INFO - Scanning for Markdown files...\n",
      "2025-07-28 12:43:10 - INFO - → Found 567 Markdown pages.\n",
      "2025-07-28 12:43:10 - INFO - Copying referenced images to ../data/context/images...\n",
      "2025-07-28 12:43:16 - INFO - → 738 unique images copied.\n",
      "2025-07-28 12:43:16 - INFO - Assembling flat JSON structure...\n",
      "2025-07-28 12:43:16 - INFO - ✅ Wiki data successfully cloned to ../data/context\n",
      "2025-07-28 12:43:17 - INFO - Cleaned up temporary directory: /tmp/tmpdw6k59dx\n",
      "2025-07-28 12:43:17 - INFO - ✅ Wiki data preparation step completed successfully.\n",
      "CPU times: user 617 ms, sys: 755 ms, total: 1.37 s\n",
      "Wall time: 33.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ADO_PAT = os.getenv(\"AIS_ADO_TOKEN\")\n",
    "if not ADO_PAT:\n",
    "    logger.info(\"Environment variable not found... Secrets Manager not properly set. Falling to secrets.yaml.\")\n",
    "    try:\n",
    "        secrets = load_secrets(SECRETS_PATH)\n",
    "        ADO_PAT = secrets.get('AIS_ADO_TOKEN')\n",
    "    except NameError:\n",
    "        logger.error(\"The 'secrets' object is not defined or available.\")\n",
    "\n",
    "try:\n",
    "    orchestrate_wiki_clone(\n",
    "        pat=ADO_PAT,\n",
    "        config=config,\n",
    "        output_dir=CONTEXT_DIR\n",
    "    )\n",
    "    logger.info(\"✅ Wiki data preparation step completed successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(\"Halting notebook execution due to a critical error in the wiki preparation step.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d135f7ca-6f89-4b9b-b132-fba91ee18ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-28 12:43:17 - WARNING - ⚠️ 94 broken image refs filtered out\n",
      "2025-07-28 12:43:17 - INFO - Docs loaded: 567 docs, avg_tokens=3099\n",
      "CPU times: user 39.1 ms, sys: 38.1 ms, total: 77.1 ms\n",
      "Wall time: 700 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "WIKI_METADATA_DIR   = Path(WIKI_METADATA_DIR)\n",
    "IMAGE_DIR = Path(IMAGE_DIR)\n",
    "\n",
    "mm_raw_docs = load_mm_docs_clean(WIKI_METADATA_DIR, Path(IMAGE_DIR))\n",
    "\n",
    "def log_stage(name: str, docs: List[Document]):\n",
    "    logger.info(f\"{name}: {len(docs)} docs, avg_tokens={sum(len(d.page_content) for d in docs)/len(docs):.0f}\")\n",
    "log_stage(\"Docs loaded\", mm_raw_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b24584a-05dc-48ad-84fe-afcaa24a0f58",
   "metadata": {},
   "source": [
    "### Creation of Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "920d2ec8-cf89-4fec-a56a-5e480325913d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-28 12:43:17 - INFO - Chunking complete: 567 docs → 2615 chunks (avg 718 chars)\n",
      "CPU times: user 70.4 ms, sys: 770 μs, total: 71.2 ms\n",
      "Wall time: 65.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def chunk_documents(\n",
    "    docs,\n",
    "    chunk_size: int = 1200,\n",
    "    overlap: int = 200,\n",
    ") -> list[Document]:\n",
    "    \"\"\"\n",
    "    1) Split each wiki page on Markdown headers (#, ## …) to keep logical\n",
    "       sections together.\n",
    "    2) Recursively break long sections to <= `chunk_size` chars with `overlap`.\n",
    "    3) Prefix every chunk with its page-title and store the title in metadata.\n",
    "    \"\"\"\n",
    "    header_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=[(\"#\", \"title\"), (\"##\", \"section\")]\n",
    "    )\n",
    "    recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "    )\n",
    "\n",
    "    all_chunks: list[Document] = []\n",
    "    for doc in docs:\n",
    "        page_title = Path(doc.metadata[\"source\"]).stem.replace(\"-\", \" \")\n",
    "\n",
    "        # 1️. section‑level split (returns list[Document])\n",
    "        section_docs = header_splitter.split_text(doc.page_content)\n",
    "\n",
    "        for section in section_docs:\n",
    "            # 2. size‑based split inside each section\n",
    "            tiny_texts = recursive_splitter.split_text(section.page_content)\n",
    "\n",
    "            for idx, tiny in enumerate(tiny_texts):\n",
    "                all_chunks.append(\n",
    "                    Document(\n",
    "                        page_content=f\"{page_title}\\n\\n{tiny.strip()}\",\n",
    "                        metadata={\n",
    "                            \"title\": page_title,\n",
    "                            \"source\": doc.metadata[\"source\"],\n",
    "                            \"section_header\": section.metadata.get(\"header\", \"\"),\n",
    "                            \"chunk_id\": idx,\n",
    "                        },\n",
    "                    )\n",
    "                )\n",
    "    if all_chunks:\n",
    "        avg_len = int(mean(len(c.page_content) for c in all_chunks))\n",
    "        logger.info(\n",
    "            \"Chunking complete: %d docs → %d chunks (avg %d chars)\",\n",
    "            len(docs),\n",
    "            len(all_chunks),\n",
    "            avg_len,\n",
    "        )\n",
    "    else:\n",
    "        logger.warning(\"Chunking produced zero chunks for %d docs\", len(docs))\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "splits = chunk_documents(mm_raw_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124f2cc6-c538-49cf-8833-725744f2bcbb",
   "metadata": {},
   "source": [
    "### Setup Text ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c33323b-bfb1-4eaf-86e9-78274b0849bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-28 12:43:17 - INFO - Loading existing Chroma index from ../data/chroma_store\n",
      "CPU times: user 190 ms, sys: 16.3 ms, total: 206 ms\n",
      "Wall time: 300 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 1) TEXT store\n",
    "def _current_manifest() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Returns a dictionary mapping every context JSON file to its SHA256 content hash.\n",
    "    This allows detecting changes in file content, not just filenames.\n",
    "    \"\"\"\n",
    "    manifest = {}\n",
    "    json_files = sorted(CONTEXT_DIR.rglob(\"*.json\"))\n",
    "\n",
    "    for file_path in json_files:\n",
    "        try:\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                file_bytes = f.read()\n",
    "                file_hash = hashlib.sha256(file_bytes).hexdigest()\n",
    "                manifest[str(file_path.resolve())] = file_hash\n",
    "        except IOError as e:\n",
    "            logger.error(f\"Could not read file {file_path} for hashing: {e}\")\n",
    "    return manifest\n",
    "\n",
    "def _needs_rebuild() -> bool:\n",
    "    \"\"\"\n",
    "    Determines if the ChromaDB needs to be rebuilt.\n",
    "    A rebuild is needed if:\n",
    "    1. The Chroma directory or manifest file doesn't exist.\n",
    "    2. The manifest is unreadable.\n",
    "    3. The stored file hashes in the manifest do not match the current file hashes.\n",
    "    \"\"\"\n",
    "    if not CHROMA_DIR.exists() or not MANIFEST_PATH.exists():\n",
    "        logger.info(\"Chroma directory or manifest not found. A rebuild is required.\")\n",
    "        return True\n",
    "    try:\n",
    "        old_manifest = json.loads(MANIFEST_PATH.read_text())\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not read manifest file. A rebuild is required. Error: {e}\")\n",
    "        return True\n",
    "\n",
    "    current_manifest = _current_manifest()\n",
    "    if old_manifest != current_manifest:\n",
    "        logger.info(\"Data content has changed. A rebuild is required.\")\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def _save_manifest(manifest: Dict[str, str]) -> None:\n",
    "    \"\"\"Saves the current data manifest (mapping file paths to hashes) to disk.\"\"\"\n",
    "    CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    MANIFEST_PATH.write_text(json.dumps(manifest, indent=2))\n",
    "\n",
    "def _build_text_db() -> Chroma:\n",
    "    collection = \"mm_text\"\n",
    "    # The rebuild check is now done outside this function.\n",
    "    # We check if the directory exists. If not, we build.\n",
    "    if not CHROMA_DIR.exists() or not (CHROMA_DIR / \"chroma.sqlite3\").exists():\n",
    "        logger.info(\"Creating new text context index in %s ...\", CHROMA_DIR)\n",
    "        chroma = Chroma.from_documents(\n",
    "            documents          = splits,\n",
    "            embedding          = txt_embeddings,\n",
    "            collection_name    = collection,\n",
    "            persist_directory  = str(CHROMA_DIR),\n",
    "        )\n",
    "        return chroma\n",
    "\n",
    "    logger.info(\"Loading existing Chroma index from %s\", CHROMA_DIR)\n",
    "    return Chroma(\n",
    "        collection_name   = collection,\n",
    "        persist_directory = str(CHROMA_DIR),\n",
    "        embedding_function= txt_embeddings,\n",
    "    )\n",
    "    \n",
    "# Check if a rebuild is needed and wipe the old DB if so.\n",
    "# This ensures both the text and image databases are rebuilt from scratch.\n",
    "if _needs_rebuild():\n",
    "    logger.warning(\"REBUILDING: Wiping old ChromaDB store at %s\", CHROMA_DIR)\n",
    "    if CHROMA_DIR.exists():\n",
    "        shutil.rmtree(CHROMA_DIR)\n",
    "    # Save the new manifest immediately after deciding to rebuild\n",
    "    _save_manifest(_current_manifest())\n",
    "\n",
    "# Now, initialize your databases. They will be created fresh if they were just deleted.\n",
    "text_db = _build_text_db()\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0722631-f6b9-44a6-a505-d5b3335a9b35",
   "metadata": {},
   "source": [
    "### Setup Image ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82062e5c-f0c1-4518-a11a-dec7612eb92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362f6f86cf93472e99671548eadd2a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/253 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e460c5961d1c4dd994000e28261e5472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1593cd48085a42f695328e9e8ee881af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/394 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10cfa249f7114ebbbe9313799099d029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fc37c91dcc94f5ab97b2ebe6139433c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8500deddd9c648309ee13f40402d7d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e143218cdd454c5fac9b9e1272adae01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-28 12:43:32 - INFO - Loaded existing image index (752 vectors).\n",
      "CPU times: user 2.19 s, sys: 7.68 s, total: 9.87 s\n",
      "Wall time: 14.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#  Helper: walk all docs once and gather *unique* image vectors + metadata\n",
    "def _collect_image_vectors():\n",
    "    \"\"\"\n",
    "    Scans every wiki page for image references and returns three parallel lists:\n",
    "        img_paths : list[str]   → full file-system paths (for SigLIP)\n",
    "        img_ids   : list[str]   → unique key per (page, image) pair\n",
    "        img_meta  : list[dict]  → {\"source\": wiki_page, \"image\": file_name}\n",
    "    Runs in < 1s even for thousands of docs.\n",
    "    \"\"\"\n",
    "    img_paths, img_ids, img_meta = [], [], []\n",
    "    seen = set()\n",
    "\n",
    "    for doc in mm_raw_docs:                         # raw wiki pages\n",
    "        src = doc.metadata[\"source\"]\n",
    "        for name in doc.metadata.get(\"images\", []): # list[str]\n",
    "            img_id = f\"{src}::{name}\"\n",
    "            if img_id in seen:\n",
    "                continue                            # de‑dupe\n",
    "            seen.add(img_id)\n",
    "\n",
    "            img_paths.append(str(IMAGE_DIR / name))\n",
    "            img_ids.append(img_id)\n",
    "            img_meta.append({\"source\": src, \"image\": name})\n",
    "\n",
    "    return img_paths, img_ids, img_meta\n",
    "\n",
    "siglip_embeddings = SiglipEmbeddings(\"google/siglip2-base-patch16-224\", DEVICE)\n",
    "\n",
    "# 2) IMAGE store\n",
    "image_db = Chroma(\n",
    "    collection_name    = \"mm_image\",\n",
    "    persist_directory  = str(CHROMA_DIR),   # SAME dir as text db\n",
    "    embedding_function = siglip_embeddings, # <-- class you kept\n",
    ")\n",
    "\n",
    "# Populate vectors *only* if it is empty\n",
    "if not image_db._collection.count():\n",
    "    img_paths, img_ids, img_meta = _collect_image_vectors()\n",
    "    image_db.add_texts(texts=img_paths, metadatas=img_meta, ids=img_ids)\n",
    "    image_db.persist()\n",
    "    logger.info(\"Indexed %d unique images.\", len(img_paths))\n",
    "else:\n",
    "    logger.info(\"Loaded existing image index (%d vectors).\",\n",
    "                image_db._collection.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1cb8d4-d58a-48bc-96e9-9bd63d43f80f",
   "metadata": {},
   "source": [
    "## Step 3: MLflow Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "769f37b3-a909-4cae-8278-015adcdcf69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalRagModel(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"\n",
    "    An MLflow PythonModel that encapsulates the entire Multimodal RAG pipeline.\n",
    "\n",
    "    This class faithfully reproduces the workflow from the `run-workflow.ipynb`, including\n",
    "    data loading, hybrid retrieval (BM25 + vector search + RRF), and multimodal\n",
    "    generation with the InternVL model.\n",
    "\n",
    "    Expected Artifacts during logging/loading:\n",
    "     - \"local_model_dir\": Path to the main InternVL model directory.\n",
    "     - \"e5_model_dir\": Path to the text embedding model (e.g., e5-large-v2).\n",
    "     - \"siglip_model_dir\": Path to the image embedding model (e.g., SigLIP).\n",
    "     - \"chroma_dir\": Path to the persisted Chroma vectorstore directory.\n",
    "     - \"context_dir\": Path to the root data directory, containing the `images` subdirectory.\n",
    "     - \"cache_dir\": Path to the directory for the semantic cache.\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Helper Class (Encapsulated from the original notebook)\n",
    "    # --------------------------------------------------------------------------\n",
    "    class InternVLMM:\n",
    "        \"\"\"Minimal, self-contained multimodal QA wrapper around InternVL3-8B-Instruct.\"\"\"\n",
    "\n",
    "        def __init__(self, model_path: str, device: str, cache: Any, text_db: Chroma, image_db: Chroma, bm25_index: BM25Okapi, doc_map: dict):\n",
    "            self.model_path = model_path\n",
    "            self.device = device\n",
    "            self.model = None\n",
    "            self.tok = None\n",
    "            self.image_processor = None\n",
    "            self.cache = cache\n",
    "            self.text_db = text_db\n",
    "            self.image_db = image_db\n",
    "            self.bm25_index = bm25_index\n",
    "            self.doc_map = doc_map\n",
    "            self._load()\n",
    "\n",
    "        @staticmethod\n",
    "        def _reciprocal_rank_fusion(results: list[list[Document]], k: int = 60) -> list[tuple[Document, float]]:\n",
    "            \"\"\"Performs RRF on multiple lists of ranked documents.\"\"\"\n",
    "            ranked_lists = [\n",
    "                {doc.page_content: (doc, i + 1) for i, doc in enumerate(res)}\n",
    "                for res in results\n",
    "            ]\n",
    "            rrf_scores = defaultdict(float)\n",
    "            all_docs = {}\n",
    "            for ranked_list in ranked_lists:\n",
    "                for content, (doc, rank) in ranked_list.items():\n",
    "                    rrf_scores[content] += 1 / (k + rank)\n",
    "                    if content not in all_docs:\n",
    "                        all_docs[content] = doc\n",
    "            fused_results = [\n",
    "                (all_docs[content], rrf_scores[content])\n",
    "                for content in sorted(rrf_scores, key=rrf_scores.get, reverse=True)\n",
    "            ]\n",
    "            return fused_results\n",
    "\n",
    "        def _retrieve_mm(self, query: str, k_text: int = 4, k_img: int = 4, recall_k: int = 20) -> dict[str, any]:\n",
    "            \"\"\"Performs hybrid search for text and retrieves contextually relevant images.\"\"\"\n",
    "            # 1. Hybrid Search for Text\n",
    "            dense_hits = self.text_db.similarity_search(query, k=recall_k)\n",
    "            tokenized_query = query.lower().split(\" \")\n",
    "            sparse_texts = self.bm25_index.get_top_n(tokenized_query, list(self.doc_map.keys()), n=recall_k)\n",
    "            sparse_hits = [self.doc_map[text] for text in sparse_texts]\n",
    "\n",
    "            if not dense_hits and not sparse_hits:\n",
    "                return {\"docs\": [], \"scores\": [], \"images\": []}\n",
    "\n",
    "            fused_results = self._reciprocal_rank_fusion([dense_hits, sparse_hits])\n",
    "            final_docs = [doc for doc, score in fused_results[:k_text]]\n",
    "            final_scores = [score for doc, score in fused_results[:k_text]]\n",
    "\n",
    "            # 2. Retrieve Relevant Images\n",
    "            retrieved_images = []\n",
    "            if final_docs:\n",
    "                final_sources = list(set(d.metadata[\"source\"] for d in final_docs))\n",
    "                image_hits = self.image_db.similarity_search(query, k=k_img, filter={\"source\": {\"$in\": final_sources}})\n",
    "                retrieved_images = [img.page_content for img in image_hits]\n",
    "\n",
    "            return {\"docs\": final_docs, \"scores\": final_scores, \"images\": retrieved_images}\n",
    "\n",
    "        def generate(self, query: str, force_regenerate: bool = False, **retrieval_kwargs) -> Dict[str, Any]:\n",
    "            \"\"\"Run retrieval, prompt assembly, and model generation.\"\"\"\n",
    "            start_gen_time = time.time()\n",
    "            \n",
    "            # 1. Cache Check\n",
    "            if not force_regenerate:\n",
    "                cached_result = self.cache.get(query, threshold=0.92)\n",
    "                if cached_result:\n",
    "                    logger.info(f\"SEMANTIC CACHE HIT for query: '{query}'\")\n",
    "                    # Ensure older cached items have the time key\n",
    "                    if \"generation_time_seconds\" not in cached_result:\n",
    "                        cached_result[\"generation_time_seconds\"] = 0.0\n",
    "                    return cached_result\n",
    "            if force_regenerate:\n",
    "                logger.info(f\"Forced regeneration for query: '{query}'. Clearing old cache entry.\")\n",
    "                self.cache.delete(query)\n",
    "            \n",
    "            logger.info(f\"CACHE MISS for query: '{query}'. Running full pipeline.\")\n",
    "            if self.model is None or self.tok is None:\n",
    "                return {\"reply\": \"Error: model not initialised.\", \"used_images\": [], \"generation_time_seconds\": 0.0}\n",
    "\n",
    "            # 2. Retrieve\n",
    "            hits = self._retrieve_mm(query, **retrieval_kwargs)\n",
    "            docs, images = hits[\"docs\"], hits[\"images\"]\n",
    "            if not docs and not images:\n",
    "                return {\"reply\": \"Based on the provided context, I cannot answer this question.\", \"used_images\": [], \"generation_time_seconds\": 0.0}\n",
    "\n",
    "            # 3. Build Prompt\n",
    "            context_str = \"\\n\\n\".join(\n",
    "                f\"<source_document name=\\\"{d.metadata.get('source', 'unknown')}\\\">\\n{d.page_content}\\n</source_document>\"\n",
    "                for d in docs\n",
    "            )\n",
    "            system_prompt = \"\"\"You are an AI Studio DevOps Assistant. Your task is to answer the user's query based ONLY on the context provided.\n",
    "            **Instructions:**\n",
    "            1.  **Analyze Context:** First, analyze the user's images (if any) and the text in the `<context>` block.\n",
    "            2.  **Synthesize Answer:** Answer the user's query directly, synthesizing information from the context.\n",
    "            3.  **Cite Sources:** List all source documents you used in a `Source Documents` section.\n",
    "            4.  **Handle Missing Information:** If the answer is not in the context, respond with ONLY this exact phrase: \"Based on the provided context, I cannot answer this question.\"\n",
    "            \n",
    "            **Output Format:**\n",
    "            Your response must follow this exact markdown structure. Do not add any other commentary.\n",
    "            \n",
    "            ### Visual Analysis\n",
    "            (Analyze the user's images here.)\n",
    "            \n",
    "            ### Synthesized Answer\n",
    "            (Your answer to the user's query goes here.)\n",
    "            \n",
    "            ### Source Documents\n",
    "            (List the sources here, like [`source-file-name.md`].)\n",
    "            \"\"\"\n",
    "            user_content = f\"\"\"<context>\n",
    "            {context_str}\n",
    "            </context>\n",
    "            \n",
    "            <user_query>\n",
    "            {query}\n",
    "            </user_query>\n",
    "            \"\"\"\n",
    "            conversation = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_content}]\n",
    "            prompt = self.tok.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "            # 4. Generate & Cache\n",
    "            try:\n",
    "                self._clear_cuda()\n",
    "                pixel_values = self._process_images(images) if images else None\n",
    "                reply = self.model.chat(\n",
    "                    self.tok, pixel_values, prompt,\n",
    "                    generation_config=dict(\n",
    "                        max_new_tokens=4096,\n",
    "                        pad_token_id=self.tok.pad_token_id,\n",
    "                        eos_token_id=self.tok.eos_token_id,\n",
    "                        repetition_penalty=1.25,\n",
    "                    ),\n",
    "                )\n",
    "                self._clear_cuda()\n",
    "                end_gen_time = time.time()\n",
    "                result_dict = {\"reply\": reply, \"used_images\": images, \"generation_time_seconds\": end_gen_time - start_gen_time}\n",
    "                self.cache.set(query, result_dict)\n",
    "                return result_dict\n",
    "            except RuntimeError as e:\n",
    "                logger.error(\"InternVL generation failed: %s\", e)\n",
    "                return {\"reply\": f\"Error during generation: {e}\", \"used_images\": images, \"generation_time_seconds\": 0.0}\n",
    "\n",
    "        def _load(self):\n",
    "            \"\"\"Load tokenizer, image_processor, & model.\"\"\"\n",
    "            logger.info(\"Loading %s...\", self.model_path)\n",
    "            self._clear_cuda()\n",
    "            self.tok = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)\n",
    "            if self.tok.pad_token is None: self.tok.pad_token = self.tok.eos_token\n",
    "            self.image_processor = AutoImageProcessor.from_pretrained(self.model_path, trust_remote_code=True, use_fast=True)\n",
    "            q_cfg = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "            ) if self.device == \"cuda\" else None\n",
    "            self.model = AutoModel.from_pretrained(\n",
    "                self.model_path,\n",
    "                quantization_config=q_cfg,\n",
    "                torch_dtype=(torch.bfloat16 if self.device == \"cuda\" else torch.float32),\n",
    "                low_cpu_mem_usage=True, use_flash_attn=False, trust_remote_code=True,\n",
    "                device_map=\"auto\" if self.device == \"cuda\" else None,\n",
    "            ).eval()\n",
    "            logger.info(\"Model loaded on %s.\", self.device)\n",
    "\n",
    "        def _process_images(self, image_paths: List[str]):\n",
    "            if not image_paths: return None\n",
    "            try:\n",
    "                pil_images = [PILImage.open(p).convert(\"RGB\") for p in image_paths]\n",
    "                processed_data = self.image_processor(images=pil_images, return_tensors=\"pt\")\n",
    "                pixel_values = processed_data['pixel_values'].to(device=self.device, dtype=next(self.model.parameters()).dtype)\n",
    "                return pixel_values\n",
    "            except Exception as e:\n",
    "                logger.error(\"Image processing failed: %s\", e)\n",
    "                return None\n",
    "\n",
    "        def _clear_cuda(self):\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # MLflow pyfunc Methods\n",
    "    # --------------------------------------------------------------------------\n",
    "    def load_context(self, context: mlflow.pyfunc.PythonModelContext) -> None:\n",
    "        \"\"\"This method is called when loading an MLflow model.\"\"\"\n",
    "        logger.info(\"--- Initializing MultimodalRagModel context ---\")\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        logger.info(f\"Running on device: {self.device}\")\n",
    "\n",
    "        # Load paths from artifacts\n",
    "        self.model_path = Path(context.artifacts[\"local_model_dir\"]).resolve()\n",
    "        self.chroma_dir = Path(context.artifacts[\"chroma_dir\"])\n",
    "        self.context_dir = Path(context.artifacts[\"context_dir\"])\n",
    "        self.cache_dir = Path(context.artifacts[\"cache_dir\"])\n",
    "        logger.info(f\"Artifacts loaded: model='{self.model_path}', chroma='{self.chroma_dir}'\")\n",
    "        \n",
    "        # Load embedding models from artifacts\n",
    "        e5_model_path = context.artifacts[\"e5_model_dir\"]\n",
    "        siglip_model_path = context.artifacts[\"siglip_model_dir\"]\n",
    "        self.text_embed_model = HuggingFaceEmbeddings(model_name=e5_model_path, model_kwargs={\"device\": self.device})\n",
    "        self.siglip_embed_model = SiglipEmbeddings(model_id=siglip_model_path, device=self.device)\n",
    "        logger.info(\"✅ Embedding models loaded from artifacts.\")\n",
    "    \n",
    "        # Load Vector DBs\n",
    "        self.text_db = Chroma(collection_name=\"mm_text\", persist_directory=str(self.chroma_dir), embedding_function=self.text_embed_model)\n",
    "        self.image_db = Chroma(collection_name=\"mm_image\", persist_directory=str(self.chroma_dir), embedding_function=self.siglip_embed_model)\n",
    "        logger.info(f\"Text DB count: {self.text_db._collection.count()}, Image DB count: {self.image_db._collection.count()}\")\n",
    "\n",
    "        # --- Build BM25 index and doc_map from the text DB ---\n",
    "        logger.info(\"Rebuilding BM25 index from ChromaDB documents...\")\n",
    "        all_docs_data = self.text_db.get(include=[\"documents\", \"metadatas\"])\n",
    "        all_docs = [Document(page_content=txt, metadata=meta) for txt, meta in zip(all_docs_data['documents'], all_docs_data['metadatas'])]\n",
    "        unique_docs_map = {doc.page_content: doc for doc in all_docs}\n",
    "        unique_splits = list(unique_docs_map.values())\n",
    "        logger.info(f\"De-duplicated {len(all_docs)} chunks down to {len(unique_splits)} unique chunks for BM25.\")\n",
    "        corpus = [doc.page_content for doc in unique_splits]\n",
    "        tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "        self.bm25_index = BM25Okapi(tokenized_corpus)\n",
    "        self.doc_map = {doc.page_content: doc for doc in unique_splits}\n",
    "        logger.info(\"✅ BM25 index and document map are ready.\")\n",
    "\n",
    "        # Initialize Cache and the main LLM\n",
    "        self.cache = SemanticCache(persist_directory=self.cache_dir, embedding_function=self.text_embed_model)\n",
    "        self.mm_llm = self.InternVLMM(\n",
    "            model_path=self.model_path, device=self.device, cache=self.cache,\n",
    "            text_db=self.text_db, image_db=self.image_db,\n",
    "            bm25_index=self.bm25_index, doc_map=self.doc_map\n",
    "        )\n",
    "\n",
    "        # Initialize Evaluation Judge\n",
    "        logger.info(\"Initializing evaluation judge by sharing the main model...\")\n",
    "        try:\n",
    "            self.judge = LocalGenAIJudge(model=self.mm_llm.model, tokenizer=self.mm_llm.tok)\n",
    "            logger.info(\"✅ Evaluation judge initialized successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize evaluation judge: {e}\")\n",
    "            self.judge = None\n",
    "        logger.info(\"--- Context initialization complete ---\")\n",
    "\n",
    "    def predict(self, context: mlflow.pyfunc.PythonModelContext, model_input: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"MLflow inference entrypoint.\"\"\"\n",
    "        logger.info(\"Received prediction request.\")\n",
    "        queries = model_input[\"query\"].tolist()\n",
    "        force_regenerate = model_input.get(\"force_regenerate\", pd.Series([False] * len(queries))).tolist()\n",
    "        results = []\n",
    "\n",
    "        for i, query in enumerate(queries):\n",
    "            logger.info(f\"Processing query: '{query}'\")\n",
    "            \n",
    "            # 1. Generate the answer\n",
    "            response_dict = self.mm_llm.generate(query, force_regenerate=force_regenerate[i])\n",
    "            response_dict[\"used_images\"] = str(response_dict.get(\"used_images\", [])) # Ensure serializable\n",
    "\n",
    "            # 2. Run evaluation\n",
    "            if self.judge:\n",
    "                retrieved_info = self.mm_llm._retrieve_mm(query)\n",
    "                context_str = \"\\n\\n\".join(d.page_content for d in retrieved_info[\"docs\"])\n",
    "                \n",
    "                eval_df = pd.DataFrame([{\"questions\": query, \"result\": response_dict[\"reply\"], \"source_documents\": context_str}])\n",
    "                response_dict[\"faithfulness\"] = self.judge.evaluate_faithfulness(eval_df).iloc[0]\n",
    "                response_dict[\"relevance\"] = self.judge.evaluate_relevance(eval_df).iloc[0]\n",
    "            else:\n",
    "                response_dict[\"faithfulness\"], response_dict[\"relevance\"] = None, None\n",
    "            \n",
    "            results.append(response_dict)\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    @classmethod\n",
    "    def log_model(cls, model_name: str, local_model: str) -> None:\n",
    "        \"\"\"Helper class method to log the MultimodalRagModel to MLflow.\"\"\"\n",
    "        logger.info(f\"--- Logging '{model_name}' to MLflow ---\")\n",
    "        \n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            logger.info(f\"Created temporary directory for models: {temp_dir}\")\n",
    "            temp_path = Path(temp_dir)\n",
    "    \n",
    "            # --- 1. Download supporting models into the temporary directory ---\n",
    "            e5_path = temp_path / \"e5-large-v2\"\n",
    "            SentenceTransformer(\"intfloat/e5-large-v2\").save(str(e5_path))\n",
    "            logger.info(f\"✅ Temporarily saved e5-large-v2 to {e5_path}\")\n",
    "    \n",
    "            siglip_path = temp_path / \"siglip2-base-patch16-224\"\n",
    "            SiglipModel.from_pretrained(\"google/siglip2-base-patch16-224\").save_pretrained(siglip_path)\n",
    "            SiglipProcessor.from_pretrained(\"google/siglip2-base-patch16-224\").save_pretrained(siglip_path)\n",
    "            logger.info(f\"✅ Temporarily saved SigLIP to {siglip_path}\")\n",
    "    \n",
    "            # --- 2. Define artifacts ---\n",
    "            project_root = Path.cwd().parent.resolve()\n",
    "            artifacts = {\n",
    "                \"local_model_dir\": local_model,\n",
    "                \"e5_model_dir\": str(e5_path),\n",
    "                \"siglip_model_dir\": str(siglip_path),\n",
    "                \"chroma_dir\": str(project_root / \"data\" / \"chroma_store\"),\n",
    "                \"context_dir\": str(project_root / \"data\" / \"context\"),\n",
    "                \"cache_dir\": str(project_root / \"data\" / \"chroma_store\" / \"semantic_cache\"),\n",
    "            }\n",
    "    \n",
    "            # --- 3. Log the model ---\n",
    "            input_schema = Schema([\n",
    "                ColSpec(DataType.string, \"query\"),\n",
    "                ColSpec(DataType.boolean, \"force_regenerate\")\n",
    "            ])\n",
    "            output_schema = Schema([\n",
    "                ColSpec(DataType.string, \"reply\"),\n",
    "                ColSpec(DataType.string, \"used_images\"),\n",
    "                ColSpec(DataType.double, \"generation_time_seconds\"),\n",
    "                ColSpec(DataType.double, \"faithfulness\"),\n",
    "                ColSpec(DataType.double, \"relevance\"),\n",
    "            ])\n",
    "            signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "    \n",
    "            mlflow.pyfunc.log_model(\n",
    "                artifact_path=model_name,\n",
    "                python_model=cls(),\n",
    "                artifacts=artifacts,\n",
    "                pip_requirements=\"../requirements.txt\",\n",
    "                signature=signature,\n",
    "                code_paths=[\"../src\"],\n",
    "            )\n",
    "    \n",
    "        logger.info(f\"✅ Successfully logged '{model_name}' and cleaned up temporary files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfee18b-a495-42c0-aaf4-a37a4d5009eb",
   "metadata": {},
   "source": [
    "## Step 4: Start Run, Log & Register Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9be0db90-8233-4b70-a3b9-1d46eb392744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-28 12:43:32 - INFO - --- Cleaning up VRAM before loading model from registry ---\n",
      "2025-07-28 12:43:32 - INFO - Deleted model and data objects from memory.\n",
      "2025-07-28 12:43:33 - INFO - ✅ VRAM cleaned and cache emptied.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# --- !!! ADD THIS NEW CELL BEFORE LOADING THE MODEL !!! ---\n",
    "# This cell explicitly deletes the large model objects and cleans up GPU memory.\n",
    "\n",
    "logger.info(\"--- Cleaning up VRAM before loading model from registry ---\")\n",
    "\n",
    "# Delete the objects holding GPU memory\n",
    "try:\n",
    "    del text_db\n",
    "    del image_db\n",
    "    del txt_embeddings\n",
    "    del siglip_embeddings\n",
    "    del splits # Also good to clean up large CPU objects\n",
    "    logger.info(\"Deleted model and data objects from memory.\")\n",
    "except NameError:\n",
    "    logger.warning(\"Some objects for cleanup were not found, they may have already been deleted.\")\n",
    "\n",
    "# Force Python's garbage collector to run\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# Now, clear the CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    logger.info(\"✅ VRAM cleaned and cache emptied.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97c7b785-0628-4358-8e8d-9d49a0d6f1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-28 12:43:33 - INFO - Started MLflow run: 219c5d20182445988e95d6e51dc51644\n",
      "2025-07-28 12:43:33 - INFO - --- Logging 'AIStudio-Multimodal-Chatbot-Model' to MLflow ---\n",
      "2025-07-28 12:43:33 - INFO - Created temporary directory for models: /tmp/tmp7l8z9drz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b84d7b11efd431792e6c55812ca4d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e409896b4314ad6adecebc7fdbdd36c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a03fcaee4c0413785439b1f9a601c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "982323a51e4149b09d5ba91f93a8ce33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a010575f0d45ae886eda0ccc4cdaf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54d22b55d9b54679be9d69cd7121fcc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6fa0cec05aa49a6a12d4815926de7f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5407df52809c4762b9262dd731469461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f14febb8672242c3a34c67bfb134fff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493c2966836c4a2f86ed8aaf5108f5e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/201 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-28 12:44:14 - INFO - ✅ Temporarily saved e5-large-v2 to /tmp/tmp7l8z9drz/e5-large-v2\n",
      "2025-07-28 12:44:19 - INFO - ✅ Temporarily saved SigLIP to /tmp/tmp7l8z9drz/siglip2-base-patch16-224\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e97b09a1de403d9553fb23d8a4ed2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e9b04e57f294b30ab0fb167761d1590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba56b36a35a740f4adcf797e1727e013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1237af6eb53b4170b717ee86463b19a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585391fb7ea64719b4a2a04519501914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/743 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1153663f68b4740b82709a9f942fab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-28 12:48:35 - INFO - ✅ Successfully logged 'AIStudio-Multimodal-Chatbot-Model' and cleaned up temporary files.\n",
      "2025-07-28 12:48:35 - INFO - Registering model from URI: runs:/219c5d20182445988e95d6e51dc51644/AIStudio-Multimodal-Chatbot-Model\n",
      "2025-07-28 12:48:35 - INFO - ✅ Successfully registered model 'AIStudio-Multimodal-Chatbot-Model'\n",
      "CPU times: user 6.08 s, sys: 49.1 s, total: 55.1 s\n",
      "Wall time: 5min 2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'AIStudio-Multimodal-Chatbot-Model' already exists. Creating a new version of this model...\n",
      "Created version '3' of model 'AIStudio-Multimodal-Chatbot-Model'.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# --- Start MLflow Run and Log the Model ---\n",
    "try:\n",
    "    with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "        run_id = run.info.run_id\n",
    "        logger.info(f\"Started MLflow run: {run_id}\")\n",
    "\n",
    "        # Use the class method to log the model and its artifacts\n",
    "        MultimodalRagModel.log_model(model_name=MODEL_NAME, local_model=LOCAL_MODEL)\n",
    "\n",
    "        model_uri = f\"runs:/{run_id}/{MODEL_NAME}\"\n",
    "        logger.info(f\"Registering model from URI: {model_uri}\")\n",
    "        \n",
    "        # Register the model in the MLflow Model Registry\n",
    "        mlflow.register_model(model_uri=model_uri, name=MODEL_NAME)\n",
    "        logger.info(f\"✅ Successfully registered model '{MODEL_NAME}'\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    logger.error(f\"Error: A required file or directory was not found. Please ensure the project structure is correct.\")\n",
    "    logger.error(f\"Details: {e}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred during the MLflow run: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c628e7f-9d9d-4e7a-9790-34ed1b003caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-28 12:48:35 - INFO - Found latest version '3' for model 'AIStudio-Multimodal-Chatbot-Model' in stage 'None'.\n"
     ]
    }
   ],
   "source": [
    "# --- Retrieve the latest version from the Model Registry ---\n",
    "try:\n",
    "    client = MlflowClient()\n",
    "    versions = client.get_latest_versions(MODEL_NAME, stages=[\"None\"])\n",
    "    if not versions:\n",
    "        raise RuntimeError(f\"No registered versions found for model '{MODEL_NAME}'.\")\n",
    "    \n",
    "    latest_version = versions[0]\n",
    "    logger.info(f\"Found latest version '{latest_version.version}' for model '{MODEL_NAME}' in stage '{latest_version.current_stage}'.\")\n",
    "    model_uri_registry = latest_version.source\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to retrieve model from registry: {e}\", exc_info=True)\n",
    "    model_uri_registry = None # Ensure variable exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c31856b3-5ff5-48b9-98b7-ddbf7dd16be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-28 12:48:35 - INFO - Loading model from: /phoenix/mlflow/747860200993380382/219c5d20182445988e95d6e51dc51644/artifacts/AIStudio-Multimodal-Chatbot-Model\n",
      "2025-07-28 12:48:35 - INFO - --- Initializing MultimodalRagModel context ---\n",
      "2025-07-28 12:48:35 - INFO - Running on device: cuda\n",
      "2025-07-28 12:48:35 - INFO - Artifacts loaded: model='/phoenix/mlflow/747860200993380382/219c5d20182445988e95d6e51dc51644/artifacts/AIStudio-Multimodal-Chatbot-Model/artifacts/InternVL3-8B-Instruct', chroma='/phoenix/mlflow/747860200993380382/219c5d20182445988e95d6e51dc51644/artifacts/AIStudio-Multimodal-Chatbot-Model/artifacts/chroma_store'\n",
      "2025-07-28 12:48:52 - INFO - ✅ Embedding models loaded from artifacts.\n",
      "2025-07-28 12:48:52 - INFO - Text DB count: 2615, Image DB count: 752\n",
      "2025-07-28 12:48:52 - INFO - Rebuilding BM25 index from ChromaDB documents...\n",
      "2025-07-28 12:48:52 - INFO - De-duplicated 2615 chunks down to 2604 unique chunks for BM25.\n",
      "2025-07-28 12:48:52 - INFO - ✅ BM25 index and document map are ready.\n",
      "2025-07-28 12:48:52 - INFO - Loading /phoenix/mlflow/747860200993380382/219c5d20182445988e95d6e51dc51644/artifacts/AIStudio-Multimodal-Chatbot-Model/artifacts/InternVL3-8B-Instruct...\n",
      "FlashAttention2 is not installed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e64010718a65420f840d00dc1e445c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-28 12:50:03 - INFO - Model loaded on cuda.\n",
      "2025-07-28 12:50:03 - INFO - Initializing evaluation judge by sharing the main model...\n",
      "2025-07-28 12:50:03 - INFO - ✅ Evaluation judge initialized successfully.\n",
      "2025-07-28 12:50:03 - INFO - --- Context initialization complete ---\n",
      "2025-07-28 12:50:03 - INFO - ✅ Successfully loaded model from registry.\n"
     ]
    }
   ],
   "source": [
    "if model_uri_registry:\n",
    "    try:\n",
    "        logger.info(f\"Loading model from: {model_uri_registry}\")\n",
    "        loaded_model = mlflow.pyfunc.load_model(model_uri=model_uri_registry)\n",
    "        logger.info(\"✅ Successfully loaded model from registry.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load model from registry URI: {e}\", exc_info=True)\n",
    "        loaded_model = None\n",
    "else:\n",
    "    logger.warning(\"Skipping model loading due to previous errors.\")\n",
    "    loaded_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2d9db6-b2dd-44e5-989d-ebb498372229",
   "metadata": {},
   "source": [
    "## Step 5: Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "279582c0-74e3-4b98-9d1d-6007c98f109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Function to Display Test Results ---\n",
    "def display_results(query: str, result_df: pd.DataFrame):\n",
    "    \"\"\"Helper to neatly print the query, reply, and display images.\"\"\"\n",
    "    if result_df.empty:\n",
    "        print(\"Received an empty result.\")\n",
    "        return\n",
    "\n",
    "    reply = result_df[\"reply\"].iloc[0]\n",
    "    image_paths_str = result_df[\"used_images\"].iloc[0]\n",
    "    image_paths = eval(image_paths_str) if isinstance(image_paths_str, str) and image_paths_str.startswith('[') else []\n",
    "    \n",
    "    gen_time = result_df[\"generation_time_seconds\"].iloc[0]\n",
    "    faithfulness = result_df[\"faithfulness\"].iloc[0]\n",
    "    relevance = result_df[\"relevance\"].iloc[0]\n",
    "\n",
    "    print(\"---\" * 20)\n",
    "    print(f\"❓ Query:\\n{query}\\n\")\n",
    "    print(f\"🤖 Reply:\\n{reply}\\n\")\n",
    "    \n",
    "    print(f\"📊 Faithfulness: {faithfulness:.4f} | Relevance: {relevance:.4f}\")\n",
    "    print(f\"⏱️ Generation Time: {gen_time:.2f}s\\n\")\n",
    "\n",
    "    if image_paths:\n",
    "        print(f\"🖼️ Displaying {len(image_paths)} retrieved image(s):\")\n",
    "        for path in image_paths:\n",
    "            print(f\" - {path}\")\n",
    "    else:\n",
    "        print(\"▶ No images were retrieved for this query.\")\n",
    "    print(\"---\" * 20 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa1c9dd-974c-475d-876c-e4621b03a202",
   "metadata": {},
   "outputs": [],
   "source": [
    "if loaded_model:\n",
    "    logger.info(\"Running sample inference with the loaded model...\")\n",
    "    \n",
    "    sample_queries = [\n",
    "        \"What are the AI Blueprints Repository best practices?\",\n",
    "        \"What are some feature flags that i can enable in AIStudio?\",\n",
    "        \"How do i manually clean my environment without hooh?\",\n",
    "    ]\n",
    "\n",
    "    for query in sample_queries:\n",
    "        try:\n",
    "            # --- MODIFIED LINE ---\n",
    "            # Add 'force_regenerate': False to the dictionary when creating the DataFrame\n",
    "            input_payload = pd.DataFrame([{\"query\": query, \"force_regenerate\": False}])\n",
    "            \n",
    "            result = loaded_model.predict(input_payload)\n",
    "\n",
    "            display_results(query, result)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Prediction failed for query '{query}': {e}\", exc_info=True)\n",
    "\n",
    "else:\n",
    "    logger.warning(\"Skipping sample inference because the model was not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83806df-79a8-4273-ab62-96c4bf1c2ded",
   "metadata": {},
   "source": [
    "## Step 6: Log Hallucinations & Relevance Evaluations to MlFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65476b0a-e314-44a6-8e51-856e237b00b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-28 12:50:13 - INFO - --- Reopening original run (219c5d20182445988e95d6e51dc51644) to log pre-computed evaluations ---\n",
      "2025-07-28 12:50:13 - INFO - Received prediction request.\n",
      "2025-07-28 12:50:13 - INFO - Processing query: 'What are the AI Blueprints Repository best practices?'\n",
      "2025-07-28 12:50:13 - INFO - Forced regeneration for query: 'What are the AI Blueprints Repository best practices?'. Clearing old cache entry.\n",
      "2025-07-28 12:50:13 - INFO - CACHE MISS for query: 'What are the AI Blueprints Repository best practices?'. Running full pipeline.\n",
      "Generate func:  ['../data/context/images/05_define_the_alert_2-91ada804-ab69-4cea-8144-91f8cca655ff.png', '../data/context/images/03_open_alerts-e0e5fa07-f652-46de-9412-5325f4016ca4.png', '../data/context/images/05_define_the_alert_1-77af4ef6-e5d5-4137-b450-98bbbab3b3c0.png', '../data/context/images/05_define_the_alert_3-7b6460b6-4bb0-4a09-8caf-b5ae7b026cc0.png']\n",
      "2025-07-28 12:50:19 - INFO - Processing query: 'What are some feature flags that i can enable in AIStudio?'\n",
      "2025-07-28 12:50:19 - INFO - Forced regeneration for query: 'What are some feature flags that i can enable in AIStudio?'. Clearing old cache entry.\n",
      "2025-07-28 12:50:19 - INFO - CACHE MISS for query: 'What are some feature flags that i can enable in AIStudio?'. Running full pipeline.\n",
      "Generate func:  ['../data/context/images/image-911c19ef-4b8d-485f-99d9-306ebea700a8.png', '../data/context/images/image-515d85ee-5a69-448b-90ae-22374815c34f.png']\n",
      "2025-07-28 12:50:23 - INFO - Processing query: 'How do i manually clean my environment without hooh?'\n",
      "2025-07-28 12:50:23 - INFO - Forced regeneration for query: 'How do i manually clean my environment without hooh?'. Clearing old cache entry.\n",
      "2025-07-28 12:50:23 - INFO - CACHE MISS for query: 'How do i manually clean my environment without hooh?'. Running full pipeline.\n",
      "Generate func:  ['../data/context/images/image-52d328df-14ce-41b9-a9c7-32d09ab04309.png', '../data/context/images/image-cadda90f-682d-4655-a6da-99cef82f80cb.png', '../data/context/images/job-container-integration-c5d8853f-c6f7-40ea-bdf1-bc551901bd41.png', '../data/context/images/image-fe32abea-eb8c-42a9-a052-061bbc4cd9f9.png']\n",
      "2025-07-28 12:50:48 - INFO - Successfully reopened existing run. Logging metrics and artifacts...\n",
      "2025-07-28 12:50:48 - INFO - ✅ Successfully logged metrics and artifacts to the original model run.\n"
     ]
    }
   ],
   "source": [
    "# In register_model.ipynb, add this as the final cell\n",
    "\n",
    "# Check if the model was loaded and the original run_id is available\n",
    "if loaded_model and 'run_id' in locals():\n",
    "    logger.info(f\"--- Reopening original run ({run_id}) to log pre-computed evaluations ---\")\n",
    "\n",
    "    # 1. Define your evaluation dataset\n",
    "    evaluation_payload = pd.DataFrame([\n",
    "        {\"query\": \"What are the AI Blueprints Repository best practices?\", \"force_regenerate\": True},\n",
    "        {\"query\": \"What are some feature flags that i can enable in AIStudio?\", \"force_regenerate\": True},\n",
    "        {\"query\": \"How do i manually clean my environment without hooh?\", \"force_regenerate\": True},\n",
    "    ])\n",
    "\n",
    "    # 2. Run predict() to get results with the embedded scores\n",
    "    results_df = loaded_model.predict(evaluation_payload)\n",
    "    \n",
    "    # Add the original query to the results for clarity in the logged table\n",
    "    results_df['query'] = evaluation_payload['query']\n",
    "\n",
    "    # 3. Reopen the existing run using its ID\n",
    "    with mlflow.start_run(run_id=run_id) as run:\n",
    "        logger.info(\"Successfully reopened existing run. Logging metrics and artifacts...\")\n",
    "\n",
    "        # 4. Calculate average scores from the DataFrame\n",
    "        avg_faithfulness = results_df[\"faithfulness\"].mean()\n",
    "        avg_relevance = results_df[\"relevance\"].mean()\n",
    "\n",
    "        # 5. Log the average scores as metrics to the original run\n",
    "        mlflow.log_metrics({\n",
    "            \"avg_faithfulness\": avg_faithfulness,\n",
    "            \"avg_relevance\": avg_relevance\n",
    "        })\n",
    "\n",
    "        # 6. Log the full results DataFrame as a table artifact to the original run\n",
    "        mlflow.log_table(data=results_df, artifact_file=\"inline_evaluation_results.json\")\n",
    "        \n",
    "        logger.info(\"✅ Successfully logged metrics and artifacts to the original model run.\")\n",
    "\n",
    "else:\n",
    "    logger.warning(\"Skipping logging because the model was not loaded or run_id was not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "725e1d0b-333c-43fd-95a7-5ce13fdfe3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-28 12:50:48 - INFO - ⏱️ Total execution time: 9m 19.73s\n",
      "2025-07-28 12:50:48 - INFO - ✅ Notebook execution completed.\n"
     ]
    }
   ],
   "source": [
    "end_time: float = time.time()\n",
    "elapsed_time: float = end_time - start_time\n",
    "elapsed_minutes: int = int(elapsed_time // 60)\n",
    "elapsed_seconds: float = elapsed_time % 60\n",
    "\n",
    "logger.info(f\"⏱️ Total execution time: {elapsed_minutes}m {elapsed_seconds:.2f}s\")\n",
    "logger.info(\"✅ Notebook execution completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e89eee-6171-446a-8da6-9381d3fe10c0",
   "metadata": {},
   "source": [
    "Built with ❤️ using Z by HP AI Studio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
