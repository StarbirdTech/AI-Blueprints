{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9ff4be-1fee-47eb-8d09-611c29a7a83f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h1 style=\"text-align: center; font-size: 50px;\">Multimodal RAG Chatbot with Langchain and ML Flow Evaluation</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbb3e26-125f-4151-9ed3-c84a3fd9081d",
   "metadata": {},
   "source": [
    "Retrieval-Augmented Generation (RAG) is an architectural approach that can enhance the effectiveness of large language model (LLM) applications using customized data. In this example, we use LangChain, an orchestrator for language pipelines, to build an assistant capable of loading information from a web page and use it for answering user questions. We'll also use the DeepEval platform to evaluate, observe and protect the LLM responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a55c55-a1dc-4697-b920-2a81f1bb0f74",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "- Imports\n",
    "- Configurations\n",
    "- Verify Assets\n",
    "- Data Loading\n",
    "- Creation of Chunks\n",
    "- Retrieval\n",
    "- Model Setup\n",
    "- Chain Creation\n",
    "- Model Service "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f78b36-56e2-4596-8325-c75901f30c76",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7156d10f-d930-4be7-a9e8-15606d466460",
   "metadata": {},
   "source": [
    "By using our Local GenAI workspace image, many of the necessary libraries to work with RAG already come pre-installed - in our case, we just need to add the connector to work with PDF documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e700f51-9011-401c-90d3-a07ea8238955",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77853772-0239-40d0-94be-7a62fcb465c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 20:14:55.547704: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-10 20:14:55.921847: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752178496.061168     373 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752178496.105455     373 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752178496.402931     373 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752178496.402958     373 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752178496.402960     373 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752178496.402961     373 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-10 20:14:56.427973: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/conda/envs/aistudio/lib/python3.12/site-packages/mlflow/pyfunc/utils/data_validation.py:168: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
      "  color_warning(\n",
      "/opt/conda/envs/aistudio/lib/python3.12/site-packages/mlflow/pyfunc/utils/data_validation.py:168: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
      "  color_warning(\n",
      "2025-07-10 20:15:01,415 - WARNING - USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# === Standard Library Imports ===\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pandas as pd\n",
    "import json\n",
    "# === MLflow integration ===\n",
    "import mlflow\n",
    "\n",
    "# Define the relative path to the 'core' directory (one level up from current working directory)\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "# === Import ChatbotService from project core ===\n",
    "from core.chatbot_service.chatbot_service import ChatbotService\n",
    "\n",
    "# === Third-Party Imports ===\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.schema.document import Document\n",
    "from langchain.document_loaders import WebBaseLoader, JSONLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import promptquality as pq\n",
    "import torch\n",
    "\n",
    "from transformers import SiglipProcessor, SiglipModel\n",
    "from llama_cpp import Llama\n",
    "# Define the relative path to the 'src' directory (one level up from current working directory)\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "# === Project-Specific Imports (from src) ===\n",
    "from src.local_genai_judge import LocalGenAIJudge\n",
    "from src.utils import (\n",
    "    load_config_and_secrets,\n",
    "    configure_proxy,\n",
    "    initialize_llm,\n",
    "    configure_hf_cache,\n",
    "    mlflow_evaluate_setup,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c43204-6cb2-48f3-ac72-f821f12585b8",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd3b8d2c-d0e1-4409-86dc-470c8454fff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "022490c4-8d8a-4c53-92ac-c62ea70a8209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logger\n",
    "logger = logging.getLogger(\"multimodal_rag_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", \n",
    "                              datefmt=\"%Y-%m-%d %H:%M:%S\") \n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75f2111d-5508-41ba-967c-256b40a9fb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"../configs/config.yaml\"\n",
    "SECRETS_PATH = \"../configs/secrets.yaml\"\n",
    "DATA_PATH = \"../data\"\n",
    "IMAGE_DIR = os.path.join(DATA_PATH, \"images\")  # PNG/JPGs\n",
    "MM_JSON = os.path.join(DATA_PATH, \"wiki_flat_structure.json\")\n",
    "\n",
    "MLFLOW_EXPERIMENT_NAME = \"AIStudio-Multimodal-Chatbot-Experiment\"\n",
    "MLFLOW_RUN_NAME = \"AIStudio-Multimodal-Chatbot-Run\"\n",
    "\n",
    "INTERNVL_MODEL_PATH = \"/home/jovyan/datafabric/InternVL3-8B-InstructQ8_0/InternVL3-8B-Instruct-Q8_0.gguf\"\n",
    "MM_PROJ_PATH = \"/home/jovyan/datafabric/mmproj-InternVL3-8B-InstructQ8_0/mmproj-InternVL3-8B-Instruct-Q8_0.gguf\"\n",
    "\n",
    "DEMO_FOLDER = \"../demo\"\n",
    "MLFLOW_MODEL_NAME = \"AIStudio-Multimodal-Chatbot-Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7384967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e411a33e-d7e2-4029-b2b9-94ea300505c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU off-load supported: False\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import llama_supports_gpu_offload\n",
    "print(\"GPU off-load supported:\", llama_supports_gpu_offload())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f912d15a-7aab-4620-8c89-33c8f07104f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 20:25:19 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "logger.info('Notebook execution started.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ca6361-dd3c-4778-9a9d-b11da7b20151",
   "metadata": {},
   "source": [
    "## Configuration of HuggingFace caches\n",
    "\n",
    "In the next cell, we configure HuggingFace cache, so that all the models downloaded from them are persisted locally, even after the workspace is closed. This is a future desired feature for AI Studio and the GenAI addon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "965e4ea4-084a-4f32-9f51-9ead89dd16fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure HuggingFace cache\n",
    "configure_hf_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41d47161-f197-409b-9a43-a3f59f1f695c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 20:25:19,460 - INFO - Use pytorch device_name: cuda:0\n",
      "2025-07-10 20:25:19,461 - INFO - Load pretrained SentenceTransformer: intfloat/e5-large-v2\n"
     ]
    }
   ],
   "source": [
    "# Initialize HuggingFace Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/e5-large-v2\",\n",
    "    cache_folder=\"/tmp/hf_cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7062ee0",
   "metadata": {},
   "source": [
    "## Configuration and Secrets Loading\n",
    "\n",
    "In this section, we load configuration parameters and API keys from separate YAML files. This separation helps maintain security by keeping sensitive information (API keys) separate from configuration settings.\n",
    "\n",
    "- **config.yaml**: Contains non-sensitive configuration parameters like model sources and URLs\n",
    "- **secrets.yaml**: Contains sensitive API keys for services like Galileo and HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e0b7464-ad76-47f2-94f0-e70c4d4cc380",
   "metadata": {},
   "outputs": [],
   "source": [
    "config, secrets = load_config_and_secrets(CONFIG_PATH, SECRETS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e80960-54f9-4ec2-b062-6d49526494d1",
   "metadata": {},
   "source": [
    "# Verify Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4417e48c-2b51-4e61-b74d-8230caf2f7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 20:25:23 - INFO - Config is properly configured. \n",
      "2025-07-10 20:25:23 - INFO - Secrets is properly configured. \n",
      "2025-07-10 20:25:23 - INFO - Local InternVL-8B model is properly configured. \n",
      "2025-07-10 20:25:23 - INFO - Vision projector (.gguf) is properly configured. \n",
      "2025-07-10 20:25:23 - INFO - wiki_flat_structure.json is properly configured. \n"
     ]
    }
   ],
   "source": [
    "def log_asset_status(asset_path: str, asset_name: str, success_message: str, failure_message: str) -> None:\n",
    "    \"\"\"\n",
    "    Logs the status of a given asset based on its existence.\n",
    "\n",
    "    Parameters:\n",
    "        asset_path (str): File or directory path to check.\n",
    "        asset_name (str): Name of the asset for logging context.\n",
    "        success_message (str): Message to log if asset exists.\n",
    "        failure_message (str): Message to log if asset does not exist.\n",
    "    \"\"\"\n",
    "    if Path(asset_path).exists():\n",
    "        logger.info(f\"{asset_name} is properly configured. {success_message}\")\n",
    "    else:\n",
    "        logger.info(f\"{asset_name} is not properly configured. {failure_message}\")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=CONFIG_PATH,\n",
    "    asset_name=\"Config\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if the configs.yaml was propely connfigured in your project on AI Studio.\"\n",
    ")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=SECRETS_PATH,\n",
    "    asset_name=\"Secrets\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if the secrets.yaml was propely connfigured in your project on AI Studio.\"\n",
    ")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=INTERNVL_MODEL_PATH,\n",
    "    asset_name=\"Local InternVL-8B model\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please create and download the required assets in your project on AI Studio if you want to use local model.\")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=MM_PROJ_PATH,\n",
    "    asset_name=\"Vision projector (.gguf)\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Download mmproj-InternVL3-8B-Instruct-Q8_0.gguf\")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=MM_JSON,\n",
    "    asset_name=\"wiki_flat_structure.json\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Place JSON Wiki Pages in data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d92659e-23ea-4965-bb27-58cd1dd0f3ec",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "\n",
    "In this step, we will use the Langchain framework to  extract the content from a local PDF file with the product documentation. Also, we have commented some example on how to use Web Loaders to load data from pages on the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb4c1589-285f-4e72-ab8e-5ae9fbd4043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mm_docs(json_path: str) -> List[Document]:\n",
    "    with open(json_path, encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    docs = []\n",
    "    for row in data:\n",
    "        meta = {\"source\": row[\"path\"], \"images\": row.get(\"images\", [])}\n",
    "        docs.append(Document(page_content=row[\"content\"], metadata=meta))\n",
    "    return docs\n",
    "\n",
    "mm_raw_docs = load_mm_docs(MM_JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc63ed05-192e-4f59-856c-cf9bff160fc8",
   "metadata": {},
   "source": [
    "# Creation of Chunks\n",
    "Here, we split the loaded documents into chunks, so we have smaller and more specific texts to add to our vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3dbd381-58b8-4569-aa75-b26d07556b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 20:25:23 - INFO - Chunks created: 4089 docs, avg_tokens=412\n"
     ]
    }
   ],
   "source": [
    "# === Initialize text splitter ===\n",
    "# - chunk_size: Maximum number of characters per text chunk.\n",
    "# - chunk_overlap: Number of overlapping characters between chunks.\n",
    "\n",
    "def chunk_documents(docs, chunk_size=600, overlap=100):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "        separators=[\"\\n## \", \"\\n# \", \"\\n\\n\", \".\", \"!\", \"?\"]\n",
    "    )\n",
    "    splits = splitter.split_documents(docs)\n",
    "    return splits\n",
    "\n",
    "splits = chunk_documents(mm_raw_docs)\n",
    "def log_stage(name: str, docs: List[Document]):\n",
    "    logger.info(f\"{name}: {len(docs)} docs, avg_tokens={sum(len(d.page_content) for d in docs)/len(docs):.0f}\")\n",
    "# e.g. after splits\n",
    "log_stage(\"Chunks created\", splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7999ff1f-1f6a-452c-848e-e3c67adf7766",
   "metadata": {},
   "source": [
    "# Retrieval\n",
    "\n",
    "We transform the texts into embeddings and store them in a vector database. This allows us to perform similarity search, and proper retrieval of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e43ae88-252a-4cbe-96e4-081daa3dc25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 20:25:23,677 - ERROR - Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "2025-07-10 20:25:23,679 - ERROR - Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "2025-07-10 20:27:45 - INFO - Text collection ready: 8178 vectors\n"
     ]
    }
   ],
   "source": [
    "from chromadb.config import Settings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "from copy import deepcopy\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from PIL import Image\n",
    "\n",
    "CHROMA_SETTINGS = Settings(anonymized_telemetry=False)\n",
    "clean_chunks = filter_complex_metadata(deepcopy(splits))\n",
    "\n",
    "# TEXT collection  (e5-large embeddings)  ― already have `clean_chunks`\n",
    "\n",
    "text_db = Chroma.from_documents(\n",
    "    documents=clean_chunks,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"wiki_text_mm\",\n",
    "    client_settings=CHROMA_SETTINGS,\n",
    ")\n",
    "logger.info(\"Text collection ready: %d vectors\", text_db._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78339655-78ad-4458-ab9a-bc156a549afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipEmbeddings(Embeddings):\n",
    "    def __init__(self,\n",
    "                 model_id: str = \"google/siglip2-base-patch16-224\",\n",
    "                 device: str | None = None):\n",
    "        self.device   = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model    = SiglipModel.from_pretrained(model_id).to(self.device)\n",
    "        self.processor = SiglipProcessor.from_pretrained(model_id)\n",
    "\n",
    "    # --- private helpers ---------------------------------------------------\n",
    "    def _embed_text(self, texts: List[str]):\n",
    "        inp = self.processor(text=texts, return_tensors=\"pt\",\n",
    "                             padding=True, truncation=True).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            return self.model.get_text_features(**inp).cpu().numpy()\n",
    "\n",
    "    def _embed_imgs(self, paths: List[str]):\n",
    "        imgs = [Image.open(p).convert(\"RGB\") for p in paths]\n",
    "        inp  = self.processor(images=imgs, return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            return self.model.get_image_features(**inp).cpu().numpy()\n",
    "\n",
    "    # --- LangChain --------------------------------------------------------\n",
    "    def embed_documents(self, docs: List[str]) -> List[List[float]]:\n",
    "        return self._embed_imgs(docs).tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self._embed_text([text])[0].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9058d95-8693-4a5b-ab32-3f6f8440084f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 20:27:50,190 - ERROR - Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "2025-07-10 20:27:50,192 - ERROR - Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    }
   ],
   "source": [
    "siglip_embeddings = SiglipEmbeddings()\n",
    "image_db = Chroma(\n",
    "    collection_name=\"wiki_image_mm\",\n",
    "    embedding_function=siglip_embeddings,\n",
    "    client_settings=CHROMA_SETTINGS,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58329084-640e-4b25-be70-ac1ec90467e9",
   "metadata": {},
   "source": [
    "# Model Setup\n",
    "\n",
    "In this notebook, we provide three different options for loading the model:\n",
    " * **local**: by loading the llama3.1-8b-instruct-Q8_0 model from the asset downloaded on the project\n",
    " * **hugging-face-local** by downloading a DeepSeek model from Hugging Face and running locally\n",
    " * **hugging-face-cloud** by accessing the Mistral model through Hugging Face cloud API (requires HuggingFace API key saved on secrets.yaml)\n",
    "\n",
    "This choice can be set in the config.yaml file. The model deployed on the bottom cells of this notebook will load the choice from the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "65b1fa71-43ae-49e1-9f2f-ad730a00ca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_source = config[\"model_source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "774e18a8-bef4-4b67-9972-ceda2af6538d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (8192) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.09 s, sys: 28.2 s, total: 30.3 s\n",
      "Wall time: 27min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "llm_mm = Llama(\n",
    "    model_path   = INTERNVL_MODEL_PATH,   # 8-B model .gguf\n",
    "    mmproj_path  = MM_PROJ_PATH,          # projector .gguf  ← the key line\n",
    "    chat_format  = \"internvl\",\n",
    "    n_gpu_layers = -1,\n",
    "    n_ctx        = 8192,\n",
    "    n_batch      = 256,\n",
    "    f16_kv       = True,\n",
    "    verbose      = False,\n",
    ")\n",
    "\n",
    "\n",
    "# llm = initialize_llm(model_source, secrets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81f5d66-d99d-4851-a48e-248b2e81e2c9",
   "metadata": {},
   "source": [
    "# Chain Creation\n",
    "In this part, we define a pipeline that receives a question and context, formats the context documents, and uses a Hugging Face (Mistral) chat model to answer the question based on the provided context. The output is then formatted as a string for easy reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b9c99b13-eea1-421e-a257-80981055b802",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Runnable' from 'langchain' (/opt/conda/envs/aistudio/lib/python3.12/site-packages/langchain/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -------------------------------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# ### ⬇ NEW  Multimodal Chain Creation ------------\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# -------------------------------------------------\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Runnable\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_b64\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'Runnable' from 'langchain' (/opt/conda/envs/aistudio/lib/python3.12/site-packages/langchain/__init__.py)"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------\n",
    "# ### ⬇ NEW  Multimodal Chain Creation ------------\n",
    "# -------------------------------------------------\n",
    "from langchain import Runnable\n",
    "\n",
    "def _b64(path: str) -> str:\n",
    "    with open(path, \"rb\") as f:\n",
    "        return base64.b64encode(f.read()).decode()\n",
    "\n",
    "def retrieve_mm(query: str, k_txt=4, k_img=4):\n",
    "    # text\n",
    "    txt_docs = text_db.similarity_search(\n",
    "        query, k=k_txt, search_type=\"mmr\", fetch_k=20)\n",
    "    # image\n",
    "    q_emb   = siglip_text_embed([query])[0]\n",
    "    img_hits = image_coll.query(query_embeddings=[q_emb], n_results=k_img)\n",
    "    img_paths = img_hits[\"documents\"][0]\n",
    "    return {\"docs\": txt_docs, \"images\": img_paths}\n",
    "\n",
    "def build_messages(inp: Dict):\n",
    "    context = \"\\n\\n\".join(d.page_content for d in inp[\"docs\"])\n",
    "    images  = [{\"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/png;base64,{_b64(p)}\"}}\n",
    "               for p in inp[\"images\"]]\n",
    "\n",
    "    msgs = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": (\"You are a technical assistant for HP’s Z by HP AI Studio.\\n\"\n",
    "                     \"Answer **only** from the <context> and the images; \"\n",
    "                     \"if the answer is missing reply: \"\n",
    "                     \"\\\"I don’t have that information in the wiki yet.\\\"\")},\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": [{\"type\": \"text\",\n",
    "                      \"text\": f\"<context>\\n{context}\\n</context>\\n\\n{inp['query']}\"}] + images}\n",
    "    ]\n",
    "    return msgs\n",
    "\n",
    "def call_llm(msgs):\n",
    "    res = llm_mm.create_chat_completion(\n",
    "        messages   = msgs,\n",
    "        max_tokens = 512,\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return res[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# LangChain Runnable pipeline\n",
    "mm_chain: Runnable = (\n",
    "    {\n",
    "        \"query\": RunnablePassthrough(),\n",
    "        # retriever returns dict with docs & images\n",
    "        **{\"retrieved\": RunnableLambda(lambda q: retrieve_mm(q))},\n",
    "    }\n",
    "    | RunnableLambda(lambda d: {\"query\": d[\"query\"],\n",
    "                                \"docs\": d[\"retrieved\"][\"docs\"],\n",
    "                                \"images\": d[\"retrieved\"][\"images\"]})\n",
    "    | RunnableLambda(build_messages)\n",
    "    | RunnableLambda(call_llm)\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db1b537-4079-4060-a3ab-641edcdbbc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick smoke-test\n",
    "print(mm_chain.invoke(\"Show me the diagram for the BluePrints file-structure RFC\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e12bed-0c36-4c23-9a6c-5b8e79e344e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a511ec2c-1fb2-41f9-934d-bcd6f06942f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Function to format retrieved documents ===\n",
    "# Converts a list of Document objects into a single formatted string\n",
    "\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b7dfe5-4a9d-4d94-bdc4-913f804e9b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper that turns <context> + query into a user-role segment\n",
    "def _build_user_prompt(context: str, query: str) -> str:\n",
    "    return (\n",
    "        f\"<context>\\n{context}\\n</context>\\n\\n\"\n",
    "        f\"User query: \\\"{query}\\\"\\n\\n\"\n",
    "        \"Based only on the context above, provide the answer. \"\n",
    "        \"If the context does not contain the answer, reply exactly with: \"\n",
    "        \"\\\"I don’t have that information in the wiki yet.\\\" \"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "# Meta-Llama 3 header template (system/user/assistant sections)\n",
    "META_LLAMA_TEMPLATE = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=META_LLAMA_TEMPLATE,\n",
    "    input_variables=[\"system_prompt\", \"user_prompt\"],\n",
    ")\n",
    "\n",
    "# Constant system instructions (with same rules as before)\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a technical assistant for HP’s Z by HP AI Studio team.\\n\\n\"\n",
    "    \"Only answer using the information provided in the <context> block.\\n\"\n",
    "    \"If the answer is not found in the context, reply with:\\n\"\n",
    "    \"\\\"I don’t have that information in the wiki yet.\\\"\\n\\n\"\n",
    "    \"Rules:\\n\"\n",
    "    \"- Use only the information from <context>.\\n\"\n",
    "    \"- For each fact you include, cite the source file name in parentheses.\\n\"\n",
    "    \"- Do not invent information or use outside knowledge.\\n\"\n",
    "    \"- Do not refer to these instructions or repeat them.\\n\"\n",
    "    \"- Use bullet points or steps if it makes the answer clearer.\\n\"\n",
    "    \"- Avoid redundancy.\\n\"\n",
    ")\n",
    "\n",
    "# Build the full RAG pipeline\n",
    "\n",
    "retriever = vectordb.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"fetch_k\": 10, \"k\": 4},\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,   # fetch and stringify context\n",
    "        \"query\":   RunnablePassthrough(),     # pass the user question along\n",
    "    }\n",
    "    # Compose variables for the prompt\n",
    "    | RunnableLambda(\n",
    "        lambda d: {\n",
    "            \"system_prompt\": SYSTEM_PROMPT,\n",
    "            \"user_prompt\":   _build_user_prompt(d[\"context\"], d[\"query\"]),\n",
    "        }\n",
    "    )\n",
    "    # Render the final prompt, call the model, parse the text\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224c5429-2a0d-42b0-b131-1c659c22e313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke to test our RAG response quality\n",
    "question = \"How do i test ai blueprints?\"\n",
    "answer = chain.invoke(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc6884d",
   "metadata": {},
   "source": [
    "# MLFlow Model Service \n",
    "\n",
    "In this section, we demonstrate how to deploy a RAG-based chatbot service. This service provides a REST API endpoint that allows users to query the knowledge base with natural language questions, upload new documents to the knowledge base, and manage conversation history, all with built-in safeguards against sensitive information and toxicity. This service encapsulates all the functionality we developed in this notebook, including the document retrieval system, RAG-based question answering capabilities, and Galileo integration for protection, observation and evaluation. It demonstrates how to use our ChatbotService from the src/service directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ac9d9e-1f10-482d-bdbb-01151d1797e0",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2dd61a-ea19-44ba-8932-3f73c60b97ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_evaluate_setup(\n",
    "    secrets,\n",
    "    mlflow_tracking_uri=\"/phoenix/mlflow\"   # your AI Studio MLflow endpoint\n",
    ")\n",
    "\n",
    "# === Set MLflow experiment context ===\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\n",
    "\n",
    "# === Validate local model file path ===\n",
    "if not os.path.exists(LOCAL_MODEL_PATH):\n",
    "    logger.info(f\"⚠️ Warning: Model file not found at {LOCAL_MODEL_PATH}. Please verify the path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee90e8d-e548-453e-902f-5856c5065be4",
   "metadata": {},
   "source": [
    "## Log & Register Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867f62bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Log and register model to MLflow ===\n",
    "with mlflow.start_run(run_name=MLFLOW_RUN_NAME) as run:\n",
    "    \n",
    "    # Log model artifacts using custom ChatbotService\n",
    "    ChatbotService.log_model(\n",
    "        artifact_path=MLFLOW_MODEL_NAME,\n",
    "        config_path=CONFIG_PATH,\n",
    "        secrets_path=SECRETS_PATH,\n",
    "        docs_path=DATA_PATH,\n",
    "        model_path=LOCAL_MODEL_PATH,\n",
    "        demo_folder=DEMO_FOLDER\n",
    "    )\n",
    "\n",
    "    # Construct the URI for the logged model\n",
    "    model_uri = f\"runs:/{run.info.run_id}/{MLFLOW_MODEL_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52dca2d-7110-4917-ac41-4e49e4dce60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the model into MLflow Model Registry\n",
    "mlflow.register_model(\n",
    "    model_uri=model_uri,\n",
    "    name=MLFLOW_MODEL_NAME\n",
    ")\n",
    "\n",
    "logger.info(f\"✅ Model registered successfully with run ID: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176a68a5-ff0d-42bc-8d48-9460a08f8941",
   "metadata": {},
   "source": [
    "## Evaluate Hallucination, Answer Relevance, Context Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b676cd-2b99-4fa6-8fd0-d47f571d6c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model(batch_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    preds, contexts = [], []\n",
    "    for q in batch_df[\"questions\"]:\n",
    "        answer = chain.invoke(q)\n",
    "        preds.append(answer)\n",
    "\n",
    "        docs = retriever.get_relevant_documents(q)\n",
    "        contexts.append(\" \".join(d.page_content for d in docs))\n",
    "\n",
    "    # keep the incoming index so every batch’s rows stay unique\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"result\": preds,\n",
    "            \"source_documents\": contexts,\n",
    "        },\n",
    "        index=batch_df.index,      #  ← key line\n",
    "    )\n",
    "\n",
    "# --- 3)  Evaluation dataset\n",
    "eval_df = pd.DataFrame({\"questions\": [\n",
    "    \"What naming convention should I use for a new blueprint project folder?\",\n",
    "    \"What is the first step in the standard blueprint testing workflow?\",\n",
    "    \"How do I fetch logs from a running Kubernetes pod?\",\n",
    "]})\n",
    "\n",
    "judge = LocalGenAIJudge(\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "faithfulness_metric = judge.to_mlflow_metric(\"faithfulness\")\n",
    "relevance_metric = judge.to_mlflow_metric(\"relevance\")\n",
    "\n",
    "results = mlflow.evaluate(\n",
    "    model,\n",
    "    eval_df,\n",
    "    predictions=\"result\",\n",
    "    evaluators=\"default\",\n",
    "    extra_metrics=[faithfulness_metric, relevance_metric],\n",
    "    evaluator_config={\n",
    "        \"col_mapping\": {\n",
    "            \"inputs\": \"questions\",\n",
    "            \"context\": \"source_documents\"\n",
    "        }\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8147f34-6e3b-47f2-9a68-7e8077f71618",
   "metadata": {},
   "source": [
    "Built with ❤️ using Z by HP AI Studio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aistudio]",
   "language": "python",
   "name": "conda-env-aistudio-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
