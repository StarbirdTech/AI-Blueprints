{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9ff4be-1fee-47eb-8d09-611c29a7a83f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h1 style=\"text-align: center; font-size: 50px;\">Multimodal RAG Chatbot with Langchain and ML Flow Evaluation</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbb3e26-125f-4151-9ed3-c84a3fd9081d",
   "metadata": {},
   "source": [
    "Retrieval-Augmented Generation (RAG) is an architectural approach that can enhance the effectiveness of large language model (LLM) applications using customized data. In this example, we use LangChain, an orchestrator for language pipelines, to build an assistant capable of loading information from a web page and use it for answering user questions. We'll also use the DeepEval platform to evaluate, observe and protect the LLM responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a55c55-a1dc-4697-b920-2a81f1bb0f74",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "- Imports\n",
    "- Configurations\n",
    "- Verify Assets\n",
    "- Data Loading\n",
    "- Creation of Chunks\n",
    "- Retrieval\n",
    "- Model Setup\n",
    "- Chain Creation\n",
    "- Model Service "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f78b36-56e2-4596-8325-c75901f30c76",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7156d10f-d930-4be7-a9e8-15606d466460",
   "metadata": {},
   "source": [
    "By using our Local GenAI workspace image, many of the necessary libraries to work with RAG already come pre-installed - in our case, we just need to add the connector to work with PDF documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e700f51-9011-401c-90d3-a07ea8238955",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77853772-0239-40d0-94be-7a62fcb465c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 17:33:46.468060: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-11 17:33:46.541332: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752255226.573276    2267 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752255226.581964    2267 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752255226.625957    2267 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752255226.625983    2267 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752255226.625985    2267 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752255226.625986    2267 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-11 17:33:46.637710: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/conda/envs/aistudio/lib/python3.12/site-packages/mlflow/pyfunc/utils/data_validation.py:168: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
      "  color_warning(\n",
      "/opt/conda/envs/aistudio/lib/python3.12/site-packages/mlflow/pyfunc/utils/data_validation.py:168: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
      "  color_warning(\n",
      "2025-07-11 17:33:49,006 - WARNING - USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# === Standard Library Imports ===\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pandas as pd\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# === MLflow integration ===\n",
    "import mlflow\n",
    "\n",
    "# Define the relative path to the 'core' directory (one level up from current working directory)\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "# === Import ChatbotService from project core ===\n",
    "from core.chatbot_service.chatbot_service import ChatbotService\n",
    "\n",
    "# === Third-Party Imports ===\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import Runnable, RunnablePassthrough, RunnableLambda\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.document import Document\n",
    "from langchain.document_loaders import WebBaseLoader, JSONLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "import promptquality as pq\n",
    "import torch\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import base64, os, mimetypes\n",
    "from chromadb.config import Settings\n",
    "from transformers import SiglipProcessor, SiglipModel\n",
    "from llama_cpp import Llama\n",
    "from llama_cpp import llama_supports_gpu_offload\n",
    "\n",
    "# Define the relative path to the 'src' directory (one level up from current working directory)\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "# === Project-Specific Imports (from src) ===\n",
    "from src.local_genai_judge import LocalGenAIJudge\n",
    "from src.utils import (\n",
    "    load_config_and_secrets,\n",
    "    configure_proxy,\n",
    "    initialize_llm,\n",
    "    configure_hf_cache,\n",
    "    mlflow_evaluate_setup,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c43204-6cb2-48f3-ac72-f821f12585b8",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd3b8d2c-d0e1-4409-86dc-470c8454fff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "022490c4-8d8a-4c53-92ac-c62ea70a8209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logger\n",
    "logger = logging.getLogger(\"multimodal_rag_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", \n",
    "                              datefmt=\"%Y-%m-%d %H:%M:%S\") \n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75f2111d-5508-41ba-967c-256b40a9fb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"../configs/config.yaml\"\n",
    "SECRETS_PATH = \"../configs/secrets.yaml\"\n",
    "DATA_PATH = \"../data\"\n",
    "IMAGE_DIR = os.path.join(DATA_PATH, \"images\")  # PNG/JPGs\n",
    "MM_JSON = os.path.join(DATA_PATH, \"wiki_flat_structure.json\")\n",
    "\n",
    "MLFLOW_EXPERIMENT_NAME = \"AIStudio-Multimodal-Chatbot-Experiment\"\n",
    "MLFLOW_RUN_NAME = \"AIStudio-Multimodal-Chatbot-Run\"\n",
    "\n",
    "LOCAL_MODEL_PATH = \"/home/jovyan/datafabric/llama3.1-8b-instruct/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\"\n",
    "INTERNVL_MODEL_PATH = \"/home/jovyan/datafabric/InternVL3-8B-Instruct-Q8_0/InternVL3-8B-Instruct-Q8_0.gguf\"\n",
    "MM_PROJ_PATH = \"/home/jovyan/datafabric/mmproj-InternVL3-8B-Instruct-Q8_0/mmproj-InternVL3-8B-Instruct-Q8_0.gguf\"\n",
    "\n",
    "DEMO_FOLDER = \"../demo\"\n",
    "MLFLOW_MODEL_NAME = \"AIStudio-Multimodal-Chatbot-Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7384967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e411a33e-d7e2-4029-b2b9-94ea300505c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4070, compute capability 8.9, VMM: yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU off-load supported: True\n"
     ]
    }
   ],
   "source": [
    "print(\"GPU off-load supported:\", llama_supports_gpu_offload())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f912d15a-7aab-4620-8c89-33c8f07104f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 17:33:49 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "logger.info('Notebook execution started.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ca6361-dd3c-4778-9a9d-b11da7b20151",
   "metadata": {},
   "source": [
    "## Configuration of HuggingFace caches\n",
    "\n",
    "In the next cell, we configure HuggingFace cache, so that all the models downloaded from them are persisted locally, even after the workspace is closed. This is a future desired feature for AI Studio and the GenAI addon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "965e4ea4-084a-4f32-9f51-9ead89dd16fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure HuggingFace cache\n",
    "configure_hf_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41d47161-f197-409b-9a43-a3f59f1f695c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 17:33:50,173 - INFO - Use pytorch device_name: cuda:0\n",
      "2025-07-11 17:33:50,173 - INFO - Load pretrained SentenceTransformer: intfloat/e5-large-v2\n"
     ]
    }
   ],
   "source": [
    "# Initialize HuggingFace Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/e5-large-v2\",\n",
    "    cache_folder=\"/tmp/hf_cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7062ee0",
   "metadata": {},
   "source": [
    "## Configuration and Secrets Loading\n",
    "\n",
    "In this section, we load configuration parameters and API keys from separate YAML files. This separation helps maintain security by keeping sensitive information (API keys) separate from configuration settings.\n",
    "\n",
    "- **config.yaml**: Contains non-sensitive configuration parameters like model sources and URLs\n",
    "- **secrets.yaml**: Contains sensitive API keys for services like Galileo and HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e0b7464-ad76-47f2-94f0-e70c4d4cc380",
   "metadata": {},
   "outputs": [],
   "source": [
    "config, secrets = load_config_and_secrets(CONFIG_PATH, SECRETS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e80960-54f9-4ec2-b062-6d49526494d1",
   "metadata": {},
   "source": [
    "# Verify Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4417e48c-2b51-4e61-b74d-8230caf2f7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 17:33:53 - INFO - Config is properly configured. \n",
      "2025-07-11 17:33:53 - INFO - Secrets is properly configured. \n",
      "2025-07-11 17:33:53 - INFO - Local InternVL-8B model is properly configured. \n",
      "2025-07-11 17:33:53 - INFO - Vision projector (.gguf) is properly configured. \n",
      "2025-07-11 17:33:53 - INFO - wiki_flat_structure.json is properly configured. \n"
     ]
    }
   ],
   "source": [
    "def log_asset_status(asset_path: str, asset_name: str, success_message: str, failure_message: str) -> None:\n",
    "    \"\"\"\n",
    "    Logs the status of a given asset based on its existence.\n",
    "\n",
    "    Parameters:\n",
    "        asset_path (str): File or directory path to check.\n",
    "        asset_name (str): Name of the asset for logging context.\n",
    "        success_message (str): Message to log if asset exists.\n",
    "        failure_message (str): Message to log if asset does not exist.\n",
    "    \"\"\"\n",
    "    if Path(asset_path).exists():\n",
    "        logger.info(f\"{asset_name} is properly configured. {success_message}\")\n",
    "    else:\n",
    "        logger.info(f\"{asset_name} is not properly configured. {failure_message}\")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=CONFIG_PATH,\n",
    "    asset_name=\"Config\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if the configs.yaml was propely connfigured in your project on AI Studio.\"\n",
    ")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=SECRETS_PATH,\n",
    "    asset_name=\"Secrets\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if the secrets.yaml was propely connfigured in your project on AI Studio.\"\n",
    ")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=INTERNVL_MODEL_PATH,\n",
    "    asset_name=\"Local InternVL-8B model\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please create and download the required assets in your project on AI Studio if you want to use local model.\")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=MM_PROJ_PATH,\n",
    "    asset_name=\"Vision projector (.gguf)\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Download mmproj-InternVL3-8B-Instruct-Q8_0.gguf\")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=MM_JSON,\n",
    "    asset_name=\"wiki_flat_structure.json\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Place JSON Wiki Pages in data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d92659e-23ea-4965-bb27-58cd1dd0f3ec",
   "metadata": {},
   "source": [
    "# Data Loading & Cleaning\n",
    "\n",
    "We load wiki-pages from `wiki_flat_structure.json`, but:\n",
    "* remove any image name that  \n",
    "  – is empty / `None`  \n",
    "  – contains invalid characters (e.g. the `==image_0==` placeholders)  \n",
    "  – has an extension not in {png, jpg, jpeg, webp, gif}  \n",
    "  – points to a file that does **not** exist in `data/images/`\n",
    "* log every discarded image so we can fix the parser later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb4c1589-285f-4e72-ab8e-5ae9fbd4043d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 17:33:54 - WARNING - ⚠️ 127 broken image refs filtered out\n",
      "2025-07-11 17:33:54 - INFO - Docs loaded: 548 docs, avg_tokens=3076\n"
     ]
    }
   ],
   "source": [
    "VALID_EXTS = {\".png\", \".jpg\", \".jpeg\", \".webp\", \".gif\"}\n",
    "\n",
    "def load_mm_docs_clean(json_path: str, img_dir: Path) -> list[Document]:\n",
    "    bad_imgs, docs = [], []\n",
    "    with open(json_path, encoding=\"utf-8\") as f:\n",
    "        rows = json.load(f)\n",
    "\n",
    "    for row in rows:\n",
    "        images_ok: list[str] = []\n",
    "        for name in row.get(\"images\", []):\n",
    "            # -- basic sanity checks --------------------------------------------------\n",
    "            if (not name) or (\"==\" in name):\n",
    "                bad_imgs.append((row[\"path\"], name, \"placeholder/empty\"))\n",
    "                continue\n",
    "            ext = Path(name).suffix.lower()\n",
    "            if ext not in VALID_EXTS:\n",
    "                bad_imgs.append((row[\"path\"], name, f\"invalid ext {ext}\"))\n",
    "                continue\n",
    "            img_path = img_dir / name\n",
    "            if not img_path.is_file():\n",
    "                bad_imgs.append((row[\"path\"], name, \"missing file\"))\n",
    "                continue\n",
    "            images_ok.append(name)\n",
    "\n",
    "        meta = {\"source\": row[\"path\"], \"images\": images_ok}\n",
    "        docs.append(Document(page_content=row[\"content\"], metadata=meta))\n",
    "\n",
    "    # -------- logging summary ------------------------------------------------------\n",
    "    if bad_imgs:\n",
    "        logger.warning(\"⚠️ %d broken image refs filtered out\", len(bad_imgs))\n",
    "        for src, name, reason in bad_imgs[:20]:                 # first 20 only\n",
    "            logger.debug(\"  » %s → %s (%s)\", src, name, reason)\n",
    "    else:\n",
    "        logger.info(\"✅ no invalid image refs found\")\n",
    "\n",
    "    return docs\n",
    "\n",
    "mm_raw_docs = load_mm_docs_clean(MM_JSON, Path(IMAGE_DIR))\n",
    "def log_stage(name: str, docs: List[Document]):\n",
    "    logger.info(f\"{name}: {len(docs)} docs, avg_tokens={sum(len(d.page_content) for d in docs)/len(docs):.0f}\")\n",
    "log_stage(\"Docs loaded\", mm_raw_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc63ed05-192e-4f59-856c-cf9bff160fc8",
   "metadata": {},
   "source": [
    "# Creation of Chunks\n",
    "Here, we split the loaded documents into chunks, so we have smaller and more specific texts to add to our vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3dbd381-58b8-4569-aa75-b26d07556b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 17:33:54 - INFO - Chunks created: 4199 docs, avg_tokens=413\n"
     ]
    }
   ],
   "source": [
    "# === Initialize text splitter ===\n",
    "# - chunk_size: Maximum number of characters per text chunk.\n",
    "# - chunk_overlap: Number of overlapping characters between chunks.\n",
    "\n",
    "def chunk_documents(docs, chunk_size=600, overlap=100):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "        separators=[\"\\n## \", \"\\n# \", \"\\n\\n\", \".\", \"!\", \"?\"]\n",
    "    )\n",
    "    splits = splitter.split_documents(docs)\n",
    "    return splits\n",
    "\n",
    "splits = chunk_documents(mm_raw_docs)\n",
    "def log_stage(name: str, docs: List[Document]):\n",
    "    logger.info(f\"{name}: {len(docs)} docs, avg_tokens={sum(len(d.page_content) for d in docs)/len(docs):.0f}\")\n",
    "# e.g. after splits\n",
    "log_stage(\"Chunks created\", splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33fda9b-e514-4ecd-b480-f07955e08233",
   "metadata": {},
   "source": [
    "# Setup Embeddings & Vector Store\n",
    "Here we setup Siglip for Image embeddings, and also store our cleaned text chunks embeddings into Chroma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7511858c-b947-4420-a3f1-1f4a1ad82b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 17:33:54,929 - ERROR - Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "2025-07-11 17:33:54,935 - ERROR - Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "2025-07-11 17:34:36 - INFO - Text collection ready: 4199 vectors\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "2025-07-11 17:34:40,716 - ERROR - Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "2025-07-11 17:34:40,718 - ERROR - Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "2025-07-11 17:35:04 - INFO - Image collection ready: 702 vectors\n"
     ]
    }
   ],
   "source": [
    "\n",
    "CHROMA_SETTINGS = Settings(anonymized_telemetry=False)\n",
    "\n",
    "# --- 1) TEXT collection (unchanged) ---------------------------------------\n",
    "clean_chunks = filter_complex_metadata(deepcopy(splits))\n",
    "text_db = Chroma.from_documents(\n",
    "    documents       = clean_chunks,\n",
    "    embedding       = embeddings,          # HF E5\n",
    "    collection_name = \"wiki_text_mm\",\n",
    "    client_settings = CHROMA_SETTINGS,\n",
    ")\n",
    "logger.info(\"Text collection ready: %d vectors\", text_db._collection.count())\n",
    "\n",
    "# --- 2) IMAGE collection ---------------------------------------------------\n",
    "class SiglipEmbeddings(Embeddings):\n",
    "    def __init__(self,\n",
    "                 model_id: str = \"google/siglip2-base-patch16-224\",\n",
    "                 device: str | None = None):\n",
    "        from transformers import SiglipModel, SiglipProcessor\n",
    "        import torch, PIL.Image as PILImage\n",
    "        self.device    = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model     = SiglipModel.from_pretrained(model_id).to(self.device)\n",
    "        self.processor = SiglipProcessor.from_pretrained(model_id)\n",
    "        self.torch     = torch\n",
    "        self.PILImage  = PILImage\n",
    "\n",
    "    def _embed_text(self, txts):  # list[str]\n",
    "        inp = self.processor(text=txts, return_tensors=\"pt\",\n",
    "                             padding=True, truncation=True).to(self.device)\n",
    "        with self.torch.no_grad():\n",
    "            return self.model.get_text_features(**inp).cpu().numpy()\n",
    "\n",
    "    def _embed_imgs(self, paths):  # list[str]\n",
    "        imgs = [self.PILImage.open(p).convert(\"RGB\") for p in paths]\n",
    "        inp  = self.processor(images=imgs, return_tensors=\"pt\").to(self.device)\n",
    "        with self.torch.no_grad():\n",
    "            return self.model.get_image_features(**inp).cpu().numpy()\n",
    "\n",
    "    # LangChain API --------------------------------------------------------\n",
    "    def embed_documents(self, docs):      # list[str]\n",
    "        return self._embed_imgs(docs).tolist()\n",
    "\n",
    "    def embed_query(self, txt):           # single str\n",
    "        return self._embed_text([txt])[0].tolist()\n",
    "\n",
    "siglip_embeddings = SiglipEmbeddings()\n",
    "\n",
    "image_db = Chroma(\n",
    "    collection_name    = \"wiki_image_mm\",\n",
    "    embedding_function = siglip_embeddings,\n",
    "    client_settings    = CHROMA_SETTINGS,\n",
    ")\n",
    "\n",
    "# --- populate image vectors (skip duplicate IDs) --------------------------\n",
    "img_paths, img_ids, img_meta = [], [], []\n",
    "seen_ids = set()\n",
    "dup_count = 0\n",
    "\n",
    "for doc in mm_raw_docs:\n",
    "    src   = doc.metadata[\"source\"]\n",
    "    for name in set(doc.metadata[\"images\"]):        # 1× per image per doc\n",
    "        img_id = f\"{src}::{name}\"\n",
    "        if img_id in seen_ids:                      # already queued\n",
    "            dup_count += 1\n",
    "            continue\n",
    "        full = str(Path(IMAGE_DIR) / name)\n",
    "        img_paths.append(full)\n",
    "        img_ids.append(img_id)\n",
    "        img_meta.append({\"source\": src, \"image\": name})\n",
    "        seen_ids.add(img_id)\n",
    "\n",
    "if dup_count:\n",
    "    logger.info(\"Skipped %d duplicate image IDs\", dup_count)\n",
    "\n",
    "image_db.add_texts(\n",
    "    texts     = img_paths,\n",
    "    metadatas = img_meta,\n",
    "    ids       = img_ids,\n",
    ")\n",
    "logger.info(\"Image collection ready: %d vectors\", image_db._collection.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7999ff1f-1f6a-452c-848e-e3c67adf7766",
   "metadata": {},
   "source": [
    "# Retrieval\n",
    "\n",
    "We transform the texts and images into embeddings and store them in a vector database. This allows us to perform similarity search, and proper retrieval of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbf1f37f-11ff-4bf7-bae6-5e15470a8ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_mm(query: str, *, k_txt: int = 4, k_img: int = 4,\n",
    "                pad_with_similarity: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Returns\n",
    "      docs   – k_txt most-relevant text chunks\n",
    "      images – up to k_img image paths\n",
    "               • first: images explicitly referenced in `docs`\n",
    "               • optional: extra similarity-based images, to reach k_img\n",
    "    \"\"\"\n",
    "    # ---------- 1) text ---------------------------------------------------\n",
    "    txt_docs = text_db.similarity_search(query, k=k_txt)\n",
    "\n",
    "    # ---------- 2) images linked to those docs ----------------------------\n",
    "    linked = [\n",
    "        str(Path(IMAGE_DIR) / name)\n",
    "        for doc in txt_docs\n",
    "        for name in doc.metadata.get(\"images\", [])\n",
    "    ]\n",
    "    # keep order, drop dups\n",
    "    seen = set()\n",
    "    linked = [p for p in linked if not (p in seen or seen.add(p))]\n",
    "\n",
    "    # ---------- 3) (optional) semantic padding ----------------------------\n",
    "    if pad_with_similarity and len(linked) < k_img:\n",
    "        q_vec = siglip_embeddings.embed_query(query)\n",
    "        extra = image_db.similarity_search_by_vector(q_vec, k=k_img * 2)\n",
    "        for d in extra:\n",
    "            p = d.page_content\n",
    "            if p not in seen:\n",
    "                linked.append(p)\n",
    "                seen.add(p)\n",
    "            if len(linked) >= k_img:\n",
    "                break\n",
    "\n",
    "    return {\n",
    "        \"docs\":   txt_docs,\n",
    "        \"images\": linked[:k_img]        # never exceed the requested cap\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58329084-640e-4b25-be70-ac1ec90467e9",
   "metadata": {},
   "source": [
    "# Model Setup\n",
    "\n",
    "In this notebook, we provide three different options for loading the model:\n",
    " * **local**: by loading the internvl3-8b-instruct-Q8_0 model from the asset downloaded on the project\n",
    " * **hugging-face-local** by downloading a DeepSeek model from Hugging Face and running locally\n",
    " * **hugging-face-cloud** by accessing the Mistral model through Hugging Face cloud API (requires HuggingFace API key saved on secrets.yaml)\n",
    "\n",
    "This choice can be set in the config.yaml file. The model deployed on the bottom cells of this notebook will load the choice from the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6379204f-a22c-4777-9d53-ef1628033554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (8192) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    }
   ],
   "source": [
    "# ## 🤖 LLM – InternVL-3 (8B Q8_0 + mmproj)\n",
    "\n",
    "llm_mm = Llama(\n",
    "    model_path   = INTERNVL_MODEL_PATH,\n",
    "    mmproj_path  = MM_PROJ_PATH,\n",
    "    chat_format  = \"qwen\",\n",
    "    n_gpu_layers = -1,\n",
    "    n_ctx        = 8192,\n",
    "    n_batch      = 256,\n",
    "    f16_kv       = True,\n",
    "    verbose      = False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81f5d66-d99d-4851-a48e-248b2e81e2c9",
   "metadata": {},
   "source": [
    "# Chain Creation\n",
    "In this part, we define a pipeline that receives a question and context, formats the context documents, and uses the Qwen chat model to answer the question based on the provided context. The output is then formatted as a string for easy reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7a05478-2c07-48a7-a0f8-0dde2d49a539",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"You are an internal DevOps assistant. \"\n",
    "    \"Use the context provided between <context> tags to answer the user’s question. \"\n",
    "    \"If the answer isn’t in the context, say you don’t know.\\n\\n\"\n",
    "    \"Context:\\n<context>\\n\\n\"\n",
    "    \"Answer clearly and concisely. Steps or bullet-points are fine when helpful.\"\n",
    ")\n",
    "\n",
    "def _b64(path: str) -> str:\n",
    "    with open(path, \"rb\") as f:\n",
    "        return base64.b64encode(f.read()).decode()\n",
    "\n",
    "def build_messages(inp: Dict) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Returns a ChatML-compatible list:\n",
    "      • system  – plain text\n",
    "      • user    – text + embedded screenshots in SAME message\n",
    "    \"\"\"\n",
    "    # ---------- context ---------------------------------------------------\n",
    "    context_txt = \"\\n\\n\".join(\n",
    "        f\"[{d.metadata['source']}]\\n{d.page_content}\"\n",
    "        for d in inp[\"docs\"]\n",
    "    )\n",
    "\n",
    "    # ---------- images ----------------------------------------------------\n",
    "    def mime(p: str) -> str:\n",
    "        from mimetypes import guess_type\n",
    "        return guess_type(p)[0] or \"image/png\"\n",
    "\n",
    "    image_blocks = [\n",
    "        {\"type\": \"image_url\",\n",
    "         \"image_url\": {\"url\": f\"data:{mime(p)};base64,{_b64(p)}\"}}\n",
    "        for p in inp[\"images\"]\n",
    "    ]\n",
    "\n",
    "    # ---------- messages list ---------------------------------------------\n",
    "    system_msg = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": SYSTEM_PROMPT.replace(\"<context>\", context_txt),\n",
    "    }\n",
    "\n",
    "    # “content” can be a list ⇒ text first, then each image dict\n",
    "    user_msg = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [inp[\"query\"], *image_blocks],\n",
    "    }\n",
    "\n",
    "    return [system_msg, user_msg]\n",
    "\n",
    "\n",
    "def call_llm(msgs: List[Dict]) -> str:\n",
    "    resp = llm_mm.create_chat_completion(messages=msgs)\n",
    "    return resp[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "mm_chain = (\n",
    "    {\n",
    "        \"query\":   RunnablePassthrough(),\n",
    "        \"results\": RunnableLambda(lambda q: retrieve_mm(q)),\n",
    "    }\n",
    "    | RunnableLambda(lambda d: {\"query\": d[\"query\"], **d[\"results\"]})\n",
    "    | RunnableLambda(build_messages)\n",
    "    | RunnableLambda(call_llm)\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85fbe6dc-68bc-4f3c-ac94-e98476ae4297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 17:35:53,942 - ERROR - Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "2025-07-11 17:35:53,993 - ERROR - Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Doc 1  •  How%2Dto-articles/How-to-manually-clean--your-environment.md\n",
      "This documentation is directed at users who don't have access to hooh for some reason. Hooh should handle these automatically when changing environments.\n",
      "\n",
      "Access the AIStudio directory by typing `%localappdata%` in your explorer.\n",
      "\n",
      "![image.png](/.attachments/image-18fc95b4-a25e-41d6-a85a-917ee67c75b1.png)\n",
      "\n",
      "Next, go to the **HP** direcotory and then **AiStudio**\n",
      "\n",
      "\n",
      "In the AIStudio direcotory, delete the directories that have either an **Account ID**, the **db** and **creds** direcotires, as circled below.\n",
      "Also delete the userconfig file, as it stores environment specific variables. …\n",
      "\n",
      "▶ Doc 2  •  Education%2DResources-and-Knowledge-Share/Learning-AI-Studio-Education-and-References/A-walk-through-of-getting-setup-(new-to-the-AI-Studio-team%3F).md\n",
      "You need to launch Windows PowerShell (WPS) as an admin and then make the hooh command line call from within that PowerShell session. This will enable hooh at the admin level which is necessary for commands like install/uninstall. To launch power shell as an admin: \n",
      "Go to the file location for PowerShell\n",
      "![==image_0==.png](/.attachments/==image_0==-8674f15a-07e5-47fb-a068-ba83aae15000.png) \n",
      "Shift Right click and select \"Run Self Elevated with Policy Pak\"\n",
      "![image.png](/.attachments/image-47604a0e-612c-4d07-9926-54c9dae2df7a …\n",
      "\n",
      "▶ Doc 3  •  Incident-Management-and-Incident-Response/Automated-Scans-&-Tools.md\n",
      "Or by running hooh\n",
      "\n",
      "```shell\n",
      "hooh gh status\n",
      "```\n",
      "\n",
      "\n",
      "It should give you an output like this one\n",
      "\n",
      "![image.png](/.attachments/image-3fc326eb-137d-41a8-9344-db4a3aa34bf2.png) …\n",
      "\n",
      "▶ Doc 4  •  Tools/Hooh.md\n",
      "## AI Studio Container Images\n",
      "```\n",
      "hooh images --environment itg --phoenix-version x.y.z\n",
      "```\n",
      "\n",
      "Requires github token.\n",
      "\n",
      "Less important now, but will download all container images in the itg config file for phoenix version x.y.z\n",
      "\n",
      "## AI Studio Data\n",
      "```\n",
      "hooh cleanup --all-files --win-creds\n",
      "```\n",
      "\n",
      "Deletes all files and windows credentials for Phoenix. Use this instead of manually deleting %localappdata%\\HP\\AIStudio! …\n",
      "\n",
      "▶ Images\n",
      "../data/images/image-871aa5a4-5f20-444c-bf3b-59b388ff0186.png\n",
      "../data/images/Incident-Management-06-33c0ada6-c392-41d0-9f17-d6358ffbc52c.png\n",
      "../data/images/image-74e45882-f2a1-4903-afeb-1428be67a34a.png\n",
      "../data/images/image-7f0dcb3e-5996-4208-8ec3-22a89f6a3526.png\n"
     ]
    }
   ],
   "source": [
    "query = \"How do I manually clean my environment without hooh?\"\n",
    "\n",
    "results = retrieve_mm(query, k_txt=4, k_img=4)\n",
    "\n",
    "# --- text context -------------------------------------------------\n",
    "for i, doc in enumerate(results[\"docs\"], 1):\n",
    "    print(f\"\\n▶ Doc {i}  •  {doc.metadata['source']}\")\n",
    "    print(doc.page_content[:700], \"…\")\n",
    "\n",
    "# --- images -------------------------------------------------------\n",
    "print(\"\\n▶ Images\")\n",
    "for p in results[\"images\"]:\n",
    "    print(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7944376-86ec-4a0f-869b-151a209e4a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To manually clean your AI Studio environment when you don't have access to hooh, follow these steps:\n",
      "\n",
      "1. **Access the AIStudio Directory:**\n",
      "   - Open File Explorer and type `%localappdata%` in the address bar to navigate to the local app data directory.\n",
      "   - Navigate to the **HP** directory and then to the **AiStudio** directory.\n",
      "\n",
      "2. **Delete Specific Directories and Files:**\n",
      "   - In the AIStudio directory, delete the directories that contain an **Account ID**, the **db** directory, and the **creds** directory. These are typically circled in the provided image.\n",
      "   - Also, delete the **userconfig** file, as it stores environment-specific variables.\n",
      "\n",
      "3. **Launch Windows PowerShell as Admin:**\n",
      "   - Go to the file location for PowerShell.\n",
      "   - Right-click on PowerShell and select \"Run Self Elevated with Policy Pak\" to launch it as an administrator.\n",
      "\n",
      "4. **Run the hooh Command:**\n",
      "   - Within the PowerShell session, run the following command to clean up AI Studio data:\n",
      "     ```shell\n",
      "     hooh cleanup --all-files --win-creds\n",
      "     ```\n",
      "   - This command will delete all files and Windows credentials for Phoenix, effectively cleaning your AI Studio environment.\n",
      "\n",
      "These steps should help you manually clean your AI Studio environment without needing access to hooh.\n"
     ]
    }
   ],
   "source": [
    "# ✅ Quick Test\n",
    "\n",
    "question = \"How do I manually clean my environment without hooh?\"\n",
    "print(mm_chain.invoke(question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00ad5add-933c-47f4-b721-c86027bd35e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'How%2Dto-articles/How-to-manually-clean--your-environment.md'}\n"
     ]
    }
   ],
   "source": [
    "doc = text_db.similarity_search(query)[0]\n",
    "print(doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc6884d",
   "metadata": {},
   "source": [
    "# MLFlow Model Service \n",
    "\n",
    "In this section, we demonstrate how to deploy a RAG-based chatbot service. This service provides a REST API endpoint that allows users to query the knowledge base with natural language questions, upload new documents to the knowledge base, and manage conversation history, all with built-in safeguards against sensitive information and toxicity. This service encapsulates all the functionality we developed in this notebook, including the document retrieval system, RAG-based question answering capabilities, and Galileo integration for protection, observation and evaluation. It demonstrates how to use our ChatbotService from the src/service directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ac9d9e-1f10-482d-bdbb-01151d1797e0",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d2dd61a-ea19-44ba-8932-3f73c60b97ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Environment ready for MLflow evaluation.\n"
     ]
    }
   ],
   "source": [
    "mlflow_evaluate_setup(\n",
    "    secrets,\n",
    "    mlflow_tracking_uri=\"/phoenix/mlflow\"\n",
    ")\n",
    "\n",
    "# === Set MLflow experiment context ===\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\n",
    "\n",
    "# === Validate local model file path ===\n",
    "if not os.path.exists(LOCAL_MODEL_PATH):\n",
    "    logger.info(f\"⚠️ Warning: Model file not found at {LOCAL_MODEL_PATH}. Please verify the path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee90e8d-e548-453e-902f-5856c5065be4",
   "metadata": {},
   "source": [
    "## Log & Register Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "867f62bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 17:39:30,914 - INFO - Use pytorch device_name: cuda:0\n",
      "2025-07-11 17:39:30,915 - INFO - Load pretrained SentenceTransformer: intfloat/e5-large-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c515f2d826a74dd0acdaa212b6f37919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "140962e8960b4bfda00cedbdd4fc0a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54670c2e14e5401f8d52deb0656272f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/732 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a02d259fd1d4605a49a1ab9ad730191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36921ac015dd48e080ac8d568439dfb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/11 17:40:37 WARNING mlflow.utils.requirements_utils: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - PyPDF (current: uninstalled, required: PyPDF)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n",
      "2025-07-11 17:41:40,585 - INFO - Model and artifacts successfully registered in MLflow.\n"
     ]
    }
   ],
   "source": [
    "# === Log and register model to MLflow ===\n",
    "with mlflow.start_run(run_name=MLFLOW_RUN_NAME) as run:\n",
    "    \n",
    "    # Log model artifacts using custom ChatbotService\n",
    "    ChatbotService.log_model(\n",
    "        artifact_path=MLFLOW_MODEL_NAME,\n",
    "        config_path=CONFIG_PATH,\n",
    "        secrets_path=SECRETS_PATH,\n",
    "        docs_path=DATA_PATH,\n",
    "        model_path=LOCAL_MODEL_PATH,\n",
    "        demo_folder=DEMO_FOLDER\n",
    "    )\n",
    "\n",
    "    # Construct the URI for the logged model\n",
    "    model_uri = f\"runs:/{run.info.run_id}/{MLFLOW_MODEL_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f52dca2d-7110-4917-ac41-4e49e4dce60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'AIStudio-Multimodal-Chatbot-Model' already exists. Creating a new version of this model...\n",
      "Created version '8' of model 'AIStudio-Multimodal-Chatbot-Model'.\n",
      "2025-07-11 17:41:41 - INFO - ✅ Model registered successfully with run ID: 61cbed93136e494791bedcdc3e18f632\n"
     ]
    }
   ],
   "source": [
    "# Register the model into MLflow Model Registry\n",
    "mlflow.register_model(\n",
    "    model_uri=model_uri,\n",
    "    name=MLFLOW_MODEL_NAME\n",
    ")\n",
    "\n",
    "logger.info(f\"✅ Model registered successfully with run ID: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176a68a5-ff0d-42bc-8d48-9460a08f8941",
   "metadata": {},
   "source": [
    "## Evaluate Hallucination, Answer Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bec39550-62cf-4c4d-ae2d-685175095bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_source = config[\"model_source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5c88c7f-77b8-4c99-9ff0-345674585144",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for LlamaCpp\n  Value error, Could not load Llama model from path: /home/jovyan/datafabric/llama3.1-8b-instruct/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf. Received error Failed to load model from file: /home/jovyan/datafabric/llama3.1-8b-instruct/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf [type=value_error, input_value={'model_path': '/home/jov...: None, 'grammar': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m<timed exec>:1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AI-Blueprints/generative-ai/multi-modal-rag-with-langchain/src/utils.py:244\u001b[39m, in \u001b[36minitialize_llm\u001b[39m\u001b[34m(model_source, secrets, local_model_path, hf_repo_id)\u001b[39m\n\u001b[32m    240\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:  \n\u001b[32m    241\u001b[39m         \u001b[38;5;66;03m# Default context window for LlamaCpp models (explicitly set)\u001b[39;00m\n\u001b[32m    242\u001b[39m         context_window = \u001b[32m8192\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m     model = \u001b[43mLlamaCpp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_gpu_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_ctx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mf16_kv\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreaming\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchat_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mllama-3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported model source: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_source\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/aistudio/lib/python3.12/site-packages/langchain_core/load/serializable.py:130\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/aistudio/lib/python3.12/site-packages/pydantic/main.py:253\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    252\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    255\u001b[39m     warnings.warn(\n\u001b[32m    256\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    257\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    258\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    259\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    260\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for LlamaCpp\n  Value error, Could not load Llama model from path: /home/jovyan/datafabric/llama3.1-8b-instruct/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf. Received error Failed to load model from file: /home/jovyan/datafabric/llama3.1-8b-instruct/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf [type=value_error, input_value={'model_path': '/home/jov...: None, 'grammar': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "llm = initialize_llm(model_source, secrets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff59737-c56c-4c98-aef0-a3cca0418f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model(batch_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    preds, contexts = [], []\n",
    "    for q in batch_df[\"questions\"]:\n",
    "        answer = mm_chain.invoke(q)\n",
    "        preds.append(answer)\n",
    "\n",
    "        docs = retriever.get_relevant_documents(q)\n",
    "        contexts.append(\" \".join(d.page_content for d in docs))\n",
    "\n",
    "    # keep the incoming index so every batch’s rows stay unique\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"result\": preds,\n",
    "            \"source_documents\": contexts,\n",
    "        },\n",
    "        index=batch_df.index,      #  ← key line\n",
    "    )\n",
    "\n",
    "# --- 3)  Evaluation dataset\n",
    "eval_df = pd.DataFrame({\"questions\": [\n",
    "    \"What naming convention should I use for a new blueprint project folder?\",\n",
    "    \"What is the first step in the standard blueprint testing workflow?\",\n",
    "    \"How do I fetch logs from a running Kubernetes pod?\",\n",
    "]})\n",
    "\n",
    "judge = LocalGenAIJudge(\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "faithfulness_metric = judge.to_mlflow_metric(\"faithfulness\")\n",
    "relevance_metric = judge.to_mlflow_metric(\"relevance\")\n",
    "\n",
    "results = mlflow.evaluate(\n",
    "    model,\n",
    "    eval_df,\n",
    "    predictions=\"result\",\n",
    "    evaluators=\"default\",\n",
    "    extra_metrics=[faithfulness_metric, relevance_metric],\n",
    "    evaluator_config={\n",
    "        \"col_mapping\": {\n",
    "            \"inputs\": \"questions\",\n",
    "            \"context\": \"source_documents\"\n",
    "        }\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8147f34-6e3b-47f2-9a68-7e8077f71618",
   "metadata": {},
   "source": [
    "Built with ❤️ using Z by HP AI Studio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aistudio]",
   "language": "python",
   "name": "conda-env-aistudio-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
