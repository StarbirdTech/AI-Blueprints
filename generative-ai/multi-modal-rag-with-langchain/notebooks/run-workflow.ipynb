{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9ff4be-1fee-47eb-8d09-611c29a7a83f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h1 style=\"text-align: center; font-size: 50px;\">Multimodal RAG Chatbot with Langchain and VLLM</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbb3e26-125f-4151-9ed3-c84a3fd9081d",
   "metadata": {},
   "source": [
    "Retrieval-Augmented Generation (RAG) is an architectural approach that can enhance the effectiveness of large language model (LLM) applications using customized data. In this example, we use LangChain, an orchestrator for language pipelines, to build an assistant capable of loading information from a web page and use it for answering user questions. We'll leverage torch and transformers for multimodal model support in Python. We'll also use the MLFlow platform to evaluate and trace the LLM responses (in `register-workflow.ipynb`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a55c55-a1dc-4697-b920-2a81f1bb0f74",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "- Configuring the Environment\n",
    "- Data Loading & Cleaning\n",
    "- Setup Embeddings & Vector Store\n",
    "- Retrieval Function\n",
    "- Model Setup & Chain Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f78b36-56e2-4596-8325-c75901f30c76",
   "metadata": {},
   "source": [
    "## Step 0: Configuring the Environment\n",
    "\n",
    "In this step, we import all the necessary libraries and internal components required to run the RAG pipeline, including modules for notebook parsing, embedding generation, vector storage, and code generation with LLMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7156d10f-d930-4be7-a9e8-15606d466460",
   "metadata": {},
   "source": [
    "By using our Local GenAI workspace image, many of the necessary libraries to work with RAG already come pre-installed - in our case, we just need to extra support for multimodal processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "022490c4-8d8a-4c53-92ac-c62ea70a8209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os \n",
    "from pathlib import Path\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "# Create logger\n",
    "logger = logging.getLogger(\"multimodal_rag_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\") \n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f912d15a-7aab-4620-8c89-33c8f07104f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 11:09:52 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()  \n",
    "\n",
    "logger.info('Notebook execution started.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e700f51-9011-401c-90d3-a07ea8238955",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77853772-0239-40d0-94be-7a62fcb465c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 11:09:58.586551: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-02 11:09:58.611515: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754132998.631916   10231 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754132998.646690   10231 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754132998.679735   10231 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754132998.679767   10231 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754132998.679769   10231 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754132998.679770   10231 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-02 11:09:58.687650: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-02 11:10:00 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "# === Standard Library Imports ===\n",
    "import gc\n",
    "import json\n",
    "import math\n",
    "import hashlib\n",
    "import shutil\n",
    "import warnings\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from rank_bm25 import BM25Okapi\n",
    "from statistics import mean\n",
    "from typing import Any, Dict, List, Optional, TypedDict\n",
    "from IPython.display import display, Markdown\n",
    "from collections import defaultdict\n",
    "\n",
    "# === Third-Party Library Imports ===\n",
    "import mlflow\n",
    "import torch\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain.schema.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from PIL import Image as PILImage\n",
    "from transformers import AutoImageProcessor, AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# === Project-Specific Imports ===\n",
    "from src.components import SemanticCache, SiglipEmbeddings\n",
    "from src.wiki_pages_clone import orchestrate_wiki_clone\n",
    "from src.utils import (\n",
    "    configure_hf_cache,\n",
    "    multimodal_rag_asset_status,\n",
    "    load_config,\n",
    "    load_secrets,\n",
    "    load_mm_docs_clean,\n",
    "    display_images,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd3b8d2c-d0e1-4409-86dc-470c8454fff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7384967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e80960-54f9-4ec2-b062-6d49526494d1",
   "metadata": {},
   "source": [
    "\n",
    "### Verify Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75f2111d-5508-41ba-967c-256b40a9fb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 11:10:01 - INFO - Local Model is properly configured. \n",
      "2025-08-02 11:10:01 - INFO - Config is properly configured. \n",
      "2025-08-02 11:10:01 - INFO - Secrets is properly configured. \n",
      "2025-08-02 11:10:01 - INFO - wiki_flat_structure.json is properly configured. \n",
      "2025-08-02 11:10:01 - INFO - CONTEXT is properly configured. \n",
      "2025-08-02 11:10:01 - INFO - CHROMA is properly configured. \n",
      "2025-08-02 11:10:01 - INFO - CACHE is properly configured. \n",
      "2025-08-02 11:10:01 - INFO - MANIFEST is properly configured. \n"
     ]
    }
   ],
   "source": [
    "CONFIG_PATH = \"../configs/config.yaml\"\n",
    "SECRETS_PATH = \"../configs/secrets.yaml\"\n",
    "\n",
    "LOCAL_MODEL_PATH: Path = Path(\"/home/jovyan/datafabric/Qwen2.5-VL-7B-Instruct-GPTQ-Int4-1\")\n",
    "CONTEXT_DIR: Path = Path(\"../data/context\")             \n",
    "CHROMA_DIR: Path = Path(\"../data/chroma_store\")     \n",
    "CACHE_DIR: Path = CHROMA_DIR / \"semantic_cache\"\n",
    "MANIFEST_PATH: Path = CHROMA_DIR / \"manifest.json\"\n",
    "\n",
    "IMAGE_DIR = CONTEXT_DIR / \"images\"\n",
    "WIKI_METADATA_DIR = CONTEXT_DIR / \"wiki_flat_structure.json\"\n",
    "\n",
    "DEMO_FOLDER = \"../demo\"\n",
    "\n",
    "CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "multimodal_rag_asset_status(\n",
    "    local_model_path=LOCAL_MODEL_PATH,\n",
    "    config_path=CONFIG_PATH,\n",
    "    secrets_path=SECRETS_PATH,\n",
    "    wiki_metadata_dir=WIKI_METADATA_DIR,\n",
    "    context_dir=CONTEXT_DIR,\n",
    "    chroma_dir=CHROMA_DIR,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    manifest_path=MANIFEST_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7062ee0",
   "metadata": {},
   "source": [
    "### Config Loading\n",
    "\n",
    "In this section, we load configuration parameters from the YAML file in the configs folder.\n",
    "\n",
    "- **config.yaml**: Contains non-sensitive configuration parameters like model sources and URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e0b7464-ad76-47f2-94f0-e70c4d4cc380",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ca6361-dd3c-4778-9a9d-b11da7b20151",
   "metadata": {},
   "source": [
    "### Config HuggingFace Caches\n",
    "\n",
    "In the next cell, we configure HuggingFace cache, so that all the models downloaded from them are persisted locally, even after the workspace is closed. This is a future desired feature for AI Studio and the GenAI addon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "965e4ea4-084a-4f32-9f51-9ead89dd16fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure HuggingFace cache\n",
    "configure_hf_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41d47161-f197-409b-9a43-a3f59f1f695c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 833 ms, sys: 820 ms, total: 1.65 s\n",
      "Wall time: 3.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Initialize HuggingFace Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/e5-large-v2\",\n",
    "    cache_folder=\"/tmp/hf_cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d92659e-23ea-4965-bb27-58cd1dd0f3ec",
   "metadata": {},
   "source": [
    "## Step 1: Data Loading & Cleaning\n",
    "\n",
    "`wiki_flat_structure.json` is a custom json metadata for ADO Wiki data. It is flatly structured, with keys for filepath, md content, and a list of images. We also have a image folder that contains all the images for every md page. We directly scrape this data from ADO and perform any cleanup if necessary.\n",
    "\n",
    "- **secrets.yaml**: For Freemium users, use secrets.yaml to store your sensitive data like API Keys. If you are a Premium user, you can use secrets manager.\n",
    "- **AIS Secrets Manager**: For Paid users, use the secrets manager in the `Project Setup` tab to configure your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "915bdc8a-0cb9-4b17-8d8f-a56acb7ffcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 11:10:05 - INFO - Starting ADO Wiki clone process...\n",
      "2025-08-02 11:10:05 - INFO - Cloning wiki 'Phoenix-DS-Platform.wiki' to temporary directory: /tmp/tmpy9w___xq\n",
      "2025-08-02 11:10:22 - INFO - Scanning for Markdown files...\n",
      "2025-08-02 11:10:22 - INFO - → Found 570 Markdown pages.\n",
      "2025-08-02 11:10:22 - INFO - Copying referenced images to ../data/context/images...\n",
      "2025-08-02 11:10:28 - INFO - → 753 unique images copied.\n",
      "2025-08-02 11:10:28 - INFO - Assembling flat JSON structure...\n",
      "2025-08-02 11:10:28 - INFO - ✅ Wiki data successfully cloned to ../data/context\n",
      "2025-08-02 11:10:28 - INFO - Cleaned up temporary directory: /tmp/tmpy9w___xq\n",
      "2025-08-02 11:10:28 - INFO - ✅ Wiki data preparation step completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 670 ms, sys: 796 ms, total: 1.47 s\n",
      "Wall time: 23.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ADO_PAT = os.getenv(\"AIS_ADO_TOKEN\")\n",
    "if not ADO_PAT:\n",
    "    logger.info(\"Environment variable not found... Secrets Manager not properly set. Falling to secrets.yaml.\")\n",
    "    try:\n",
    "        secrets = load_secrets(SECRETS_PATH)\n",
    "        ADO_PAT = secrets.get('AIS_ADO_TOKEN')\n",
    "    except NameError:\n",
    "        logger.error(\"The 'secrets' object is not defined or available.\")\n",
    "\n",
    "try:\n",
    "    orchestrate_wiki_clone(\n",
    "        pat=ADO_PAT,\n",
    "        config=config,\n",
    "        output_dir=CONTEXT_DIR\n",
    "    )\n",
    "    logger.info(\"✅ Wiki data preparation step completed successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(\"Halting notebook execution due to a critical error in the wiki preparation step.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb4c1589-285f-4e72-ab8e-5ae9fbd4043d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 11:10:29 - WARNING - ⚠️ 94 broken image refs filtered out\n",
      "2025-08-02 11:10:29 - INFO - Docs loaded: 570 docs, avg_tokens=3127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.5 ms, sys: 45.4 ms, total: 81.9 ms\n",
      "Wall time: 765 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "WIKI_METADATA_DIR   = Path(WIKI_METADATA_DIR)\n",
    "IMAGE_DIR = Path(IMAGE_DIR)\n",
    "\n",
    "mm_raw_docs = load_mm_docs_clean(WIKI_METADATA_DIR, Path(IMAGE_DIR))\n",
    "\n",
    "def log_stage(name: str, docs: List[Document]):\n",
    "    logger.info(f\"{name}: {len(docs)} docs, avg_tokens={sum(len(d.page_content) for d in docs)/len(docs):.0f}\")\n",
    "log_stage(\"Docs loaded\", mm_raw_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc63ed05-192e-4f59-856c-cf9bff160fc8",
   "metadata": {},
   "source": [
    "## Step 2: Creation of Chunks\n",
    "\n",
    "Here, we split the loaded documents into chunks, so we have smaller and more specific texts to add to our vector database. \n",
    "\n",
    "We chunk based on header style, and then within each header style we futher chunk based on the provided chunk size. Each chunk retains the page name, which preserves the relevance of each chunk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3dbd381-58b8-4569-aa75-b26d07556b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 11:10:29 - INFO - Chunking complete: 570 docs → 2646 chunks (avg 720 chars)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 68.4 ms, sys: 4.44 ms, total: 72.9 ms\n",
      "Wall time: 64.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def chunk_documents(\n",
    "    docs,\n",
    "    chunk_size: int = 1200,\n",
    "    overlap: int = 200,\n",
    ") -> list[Document]:\n",
    "    \"\"\"\n",
    "    1) Split each wiki page on Markdown headers (#, ## …) to keep logical\n",
    "       sections together.\n",
    "    2) Recursively break long sections to <= `chunk_size` chars with `overlap`.\n",
    "    3) Prefix every chunk with its page-title and store the title in metadata.\n",
    "    \"\"\"\n",
    "    header_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=[(\"#\", \"title\"), (\"##\", \"section\")]\n",
    "    )\n",
    "    recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "    )\n",
    "\n",
    "    all_chunks: list[Document] = []\n",
    "    for doc in docs:\n",
    "        page_title = Path(doc.metadata[\"source\"]).stem.replace(\"-\", \" \")\n",
    "\n",
    "        # 1️. section‑level split (returns list[Document])\n",
    "        section_docs = header_splitter.split_text(doc.page_content)\n",
    "\n",
    "        for section in section_docs:\n",
    "            # 2. size‑based split inside each section\n",
    "            tiny_texts = recursive_splitter.split_text(section.page_content)\n",
    "\n",
    "            for idx, tiny in enumerate(tiny_texts):\n",
    "                all_chunks.append(\n",
    "                    Document(\n",
    "                        page_content=f\"{page_title}\\n\\n{tiny.strip()}\",\n",
    "                        metadata={\n",
    "                            \"title\": page_title,\n",
    "                            \"source\": doc.metadata[\"source\"],\n",
    "                            \"section_header\": section.metadata.get(\"header\", \"\"),\n",
    "                            \"chunk_id\": idx,\n",
    "                        },\n",
    "                    )\n",
    "                )\n",
    "    if all_chunks:\n",
    "        avg_len = int(mean(len(c.page_content) for c in all_chunks))\n",
    "        logger.info(\n",
    "            \"Chunking complete: %d docs → %d chunks (avg %d chars)\",\n",
    "            len(docs),\n",
    "            len(all_chunks),\n",
    "            avg_len,\n",
    "        )\n",
    "    else:\n",
    "        logger.warning(\"Chunking produced zero chunks for %d docs\", len(docs))\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "splits = chunk_documents(mm_raw_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33fda9b-e514-4ecd-b480-f07955e08233",
   "metadata": {},
   "source": [
    "## Step 3: Setup Embeddings & Vector Store\n",
    "Here we setup Siglip for Image embeddings, and also transform our cleaned text chunks into embeddings to be stored in Chroma. We store the chroma data locally on the disk to reduce memory usage. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cd5fbc",
   "metadata": {},
   "source": [
    "### Setup Text ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e83d3d3f-87a8-4209-a805-59983479d629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 11:10:29 - INFO - Loading existing Chroma index from ../data/chroma_store\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 183 ms, sys: 13 ms, total: 196 ms\n",
      "Wall time: 263 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 1) TEXT store\n",
    "def _current_manifest() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Returns a dictionary mapping every context JSON file to its SHA256 content hash.\n",
    "    This allows detecting changes in file content, not just filenames.\n",
    "    \"\"\"\n",
    "    manifest = {}\n",
    "    json_files = sorted(CONTEXT_DIR.rglob(\"*.json\"))\n",
    "\n",
    "    for file_path in json_files:\n",
    "        try:\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                file_bytes = f.read()\n",
    "                file_hash = hashlib.sha256(file_bytes).hexdigest()\n",
    "                manifest[str(file_path.resolve())] = file_hash\n",
    "        except IOError as e:\n",
    "            logger.error(f\"Could not read file {file_path} for hashing: {e}\")\n",
    "    return manifest\n",
    "\n",
    "def _needs_rebuild() -> bool:\n",
    "    \"\"\"\n",
    "    Determines if the ChromaDB needs to be rebuilt.\n",
    "    A rebuild is needed if:\n",
    "    1. The Chroma directory or manifest file doesn't exist.\n",
    "    2. The manifest is unreadable.\n",
    "    3. The stored file hashes in the manifest do not match the current file hashes.\n",
    "    \"\"\"\n",
    "    if not CHROMA_DIR.exists() or not MANIFEST_PATH.exists():\n",
    "        logger.info(\"Chroma directory or manifest not found. A rebuild is required.\")\n",
    "        return True\n",
    "    try:\n",
    "        old_manifest = json.loads(MANIFEST_PATH.read_text())\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not read manifest file. A rebuild is required. Error: {e}\")\n",
    "        return True\n",
    "\n",
    "    current_manifest = _current_manifest()\n",
    "    if old_manifest != current_manifest:\n",
    "        logger.info(\"Data content has changed. A rebuild is required.\")\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def _save_manifest(manifest: Dict[str, str]) -> None:\n",
    "    \"\"\"Saves the current data manifest (mapping file paths to hashes) to disk.\"\"\"\n",
    "    CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    MANIFEST_PATH.write_text(json.dumps(manifest, indent=2))\n",
    "\n",
    "def _build_text_db() -> Chroma:\n",
    "    collection = \"mm_text\"\n",
    "    # The rebuild check is now done outside this function.\n",
    "    # We check if the directory exists. If not, we build.\n",
    "    if not CHROMA_DIR.exists() or not (CHROMA_DIR / \"chroma.sqlite3\").exists():\n",
    "        logger.info(\"Creating new text context index in %s ...\", CHROMA_DIR)\n",
    "        chroma = Chroma.from_documents(\n",
    "            documents          = splits,\n",
    "            embedding          = embeddings,\n",
    "            collection_name    = collection,\n",
    "            persist_directory  = str(CHROMA_DIR),\n",
    "        )\n",
    "        return chroma\n",
    "\n",
    "    logger.info(\"Loading existing Chroma index from %s\", CHROMA_DIR)\n",
    "    return Chroma(\n",
    "        collection_name   = collection,\n",
    "        persist_directory = str(CHROMA_DIR),\n",
    "        embedding_function= embeddings,\n",
    "    )\n",
    "    \n",
    "# Check if a rebuild is needed and wipe the old DB if so.\n",
    "# This ensures both the text and image databases are rebuilt from scratch.\n",
    "if _needs_rebuild():\n",
    "    logger.warning(\"REBUILDING: Wiping old ChromaDB store at %s\", CHROMA_DIR)\n",
    "    if CHROMA_DIR.exists():\n",
    "        shutil.rmtree(CHROMA_DIR)\n",
    "    # Save the new manifest immediately after deciding to rebuild\n",
    "    _save_manifest(_current_manifest())\n",
    "\n",
    "# Now, initialize your databases. They will be created fresh if they were just deleted.\n",
    "text_db = _build_text_db()\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2c64bd",
   "metadata": {},
   "source": [
    "### Setup Image ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50a39492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "2025-08-02 11:10:33 - INFO - Loaded existing image index (767 vectors).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.82 s, sys: 752 ms, total: 2.57 s\n",
      "Wall time: 3.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#  Helper: walk all docs once and gather *unique* image vectors + metadata\n",
    "def _collect_image_vectors():\n",
    "    \"\"\"\n",
    "    Scans every wiki page for image references and returns three parallel lists:\n",
    "        img_paths : list[str]   → full file-system paths (for SigLIP)\n",
    "        img_ids   : list[str]   → unique key per (page, image) pair\n",
    "        img_meta  : list[dict]  → {\"source\": wiki_page, \"image\": file_name}\n",
    "    Runs in < 1s even for thousands of docs.\n",
    "    \"\"\"\n",
    "    img_paths, img_ids, img_meta = [], [], []\n",
    "    seen = set()\n",
    "\n",
    "    for doc in mm_raw_docs:                         # raw wiki pages\n",
    "        src = doc.metadata[\"source\"]\n",
    "        for name in doc.metadata.get(\"images\", []): # list[str]\n",
    "            img_id = f\"{src}::{name}\"\n",
    "            if img_id in seen:\n",
    "                continue                            # de‑dupe\n",
    "            seen.add(img_id)\n",
    "\n",
    "            img_paths.append(str(IMAGE_DIR / name))\n",
    "            img_ids.append(img_id)\n",
    "            img_meta.append({\"source\": src, \"image\": name})\n",
    "\n",
    "    return img_paths, img_ids, img_meta\n",
    "\n",
    "siglip_embeddings = SiglipEmbeddings(\"google/siglip2-base-patch16-224\", DEVICE)\n",
    "\n",
    "# 2) IMAGE store\n",
    "image_db = Chroma(\n",
    "    collection_name    = \"mm_image\",\n",
    "    persist_directory  = str(CHROMA_DIR),   # SAME dir as text db\n",
    "    embedding_function = siglip_embeddings, # <-- class you kept\n",
    ")\n",
    "\n",
    "# Populate vectors *only* if it is empty\n",
    "if not image_db._collection.count():\n",
    "    img_paths, img_ids, img_meta = _collect_image_vectors()\n",
    "    image_db.add_texts(texts=img_paths, metadatas=img_meta, ids=img_ids)\n",
    "    image_db.persist()\n",
    "    logger.info(\"Indexed %d unique images.\", len(img_paths))\n",
    "else:\n",
    "    logger.info(\"Loaded existing image index (%d vectors).\",\n",
    "                image_db._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b7a32f-81e5-49bb-94c3-04a480e4be89",
   "metadata": {},
   "source": [
    "### Setup Memory Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9473fbc-bbbb-40b2-b344-decd9322a303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the semantic cache\n",
    "semantic_cache = SemanticCache(persist_directory=CACHE_DIR, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fd4358-0385-4cd3-bd2d-7da53a627418",
   "metadata": {},
   "source": [
    "## Step 4: Retrieval Function\n",
    "\n",
    "This code implements a hybrid retrieval process that combines two powerful search techniques to find the most relevant text documents and associated images.\n",
    "\n",
    "1.  **Initial Recall (Hybrid Search)**: The system performs two searches in parallel:\n",
    "    * **Dense Search**: A vector similarity search against `text_db` (ChromaDB) to find semantically related documents.\n",
    "    * **Sparse Search**: A keyword-based search using a `BM25` index to find documents with exact term matches.\n",
    "\n",
    "2.  **Fusion (RRF)**: The results from both searches are combined into a single, more robust ranked list using **Reciprocal Rank Fusion (RRF)**. This method intelligently merges the rankings without needing complex parameter tuning.\n",
    "\n",
    "3.  **Image Retrieval**: Using the top text documents from the fused list, the system performs a targeted search in the `image_db` to find images that are on the same source pages, ensuring contextual relevance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbf1f37f-11ff-4bf7-bae6-5e15470a8ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 11:10:33 - INFO - De-duplicated 2646 chunks down to 2635 unique chunks.\n"
     ]
    }
   ],
   "source": [
    "# This is necessary because the chunking process can sometimes create identical chunks.\n",
    "unique_docs_map = {doc.page_content: doc for doc in splits}\n",
    "unique_splits = list(unique_docs_map.values())\n",
    "\n",
    "logger.info(f\"De-duplicated {len(splits)} chunks down to {len(unique_splits)} unique chunks.\")\n",
    "\n",
    "# Now, build the BM25 index and the final doc_map using only the unique documents.\n",
    "# This ensures the index and the search corpus are perfectly aligned.\n",
    "corpus = [doc.page_content for doc in unique_splits]\n",
    "tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "doc_map = {doc.page_content: doc for doc in unique_splits}\n",
    "\n",
    "# %%\n",
    "# Helper function for Reciprocal Rank Fusion\n",
    "def reciprocal_rank_fusion(\n",
    "    results: list[list[Document]], k: int = 60\n",
    ") -> list[tuple[Document, float]]:\n",
    "    \"\"\"Performs RRF on multiple lists of ranked documents.\"\"\"\n",
    "    ranked_lists = [\n",
    "        {doc.page_content: (doc, i + 1) for i, doc in enumerate(res)}\n",
    "        for res in results\n",
    "    ]\n",
    "    rrf_scores = defaultdict(float)\n",
    "    all_docs = {}\n",
    "    for ranked_list in ranked_lists:\n",
    "        for content, (doc, rank) in ranked_list.items():\n",
    "            rrf_scores[content] += 1 / (k + rank)\n",
    "            if content not in all_docs:\n",
    "                all_docs[content] = doc\n",
    "    fused_results = [\n",
    "        (all_docs[content], rrf_scores[content])\n",
    "        for content in sorted(rrf_scores, key=rrf_scores.get, reverse=True)\n",
    "    ]\n",
    "    return fused_results\n",
    "\n",
    "\n",
    "def retrieve_mm(\n",
    "    query: str,\n",
    "    text_db: Chroma,\n",
    "    image_db: Chroma,\n",
    "    bm25_index: BM25Okapi,\n",
    "    doc_map: dict,\n",
    "    k_text: int = 3,\n",
    "    k_img: int = 2,\n",
    "    recall_k: int = 20,\n",
    ") -> dict[str, any]:\n",
    "    \"\"\"\n",
    "    Performs hybrid search for text and retrieves contextually relevant images.\n",
    "    \"\"\"\n",
    "    # 1. Hybrid Search for Text\n",
    "    dense_hits = text_db.similarity_search(query, k=recall_k)\n",
    "    tokenized_query = query.lower().split(\" \")\n",
    "    sparse_texts = bm25_index.get_top_n(tokenized_query, list(doc_map.keys()), n=recall_k)\n",
    "    sparse_hits = [doc_map[text] for text in sparse_texts]\n",
    "\n",
    "    if not dense_hits and not sparse_hits:\n",
    "        return {\"docs\": [], \"scores\": [], \"images\": []}\n",
    "\n",
    "    fused_results = reciprocal_rank_fusion([dense_hits, sparse_hits])\n",
    "    final_docs = [doc for doc, score in fused_results[:k_text]]\n",
    "    final_scores = [score for doc, score in fused_results[:k_text]]\n",
    "\n",
    "    # 2. Retrieve Relevant Images\n",
    "    retrieved_images = []\n",
    "    if final_docs:\n",
    "        # Get the source pages of the top text results\n",
    "        final_sources = list(set(d.metadata[\"source\"] for d in final_docs))\n",
    "\n",
    "        # Perform a vector search for images, filtered by the relevant sources\n",
    "        # The image_db's embedding function (SigLIP) will automatically handle the text query.\n",
    "        image_hits = image_db.similarity_search(\n",
    "            query,\n",
    "            k=k_img,\n",
    "            filter={\"source\": {\"$in\": final_sources}}\n",
    "        )\n",
    "        # The `page_content` of an image document is its path/name\n",
    "        retrieved_images = [img.page_content for img in image_hits]\n",
    "\n",
    "    return {\n",
    "        \"docs\": final_docs,\n",
    "        \"scores\": final_scores,\n",
    "        \"images\": retrieved_images,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58329084-640e-4b25-be70-ac1ec90467e9",
   "metadata": {},
   "source": [
    "## Step 5: Model Setup & Chain Creation\n",
    "\n",
    "In this section, we set up our local Large Language Model (LLM) and integrate it into a Question Answering (QA) pipeline. We're using `internvl3-8b-instruct` as our multimodal model, which can process both text and images. This setup is encapsulated within the InternVLMM class, designed for efficient and robust multimodal interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fcccbf",
   "metadata": {},
   "source": [
    "### Cleanup Previous Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3679775b-14ad-44bc-8eb7-c24a3a9a88bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 11:10:33 - INFO - ✅ Embeddings and vector stores are ready. Offloading embedding models to free up VRAM.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"✅ Embeddings and vector stores are ready. Offloading embedding models to free up VRAM.\")\n",
    "\n",
    "# Explicitly delete the objects to free memory\n",
    "del embeddings\n",
    "del siglip_embeddings\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# For PyTorch, you can also empty the CUDA cache\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63929d26",
   "metadata": {},
   "source": [
    "### QwenVLMM QA Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "910ccb5a-fb0c-4f7c-bc2a-69f464f3ffa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 11:10:34 - INFO - Loading Qwen2.5-VL via vLLM...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-02 11:10:40 [config.py:3443] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 08-02 11:10:40 [config.py:1604] Using max model len 4096\n",
      "INFO 08-02 11:10:41 [gptq_marlin.py:174] Detected that the model can run with gptq_marlin, however you specified quantization=gptq explicitly, so forcing gptq. Use quantization=gptq_marlin for faster inference\n",
      "WARNING 08-02 11:10:41 [config.py:1084] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 08-02 11:10:41 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 11:10:43.879398: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754133043.891058   10394 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754133043.894545   10394 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754133043.903574   10394 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754133043.903599   10394 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754133043.903601   10394 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754133043.903602   10394 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-02 11:10:45 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 08-02 11:10:46 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 08-02 11:10:46 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='/home/jovyan/datafabric/Qwen2.5-VL-7B-Instruct-GPTQ-Int4-1', speculative_config=None, tokenizer='/home/jovyan/datafabric/Qwen2.5-VL-7B-Instruct-GPTQ-Int4-1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=gptq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/jovyan/datafabric/Qwen2.5-VL-7B-Instruct-GPTQ-Int4-1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}\n",
      "INFO 08-02 11:10:47 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 08-02 11:10:47 [interface.py:380] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "WARNING 08-02 11:10:48 [profiling.py:276] The sequence length (4096) is smaller than the pre-defined worst-case total number of multimodal tokens (32768). This may cause certain multi-modal inputs to fail during inference. To avoid this, you should increase `max_model_len` or reduce `mm_counts`.\n",
      "INFO 08-02 11:10:48 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.\n",
      "INFO 08-02 11:10:48 [gpu_model_runner.py:1843] Starting to load model /home/jovyan/datafabric/Qwen2.5-VL-7B-Instruct-GPTQ-Int4-1...\n",
      "INFO 08-02 11:10:48 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "WARNING 08-02 11:10:48 [vision.py:91] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "INFO 08-02 11:10:48 [cuda.py:290] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:10<00:10, 10.76s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:19<00:00,  9.54s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:19<00:00,  9.72s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-02 11:11:08 [default_loader.py:262] Loading weights took 18.92 seconds\n",
      "INFO 08-02 11:11:08 [gpu_model_runner.py:1892] Model loading took 6.5705 GiB and 19.315125 seconds\n",
      "INFO 08-02 11:11:08 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n",
      "/opt/conda/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-02 11:11:17 [gpu_worker.py:255] Available KV cache memory: 1.85 GiB\n",
      "INFO 08-02 11:11:17 [kv_cache_utils.py:833] GPU KV cache size: 34,640 tokens\n",
      "INFO 08-02 11:11:17 [kv_cache_utils.py:837] Maximum concurrency for 4,096 tokens per request: 8.46x\n",
      "INFO 08-02 11:11:17 [core.py:193] init engine (profile, create kv cache, warmup model) took 9.28 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 11:11:18 - INFO - vLLM model loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.27 s, sys: 188 ms, total: 1.46 s\n",
      "Wall time: 44.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "class QwenVLMM:\n",
    "    \"\"\"\n",
    "    Multimodal QA wrapper around the quantized Qwen2.5-VL model using vLLM.\n",
    "    Requires:\n",
    "      * `vllm` installed and importable.\n",
    "      * `qwen_vl_utils.process_vision_info` for multimodal image handling.\n",
    "      * HuggingFace transformers for tokenizer / image processor.\n",
    "      * External retrieval function (e.g., `retrieve_mm`) and a `SemanticCache`-like cache.\n",
    "    Expects the quantized safetensors model `RedHatAI/Qwen2.5-VL-7B-Instruct-quantized.w8a8`\n",
    "    to be accessible (vLLM will pull it from HuggingFace).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cache,\n",
    "        text_db,\n",
    "        image_db,\n",
    "        bm25_index,\n",
    "        doc_map: dict,\n",
    "        model_name: str = \"RedHatAI/Qwen2.5-VL-7B-Instruct-quantized.w8a8\",\n",
    "        base_for_tokenizer: str = \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "        device: str = \"cuda\",\n",
    "    ):\n",
    "        self.cache = cache\n",
    "        self.text_db = text_db\n",
    "        self.image_db = image_db\n",
    "        self.bm25_index = bm25_index\n",
    "        self.doc_map = doc_map\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.base_for_tokenizer = base_for_tokenizer\n",
    "        self.device = device\n",
    "\n",
    "        self.tok = None\n",
    "        self.image_processor = None\n",
    "        self.llm = None  # vLLM instance\n",
    "\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "        self._load()\n",
    "\n",
    "    # ---------- public function ----------\n",
    "    def generate(self, query: str, force_regenerate: bool = False, **retrieval_kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Run retrieval, prompt assembly, and model generation via vLLM.\n",
    "        \"\"\"\n",
    "        # 1. Cache check\n",
    "        if not force_regenerate:\n",
    "            cached_result = self.cache.get(query, threshold=0.92)\n",
    "            if cached_result:\n",
    "                logger.info(f\"SEMANTIC CACHE HIT for query: '{query}'\")\n",
    "                return cached_result\n",
    "        if force_regenerate:\n",
    "            logger.info(f\"Forced regeneration for query: '{query}'. Clearing old cache entry.\")\n",
    "            self.cache.delete(query)\n",
    "        logger.info(f\"CACHE MISS for query: '{query}'. Running full pipeline.\")\n",
    "\n",
    "        if self.llm is None or self.tok is None:\n",
    "            return {\"reply\": \"Error: model not initialised.\", \"used_images\": []}\n",
    "\n",
    "        # 2. Retrieval\n",
    "        hits = retrieve_mm(\n",
    "            query,\n",
    "            text_db=self.text_db,\n",
    "            image_db=self.image_db,\n",
    "            bm25_index=self.bm25_index,\n",
    "            doc_map=self.doc_map,\n",
    "            **retrieval_kwargs\n",
    "        )\n",
    "        docs = hits.get(\"docs\", [])\n",
    "        images = hits.get(\"images\", [])\n",
    "\n",
    "        if not docs and not images:\n",
    "            return {\n",
    "                \"reply\": \"Based on the provided context, I cannot answer this question.\", \n",
    "                \"used_images\": [],\n",
    "                \"retrieved_sources\": {\"text_documents\": [], \"images\": []},\n",
    "            }\n",
    "\n",
    "        # Limit number of images to reduce memory usage\n",
    "        if len(images) > 2:\n",
    "            logger.warning(f\"Limiting images from {len(images)} to 2 to save memory\")\n",
    "            images = images[:2]\n",
    "\n",
    "        # 3. Build prompt\n",
    "        context_str = \"\\n\\n\".join(\n",
    "            f\"<source_document name=\\\"{d.metadata.get('source', 'unknown')}\\\">\\n{d.page_content}\\n</source_document>\"\n",
    "            for d in docs\n",
    "        )\n",
    "\n",
    "        system_prompt = \"\"\"You are a Multimodal RAG Assistant. Your task is to answer the user's query using ONLY the provided context from retrieved documents and images.\n",
    "            \n",
    "            **Instructions:**\n",
    "            1. **Analyze Context:** Carefully examine the retrieved images and text documents provided in the context.\n",
    "            2. **Answer Directly:** Provide a clear, comprehensive answer to the user's query by synthesizing information from both text and image sources.\n",
    "            3. **Stay Focused:** Do not include unnecessary sections or verbose explanations. Answer the question directly and concisely.\n",
    "            4. **No Hallucination:** Use ONLY the information provided in the context. Do not make up facts or add information not present in the retrieved materials.\n",
    "            \n",
    "            **Output Format:**\n",
    "            - If the context is relevant: Provide a direct answer using the retrieved context.\n",
    "            - If the context is irrelevant: Respond with \"The provided context does not contain relevant information to answer the query.\"\n",
    "            \"\"\"\n",
    "                    \n",
    "        # Build user content with proper image placeholders for Qwen2.5-VL\n",
    "        if images:\n",
    "            # Use the standard Qwen2.5-VL image token format\n",
    "            image_tokens = \"\"\n",
    "            for i in range(len(images)):\n",
    "                image_tokens += f\"<|vision_start|><|image_pad|><|vision_end|>\"\n",
    "            \n",
    "            user_content = f\"\"\"{image_tokens}\n",
    "\n",
    "            <context>\n",
    "            {context_str}\n",
    "            </context>\n",
    "            \n",
    "            <user_query>\n",
    "            {query}\n",
    "            </user_query>\"\"\"\n",
    "        else:\n",
    "            user_content = f\"\"\"<context>\n",
    "            {context_str}\n",
    "            </context>\n",
    "            \n",
    "            <user_query>\n",
    "            {query}\n",
    "            </user_query>\"\"\"\n",
    "\n",
    "        # Use chat template if available\n",
    "        if hasattr(self.tok, 'apply_chat_template') and self.tok.chat_template:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_content}\n",
    "            ]\n",
    "            try:\n",
    "                prompt_string = self.tok.apply_chat_template(\n",
    "                    messages, \n",
    "                    tokenize=False, \n",
    "                    add_generation_prompt=True\n",
    "                )\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Chat template failed: {e}, using fallback\")\n",
    "                prompt_string = f\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n<|im_start|>user\\n{user_content}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "        else:\n",
    "            # Fallback to manual template\n",
    "            prompt_string = f\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n<|im_start|>user\\n{user_content}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "        # 4. Generation via vLLM\n",
    "        try:\n",
    "            self._clear_cuda()\n",
    "\n",
    "            # More conservative sampling parameters\n",
    "            sampling_params = SamplingParams(\n",
    "                temperature=0.0,     # Deterministic\n",
    "                top_p=1.0,\n",
    "                max_tokens=2048,\n",
    "            )\n",
    "\n",
    "            if images:\n",
    "                # Process images with size limit\n",
    "                pil_images = []\n",
    "                for i, img_path in enumerate(images):\n",
    "                    try:\n",
    "                        img = PILImage.open(img_path).convert(\"RGB\")\n",
    "                        # Resize large images to save memory\n",
    "                        if img.size[0] > 512 or img.size[1] > 512:\n",
    "                            img.thumbnail((512, 512), PILImage.Resampling.LANCZOS)\n",
    "                        pil_images.append(img)\n",
    "                        logger.info(f\"Processed image {i+1}: {img_path}\")\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Failed to process image {img_path}: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                if not pil_images:\n",
    "                    logger.warning(\"No images successfully processed, proceeding text-only\")\n",
    "                    request_payload = {\"prompt\": prompt_string}\n",
    "                else:\n",
    "                    request_payload = {\n",
    "                        \"prompt\": prompt_string,\n",
    "                        \"multi_modal_data\": {\n",
    "                            \"image\": pil_images\n",
    "                        },\n",
    "                    }\n",
    "            else:\n",
    "                request_payload = {\"prompt\": prompt_string}\n",
    "            \n",
    "            output_list = self.llm.generate(request_payload, sampling_params=sampling_params)\n",
    "            \n",
    "            if output_list and output_list[0].outputs:\n",
    "                reply = output_list[0].outputs[0].text.strip()\n",
    "            else:\n",
    "                reply = \"Error: no output from LLM.\"\n",
    "\n",
    "            self._clear_cuda()\n",
    "            \n",
    "            # Prepare retrieved sources for programmatic return\n",
    "            retrieved_sources = {\n",
    "                \"text_documents\": [\n",
    "                    {\n",
    "                        \"source\": d.metadata.get('source', 'unknown'),\n",
    "                        \"content\": d.page_content[:500] + \"...\" if len(d.page_content) > 500 else d.page_content,\n",
    "                        \"metadata\": d.metadata\n",
    "                    }\n",
    "                    for d in docs\n",
    "                ],\n",
    "                \"images\": [\n",
    "                    {\n",
    "                        \"path\": img_path,\n",
    "                        \"filename\": img_path.split('/')[-1] if '/' in img_path else img_path\n",
    "                    }\n",
    "                    for img_path in images\n",
    "                ]\n",
    "            }\n",
    "\n",
    "            if reply == \"The provided context does not contain relevant information to answer the query.\":\n",
    "                images = []\n",
    "            \n",
    "            result = {\n",
    "                \"reply\": reply, \n",
    "                \"used_images\": images,\n",
    "                \"retrieved_sources\": retrieved_sources,\n",
    "            }\n",
    "            self.cache.set(query, result)\n",
    "            return result\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            msg = str(e).lower()\n",
    "            if \"cuda\" in msg or \"out of memory\" in msg:\n",
    "                logger.warning(\"CUDA error – resetting model: %s\", e)\n",
    "                self._reset()\n",
    "                error_reply = \"I ran into a GPU memory error – please try again.\"\n",
    "            else:\n",
    "                logger.error(\"Runtime error: %s\", e)\n",
    "                error_reply = f\"Error: {e}\"\n",
    "            return {\n",
    "                \"reply\": error_reply, \n",
    "                \"used_images\": images,\n",
    "                \"retrieved_sources\": {\"text_documents\": [], \"images\": []},\n",
    "            }\n",
    "\n",
    "    # ---------- internal helpers ----------\n",
    "\n",
    "    def _load(self):\n",
    "        \"\"\"Load tokenizer, image_processor, & vLLM model.\"\"\"\n",
    "        logger.info(\"Loading Qwen2.5-VL via vLLM...\")\n",
    "        gc.collect()\n",
    "        self._clear_cuda()\n",
    "    \n",
    "        # Tokenizer & image processor (base model)\n",
    "        self.tok = AutoTokenizer.from_pretrained(\n",
    "            self.base_for_tokenizer, trust_remote_code=True\n",
    "        )\n",
    "        if self.tok.pad_token is None:\n",
    "            self.tok.pad_token = self.tok.eos_token\n",
    "    \n",
    "        self.image_processor = AutoImageProcessor.from_pretrained(\n",
    "            self.base_for_tokenizer, trust_remote_code=True, use_fast=True\n",
    "        )\n",
    "    \n",
    "        # Load vLLM with the quantized safetensors model (no use_mlock)\n",
    "        self.llm = LLM(\n",
    "            model=self.model_name,\n",
    "            quantization=\"gptq\",\n",
    "            gpu_memory_utilization=0.90,    # Leave headroom for image tensors\n",
    "            max_model_len=4096,\n",
    "            enforce_eager=True,\n",
    "            limit_mm_per_prompt={\"image\": 2},  # No more than 2 images\n",
    "            disable_custom_all_reduce=True,\n",
    "            tensor_parallel_size=1,\n",
    "            dtype=\"float16\",\n",
    "        )\n",
    "\n",
    "        logger.info(\"vLLM model loaded.\")\n",
    "\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Free everything and reload on error.\"\"\"\n",
    "        logger.warning(\"Resetting InternQwenVLMM model …\")\n",
    "        try:\n",
    "            del self.llm, self.tok, self.image_processor\n",
    "        except Exception:\n",
    "            pass\n",
    "        self.llm = self.tok = self.image_processor = None\n",
    "        gc.collect()\n",
    "        self._clear_cuda()\n",
    "        time.sleep(1)\n",
    "        self._load()\n",
    "\n",
    "    @staticmethod\n",
    "    def _clear_cuda():\n",
    "        try:\n",
    "            import torch\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "        except ImportError:\n",
    "            pass\n",
    "\n",
    "\n",
    "# Initalize mm llm\n",
    "mm = QwenVLMM(\n",
    "    cache=semantic_cache,\n",
    "    text_db=text_db,\n",
    "    image_db=image_db,\n",
    "    bm25_index=bm25,\n",
    "    doc_map=doc_map,\n",
    "    model_name=str(LOCAL_MODEL_PATH),  # quantized safetensors model\n",
    "    base_for_tokenizer=\"Qwen/Qwen2.5-VL-7B-Instruct\",  # for tokenizer / image processor\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0057f873",
   "metadata": {},
   "source": [
    "## Step 6: Test Generation and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a24f50-8384-488e-87e0-eafae305f2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the AI Blueprints Repository best practices?\"\n",
    "results = mm.generate(question, force_regenerate=True)\n",
    "print(\"--- MODEL CONTEXT ---\")\n",
    "print(results[\"retrieved_sources\"])\n",
    "\n",
    "print(\"\\n--- MODEL RESPONSE ---\")\n",
    "display(Markdown(results[\"reply\"]))\n",
    "print(\"----------------------\\n\")\n",
    "\n",
    "display_images(results[\"used_images\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98ead24-203d-41fd-b60a-74f64445e800",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "question1 = \"What is the capital of paris?\"\n",
    "results = mm.generate(question1, force_regenerate=True)\n",
    "print(\"--- MODEL CONTEXT ---\")\n",
    "print(results[\"retrieved_sources\"])\n",
    "\n",
    "print(\"\\n--- MODEL RESPONSE ---\")\n",
    "display(Markdown(results[\"reply\"]))\n",
    "print(\"----------------------\\n\")\n",
    "\n",
    "display_images(results[\"used_images\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a3a494-8f3c-4bce-8494-5c8bd7e150cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "question2 = \"What is ITG, STG, and Prod?\"\n",
    "results = mm.generate(question2, force_regenerate=True)\n",
    "print(\"--- MODEL CONTEXT ---\")\n",
    "print(results[\"retrieved_sources\"])\n",
    "\n",
    "print(\"\\n--- MODEL RESPONSE ---\")\n",
    "display(Markdown(results[\"reply\"]))\n",
    "print(\"----------------------\\n\")\n",
    "\n",
    "display_images(results[\"used_images\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbca87f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 11:11:33 - INFO - ⏱️ Total execution time: 1m 41.27s\n",
      "2025-08-02 11:11:33 - INFO - ✅ Notebook execution completed successfully.\n"
     ]
    }
   ],
   "source": [
    "end_time: float = time.time()\n",
    "elapsed_time: float = end_time - start_time\n",
    "elapsed_minutes: int = int(elapsed_time // 60)\n",
    "elapsed_seconds: float = elapsed_time % 60\n",
    "\n",
    "logger.info(f\"⏱️ Total execution time: {elapsed_minutes}m {elapsed_seconds:.2f}s\")\n",
    "logger.info(\"✅ Notebook execution completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8147f34-6e3b-47f2-9a68-7e8077f71618",
   "metadata": {},
   "source": [
    "Built with ❤️ using Z by HP AI Studio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
