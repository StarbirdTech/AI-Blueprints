{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9ff4be-1fee-47eb-8d09-611c29a7a83f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h1 style=\"text-align: center; font-size: 50px;\">Multimodal RAG Chatbot with Langchain, Torch, Transformers</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbb3e26-125f-4151-9ed3-c84a3fd9081d",
   "metadata": {},
   "source": [
    "Retrieval-Augmented Generation (RAG) is an architectural approach that can enhance the effectiveness of large language model (LLM) applications using customized data. In this example, we use LangChain, an orchestrator for language pipelines, to build an assistant capable of loading information from a web page and use it for answering user questions. We'll leverage torch and transformers for multimodal model support in Python. We'll also use the MLFlow platform to evaluate and trace the LLM responses (in `register-workflow.ipynb`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a55c55-a1dc-4697-b920-2a81f1bb0f74",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "- Configuring the Environment\n",
    "- Data Loading & Cleaning\n",
    "- Setup Embeddings & Vector Store\n",
    "- Retrieval Function\n",
    "- Model Setup & Chain Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f78b36-56e2-4596-8325-c75901f30c76",
   "metadata": {},
   "source": [
    "## Step 0: Configuring the Environment\n",
    "\n",
    "In this step, we import all the necessary libraries and internal components required to run the RAG pipeline, including modules for notebook parsing, embedding generation, vector storage, and code generation with LLMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7156d10f-d930-4be7-a9e8-15606d466460",
   "metadata": {},
   "source": [
    "By using our Local GenAI workspace image, many of the necessary libraries to work with RAG already come pre-installed - in our case, we just need to extra support for multimodal processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e700f51-9011-401c-90d3-a07ea8238955",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77853772-0239-40d0-94be-7a62fcb465c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "2025-07-18 06:44:09.915119: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-18 06:44:09.972947: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752821050.002323    6903 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752821050.011476    6903 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752821050.054241    6903 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752821050.054274    6903 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752821050.054276    6903 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752821050.054278    6903 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-18 06:44:10.065863: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# === Standard Library Imports ===\n",
    "import gc\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from statistics import mean\n",
    "from typing import Any, Dict, List, Optional, TypedDict\n",
    "\n",
    "# === Third-Party Library Imports ===\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import torch\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain.schema.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from PIL import Image as PILImage\n",
    "from sentence_transformers import CrossEncoder\n",
    "from transformers import AutoImageProcessor, AutoModel, AutoTokenizer, BitsAndBytesConfig, SiglipModel, SiglipProcessor\n",
    "\n",
    "\n",
    "# Define the relative path to the 'core' directory (one level up from current working directory)\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "# === Project-Specific Imports ===\n",
    "from src.utils import (\n",
    "    configure_hf_cache,\n",
    "    load_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f912d15a-7aab-4620-8c89-33c8f07104f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-18 06:44:13 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()  \n",
    "\n",
    "logger.info('Notebook execution started.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd3b8d2c-d0e1-4409-86dc-470c8454fff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "022490c4-8d8a-4c53-92ac-c62ea70a8209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logger\n",
    "logger = logging.getLogger(\"multimodal_rag_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\") \n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7384967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e80960-54f9-4ec2-b062-6d49526494d1",
   "metadata": {},
   "source": [
    "\n",
    "### Verify Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f2111d-5508-41ba-967c-256b40a9fb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"../configs/config.yaml\"\n",
    "\n",
    "CONTEXT_DIR: Path = Path(\"../data/context\")             \n",
    "CHROMA_DIR: Path = Path(\"../data/chroma_store\")     \n",
    "MEMORY_PATH: Path = Path(\"../data/memory/memory.json\")\n",
    "MANIFEST_PATH: Path = CHROMA_DIR / \"manifest.json\"\n",
    "\n",
    "IMAGE_DIR = CONTEXT_DIR / \"images\"\n",
    "WIKI_METADATA_DIR = CONTEXT_DIR / \"wiki_flat_structure.json\"\n",
    "\n",
    "MLFLOW_EXPERIMENT_NAME = \"AIStudio-Multimodal-Chatbot-Experiment\"\n",
    "MLFLOW_RUN_NAME = \"AIStudio-Multimodal-Chatbot-Run\"\n",
    "MLFLOW_MODEL_NAME = \"AIStudio-Multimodal-Model\"\n",
    "\n",
    "DEMO_FOLDER = \"../demo\"\n",
    "MLFLOW_MODEL_NAME = \"AIStudio-Multimodal-Chatbot-Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa07ae5b-c46e-4c26-9f8d-2a3d732348da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If folders do not exist, create them automatically\n",
    "for _dir in (CHROMA_DIR, MEMORY_PATH.parent):\n",
    "    _dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not MEMORY_PATH.exists():\n",
    "    MEMORY_PATH.write_text(\"{}\", encoding=\"utf‑8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4417e48c-2b51-4e61-b74d-8230caf2f7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-18 06:44:13 - INFO - Config is properly configured. \n",
      "2025-07-18 06:44:13 - INFO - Local InternVL-8B model is properly configured. \n",
      "2025-07-18 06:44:13 - INFO - Vision projector (.gguf) is properly configured. \n",
      "2025-07-18 06:44:13 - INFO - wiki_flat_structure.json is properly configured. \n",
      "2025-07-18 06:44:13 - INFO - CONTEXT is properly configured. \n",
      "2025-07-18 06:44:13 - INFO - CHROMA is properly configured. \n",
      "2025-07-18 06:44:13 - INFO - MEMORY is properly configured. \n",
      "2025-07-18 06:44:13 - INFO - MANIFEST is properly configured. \n"
     ]
    }
   ],
   "source": [
    "def log_asset_status(asset_path: str, asset_name: str, success_message: str, failure_message: str) -> None:\n",
    "    \"\"\"\n",
    "    Logs the status of a given asset based on its existence.\n",
    "\n",
    "    Parameters:\n",
    "        asset_path (str): File or directory path to check.\n",
    "        asset_name (str): Name of the asset for logging context.\n",
    "        success_message (str): Message to log if asset exists.\n",
    "        failure_message (str): Message to log if asset does not exist.\n",
    "    \"\"\"\n",
    "    if Path(asset_path).exists():\n",
    "        logger.info(f\"{asset_name} is properly configured. {success_message}\")\n",
    "    else:\n",
    "        logger.info(f\"{asset_name} is not properly configured. {failure_message}\")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=CONFIG_PATH,\n",
    "    asset_name=\"Config\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if the configs.yaml was propely connfigured in your project on AI Studio.\"\n",
    ")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=WIKI_METADATA_DIR,\n",
    "    asset_name=\"wiki_flat_structure.json\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Place JSON Wiki Pages in data/\")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=CONTEXT_DIR,\n",
    "    asset_name=\"CONTEXT\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if CONTEXT path was downloaded correctly in your project on AI Studio.\"\n",
    ")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=CHROMA_DIR,\n",
    "    asset_name=\"CHROMA\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if CHROMA path was downloaded correctly in your project on AI Studio.\"\n",
    ")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=MEMORY_PATH,\n",
    "    asset_name=\"MEMORY\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if the MEMORY path was propely connfigured in your project on AI Studio.\"\n",
    ")\n",
    "\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=MANIFEST_PATH,\n",
    "    asset_name=\"MANIFEST\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if the MANIFEST path was propely connfigured in your project on AI Studio.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7062ee0",
   "metadata": {},
   "source": [
    "### Config Loading\n",
    "\n",
    "In this section, we load configuration parameters from the YAML file in the configs folder.\n",
    "\n",
    "- **config.yaml**: Contains non-sensitive configuration parameters like model sources and URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0b7464-ad76-47f2-94f0-e70c4d4cc380",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.5' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/Users/ChJi083/AppData/Local/Microsoft/WindowsApps/python3.13.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "config = load_config(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ca6361-dd3c-4778-9a9d-b11da7b20151",
   "metadata": {},
   "source": [
    "### Config HuggingFace Caches\n",
    "\n",
    "In the next cell, we configure HuggingFace cache, so that all the models downloaded from them are persisted locally, even after the workspace is closed. This is a future desired feature for AI Studio and the GenAI addon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "965e4ea4-084a-4f32-9f51-9ead89dd16fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure HuggingFace cache\n",
    "configure_hf_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d47161-f197-409b-9a43-a3f59f1f695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Initialize HuggingFace Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/e5-large-v2\",\n",
    "    cache_folder=\"/tmp/hf_cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b7a32f-81e5-49bb-94c3-04a480e4be89",
   "metadata": {},
   "source": [
    "## Memory Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9473fbc-bbbb-40b2-b344-decd9322a303",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleKVMemory:\n",
    "    \"\"\"Very small persistent key-value store (JSON on disk).\"\"\"\n",
    "    def __init__(self, file_path: Path) -> None:\n",
    "        self.file_path = file_path\n",
    "        self._store    = self._load()\n",
    "\n",
    "    # Public ------------\n",
    "    def get(self, key: str) -> Optional[str]:\n",
    "        return self._store.get(key)\n",
    "\n",
    "    def set(self, key: str, value: str) -> None:\n",
    "        self._store[key] = value\n",
    "        self._dump()\n",
    "\n",
    "    # Private -----------\n",
    "    def _load(self) -> Dict[str, str]:\n",
    "        if self.file_path.exists():\n",
    "            try:\n",
    "                with self.file_path.open(\"r\", encoding=\"utf‑8\") as f:\n",
    "                    return json.load(f)\n",
    "            except Exception as exc:\n",
    "                logger.warning(\"Memory load failed (%s). Starting fresh.\", exc)\n",
    "        return {}\n",
    "\n",
    "    def _dump(self) -> None:\n",
    "        self.file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with self.file_path.open(\"w\", encoding=\"utf‑8\") as f:\n",
    "            json.dump(self._store, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "memory = SimpleKVMemory(MEMORY_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d92659e-23ea-4965-bb27-58cd1dd0f3ec",
   "metadata": {},
   "source": [
    "## Step 1: Data Loading & Cleaning\n",
    "\n",
    "We load wiki-pages from `wiki_flat_structure.json`, and:\n",
    "* remove any image name that  \n",
    "  – is empty / `None`  \n",
    "  – has an extension not in {png, jpg, jpeg, webp, gif}  \n",
    "  – points to a file that does **not** exist in `data/images/`\n",
    "* log every discarded image so we can fix the parser later.\n",
    "\n",
    "`wiki_flat_structure.json` is a custom json metadata for ADO Wiki data. It is flatly structured, with keys for filepath, md content, and a list of images. We also have a image folder that contains all the images for every md page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4c1589-285f-4e72-ab8e-5ae9fbd4043d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.5' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/Users/ChJi083/AppData/Local/Microsoft/WindowsApps/python3.13.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "VALID_EXTS = {\".png\", \".jpg\", \".jpeg\", \".webp\", \".gif\"}\n",
    "\n",
    "WIKI_METADATA_DIR   = Path(WIKI_METADATA_DIR)\n",
    "IMAGE_DIR = Path(IMAGE_DIR)\n",
    "\n",
    "def load_mm_docs_clean(json_path: Path, img_dir: Path) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load wiki Markdown + image references from *json_path*.\n",
    "    • Filters out images with bad extensions or missing files.\n",
    "    • Logs the first 20 broken refs.\n",
    "    • Returns a list[Document] where metadata = {source, images}\n",
    "    \"\"\"\n",
    "    bad_imgs, docs = [], []\n",
    "\n",
    "    rows = json.loads(json_path.read_text(\"utf-8\"))\n",
    "    for row in rows:\n",
    "        images_ok = []\n",
    "        for name in row.get(\"images\", []):\n",
    "            if not name: # empty\n",
    "                bad_imgs.append((row[\"path\"], name, \"empty\"))\n",
    "                continue\n",
    "            ext = Path(name).suffix.lower()\n",
    "            if ext not in VALID_EXTS: # unsupported ext\n",
    "                bad_imgs.append((row[\"path\"], name, f\"ext {ext}\"))\n",
    "                continue\n",
    "            img_path = img_dir / name\n",
    "            if not img_path.is_file(): # missing on disk\n",
    "                bad_imgs.append((row[\"path\"], name, \"missing file\"))\n",
    "                continue\n",
    "            images_ok.append(name)\n",
    "\n",
    "        docs.append(\n",
    "            Document(\n",
    "                page_content=row[\"content\"],\n",
    "                metadata={\"source\": row[\"path\"], \"images\": images_ok},\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # ---- summary logging ----------------------------------------------------\n",
    "    if bad_imgs:\n",
    "        logger.warning(\"⚠️ %d broken image refs filtered out\", len(bad_imgs))\n",
    "        for src, name, reason in bad_imgs[:20]:\n",
    "            logger.debug(\"  » %s → %s (%s)\", src, name or \"<EMPTY>\", reason)\n",
    "    else:\n",
    "        logger.info(\"✅ no invalid image refs found\")\n",
    "\n",
    "    return docs\n",
    "\n",
    "mm_raw_docs = load_mm_docs_clean(WIKI_METADATA_DIR, Path(IMAGE_DIR))\n",
    "\n",
    "def log_stage(name: str, docs: List[Document]):\n",
    "    logger.info(f\"{name}: {len(docs)} docs, avg_tokens={sum(len(d.page_content) for d in docs)/len(docs):.0f}\")\n",
    "log_stage(\"Docs loaded\", mm_raw_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc63ed05-192e-4f59-856c-cf9bff160fc8",
   "metadata": {},
   "source": [
    "## Step 2: Creation of Chunks\n",
    "\n",
    "Here, we split the loaded documents into chunks, so we have smaller and more specific texts to add to our vector database. \n",
    "\n",
    "We chunk based on header style, and then within each header style we futher chunk based on the provided chunk size. Each chunk retains the page name, which preserves the relevance of each chunk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dbd381-58b8-4569-aa75-b26d07556b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-18 06:44:19 - INFO - Chunking complete: 558 docs → 2574 chunks (avg 713 chars)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def chunk_documents(\n",
    "    docs,\n",
    "    chunk_size: int = 1200,\n",
    "    overlap: int = 200,\n",
    ") -> list[Document]:\n",
    "    \"\"\"\n",
    "    1) Split each wiki page on Markdown headers (#, ## …) to keep logical\n",
    "       sections together.\n",
    "    2) Recursively break long sections to <= `chunk_size` chars with `overlap`.\n",
    "    3) Prefix every chunk with its page-title and store the title in metadata.\n",
    "    \"\"\"\n",
    "    header_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=[(\"#\", \"title\"), (\"##\", \"section\")]\n",
    "    )\n",
    "    recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "    )\n",
    "\n",
    "    all_chunks: list[Document] = []\n",
    "    for doc in docs:\n",
    "        page_title = Path(doc.metadata[\"source\"]).stem.replace(\"-\", \" \")\n",
    "\n",
    "        # 1️. section‑level split (returns list[Document])\n",
    "        section_docs = header_splitter.split_text(doc.page_content)\n",
    "\n",
    "        for section in section_docs:\n",
    "            # 2. size‑based split inside each section\n",
    "            tiny_texts = recursive_splitter.split_text(section.page_content)\n",
    "\n",
    "            for idx, tiny in enumerate(tiny_texts):\n",
    "                all_chunks.append(\n",
    "                    Document(\n",
    "                        page_content=f\"{page_title}\\n\\n{tiny.strip()}\",\n",
    "                        metadata={\n",
    "                            \"title\": page_title,\n",
    "                            \"source\": doc.metadata[\"source\"],\n",
    "                            \"section_header\": section.metadata.get(\"header\", \"\"),\n",
    "                            \"chunk_id\": idx,\n",
    "                        },\n",
    "                    )\n",
    "                )\n",
    "    if all_chunks:\n",
    "        avg_len = int(mean(len(c.page_content) for c in all_chunks))\n",
    "        logger.info(\n",
    "            \"Chunking complete: %d docs → %d chunks (avg %d chars)\",\n",
    "            len(docs),\n",
    "            len(all_chunks),\n",
    "            avg_len,\n",
    "        )\n",
    "    else:\n",
    "        logger.warning(\"Chunking produced zero chunks for %d docs\", len(docs))\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "splits = chunk_documents(mm_raw_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33fda9b-e514-4ecd-b480-f07955e08233",
   "metadata": {},
   "source": [
    "## Step 3: Setup Embeddings & Vector Store\n",
    "Here we setup Siglip for Image embeddings, and also transform our cleaned text chunks into embeddings to be stored in Chroma. We store the chroma data locally on the disk to reduce memory usage. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cd5fbc",
   "metadata": {},
   "source": [
    "### Setup Text ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83d3d3f-87a8-4209-a805-59983479d629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-18 06:44:25 - INFO - Loading existing Chroma index from ../data/chroma_store\n",
      "2025-07-18 06:44:25 - INFO - Loaded existing image index (738 vectors).\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 1) TEXT store\n",
    "def _current_manifest() -> List[str]:\n",
    "    \"\"\"Returns an ordered list of every Markdown/JSON context file we index.\"\"\"\n",
    "    return sorted(str(p.resolve()) for p in CONTEXT_DIR.rglob(\"*.json\"))\n",
    "\n",
    "def _needs_rebuild() -> bool:\n",
    "    if not CHROMA_DIR.exists() or not MANIFEST_PATH.exists():\n",
    "        return True\n",
    "    try:\n",
    "        old = json.loads(MANIFEST_PATH.read_text())\n",
    "    except Exception:\n",
    "        return True\n",
    "    return old != _current_manifest()\n",
    "\n",
    "def _save_manifest(manifest: List[str]) -> None:\n",
    "    CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    MANIFEST_PATH.write_text(json.dumps(manifest, indent=2))\n",
    "\n",
    "def _build_text_db() -> Chroma:\n",
    "    collection = \"mm_text\"\n",
    "    if _needs_rebuild():\n",
    "        logger.info(\"Re‑indexing text context → %s …\", CHROMA_DIR)\n",
    "        chroma = Chroma.from_documents(\n",
    "            documents        = splits,            # you already created these\n",
    "            embedding        = embeddings,\n",
    "            collection_name  = collection,\n",
    "            persist_directory= str(CHROMA_DIR),\n",
    "        )\n",
    "        _save_manifest(_current_manifest())\n",
    "        return chroma\n",
    "    logger.info(\"Loading existing Chroma index from %s\", CHROMA_DIR)\n",
    "    return Chroma(\n",
    "        collection_name   = collection,\n",
    "        persist_directory = str(CHROMA_DIR),\n",
    "        embedding_function= embeddings,\n",
    "    )\n",
    "\n",
    "text_db = _build_text_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2c64bd",
   "metadata": {},
   "source": [
    "### Setup Image ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7511858c-b947-4420-a3f1-1f4a1ad82b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# --- Image‑embedding helper\n",
    "class SiglipEmbeddings(Embeddings):\n",
    "    def __init__(self,\n",
    "                 model_id: str = \"google/siglip2-base-patch16-224\",\n",
    "                 device: str | None = None):\n",
    "        self.device    = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model     = SiglipModel.from_pretrained(model_id).to(self.device)\n",
    "        self.processor = SiglipProcessor.from_pretrained(model_id)\n",
    "        self.torch     = torch\n",
    "        self.PILImage  = PILImage\n",
    "\n",
    "    # ---- private helpers ---------------------------------------------------\n",
    "    def _embed_text(self, txts: List[str]) -> np.ndarray:\n",
    "        inp = self.processor(text=txts, return_tensors=\"pt\",\n",
    "                             padding=True, truncation=True).to(self.device)\n",
    "        with self.torch.no_grad():\n",
    "            return self.model.get_text_features(**inp).cpu().numpy()\n",
    "\n",
    "    def _embed_imgs(self, paths):\n",
    "        imgs = [self.PILImage.open(p).convert(\"RGB\") for p in paths]\n",
    "        inp  = self.processor(images=imgs, return_tensors=\"pt\").to(self.device)\n",
    "        with self.torch.no_grad():\n",
    "            return self.model.get_image_features(**inp).cpu().numpy()\n",
    "\n",
    "    # ---- LangChain API -----------------------------------------------------\n",
    "    def embed_documents(self, docs: List[str]) -> List[List[float]]:\n",
    "        return self._embed_imgs(docs).tolist()\n",
    "\n",
    "    def embed_query(self, txt: str) -> List[float]:               # single str  (textual query)\n",
    "        return self._embed_text([txt])[0].tolist()\n",
    "\n",
    "siglip_embeddings = SiglipEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a39492",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#  Helper: walk all docs once and gather *unique* image vectors + metadata\n",
    "def _collect_image_vectors():\n",
    "    \"\"\"\n",
    "    Scans every wiki page for image references and returns three parallel lists:\n",
    "        img_paths : list[str]   → full file-system paths (for SigLIP)\n",
    "        img_ids   : list[str]   → unique key per (page, image) pair\n",
    "        img_meta  : list[dict]  → {\"source\": wiki_page, \"image\": file_name}\n",
    "    Runs in < 1s even for thousands of docs.\n",
    "    \"\"\"\n",
    "    img_paths, img_ids, img_meta = [], [], []\n",
    "    seen = set()\n",
    "\n",
    "    for doc in mm_raw_docs:                         # raw wiki pages\n",
    "        src = doc.metadata[\"source\"]\n",
    "        for name in doc.metadata.get(\"images\", []): # list[str]\n",
    "            img_id = f\"{src}::{name}\"\n",
    "            if img_id in seen:\n",
    "                continue                            # de‑dupe\n",
    "            seen.add(img_id)\n",
    "\n",
    "            img_paths.append(str(IMAGE_DIR / name))\n",
    "            img_ids.append(img_id)\n",
    "            img_meta.append({\"source\": src, \"image\": name})\n",
    "\n",
    "    return img_paths, img_ids, img_meta\n",
    "\n",
    "\n",
    "# 2) IMAGE store\n",
    "image_db = Chroma(\n",
    "    collection_name    = \"mm_image\",\n",
    "    persist_directory  = str(CHROMA_DIR),   # SAME dir as text db\n",
    "    embedding_function = siglip_embeddings, # <-- class you kept\n",
    ")\n",
    "\n",
    "# Populate vectors *only* if it is empty\n",
    "if not image_db._collection.count():\n",
    "    img_paths, img_ids, img_meta = _collect_image_vectors()\n",
    "    image_db.add_texts(texts=img_paths, metadatas=img_meta, ids=img_ids)\n",
    "    image_db.persist()\n",
    "    logger.info(\"Indexed %d unique images.\", len(img_paths))\n",
    "else:\n",
    "    logger.info(\"Loaded existing image index (%d vectors).\",\n",
    "                image_db._collection.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7999ff1f-1f6a-452c-848e-e3c67adf7766",
   "metadata": {},
   "source": [
    "## Step 4: Retrieval Function\n",
    "\n",
    "This code implements a multi-stage retrieval process that combines vector similarity search, cross-encoder reranking, and a hybrid scoring mechanism to select the most relevant text documents and associated images.\n",
    "\n",
    "Here, the system performs an initial similarity search against a `text_db` (likely a vector store like ChromaDB, given your imports). It uses the `query` to find the top `fetch_k` most similar text documents based on their initial embedding similarity. This step acts as a broad filter, quickly identifying a larger set of potentially relevant documents. The result includes both the documents `(docs)` and their initial similarity scores (init_scores). After the intial recall, we use a cross-encoder to rerank these `fetch_k` documents. Unlike the initial embedding similarity, a cross-encoder takes the query and each document content as a pair and provides a more nuanced relevance score by considering their interaction. We also implement Hybrid scoring and select the `top-k` documents at the end.\n",
    "\n",
    "Using the `top-k` documents, we retrieve images associated with those documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf1f37f-11ff-4bf7-bae6-5e15470a8ff8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.5' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/Users/ChJi083/AppData/Local/Microsoft/WindowsApps/python3.13.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "_cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "\n",
    "def retrieve_mm(\n",
    "    query: str,\n",
    "    k_txt: int = 8,\n",
    "    k_img: int = 8,\n",
    "    fetch_k: int = 20,\n",
    "    hybrid_weights: Dict[str, float] = {\"init\": 0.6, \"rerank\": 0.4},\n",
    "    boost_slug: float = 0.1,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    1) Coarse recall: top `fetch_k` docs + init_scores\n",
    "    2) Cross-encoder rerank: rerank_scores\n",
    "    3) Build hybrid_score = w1*init + w2*rerank + optional slug boost\n",
    "    4) Sort by hybrid_score, then pick top-k_txt **without repeating sources**\n",
    "    5) Image retrieval with $in filter\n",
    "    \"\"\"\n",
    "    # 1) Coarse recall\n",
    "    docs_and_init = text_db.similarity_search_with_score(query, k=fetch_k)\n",
    "    docs, init_scores = zip(*docs_and_init)\n",
    "\n",
    "    # 2) Rerank\n",
    "    pairs = [(query, d.page_content) for d in docs]\n",
    "    rerank_scores = _cross_encoder.predict(pairs)\n",
    "\n",
    "    # 3) Compute hybrid scores (+ slug boost)\n",
    "    slug = query.lower().replace(\" \", \"-\")\n",
    "    hybrid_scores = []\n",
    "    for d, init, rerank in zip(docs, init_scores, rerank_scores):\n",
    "        score = hybrid_weights[\"init\"] * init + hybrid_weights[\"rerank\"] * rerank\n",
    "        if slug in d.metadata.get(\"source\", \"\").lower():\n",
    "            score += boost_slug\n",
    "        hybrid_scores.append(score)\n",
    "\n",
    "    # 4) select top‑k_txt\n",
    "    scored_docs = list(zip(docs, hybrid_scores))\n",
    "    scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    top_docs_and_scores = scored_docs[:k_txt]\n",
    "\n",
    "    if not top_docs_and_scores:\n",
    "        return {\"docs\": [], \"images\": [], \"scores\": []}\n",
    "    selected_docs, final_scores = zip(*top_docs_and_scores)\n",
    "\n",
    "    # 5) Image retrieval\n",
    "    sources = [d.metadata[\"source\"] for d in selected_docs]\n",
    "    q_emb = siglip_embeddings.embed_query(query)\n",
    "    img_hits = image_db.similarity_search_by_vector(\n",
    "        q_emb,\n",
    "        k=k_img * 2,\n",
    "        filter={\"source\": {\"$in\": sources}},\n",
    "    )\n",
    "    images = [img.page_content for img in img_hits[:k_img]]\n",
    "\n",
    "    return {\n",
    "        \"docs\": list(selected_docs),\n",
    "        \"images\": images,\n",
    "        \"scores\": list(final_scores),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58329084-640e-4b25-be70-ac1ec90467e9",
   "metadata": {},
   "source": [
    "## Step 5: Model Setup & Chain Creation\n",
    "\n",
    "In this section, we set up our local Large Language Model (LLM) and integrate it into a Question Answering (QA) pipeline. We're using `internvl3-8b-instruct` as our multimodal model, which can process both text and images. This setup is encapsulated within the InternVLMM class, designed for efficient and robust multimodal interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fcccbf",
   "metadata": {},
   "source": [
    "### System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd3cdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are **AI Studio DevOps Assistant**. Everything you know for this turn is inside\n",
    "the <context> block. Follow *all* rules below:\n",
    "1. **Answer comprehensively from the context.**\n",
    "   - Provide detailed, thorough responses using all relevant information\n",
    "   - If the answer is missing, write: \"I don't know based on the provided context.\"\n",
    "   - Never invent facts or rely on outside knowledge.\n",
    "2. **Be detailed and structured.**\n",
    "   - For procedures, provide complete numbered steps with explanations\n",
    "   - Include the source from the context at the end of your answer in brackets and back ticks e.g. [``].\n",
    "   - Quote file paths / commands in back-ticks\n",
    "   - Include relevant examples and details from the context\n",
    "3. **Use all available information.**\n",
    "   - Draw from multiple documents when relevant\n",
    "   - Synthesize information to provide complete answers\n",
    "   - Explain what's in the images provided\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63929d26",
   "metadata": {},
   "source": [
    "### InternVLMM QA Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910ccb5a-fb0c-4f7c-bc2a-69f464f3ffa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashAttention2 is not installed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b0ce596a6b34915b44fa6c32c5201c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "class InternVLMM:\n",
    "    \"\"\"\n",
    "    Minimal, self-contained multimodal QA wrapper around InternVL3-8B-Instruct.\n",
    "    This class:\n",
    "      • loads / resets the model\n",
    "      • builds the prompt (<context>…)\n",
    "      • returns the model's answer (based on img and text) and also the top retrieved images\n",
    "    \"\"\"\n",
    "    MODEL_NAME = \"OpenGVLab/InternVL3-8B-Instruct\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.tok   = None\n",
    "        self.image_processor = None\n",
    "        self._load()\n",
    "\n",
    "    # ---------- public function ----------\n",
    "    def generate(self, query: str, **retrieval_kwargs) -> Dict[str, Any]: # <-- Update return type hint\n",
    "        \"\"\"\n",
    "        Run retrieval, prompt assembly, and model generation.\n",
    "        Returns a dictionary with the text reply and a list of used image paths.\n",
    "        \"\"\"\n",
    "        if self.model is None or self.tok is None:\n",
    "            return {\"reply\": \"Error: model not initialised.\", \"used_images\": []}\n",
    "    \n",
    "        # retrieve docs & images\n",
    "        hits = retrieve_mm(query, **retrieval_kwargs)\n",
    "        docs: List[Any]   = hits[\"docs\"]\n",
    "        images: List[str] = hits[\"images\"] # This is the list of paths you want to return\n",
    "    \n",
    "        if not docs and not images:\n",
    "            return {\"reply\": \"I don't know based on the provided context.\", \"used_images\": []}\n",
    "    \n",
    "        # ... (The middle of the function remains the same) ...\n",
    "        context = \"\\n\\n\".join(\n",
    "            f\"### Source: {d.metadata.get('source','unknown')}\\n{d.page_content}\"\n",
    "            for d in docs\n",
    "        )\n",
    "        prompt = (\n",
    "            f\"{SYSTEM_PROMPT}\\n\\n<context>\\n{context}\\n</context>\\n\\nUser query:\\n{query}\"\n",
    "        )\n",
    "        if images:\n",
    "            prompt += f\"\\n\\n[{len(images)} image(s) are provided for analysis]\"\n",
    "    \n",
    "        # generate\n",
    "        try:\n",
    "            self._clear_cuda()\n",
    "            pixel_values = self._process_images(images) if images else None\n",
    "            reply = self.model.chat(\n",
    "                self.tok,\n",
    "                pixel_values,\n",
    "                prompt,\n",
    "                generation_config=dict(\n",
    "                    max_new_tokens=8916,\n",
    "                    pad_token_id=self.tok.pad_token_id,\n",
    "                    eos_token_id=self.tok.eos_token_id,\n",
    "                ),\n",
    "            )\n",
    "            self._clear_cuda()\n",
    "            \n",
    "            # Return a dictionary instead of just the text string ## MODIFIED ##\n",
    "            return {\"reply\": reply, \"used_images\": images}\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            msg = str(e).lower()\n",
    "            if \"cuda\" in msg or \"out of memory\" in msg:\n",
    "                logger.warning(\"CUDA error – resetting model: %s\", e)\n",
    "                self._reset()\n",
    "                error_reply = \"I ran into a GPU memory error – please try again.\"\n",
    "            else:\n",
    "                logger.error(\"Runtime error: %s\", e)\n",
    "                error_reply = f\"Error: {e}\"\n",
    "            return {\"reply\": error_reply, \"used_images\": images}\n",
    "    # ---------- internal helpers ----------\n",
    "    def _load(self):\n",
    "        \"\"\"Load tokenizer, image_processor, & model. Handles 8-bit quant on GPUs, fp32 on CPU.\"\"\" # <-- (Updated docstring)\n",
    "        logger.info(\"Loading %s ...\", self.MODEL_NAME)\n",
    "        gc.collect()\n",
    "        self._clear_cuda()\n",
    "\n",
    "        self.tok = AutoTokenizer.from_pretrained(\n",
    "            self.MODEL_NAME, trust_remote_code=True\n",
    "        )\n",
    "        if self.tok.pad_token is None:\n",
    "            self.tok.pad_token = self.tok.eos_token\n",
    "\n",
    "        # ADD THIS BLOCK to load the image processor\n",
    "        self.image_processor = AutoImageProcessor.from_pretrained(\n",
    "            self.MODEL_NAME, trust_remote_code=True\n",
    "        )\n",
    "\n",
    "        q_cfg = (\n",
    "            BitsAndBytesConfig(load_in_8bit=True, llm_int8_threshold=6.0)\n",
    "            if DEVICE == \"cuda\"\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            self.MODEL_NAME,\n",
    "            quantization_config=q_cfg,\n",
    "            torch_dtype=(torch.bfloat16 if DEVICE == \"cuda\" else torch.float32),\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\" if DEVICE == \"cuda\" else None,\n",
    "        ).eval()\n",
    "        logger.info(\"Model loaded on %s.\", DEVICE)\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Free everything and reload (called after persistent CUDA errors).\"\"\"\n",
    "        logger.warning(\"Resetting InternVL model …\")\n",
    "        del self.model, self.tok, self.image_processor # <-- ADD self.image_processor\n",
    "        self.model = self.tok = self.image_processor = None # <-- ADD self.image_processor\n",
    "        gc.collect()\n",
    "        self._clear_cuda()\n",
    "        time.sleep(1)\n",
    "        self._load()\n",
    "\n",
    "    def _process_images(self, image_paths: List[str]):\n",
    "        \"\"\"\n",
    "        Convert a list of image filepaths to a single batched tensor.\n",
    "        \"\"\"\n",
    "        if not image_paths:\n",
    "            return None\n",
    "        try:\n",
    "            # Open all images from their file paths\n",
    "            pil_images = [PILImage.open(p).convert(\"RGB\") for p in image_paths]\n",
    "            \n",
    "            # The processor naturally handles a list of PIL images\n",
    "            processed_data = self.image_processor(images=pil_images, return_tensors=\"pt\")\n",
    "            pixel_values = processed_data['pixel_values']\n",
    "\n",
    "            # Match model device/dtype\n",
    "            target_dtype = next(self.model.parameters()).dtype if self.model else torch.float32\n",
    "            pixel_values = pixel_values.to(device=DEVICE, dtype=target_dtype)\n",
    "            \n",
    "            return pixel_values\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(\"Image processing failed for one or more images: %s\", e)\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def _clear_cuda():\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "# Initalize mm llm\n",
    "mm = InternVLMM()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be45e0ce",
   "metadata": {},
   "source": [
    "### Helper Function to Display Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5cd2b3-e88d-4172-8c9f-fec08ee55d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(image_paths: List[str], max_cols: int = 4):\n",
    "    \"\"\"\n",
    "    Opens and displays a list of images from their file paths in a grid.\n",
    "\n",
    "    Args:\n",
    "        image_paths: A list of file paths to the images.\n",
    "        max_cols: The maximum number of columns in the display grid.\n",
    "    \"\"\"\n",
    "    if not image_paths:\n",
    "        print(\"▶ No images to display.\")\n",
    "        return\n",
    "\n",
    "    print(f\"🖼️ Displaying {len(image_paths)} image(s):\")\n",
    "    \n",
    "    # --- Calculate grid size ---\n",
    "    num_images = len(image_paths)\n",
    "    cols = min(num_images, max_cols)\n",
    "    rows = math.ceil(num_images / cols)\n",
    "\n",
    "    # --- Create figure and axes ---\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 4))\n",
    "    \n",
    "    # Flatten the axes array for easy iteration, and handle the case of a single image\n",
    "    if num_images == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    # --- Loop through images and display them ---\n",
    "    for i, path in enumerate(image_paths):\n",
    "        ax = axes[i]\n",
    "        try:\n",
    "            # Open the image file\n",
    "            img = PILImage.open(path)\n",
    "            \n",
    "            # Display the image on the appropriate subplot\n",
    "            ax.imshow(img)\n",
    "            ax.set_title(path.split('/')[-1], fontsize=8) # Use filename as title\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            ax.text(0.5, 0.5, 'Image not found', ha='center', va='center', fontsize=9)\n",
    "        except Exception as e:\n",
    "            ax.text(0.5, 0.5, 'Error loading image', ha='center', va='center', fontsize=9)\n",
    "            print(f\"  - Error loading image '{path}': {e}\")\n",
    "        \n",
    "        ax.axis('off') # Hide the x/y axis for a cleaner look\n",
    "\n",
    "    # --- Hide any unused subplots ---\n",
    "    for j in range(num_images, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0057f873",
   "metadata": {},
   "source": [
    "## Step 6: Test Generation and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98ead24-203d-41fd-b60a-74f64445e800",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "\n",
    "question = \"How do i run blueprints locally?\"\n",
    "\n",
    "results = mm.generate(question)\n",
    "\n",
    "print(\"--- MODEL RESPONSE ---\")\n",
    "print(results[\"reply\"])\n",
    "print(\"----------------------\\n\")\n",
    "\n",
    "display_images(results[\"used_images\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64007551-edfa-4d50-a50e-e242194433ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "question2 = \"What are some feature flags that i can enable in AIStudio?\"\n",
    "results = mm.generate(question2)\n",
    "\n",
    "print(\"--- MODEL RESPONSE ---\")\n",
    "print(results[\"reply\"])\n",
    "print(\"----------------------\\n\")\n",
    "\n",
    "display_images(results[\"used_images\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6014059-66ef-47a9-b37b-868c15e32039",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "question3 = \"How do i manually clean my environment without hooh?\"\n",
    "results = mm.generate(question3)\n",
    "\n",
    "print(\"--- MODEL RESPONSE ---\")\n",
    "print(results[\"reply\"])\n",
    "print(\"----------------------\\n\")\n",
    "\n",
    "display_images(results[\"used_images\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a3a494-8f3c-4bce-8494-5c8bd7e150cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "question4 = \"How do I test a config after i sign it?\"\n",
    "results = mm.generate(question4)\n",
    "\n",
    "print(\"--- MODEL RESPONSE ---\")\n",
    "print(results[\"reply\"])\n",
    "print(\"----------------------\\n\")\n",
    "\n",
    "display_images(results[\"used_images\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbca87f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time: float = time.time()\n",
    "elapsed_time: float = end_time - start_time\n",
    "elapsed_minutes: int = int(elapsed_time // 60)\n",
    "elapsed_seconds: float = elapsed_time % 60\n",
    "\n",
    "logger.info(f\"⏱️ Total execution time: {elapsed_minutes}m {elapsed_seconds:.2f}s\")\n",
    "logger.info(\"✅ Notebook execution completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8147f34-6e3b-47f2-9a68-7e8077f71618",
   "metadata": {},
   "source": [
    "Built with ❤️ using Z by HP AI Studio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
