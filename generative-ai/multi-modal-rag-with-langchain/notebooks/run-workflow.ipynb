{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9ff4be-1fee-47eb-8d09-611c29a7a83f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h1 style=\"text-align: center; font-size: 50px;\">Multimodal RAG Chatbot with Langchain, Torch, Transformers</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbb3e26-125f-4151-9ed3-c84a3fd9081d",
   "metadata": {},
   "source": [
    "Retrieval-Augmented Generation (RAG) is an architectural approach that can enhance the effectiveness of large language model (LLM) applications using customized data. In this example, we use LangChain, an orchestrator for language pipelines, to build an assistant capable of loading information from a web page and use it for answering user questions. We'll leverage torch and transformers for multimodal model support in Python. We'll also use the MLFlow platform to evaluate and trace the LLM responses (in `register-workflow.ipynb`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a55c55-a1dc-4697-b920-2a81f1bb0f74",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "- Configuring the Environment\n",
    "- Data Loading & Cleaning\n",
    "- Setup Embeddings & Vector Store\n",
    "- Retrieval Function\n",
    "- Model Setup & Chain Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f78b36-56e2-4596-8325-c75901f30c76",
   "metadata": {},
   "source": [
    "## Step 0: Configuring the Environment\n",
    "\n",
    "In this step, we import all the necessary libraries and internal components required to run the RAG pipeline, including modules for notebook parsing, embedding generation, vector storage, and code generation with LLMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7156d10f-d930-4be7-a9e8-15606d466460",
   "metadata": {},
   "source": [
    "By using our Local GenAI workspace image, many of the necessary libraries to work with RAG already come pre-installed - in our case, we just need to extra support for multimodal processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e700f51-9011-401c-90d3-a07ea8238955",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77853772-0239-40d0-94be-7a62fcb465c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 07:22:43.991995: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-20 07:22:44.004616: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752996164.016306    4357 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752996164.019822    4357 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752996164.030290    4357 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752996164.030304    4357 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752996164.030305    4357 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752996164.030306    4357 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-20 07:22:44.033689: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# === Standard Library Imports ===\n",
    "import gc\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from statistics import mean\n",
    "from typing import Any, Dict, List, Optional, TypedDict\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "# === Third-Party Library Imports ===\n",
    "import mlflow\n",
    "import torch\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain.schema.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from PIL import Image as PILImage\n",
    "from sentence_transformers import CrossEncoder\n",
    "from transformers import AutoImageProcessor, AutoModel, AutoTokenizer, BitsAndBytesConfig, SiglipModel, SiglipProcessor\n",
    "\n",
    "\n",
    "# Define the relative path to the 'core' directory (one level up from current working directory)\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "# === Project-Specific Imports ===\n",
    "from src.utils import (\n",
    "    configure_hf_cache,\n",
    "    load_config,\n",
    "    display_images,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "022490c4-8d8a-4c53-92ac-c62ea70a8209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logger\n",
    "logger = logging.getLogger(\"multimodal_rag_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\") \n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f912d15a-7aab-4620-8c89-33c8f07104f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 07:22:46 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()  \n",
    "\n",
    "logger.info('Notebook execution started.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd3b8d2c-d0e1-4409-86dc-470c8454fff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7384967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e80960-54f9-4ec2-b062-6d49526494d1",
   "metadata": {},
   "source": [
    "\n",
    "### Verify Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75f2111d-5508-41ba-967c-256b40a9fb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"../configs/config.yaml\"\n",
    "\n",
    "LOCAL_MODEL: Path = Path(\"/home/jovyan/datafabric/InternVL3-8B-Instruct-1\")\n",
    "CONTEXT_DIR: Path = Path(\"../data/context\")             \n",
    "CHROMA_DIR: Path = Path(\"../data/chroma_store\")     \n",
    "CACHE_DIR: Path = CHROMA_DIR / \"semantic_cache\"\n",
    "MANIFEST_PATH: Path = CHROMA_DIR / \"manifest.json\"\n",
    "\n",
    "IMAGE_DIR = CONTEXT_DIR / \"images\"\n",
    "WIKI_METADATA_DIR = CONTEXT_DIR / \"wiki_flat_structure.json\"\n",
    "\n",
    "MLFLOW_EXPERIMENT_NAME = \"AIStudio-Multimodal-Chatbot-Experiment\"\n",
    "MLFLOW_RUN_NAME = \"AIStudio-Multimodal-Chatbot-Run\"\n",
    "MLFLOW_MODEL_NAME = \"AIStudio-Multimodal-Model\"\n",
    "\n",
    "DEMO_FOLDER = \"../demo\"\n",
    "MLFLOW_MODEL_NAME = \"AIStudio-Multimodal-Chatbot-Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4417e48c-2b51-4e61-b74d-8230caf2f7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 07:22:46 - INFO - Local Model is properly configured. \n",
      "2025-07-20 07:22:46 - INFO - Config is properly configured. \n",
      "2025-07-20 07:22:46 - INFO - wiki_flat_structure.json is properly configured. \n",
      "2025-07-20 07:22:46 - INFO - CONTEXT is properly configured. \n",
      "2025-07-20 07:22:46 - INFO - CHROMA is properly configured. \n",
      "2025-07-20 07:22:46 - INFO - CACHE is properly configured. \n",
      "2025-07-20 07:22:46 - INFO - MANIFEST is properly configured. \n"
     ]
    }
   ],
   "source": [
    "def log_asset_status(asset_path: str, asset_name: str, success_message: str, failure_message: str) -> None:\n",
    "    \"\"\"\n",
    "    Logs the status of a given asset based on its existence.\n",
    "\n",
    "    Parameters:\n",
    "        asset_path (str): File or directory path to check.\n",
    "        asset_name (str): Name of the asset for logging context.\n",
    "        success_message (str): Message to log if asset exists.\n",
    "        failure_message (str): Message to log if asset does not exist.\n",
    "    \"\"\"\n",
    "    if Path(asset_path).exists():\n",
    "        logger.info(f\"{asset_name} is properly configured. {success_message}\")\n",
    "    else:\n",
    "        logger.info(f\"{asset_name} is not properly configured. {failure_message}\")\n",
    "\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=LOCAL_MODEL,\n",
    "    asset_name=\"Local Model\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if the local model was properly configured in your project in your datafabrics folder.\"\n",
    ")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=CONFIG_PATH,\n",
    "    asset_name=\"Config\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if the configs.yaml was properly connfigured in your project on AI Studio.\"\n",
    ")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=WIKI_METADATA_DIR,\n",
    "    asset_name=\"wiki_flat_structure.json\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Place JSON Wiki Pages in data/\")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=CONTEXT_DIR,\n",
    "    asset_name=\"CONTEXT\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if CONTEXT path was downloaded correctly in your project on AI Studio.\"\n",
    ")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=CHROMA_DIR,\n",
    "    asset_name=\"CHROMA\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if CHROMA path was downloaded correctly in your project on AI Studio.\"\n",
    ")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=CACHE_DIR,\n",
    "    asset_name=\"CACHE\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if the CHROMA/CACHE path was properly connfigured in your project on AI Studio.\"\n",
    ")\n",
    "\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=MANIFEST_PATH,\n",
    "    asset_name=\"MANIFEST\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if the MANIFEST path was properly connfigured in your project on AI Studio.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7062ee0",
   "metadata": {},
   "source": [
    "### Config Loading\n",
    "\n",
    "In this section, we load configuration parameters from the YAML file in the configs folder.\n",
    "\n",
    "- **config.yaml**: Contains non-sensitive configuration parameters like model sources and URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e0b7464-ad76-47f2-94f0-e70c4d4cc380",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ca6361-dd3c-4778-9a9d-b11da7b20151",
   "metadata": {},
   "source": [
    "### Config HuggingFace Caches\n",
    "\n",
    "In the next cell, we configure HuggingFace cache, so that all the models downloaded from them are persisted locally, even after the workspace is closed. This is a future desired feature for AI Studio and the GenAI addon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "965e4ea4-084a-4f32-9f51-9ead89dd16fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure HuggingFace cache\n",
    "configure_hf_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41d47161-f197-409b-9a43-a3f59f1f695c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 639 ms, sys: 708 ms, total: 1.35 s\n",
      "Wall time: 13.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Initialize HuggingFace Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/e5-large-v2\",\n",
    "    cache_folder=\"/tmp/hf_cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b7a32f-81e5-49bb-94c3-04a480e4be89",
   "metadata": {},
   "source": [
    "## Memory Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9473fbc-bbbb-40b2-b344-decd9322a303",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticCache:\n",
    "    \"\"\"\n",
    "    A semantic cache using a Chroma vector store to find similar queries.\n",
    "    \"\"\"\n",
    "    def __init__(self, persist_directory: Path, embedding_function: Embeddings, collection_name: str = \"multimodal_cache\"):\n",
    "        self.embedding_function = embedding_function\n",
    "        self._vectorstore = Chroma(\n",
    "            collection_name=collection_name,\n",
    "            persist_directory=str(persist_directory),\n",
    "            embedding_function=self.embedding_function\n",
    "        )\n",
    "\n",
    "    def get(self, query: str, threshold: float = 0.90) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Searches for a semantically similar query in the cache.\n",
    "\n",
    "        Args:\n",
    "            query: The user's query.\n",
    "            threshold: The similarity threshold (1-cosine_distance) to consider a match.\n",
    "                       A higher value means a stricter match.\n",
    "\n",
    "        Returns:\n",
    "            The cached result dictionary if a sufficiently similar query is found, otherwise None.\n",
    "        \"\"\"\n",
    "        if self._vectorstore._collection.count() == 0:\n",
    "            return None # Cache is empty\n",
    "\n",
    "        # Find the most similar document (question) in the cache\n",
    "        results = self._vectorstore.similarity_search_with_score(query, k=1)\n",
    "        if not results:\n",
    "            return None\n",
    "\n",
    "        # LangChain's Chroma wrapper returns a distance score (0=identical, 1=dissimilar).\n",
    "        # We convert it to a similarity score.\n",
    "        most_similar_doc, score = results[0]\n",
    "        similarity = 1.0 - score\n",
    "\n",
    "        logger.info(f\"Most similar cached query: '{most_similar_doc.page_content}' (Similarity: {similarity:.4f})\")\n",
    "        \n",
    "        if similarity >= threshold:\n",
    "            # The answer and image paths are stored in the metadata\n",
    "            cached_result = most_similar_doc.metadata\n",
    "            # Metadata values are strings, so we need to parse the image list back\n",
    "            cached_result['used_images'] = json.loads(cached_result.get('used_images', '[]'))\n",
    "            return cached_result\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def set(self, query: str, result_dict: Dict[str, Any]) -> None:\n",
    "        \"\"\"\n",
    "        Adds a new query and its result to the cache.\n",
    "        The result dict's 'used_images' list is converted to a JSON string for storage.\n",
    "        \"\"\"\n",
    "        metadata_to_store = {\n",
    "            'reply': result_dict.get('reply', ''),\n",
    "            'used_images': json.dumps(result_dict.get('used_images', []))\n",
    "        }\n",
    "        \n",
    "        doc = Document(page_content=query, metadata=metadata_to_store)\n",
    "        self._vectorstore.add_documents([doc])\n",
    "        logger.info(f\"Added query to semantic cache: '{query}'\")\n",
    "\n",
    "    def delete(self, query: str) -> None:\n",
    "        \"\"\"\n",
    "        Finds and deletes a query and its cached response from the vector store.\n",
    "        \"\"\"\n",
    "        # Find the document to delete based on the query content\n",
    "        results = self._vectorstore.get(where={\"page_content\": query})\n",
    "        if results and 'ids' in results and results['ids']:\n",
    "            doc_id_to_delete = results['ids'][0]\n",
    "            self._vectorstore._collection.delete(ids=[doc_id_to_delete])\n",
    "            logger.info(f\"Cleared old cache for query: '{query}'\")\n",
    "\n",
    "# Initialize the semantic cache\n",
    "semantic_cache = SemanticCache(persist_directory=CACHE_DIR, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d92659e-23ea-4965-bb27-58cd1dd0f3ec",
   "metadata": {},
   "source": [
    "## Step 1: Data Loading & Cleaning\n",
    "\n",
    "We load wiki-pages from `wiki_flat_structure.json`, and:\n",
    "* remove any image name that  \n",
    "  – is empty / `None`  \n",
    "  – has an extension not in {png, jpg, jpeg, webp, gif}  \n",
    "  – points to a file that does **not** exist in `data/images/`\n",
    "* log every discarded image so we can fix the parser later.\n",
    "\n",
    "`wiki_flat_structure.json` is a custom json metadata for ADO Wiki data. It is flatly structured, with keys for filepath, md content, and a list of images. We also have a image folder that contains all the images for every md page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb4c1589-285f-4e72-ab8e-5ae9fbd4043d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 07:23:01 - WARNING - ⚠️ 90 broken image refs filtered out\n",
      "2025-07-20 07:23:01 - INFO - Docs loaded: 548 docs, avg_tokens=3076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 99 ms, sys: 34.6 ms, total: 134 ms\n",
      "Wall time: 1.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "VALID_EXTS = {\".png\", \".jpg\", \".jpeg\", \".webp\", \".gif\"}\n",
    "\n",
    "WIKI_METADATA_DIR   = Path(WIKI_METADATA_DIR)\n",
    "IMAGE_DIR = Path(IMAGE_DIR)\n",
    "\n",
    "def load_mm_docs_clean(json_path: Path, img_dir: Path) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load wiki Markdown + image references from *json_path*.\n",
    "    • Filters out images with bad extensions or missing files.\n",
    "    • Logs the first 20 broken refs.\n",
    "    • Returns a list[Document] where metadata = {source, images}\n",
    "    \"\"\"\n",
    "    bad_imgs, docs = [], []\n",
    "\n",
    "    rows = json.loads(json_path.read_text(\"utf-8\"))\n",
    "    for row in rows:\n",
    "        images_ok = []\n",
    "        for name in row.get(\"images\", []):\n",
    "            if not name: # empty\n",
    "                bad_imgs.append((row[\"path\"], name, \"empty\"))\n",
    "                continue\n",
    "            ext = Path(name).suffix.lower()\n",
    "            if ext not in VALID_EXTS: # unsupported ext\n",
    "                bad_imgs.append((row[\"path\"], name, f\"ext {ext}\"))\n",
    "                continue\n",
    "            img_path = img_dir / name\n",
    "            if not img_path.is_file(): # missing on disk\n",
    "                bad_imgs.append((row[\"path\"], name, \"missing file\"))\n",
    "                continue\n",
    "            images_ok.append(name)\n",
    "\n",
    "        docs.append(\n",
    "            Document(\n",
    "                page_content=row[\"content\"],\n",
    "                metadata={\"source\": row[\"path\"], \"images\": images_ok},\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # ---- summary logging ----------------------------------------------------\n",
    "    if bad_imgs:\n",
    "        logger.warning(\"⚠️ %d broken image refs filtered out\", len(bad_imgs))\n",
    "        for src, name, reason in bad_imgs[:20]:\n",
    "            logger.debug(\"  » %s → %s (%s)\", src, name or \"<EMPTY>\", reason)\n",
    "    else:\n",
    "        logger.info(\"✅ no invalid image refs found\")\n",
    "\n",
    "    return docs\n",
    "\n",
    "mm_raw_docs = load_mm_docs_clean(WIKI_METADATA_DIR, Path(IMAGE_DIR))\n",
    "\n",
    "def log_stage(name: str, docs: List[Document]):\n",
    "    logger.info(f\"{name}: {len(docs)} docs, avg_tokens={sum(len(d.page_content) for d in docs)/len(docs):.0f}\")\n",
    "log_stage(\"Docs loaded\", mm_raw_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc63ed05-192e-4f59-856c-cf9bff160fc8",
   "metadata": {},
   "source": [
    "## Step 2: Creation of Chunks\n",
    "\n",
    "Here, we split the loaded documents into chunks, so we have smaller and more specific texts to add to our vector database. \n",
    "\n",
    "We chunk based on header style, and then within each header style we futher chunk based on the provided chunk size. Each chunk retains the page name, which preserves the relevance of each chunk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3dbd381-58b8-4569-aa75-b26d07556b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 07:23:01 - INFO - Chunking complete: 548 docs → 2533 chunks (avg 711 chars)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 71.2 ms, sys: 7.66 ms, total: 78.8 ms\n",
      "Wall time: 77.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def chunk_documents(\n",
    "    docs,\n",
    "    chunk_size: int = 1200,\n",
    "    overlap: int = 200,\n",
    ") -> list[Document]:\n",
    "    \"\"\"\n",
    "    1) Split each wiki page on Markdown headers (#, ## …) to keep logical\n",
    "       sections together.\n",
    "    2) Recursively break long sections to <= `chunk_size` chars with `overlap`.\n",
    "    3) Prefix every chunk with its page-title and store the title in metadata.\n",
    "    \"\"\"\n",
    "    header_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=[(\"#\", \"title\"), (\"##\", \"section\")]\n",
    "    )\n",
    "    recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "    )\n",
    "\n",
    "    all_chunks: list[Document] = []\n",
    "    for doc in docs:\n",
    "        page_title = Path(doc.metadata[\"source\"]).stem.replace(\"-\", \" \")\n",
    "\n",
    "        # 1️. section‑level split (returns list[Document])\n",
    "        section_docs = header_splitter.split_text(doc.page_content)\n",
    "\n",
    "        for section in section_docs:\n",
    "            # 2. size‑based split inside each section\n",
    "            tiny_texts = recursive_splitter.split_text(section.page_content)\n",
    "\n",
    "            for idx, tiny in enumerate(tiny_texts):\n",
    "                all_chunks.append(\n",
    "                    Document(\n",
    "                        page_content=f\"{page_title}\\n\\n{tiny.strip()}\",\n",
    "                        metadata={\n",
    "                            \"title\": page_title,\n",
    "                            \"source\": doc.metadata[\"source\"],\n",
    "                            \"section_header\": section.metadata.get(\"header\", \"\"),\n",
    "                            \"chunk_id\": idx,\n",
    "                        },\n",
    "                    )\n",
    "                )\n",
    "    if all_chunks:\n",
    "        avg_len = int(mean(len(c.page_content) for c in all_chunks))\n",
    "        logger.info(\n",
    "            \"Chunking complete: %d docs → %d chunks (avg %d chars)\",\n",
    "            len(docs),\n",
    "            len(all_chunks),\n",
    "            avg_len,\n",
    "        )\n",
    "    else:\n",
    "        logger.warning(\"Chunking produced zero chunks for %d docs\", len(docs))\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "splits = chunk_documents(mm_raw_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33fda9b-e514-4ecd-b480-f07955e08233",
   "metadata": {},
   "source": [
    "## Step 3: Setup Embeddings & Vector Store\n",
    "Here we setup Siglip for Image embeddings, and also transform our cleaned text chunks into embeddings to be stored in Chroma. We store the chroma data locally on the disk to reduce memory usage. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cd5fbc",
   "metadata": {},
   "source": [
    "### Setup Text ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e83d3d3f-87a8-4209-a805-59983479d629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 07:23:01 - INFO - Loading existing Chroma index from ../data/chroma_store\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.95 ms, sys: 5.54 ms, total: 13.5 ms\n",
      "Wall time: 56.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 1) TEXT store\n",
    "def _current_manifest() -> List[str]:\n",
    "    \"\"\"Returns an ordered list of every Markdown/JSON context file we index.\"\"\"\n",
    "    return sorted(str(p.resolve()) for p in CONTEXT_DIR.rglob(\"*.json\"))\n",
    "\n",
    "def _needs_rebuild() -> bool:\n",
    "    if not CHROMA_DIR.exists() or not MANIFEST_PATH.exists():\n",
    "        return True\n",
    "    try:\n",
    "        old = json.loads(MANIFEST_PATH.read_text())\n",
    "    except Exception:\n",
    "        return True\n",
    "    return old != _current_manifest()\n",
    "\n",
    "def _save_manifest(manifest: List[str]) -> None:\n",
    "    CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    MANIFEST_PATH.write_text(json.dumps(manifest, indent=2))\n",
    "\n",
    "def _build_text_db() -> Chroma:\n",
    "    collection = \"mm_text\"\n",
    "    if _needs_rebuild():\n",
    "        logger.info(\"Re‑indexing text context → %s …\", CHROMA_DIR)\n",
    "        chroma = Chroma.from_documents(\n",
    "            documents        = splits,            # you already created these\n",
    "            embedding        = embeddings,\n",
    "            collection_name  = collection,\n",
    "            persist_directory= str(CHROMA_DIR),\n",
    "        )\n",
    "        _save_manifest(_current_manifest())\n",
    "        return chroma\n",
    "    logger.info(\"Loading existing Chroma index from %s\", CHROMA_DIR)\n",
    "    return Chroma(\n",
    "        collection_name   = collection,\n",
    "        persist_directory = str(CHROMA_DIR),\n",
    "        embedding_function= embeddings,\n",
    "    )\n",
    "\n",
    "text_db = _build_text_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2c64bd",
   "metadata": {},
   "source": [
    "### Setup Image ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7511858c-b947-4420-a3f1-1f4a1ad82b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.9 s, sys: 1.02 s, total: 2.93 s\n",
      "Wall time: 4.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# --- Image‑embedding helper\n",
    "class SiglipEmbeddings(Embeddings):\n",
    "    def __init__(self,\n",
    "                 model_id: str = \"google/siglip2-base-patch16-224\",\n",
    "                 device: str | None = None):\n",
    "        self.device    = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model     = SiglipModel.from_pretrained(model_id).to(self.device)\n",
    "        self.processor = SiglipProcessor.from_pretrained(model_id)\n",
    "        self.torch     = torch\n",
    "        self.PILImage  = PILImage\n",
    "\n",
    "    # ---- private helpers ---------------------------------------------------\n",
    "    def _embed_text(self, txts: List[str]) -> np.ndarray:\n",
    "        inp = self.processor(text=txts, return_tensors=\"pt\",\n",
    "                             padding=True, truncation=True).to(self.device)\n",
    "        with self.torch.no_grad():\n",
    "            return self.model.get_text_features(**inp).cpu().numpy()\n",
    "\n",
    "    def _embed_imgs(self, paths):\n",
    "        imgs = [self.PILImage.open(p).convert(\"RGB\") for p in paths]\n",
    "        inp  = self.processor(images=imgs, return_tensors=\"pt\").to(self.device)\n",
    "        with self.torch.no_grad():\n",
    "            return self.model.get_image_features(**inp).cpu().numpy()\n",
    "\n",
    "    # ---- LangChain API -----------------------------------------------------\n",
    "    def embed_documents(self, docs: List[str]) -> List[List[float]]:\n",
    "        return self._embed_imgs(docs).tolist()\n",
    "\n",
    "    def embed_query(self, txt: str) -> List[float]:               # single str  (textual query)\n",
    "        return self._embed_text([txt])[0].tolist()\n",
    "\n",
    "siglip_embeddings = SiglipEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50a39492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 07:23:06 - INFO - Loaded existing image index (739 vectors).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.88 ms, sys: 3.95 ms, total: 11.8 ms\n",
      "Wall time: 23.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#  Helper: walk all docs once and gather *unique* image vectors + metadata\n",
    "def _collect_image_vectors():\n",
    "    \"\"\"\n",
    "    Scans every wiki page for image references and returns three parallel lists:\n",
    "        img_paths : list[str]   → full file-system paths (for SigLIP)\n",
    "        img_ids   : list[str]   → unique key per (page, image) pair\n",
    "        img_meta  : list[dict]  → {\"source\": wiki_page, \"image\": file_name}\n",
    "    Runs in < 1s even for thousands of docs.\n",
    "    \"\"\"\n",
    "    img_paths, img_ids, img_meta = [], [], []\n",
    "    seen = set()\n",
    "\n",
    "    for doc in mm_raw_docs:                         # raw wiki pages\n",
    "        src = doc.metadata[\"source\"]\n",
    "        for name in doc.metadata.get(\"images\", []): # list[str]\n",
    "            img_id = f\"{src}::{name}\"\n",
    "            if img_id in seen:\n",
    "                continue                            # de‑dupe\n",
    "            seen.add(img_id)\n",
    "\n",
    "            img_paths.append(str(IMAGE_DIR / name))\n",
    "            img_ids.append(img_id)\n",
    "            img_meta.append({\"source\": src, \"image\": name})\n",
    "\n",
    "    return img_paths, img_ids, img_meta\n",
    "\n",
    "\n",
    "# 2) IMAGE store\n",
    "image_db = Chroma(\n",
    "    collection_name    = \"mm_image\",\n",
    "    persist_directory  = str(CHROMA_DIR),   # SAME dir as text db\n",
    "    embedding_function = siglip_embeddings, # <-- class you kept\n",
    ")\n",
    "\n",
    "# Populate vectors *only* if it is empty\n",
    "if not image_db._collection.count():\n",
    "    img_paths, img_ids, img_meta = _collect_image_vectors()\n",
    "    image_db.add_texts(texts=img_paths, metadatas=img_meta, ids=img_ids)\n",
    "    image_db.persist()\n",
    "    logger.info(\"Indexed %d unique images.\", len(img_paths))\n",
    "else:\n",
    "    logger.info(\"Loaded existing image index (%d vectors).\",\n",
    "                image_db._collection.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7999ff1f-1f6a-452c-848e-e3c67adf7766",
   "metadata": {},
   "source": [
    "## Step 4: Retrieval Function\n",
    "\n",
    "This code implements a multi-stage retrieval process that combines vector similarity search, cross-encoder reranking, and a hybrid scoring mechanism to select the most relevant text documents and associated images.\n",
    "\n",
    "Here, the system performs an initial similarity search against a `text_db` (likely a vector store like ChromaDB, given your imports). It uses the `query` to find the top `fetch_k` most similar text documents based on their initial embedding similarity. This step acts as a broad filter, quickly identifying a larger set of potentially relevant documents. The result includes both the documents `(docs)` and their initial similarity scores (init_scores). After the intial recall, we use a cross-encoder to rerank these `fetch_k` documents. Unlike the initial embedding similarity, a cross-encoder takes the query and each document content as a pair and provides a more nuanced relevance score by considering their interaction. We also implement Hybrid scoring and select the `top-k` documents at the end.\n",
    "\n",
    "Using the `top-k` documents, we retrieve images associated with those documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbf1f37f-11ff-4bf7-bae6-5e15470a8ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "\n",
    "def retrieve_mm(\n",
    "    query: str,\n",
    "    k_txt: int = 8,\n",
    "    k_img: int = 8,\n",
    "    fetch_k: int = 20,\n",
    "    hybrid_weights: Dict[str, float] = {\"init\": 0.6, \"rerank\": 0.4},\n",
    "    boost_slug: float = 0.1,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    1) Coarse recall: top `fetch_k` docs + init_scores\n",
    "    2) Cross-encoder rerank: rerank_scores\n",
    "    3) Build hybrid_score = w1*init + w2*rerank + optional slug boost\n",
    "    4) Sort by hybrid_score, then pick top-k_txt **without repeating sources**\n",
    "    5) Image retrieval with $in filter\n",
    "    \"\"\"\n",
    "    # 1) Coarse recall\n",
    "    docs_and_init = text_db.similarity_search_with_score(query, k=fetch_k)\n",
    "    docs, init_scores = zip(*docs_and_init)\n",
    "\n",
    "    # 2) Rerank\n",
    "    pairs = [(query, d.page_content) for d in docs]\n",
    "    rerank_scores = _cross_encoder.predict(pairs)\n",
    "\n",
    "    # 3) Compute hybrid scores (+ slug boost)\n",
    "    slug = query.lower().replace(\" \", \"-\")\n",
    "    hybrid_scores = []\n",
    "    for d, init, rerank in zip(docs, init_scores, rerank_scores):\n",
    "        score = hybrid_weights[\"init\"] * init + hybrid_weights[\"rerank\"] * rerank\n",
    "        if slug in d.metadata.get(\"source\", \"\").lower():\n",
    "            score += boost_slug\n",
    "        hybrid_scores.append(score)\n",
    "\n",
    "    # 4) select top‑k_txt\n",
    "    scored_docs = list(zip(docs, hybrid_scores))\n",
    "    scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    top_docs_and_scores = scored_docs[:k_txt]\n",
    "\n",
    "    if not top_docs_and_scores:\n",
    "        return {\"docs\": [], \"images\": [], \"scores\": []}\n",
    "    selected_docs, final_scores = zip(*top_docs_and_scores)\n",
    "\n",
    "    # 5) Image retrieval\n",
    "    sources = [d.metadata[\"source\"] for d in selected_docs]\n",
    "    q_emb = siglip_embeddings.embed_query(query)\n",
    "    img_hits = image_db.similarity_search_by_vector(\n",
    "        q_emb,\n",
    "        k=k_img * 2,\n",
    "        filter={\"source\": {\"$in\": sources}},\n",
    "    )\n",
    "    images = [img.page_content for img in img_hits[:k_img]]\n",
    "\n",
    "    return {\n",
    "        \"docs\": list(selected_docs),\n",
    "        \"images\": images,\n",
    "        \"scores\": list(final_scores),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58329084-640e-4b25-be70-ac1ec90467e9",
   "metadata": {},
   "source": [
    "## Step 5: Model Setup & Chain Creation\n",
    "\n",
    "In this section, we set up our local Large Language Model (LLM) and integrate it into a Question Answering (QA) pipeline. We're using `internvl3-8b-instruct` as our multimodal model, which can process both text and images. This setup is encapsulated within the InternVLMM class, designed for efficient and robust multimodal interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fcccbf",
   "metadata": {},
   "source": [
    "### System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffd3cdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are AI Studio DevOps Assistant. You MUST follow these rules and the response structure below:\n",
    "\n",
    "---\n",
    "## Core Rules\n",
    "1.  **Strictly Adhere to Context:** Your entire response MUST be based ONLY on the information within the `<context>` block and the provided image(s).\n",
    "2.  **No Outside Knowledge:** DO NOT use any of your pre-trained knowledge.\n",
    "3.  **Never Hallucinate:** Do not invent facts, details, or examples. If a detail is not explicitly mentioned, do not include it.\n",
    "4.  **Handle Missing Information:** If the answer to the user's query cannot be found in the provided context, you MUST stop and respond ONLY with the phrase: \"Based on the provided context, I cannot answer this question.\"\n",
    "---\n",
    "\n",
    "## Response Structure\n",
    "\n",
    "**1. Visual Analysis**\n",
    "First, provide a detailed description of what is shown in the provided image(s), based only on what you can see.\n",
    "\n",
    "**2. Synthesized Answer**\n",
    "Next, answer the user's original query. Your answer must synthesize information STRICTLY from your **Visual Analysis** and the provided text `<context>`.\n",
    "\n",
    "**3. Source Documents**\n",
    "At the very end of your response, cite the source from the context in brackets and backticks, like this: [`source-file-name.md`].\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63929d26",
   "metadata": {},
   "source": [
    "### InternVLMM QA Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "910ccb5a-fb0c-4f7c-bc2a-69f464f3ffa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 07:23:07 - INFO - Loading /home/jovyan/datafabric/InternVL3-8B-Instruct-1 ...\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashAttention2 is not installed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdaef4efc35c4c1799c856abb9dfde9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 07:24:34 - INFO - Model loaded on cuda.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.87 s, sys: 14.3 s, total: 24.2 s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class InternVLMM:\n",
    "    \"\"\"\n",
    "    Minimal, self-contained multimodal QA wrapper around InternVL3-8B-Instruct.\n",
    "    This class:\n",
    "      • loads / resets the model\n",
    "      • builds the prompt (<context>…)\n",
    "      • returns the model's answer (based on img and text) and also the top retrieved images\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cache: SemanticCache):\n",
    "        self.tok   = None\n",
    "        self.image_processor = None\n",
    "        self.cache = cache\n",
    "        self._load()\n",
    "\n",
    "    # ---------- public function ----------\n",
    "    def generate(self, query: str, force_regenerate: bool = False, **retrieval_kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run retrieval, prompt assembly, and model generation.\n",
    "        Returns a dictionary with the text reply and a list of used image paths.\n",
    "        \"\"\"\n",
    "        \n",
    "        # === 1. CHECK SEMANTIC CACHE (or bypass if forced) ===\n",
    "        if not force_regenerate:\n",
    "            cached_result = self.cache.get(query, threshold=0.98)\n",
    "            if cached_result:\n",
    "                logger.info(f\"SEMANTIC CACHE HIT for query: '{query}'\")\n",
    "                return cached_result\n",
    "        \n",
    "        if force_regenerate:\n",
    "            logger.info(f\"Forced regeneration for query: '{query}'. Clearing old cache entry.\")\n",
    "            self.cache.delete(query)\n",
    "\n",
    "        logger.info(f\"CACHE MISS for query: '{query}'. Running full pipeline.\")\n",
    "        if self.model is None or self.tok is None:\n",
    "            return {\"reply\": \"Error: model not initialised.\", \"used_images\": []}\n",
    "    \n",
    "        # === 2. RETRIEVE (if not cached) ===\n",
    "        hits = retrieve_mm(query, **retrieval_kwargs)\n",
    "        docs: List[Any]   = hits[\"docs\"]\n",
    "        images: List[str] = hits[\"images\"] # This is the list of paths you want to return\n",
    "    \n",
    "        if not docs and not images:\n",
    "            return {\"reply\": \"I don't know based on the provided context.\", \"used_images\": []}\n",
    "\n",
    "        # === 3. BUILD PROMPT & GENERATE ===\n",
    "        context = \"\\n\\n\".join(\n",
    "            f\"### Source: {d.metadata.get('source','unknown')}\\n{d.page_content}\"\n",
    "            for d in docs\n",
    "        )\n",
    "        prompt = (\n",
    "            f\"{SYSTEM_PROMPT}\\n\\n<context>\\n{context}\\n</context>\\n\\nUser query:\\n{query}\"\n",
    "        )\n",
    "        if images:\n",
    "            prompt += f\"\\n\\n[{len(images)} image(s) are provided for analysis]\"\n",
    "\n",
    "        # generate\n",
    "        try:\n",
    "            self._clear_cuda()\n",
    "            pixel_values = self._process_images(images) if images else None\n",
    "            reply = self.model.chat(\n",
    "                self.tok,\n",
    "                pixel_values,\n",
    "                prompt,\n",
    "                generation_config=dict(\n",
    "                    max_new_tokens=8916,\n",
    "                    pad_token_id=self.tok.pad_token_id,\n",
    "                    eos_token_id=self.tok.eos_token_id,\n",
    "                ),\n",
    "            )\n",
    "            self._clear_cuda()\n",
    "            result_dict = {\"reply\": reply, \"used_images\": images}\n",
    "            \n",
    "            # === 4. UPDATE CACHE ===\n",
    "            self.cache.set(query, result_dict)\n",
    "            \n",
    "            return result_dict\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            msg = str(e).lower()\n",
    "            if \"cuda\" in msg or \"out of memory\" in msg:\n",
    "                logger.warning(\"CUDA error – resetting model: %s\", e)\n",
    "                self._reset()\n",
    "                error_reply = \"I ran into a GPU memory error – please try again.\"\n",
    "            else:\n",
    "                logger.error(\"Runtime error: %s\", e)\n",
    "                error_reply = f\"Error: {e}\"\n",
    "            return {\"reply\": error_reply, \"used_images\": images}\n",
    "            \n",
    "    # ---------- internal helpers ----------\n",
    "    \n",
    "    def _load(self):\n",
    "        \"\"\"Load tokenizer, image_processor, & model. Handles 8-bit quant on GPUs, fp32 on CPU.\"\"\" # <-- (Updated docstring)\n",
    "        logger.info(\"Loading %s ...\", LOCAL_MODEL)\n",
    "        gc.collect()\n",
    "        self._clear_cuda()\n",
    "\n",
    "        self.tok = AutoTokenizer.from_pretrained(\n",
    "            LOCAL_MODEL, trust_remote_code=True\n",
    "        )\n",
    "        if self.tok.pad_token is None:\n",
    "            self.tok.pad_token = self.tok.eos_token\n",
    "\n",
    "        self.image_processor = AutoImageProcessor.from_pretrained(\n",
    "            LOCAL_MODEL, trust_remote_code=True\n",
    "        )\n",
    "\n",
    "        # q_cfg = (\n",
    "        #     BitsAndBytesConfig(load_in_8bit=True, llm_int8_threshold=6.0)\n",
    "        #     if DEVICE == \"cuda\"\n",
    "        #     else None\n",
    "        # )\n",
    "\n",
    "        q_cfg = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\", # Use the modern \"Normal Float 4\"\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16, # Speeds up computation\n",
    "            bnb_4bit_use_double_quant=True, # Minor memory improvement\n",
    "        )\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            LOCAL_MODEL,\n",
    "            quantization_config=q_cfg,\n",
    "            torch_dtype=(torch.bfloat16 if DEVICE == \"cuda\" else torch.float32),\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\" if DEVICE == \"cuda\" else None,\n",
    "        ).eval()\n",
    "        logger.info(\"Model loaded on %s.\", DEVICE)\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Free everything and reload (called after persistent CUDA errors).\"\"\"\n",
    "        logger.warning(\"Resetting InternVL model …\")\n",
    "        del self.model, self.tok, self.image_processor\n",
    "        self.model = self.tok = self.image_processor = None\n",
    "        gc.collect()\n",
    "        self._clear_cuda()\n",
    "        time.sleep(1)\n",
    "        self._load()\n",
    "\n",
    "    def _process_images(self, image_paths: List[str]):\n",
    "        \"\"\"\n",
    "        Convert a list of image filepaths to a single batched tensor.\n",
    "        \"\"\"\n",
    "        if not image_paths:\n",
    "            return None\n",
    "        try:\n",
    "            # Open all images from their file paths\n",
    "            pil_images = [PILImage.open(p).convert(\"RGB\") for p in image_paths]\n",
    "            \n",
    "            # The processor naturally handles a list of PIL images\n",
    "            processed_data = self.image_processor(images=pil_images, return_tensors=\"pt\")\n",
    "            pixel_values = processed_data['pixel_values']\n",
    "\n",
    "            # Match model device/dtype\n",
    "            target_dtype = next(self.model.parameters()).dtype if self.model else torch.float32\n",
    "            pixel_values = pixel_values.to(device=DEVICE, dtype=target_dtype)\n",
    "            \n",
    "            return pixel_values\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(\"Image processing failed for one or more images: %s\", e)\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def _clear_cuda():\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "# Initalize mm llm\n",
    "mm = InternVLMM(semantic_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0057f873",
   "metadata": {},
   "source": [
    "## Step 6: Test Generation and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98ead24-203d-41fd-b60a-74f64445e800",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "question = \"How do i run blueprints locally?\"\n",
    "\n",
    "results = mm.generate(question, force_regenerate=True)\n",
    "\n",
    "print(\"--- MODEL RESPONSE ---\")\n",
    "display(Markdown(results[\"reply\"]))\n",
    "\n",
    "print(\"----------------------\\n\")\n",
    "\n",
    "display_images(results[\"used_images\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64007551-edfa-4d50-a50e-e242194433ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "question2 = \"What are some feature flags that i can enable in AIStudio?\"\n",
    "results = mm.generate(question2, force_regenerate=True)\n",
    "\n",
    "print(\"--- MODEL RESPONSE ---\")\n",
    "display(Markdown(results[\"reply\"]))\n",
    "print(\"----------------------\\n\")\n",
    "\n",
    "display_images(results[\"used_images\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6014059-66ef-47a9-b37b-868c15e32039",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "question3 = \"How do i manually clean my environment without hooh?\"\n",
    "results = mm.generate(question3, force_regenerate=True)\n",
    "\n",
    "print(\"--- MODEL RESPONSE ---\")\n",
    "display(Markdown(results[\"reply\"]))\n",
    "print(\"----------------------\\n\")\n",
    "\n",
    "display_images(results[\"used_images\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a3a494-8f3c-4bce-8494-5c8bd7e150cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "question4 = \"How do i sign a config file?\"\n",
    "results = mm.generate(question4, force_regenerate=True)\n",
    "\n",
    "print(\"--- MODEL RESPONSE ---\")\n",
    "display(Markdown(results[\"reply\"]))\n",
    "print(\"----------------------\\n\")\n",
    "\n",
    "display_images(results[\"used_images\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbca87f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time: float = time.time()\n",
    "elapsed_time: float = end_time - start_time\n",
    "elapsed_minutes: int = int(elapsed_time // 60)\n",
    "elapsed_seconds: float = elapsed_time % 60\n",
    "\n",
    "logger.info(f\"⏱️ Total execution time: {elapsed_minutes}m {elapsed_seconds:.2f}s\")\n",
    "logger.info(\"✅ Notebook execution completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8147f34-6e3b-47f2-9a68-7e8077f71618",
   "metadata": {},
   "source": [
    "Built with ❤️ using Z by HP AI Studio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
