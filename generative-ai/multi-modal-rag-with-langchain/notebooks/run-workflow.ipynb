{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bd9ff4be-1fee-47eb-8d09-611c29a7a83f",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "<h1 style=\"text-align: center; font-size: 50px;\">Multimodal RAG Chatbot with Langchain, Torch, Transformers</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdbb3e26-125f-4151-9ed3-c84a3fd9081d",
      "metadata": {},
      "source": [
        "Retrieval-Augmented Generation (RAG) is an architectural approach that can enhance the effectiveness of large language model (LLM) applications using customized data. In this example, we use LangChain, an orchestrator for language pipelines, to build an assistant capable of loading information from a web page and use it for answering user questions. We'll leverage torch and transformers for multimodal model support in Python. We'll also use the MLFlow platform to evaluate and trace the LLM responses (in `register-workflow.ipynb`)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6a55c55-a1dc-4697-b920-2a81f1bb0f74",
      "metadata": {},
      "source": [
        "# Notebook Overview\n",
        "- Configuring the Environment\n",
        "- Data Loading & Cleaning\n",
        "- Setup Embeddings & Vector Store\n",
        "- Retrieval Function\n",
        "- Model Setup & Chain Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56f78b36-56e2-4596-8325-c75901f30c76",
      "metadata": {},
      "source": [
        "## Step 0: Configuring the Environment\n",
        "\n",
        "In this step, we import all the necessary libraries and internal components required to run the RAG pipeline, including modules for notebook parsing, embedding generation, vector storage, and code generation with LLMs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7156d10f-d930-4be7-a9e8-15606d466460",
      "metadata": {},
      "source": [
        "By using our Local GenAI workspace image, many of the necessary libraries to work with RAG already come pre-installed - in our case, we just need to extra support for multimodal processes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "022490c4-8d8a-4c53-92ac-c62ea70a8209",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import os \n",
        "from pathlib import Path\n",
        "import sys\n",
        "import logging\n",
        "\n",
        "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
        "\n",
        "# Create logger\n",
        "logger = logging.getLogger(\"multimodal_rag_logger\")\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\") \n",
        "stream_handler = logging.StreamHandler()\n",
        "stream_handler.setFormatter(formatter)\n",
        "logger.addHandler(stream_handler)\n",
        "logger.propagate = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f912d15a-7aab-4620-8c89-33c8f07104f0",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-02 04:23:58 - INFO - Notebook execution started.\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()  \n",
        "\n",
        "logger.info('Notebook execution started.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8e700f51-9011-401c-90d3-a07ea8238955",
      "metadata": {
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-aws 0.2.2 requires numpy<2.0.0,>=1.26.0; python_version >= \"3.12\", but you have numpy 2.1.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -r ../requirements.txt --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "77853772-0239-40d0-94be-7a62fcb465c6",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-02 04:27:33.027810: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-08-02 04:27:33.037554: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754108853.047694     285 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754108853.051199     285 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1754108853.064782     285 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754108853.064798     285 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754108853.064800     285 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754108853.064801     285 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-02 04:27:33.069091: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 08-02 04:27:35 [__init__.py:235] Automatically detected platform cuda.\n"
          ]
        }
      ],
      "source": [
        "# === Standard Library Imports ===\n",
        "import gc\n",
        "import json\n",
        "import math\n",
        "import hashlib\n",
        "import shutil\n",
        "import warnings\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from rank_bm25 import BM25Okapi\n",
        "from statistics import mean\n",
        "from typing import Any, Dict, List, Optional, TypedDict\n",
        "from IPython.display import display, Markdown\n",
        "from collections import defaultdict\n",
        "\n",
        "# === Third-Party Library Imports ===\n",
        "import mlflow\n",
        "import torch\n",
        "from langchain_core.embeddings import Embeddings\n",
        "from langchain.schema.document import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from PIL import Image as PILImage\n",
        "from transformers import AutoImageProcessor, AutoTokenizer\n",
        "from vllm import LLM, SamplingParams\n",
        "from qwen_vl_utils import process_vision_info\n",
        "\n",
        "# === Project-Specific Imports ===\n",
        "from src.components import SemanticCache, SiglipEmbeddings\n",
        "from src.wiki_pages_clone import orchestrate_wiki_clone\n",
        "from src.utils import (\n",
        "    configure_hf_cache,\n",
        "    multimodal_rag_asset_status,\n",
        "    load_config,\n",
        "    load_secrets,\n",
        "    load_mm_docs_clean,\n",
        "    display_images,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cd3b8d2c-d0e1-4409-86dc-470c8454fff8",
      "metadata": {},
      "outputs": [],
      "source": [
        "warnings.filterwarnings(\"ignore\")\n",
        "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e7384967",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09e80960-54f9-4ec2-b062-6d49526494d1",
      "metadata": {},
      "source": [
        "\n",
        "### Verify Assets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "75f2111d-5508-41ba-967c-256b40a9fb67",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-02 04:27:36 - INFO - Local Model is properly configured. \n",
            "2025-08-02 04:27:36 - INFO - Config is properly configured. \n",
            "2025-08-02 04:27:36 - INFO - Secrets is properly configured. \n",
            "2025-08-02 04:27:36 - INFO - wiki_flat_structure.json is properly configured. \n",
            "2025-08-02 04:27:36 - INFO - CONTEXT is properly configured. \n",
            "2025-08-02 04:27:36 - INFO - CHROMA is properly configured. \n",
            "2025-08-02 04:27:36 - INFO - CACHE is properly configured. \n",
            "2025-08-02 04:27:36 - INFO - MANIFEST is properly configured. \n"
          ]
        }
      ],
      "source": [
        "CONFIG_PATH = \"../configs/config.yaml\"\n",
        "SECRETS_PATH = \"../configs/secrets.yaml\"\n",
        "\n",
        "LOCAL_MODEL_PATH: Path = Path(\"/home/jovyan/datafabric/Qwen2.5-VL-7B-Instruct-quantized.w4a16\")\n",
        "CONTEXT_DIR: Path = Path(\"../data/context\")             \n",
        "CHROMA_DIR: Path = Path(\"../data/chroma_store\")     \n",
        "CACHE_DIR: Path = CHROMA_DIR / \"semantic_cache\"\n",
        "MANIFEST_PATH: Path = CHROMA_DIR / \"manifest.json\"\n",
        "\n",
        "IMAGE_DIR = CONTEXT_DIR / \"images\"\n",
        "WIKI_METADATA_DIR = CONTEXT_DIR / \"wiki_flat_structure.json\"\n",
        "\n",
        "DEMO_FOLDER = \"../demo\"\n",
        "\n",
        "CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "multimodal_rag_asset_status(\n",
        "    local_model_path=LOCAL_MODEL_PATH,\n",
        "    config_path=CONFIG_PATH,\n",
        "    secrets_path=SECRETS_PATH,\n",
        "    wiki_metadata_dir=WIKI_METADATA_DIR,\n",
        "    context_dir=CONTEXT_DIR,\n",
        "    chroma_dir=CHROMA_DIR,\n",
        "    cache_dir=CACHE_DIR,\n",
        "    manifest_path=MANIFEST_PATH\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7062ee0",
      "metadata": {},
      "source": [
        "### Config Loading\n",
        "\n",
        "In this section, we load configuration parameters from the YAML file in the configs folder.\n",
        "\n",
        "- **config.yaml**: Contains non-sensitive configuration parameters like model sources and URLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4e0b7464-ad76-47f2-94f0-e70c4d4cc380",
      "metadata": {},
      "outputs": [],
      "source": [
        "config = load_config(CONFIG_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20ca6361-dd3c-4778-9a9d-b11da7b20151",
      "metadata": {},
      "source": [
        "### Config HuggingFace Caches\n",
        "\n",
        "In the next cell, we configure HuggingFace cache, so that all the models downloaded from them are persisted locally, even after the workspace is closed. This is a future desired feature for AI Studio and the GenAI addon."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "965e4ea4-084a-4f32-9f51-9ead89dd16fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure HuggingFace cache\n",
        "configure_hf_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "41d47161-f197-409b-9a43-a3f59f1f695c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a970186bf0f54efd80675f65a8a51b26",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a584b38ac35849348cf7c512c91c8247",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a110d85c43d044199561e51b38b9b961",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac8b8bdbc8ad4c5ca6ef68029cbd794d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26f55ba298864adca9db6cfcbd898bcb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Initialize HuggingFace Embeddings\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"intfloat/e5-large-v2\",\n",
        "    cache_folder=\"/tmp/hf_cache\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d92659e-23ea-4965-bb27-58cd1dd0f3ec",
      "metadata": {},
      "source": [
        "## Step 1: Data Loading & Cleaning\n",
        "\n",
        "`wiki_flat_structure.json` is a custom json metadata for ADO Wiki data. It is flatly structured, with keys for filepath, md content, and a list of images. We also have a image folder that contains all the images for every md page. We directly scrape this data from ADO and perform any cleanup if necessary.\n",
        "\n",
        "- **secrets.yaml**: For Freemium users, use secrets.yaml to store your sensitive data like API Keys. If you are a Premium user, you can use secrets manager.\n",
        "- **AIS Secrets Manager**: For Paid users, use the secrets manager in the `Project Setup` tab to configure your API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "915bdc8a-0cb9-4b17-8d8f-a56acb7ffcd2",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-01 04:20:03 - INFO - Environment variable not found... Secrets Manager not properly set. Falling to secrets.yaml.\n",
            "2025-08-01 04:20:03 - INFO - Starting ADO Wiki clone process...\n",
            "2025-08-01 04:20:03 - INFO - Cloning wiki 'Phoenix-DS-Platform.wiki' to temporary directory: /tmp/tmptckb87n6\n",
            "2025-08-01 04:20:24 - INFO - Scanning for Markdown files...\n",
            "2025-08-01 04:20:24 - INFO - → Found 570 Markdown pages.\n",
            "2025-08-01 04:20:24 - INFO - Copying referenced images to ../data/context/images...\n",
            "2025-08-01 04:20:31 - INFO - → 753 unique images copied.\n",
            "2025-08-01 04:20:31 - INFO - Assembling flat JSON structure...\n",
            "2025-08-01 04:20:31 - INFO - ✅ Wiki data successfully cloned to ../data/context\n",
            "2025-08-01 04:20:31 - INFO - Cleaned up temporary directory: /tmp/tmptckb87n6\n",
            "2025-08-01 04:20:31 - INFO - ✅ Wiki data preparation step completed successfully.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 689 ms, sys: 808 ms, total: 1.5 s\n",
            "Wall time: 27.9 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "ADO_PAT = os.getenv(\"AIS_ADO_TOKEN\")\n",
        "if not ADO_PAT:\n",
        "    logger.info(\"Environment variable not found... Secrets Manager not properly set. Falling to secrets.yaml.\")\n",
        "    try:\n",
        "        secrets = load_secrets(SECRETS_PATH)\n",
        "        ADO_PAT = secrets.get('AIS_ADO_TOKEN')\n",
        "    except NameError:\n",
        "        logger.error(\"The 'secrets' object is not defined or available.\")\n",
        "\n",
        "try:\n",
        "    orchestrate_wiki_clone(\n",
        "        pat=ADO_PAT,\n",
        "        config=config,\n",
        "        output_dir=CONTEXT_DIR\n",
        "    )\n",
        "    logger.info(\"✅ Wiki data preparation step completed successfully.\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(\"Halting notebook execution due to a critical error in the wiki preparation step.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "fb4c1589-285f-4e72-ab8e-5ae9fbd4043d",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-01 04:20:32 - WARNING - ⚠️ 94 broken image refs filtered out\n",
            "2025-08-01 04:20:32 - INFO - Docs loaded: 570 docs, avg_tokens=3126\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 31.3 ms, sys: 45 ms, total: 76.2 ms\n",
            "Wall time: 753 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "WIKI_METADATA_DIR   = Path(WIKI_METADATA_DIR)\n",
        "IMAGE_DIR = Path(IMAGE_DIR)\n",
        "\n",
        "mm_raw_docs = load_mm_docs_clean(WIKI_METADATA_DIR, Path(IMAGE_DIR))\n",
        "\n",
        "def log_stage(name: str, docs: List[Document]):\n",
        "    logger.info(f\"{name}: {len(docs)} docs, avg_tokens={sum(len(d.page_content) for d in docs)/len(docs):.0f}\")\n",
        "log_stage(\"Docs loaded\", mm_raw_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc63ed05-192e-4f59-856c-cf9bff160fc8",
      "metadata": {},
      "source": [
        "## Step 2: Creation of Chunks\n",
        "\n",
        "Here, we split the loaded documents into chunks, so we have smaller and more specific texts to add to our vector database. \n",
        "\n",
        "We chunk based on header style, and then within each header style we futher chunk based on the provided chunk size. Each chunk retains the page name, which preserves the relevance of each chunk. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c3dbd381-58b8-4569-aa75-b26d07556b09",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-01 04:20:32 - INFO - Chunking complete: 570 docs → 2646 chunks (avg 720 chars)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 64.9 ms, sys: 7.4 ms, total: 72.3 ms\n",
            "Wall time: 68.2 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "def chunk_documents(\n",
        "    docs,\n",
        "    chunk_size: int = 1200,\n",
        "    overlap: int = 200,\n",
        ") -> list[Document]:\n",
        "    \"\"\"\n",
        "    1) Split each wiki page on Markdown headers (#, ## …) to keep logical\n",
        "       sections together.\n",
        "    2) Recursively break long sections to <= `chunk_size` chars with `overlap`.\n",
        "    3) Prefix every chunk with its page-title and store the title in metadata.\n",
        "    \"\"\"\n",
        "    header_splitter = MarkdownHeaderTextSplitter(\n",
        "        headers_to_split_on=[(\"#\", \"title\"), (\"##\", \"section\")]\n",
        "    )\n",
        "    recursive_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=overlap,\n",
        "    )\n",
        "\n",
        "    all_chunks: list[Document] = []\n",
        "    for doc in docs:\n",
        "        page_title = Path(doc.metadata[\"source\"]).stem.replace(\"-\", \" \")\n",
        "\n",
        "        # 1️. section‑level split (returns list[Document])\n",
        "        section_docs = header_splitter.split_text(doc.page_content)\n",
        "\n",
        "        for section in section_docs:\n",
        "            # 2. size‑based split inside each section\n",
        "            tiny_texts = recursive_splitter.split_text(section.page_content)\n",
        "\n",
        "            for idx, tiny in enumerate(tiny_texts):\n",
        "                all_chunks.append(\n",
        "                    Document(\n",
        "                        page_content=f\"{page_title}\\n\\n{tiny.strip()}\",\n",
        "                        metadata={\n",
        "                            \"title\": page_title,\n",
        "                            \"source\": doc.metadata[\"source\"],\n",
        "                            \"section_header\": section.metadata.get(\"header\", \"\"),\n",
        "                            \"chunk_id\": idx,\n",
        "                        },\n",
        "                    )\n",
        "                )\n",
        "    if all_chunks:\n",
        "        avg_len = int(mean(len(c.page_content) for c in all_chunks))\n",
        "        logger.info(\n",
        "            \"Chunking complete: %d docs → %d chunks (avg %d chars)\",\n",
        "            len(docs),\n",
        "            len(all_chunks),\n",
        "            avg_len,\n",
        "        )\n",
        "    else:\n",
        "        logger.warning(\"Chunking produced zero chunks for %d docs\", len(docs))\n",
        "\n",
        "    return all_chunks\n",
        "\n",
        "splits = chunk_documents(mm_raw_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c33fda9b-e514-4ecd-b480-f07955e08233",
      "metadata": {},
      "source": [
        "## Step 3: Setup Embeddings & Vector Store\n",
        "Here we setup Siglip for Image embeddings, and also transform our cleaned text chunks into embeddings to be stored in Chroma. We store the chroma data locally on the disk to reduce memory usage. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21cd5fbc",
      "metadata": {},
      "source": [
        "### Setup Text ChromaDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "e83d3d3f-87a8-4209-a805-59983479d629",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-01 04:20:32 - INFO - Loading existing Chroma index from ../data/chroma_store\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 163 ms, sys: 41.3 ms, total: 205 ms\n",
            "Wall time: 278 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# 1) TEXT store\n",
        "def _current_manifest() -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Returns a dictionary mapping every context JSON file to its SHA256 content hash.\n",
        "    This allows detecting changes in file content, not just filenames.\n",
        "    \"\"\"\n",
        "    manifest = {}\n",
        "    json_files = sorted(CONTEXT_DIR.rglob(\"*.json\"))\n",
        "\n",
        "    for file_path in json_files:\n",
        "        try:\n",
        "            with open(file_path, \"rb\") as f:\n",
        "                file_bytes = f.read()\n",
        "                file_hash = hashlib.sha256(file_bytes).hexdigest()\n",
        "                manifest[str(file_path.resolve())] = file_hash\n",
        "        except IOError as e:\n",
        "            logger.error(f\"Could not read file {file_path} for hashing: {e}\")\n",
        "    return manifest\n",
        "\n",
        "def _needs_rebuild() -> bool:\n",
        "    \"\"\"\n",
        "    Determines if the ChromaDB needs to be rebuilt.\n",
        "    A rebuild is needed if:\n",
        "    1. The Chroma directory or manifest file doesn't exist.\n",
        "    2. The manifest is unreadable.\n",
        "    3. The stored file hashes in the manifest do not match the current file hashes.\n",
        "    \"\"\"\n",
        "    if not CHROMA_DIR.exists() or not MANIFEST_PATH.exists():\n",
        "        logger.info(\"Chroma directory or manifest not found. A rebuild is required.\")\n",
        "        return True\n",
        "    try:\n",
        "        old_manifest = json.loads(MANIFEST_PATH.read_text())\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Could not read manifest file. A rebuild is required. Error: {e}\")\n",
        "        return True\n",
        "\n",
        "    current_manifest = _current_manifest()\n",
        "    if old_manifest != current_manifest:\n",
        "        logger.info(\"Data content has changed. A rebuild is required.\")\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def _save_manifest(manifest: Dict[str, str]) -> None:\n",
        "    \"\"\"Saves the current data manifest (mapping file paths to hashes) to disk.\"\"\"\n",
        "    CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    MANIFEST_PATH.write_text(json.dumps(manifest, indent=2))\n",
        "\n",
        "def _build_text_db() -> Chroma:\n",
        "    collection = \"mm_text\"\n",
        "    # The rebuild check is now done outside this function.\n",
        "    # We check if the directory exists. If not, we build.\n",
        "    if not CHROMA_DIR.exists() or not (CHROMA_DIR / \"chroma.sqlite3\").exists():\n",
        "        logger.info(\"Creating new text context index in %s ...\", CHROMA_DIR)\n",
        "        chroma = Chroma.from_documents(\n",
        "            documents          = splits,\n",
        "            embedding          = embeddings,\n",
        "            collection_name    = collection,\n",
        "            persist_directory  = str(CHROMA_DIR),\n",
        "        )\n",
        "        return chroma\n",
        "\n",
        "    logger.info(\"Loading existing Chroma index from %s\", CHROMA_DIR)\n",
        "    return Chroma(\n",
        "        collection_name   = collection,\n",
        "        persist_directory = str(CHROMA_DIR),\n",
        "        embedding_function= embeddings,\n",
        "    )\n",
        "    \n",
        "# Check if a rebuild is needed and wipe the old DB if so.\n",
        "# This ensures both the text and image databases are rebuilt from scratch.\n",
        "if _needs_rebuild():\n",
        "    logger.warning(\"REBUILDING: Wiping old ChromaDB store at %s\", CHROMA_DIR)\n",
        "    if CHROMA_DIR.exists():\n",
        "        shutil.rmtree(CHROMA_DIR)\n",
        "    # Save the new manifest immediately after deciding to rebuild\n",
        "    _save_manifest(_current_manifest())\n",
        "\n",
        "# Now, initialize your databases. They will be created fresh if they were just deleted.\n",
        "text_db = _build_text_db()\n",
        "CACHE_DIR.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b2c64bd",
      "metadata": {},
      "source": [
        "### Setup Image ChromaDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "50a39492",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c44217dc3fc5473f83dc98d6d2c60c6b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/253 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac402154fc394489ad741582711292be",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.50G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "030bfb29e3294d8e9025d94a41b0a075",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/394 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "42b61845b9564846a7bbcf7e18adb13c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ceef3802f58d4efaa30b7432810ffaee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9bddf515fabd440a9c733f71b560ab12",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30f9b207f3804d16951cc8d3825b4d9f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-01 04:21:49 - INFO - Indexed 767 unique images.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 34.7 s, sys: 9.76 s, total: 44.4 s\n",
            "Wall time: 1min 16s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "#  Helper: walk all docs once and gather *unique* image vectors + metadata\n",
        "def _collect_image_vectors():\n",
        "    \"\"\"\n",
        "    Scans every wiki page for image references and returns three parallel lists:\n",
        "        img_paths : list[str]   → full file-system paths (for SigLIP)\n",
        "        img_ids   : list[str]   → unique key per (page, image) pair\n",
        "        img_meta  : list[dict]  → {\"source\": wiki_page, \"image\": file_name}\n",
        "    Runs in < 1s even for thousands of docs.\n",
        "    \"\"\"\n",
        "    img_paths, img_ids, img_meta = [], [], []\n",
        "    seen = set()\n",
        "\n",
        "    for doc in mm_raw_docs:                         # raw wiki pages\n",
        "        src = doc.metadata[\"source\"]\n",
        "        for name in doc.metadata.get(\"images\", []): # list[str]\n",
        "            img_id = f\"{src}::{name}\"\n",
        "            if img_id in seen:\n",
        "                continue                            # de‑dupe\n",
        "            seen.add(img_id)\n",
        "\n",
        "            img_paths.append(str(IMAGE_DIR / name))\n",
        "            img_ids.append(img_id)\n",
        "            img_meta.append({\"source\": src, \"image\": name})\n",
        "\n",
        "    return img_paths, img_ids, img_meta\n",
        "\n",
        "siglip_embeddings = SiglipEmbeddings(\"google/siglip2-base-patch16-224\", DEVICE)\n",
        "\n",
        "# 2) IMAGE store\n",
        "image_db = Chroma(\n",
        "    collection_name    = \"mm_image\",\n",
        "    persist_directory  = str(CHROMA_DIR),   # SAME dir as text db\n",
        "    embedding_function = siglip_embeddings, # <-- class you kept\n",
        ")\n",
        "\n",
        "# Populate vectors *only* if it is empty\n",
        "if not image_db._collection.count():\n",
        "    img_paths, img_ids, img_meta = _collect_image_vectors()\n",
        "    image_db.add_texts(texts=img_paths, metadatas=img_meta, ids=img_ids)\n",
        "    image_db.persist()\n",
        "    logger.info(\"Indexed %d unique images.\", len(img_paths))\n",
        "else:\n",
        "    logger.info(\"Loaded existing image index (%d vectors).\",\n",
        "                image_db._collection.count())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9b7a32f-81e5-49bb-94c3-04a480e4be89",
      "metadata": {},
      "source": [
        "### Setup Memory Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c9473fbc-bbbb-40b2-b344-decd9322a303",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the semantic cache\n",
        "semantic_cache = SemanticCache(persist_directory=CACHE_DIR, embedding_function=embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88fd4358-0385-4cd3-bd2d-7da53a627418",
      "metadata": {},
      "source": [
        "## Step 4: Retrieval Function\n",
        "\n",
        "This code implements a hybrid retrieval process that combines two powerful search techniques to find the most relevant text documents and associated images.\n",
        "\n",
        "1.  **Initial Recall (Hybrid Search)**: The system performs two searches in parallel:\n",
        "    * **Dense Search**: A vector similarity search against `text_db` (ChromaDB) to find semantically related documents.\n",
        "    * **Sparse Search**: A keyword-based search using a `BM25` index to find documents with exact term matches.\n",
        "\n",
        "2.  **Fusion (RRF)**: The results from both searches are combined into a single, more robust ranked list using **Reciprocal Rank Fusion (RRF)**. This method intelligently merges the rankings without needing complex parameter tuning.\n",
        "\n",
        "3.  **Image Retrieval**: Using the top text documents from the fused list, the system performs a targeted search in the `image_db` to find images that are on the same source pages, ensuring contextual relevance.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "cbf1f37f-11ff-4bf7-bae6-5e15470a8ff8",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-01 04:21:49 - INFO - De-duplicated 2646 chunks down to 2635 unique chunks.\n"
          ]
        }
      ],
      "source": [
        "# This is necessary because the chunking process can sometimes create identical chunks.\n",
        "unique_docs_map = {doc.page_content: doc for doc in splits}\n",
        "unique_splits = list(unique_docs_map.values())\n",
        "\n",
        "logger.info(f\"De-duplicated {len(splits)} chunks down to {len(unique_splits)} unique chunks.\")\n",
        "\n",
        "# Now, build the BM25 index and the final doc_map using only the unique documents.\n",
        "# This ensures the index and the search corpus are perfectly aligned.\n",
        "corpus = [doc.page_content for doc in unique_splits]\n",
        "tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "doc_map = {doc.page_content: doc for doc in unique_splits}\n",
        "\n",
        "# %%\n",
        "# Helper function for Reciprocal Rank Fusion\n",
        "def reciprocal_rank_fusion(\n",
        "    results: list[list[Document]], k: int = 60\n",
        ") -> list[tuple[Document, float]]:\n",
        "    \"\"\"Performs RRF on multiple lists of ranked documents.\"\"\"\n",
        "    ranked_lists = [\n",
        "        {doc.page_content: (doc, i + 1) for i, doc in enumerate(res)}\n",
        "        for res in results\n",
        "    ]\n",
        "    rrf_scores = defaultdict(float)\n",
        "    all_docs = {}\n",
        "    for ranked_list in ranked_lists:\n",
        "        for content, (doc, rank) in ranked_list.items():\n",
        "            rrf_scores[content] += 1 / (k + rank)\n",
        "            if content not in all_docs:\n",
        "                all_docs[content] = doc\n",
        "    fused_results = [\n",
        "        (all_docs[content], rrf_scores[content])\n",
        "        for content in sorted(rrf_scores, key=rrf_scores.get, reverse=True)\n",
        "    ]\n",
        "    return fused_results\n",
        "\n",
        "\n",
        "def retrieve_mm(\n",
        "    query: str,\n",
        "    text_db: Chroma,\n",
        "    image_db: Chroma,\n",
        "    bm25_index: BM25Okapi,\n",
        "    doc_map: dict,\n",
        "    k_text: int = 3,\n",
        "    k_img: int = 4,\n",
        "    recall_k: int = 20,\n",
        ") -> dict[str, any]:\n",
        "    \"\"\"\n",
        "    Performs hybrid search for text and retrieves contextually relevant images.\n",
        "    \"\"\"\n",
        "    # 1. Hybrid Search for Text\n",
        "    dense_hits = text_db.similarity_search(query, k=recall_k)\n",
        "    tokenized_query = query.lower().split(\" \")\n",
        "    sparse_texts = bm25_index.get_top_n(tokenized_query, list(doc_map.keys()), n=recall_k)\n",
        "    sparse_hits = [doc_map[text] for text in sparse_texts]\n",
        "\n",
        "    if not dense_hits and not sparse_hits:\n",
        "        return {\"docs\": [], \"scores\": [], \"images\": []}\n",
        "\n",
        "    fused_results = reciprocal_rank_fusion([dense_hits, sparse_hits])\n",
        "    final_docs = [doc for doc, score in fused_results[:k_text]]\n",
        "    final_scores = [score for doc, score in fused_results[:k_text]]\n",
        "\n",
        "    # 2. Retrieve Relevant Images\n",
        "    retrieved_images = []\n",
        "    if final_docs:\n",
        "        # Get the source pages of the top text results\n",
        "        final_sources = list(set(d.metadata[\"source\"] for d in final_docs))\n",
        "\n",
        "        # Perform a vector search for images, filtered by the relevant sources\n",
        "        # The image_db's embedding function (SigLIP) will automatically handle the text query.\n",
        "        image_hits = image_db.similarity_search(\n",
        "            query,\n",
        "            k=k_img,\n",
        "            filter={\"source\": {\"$in\": final_sources}}\n",
        "        )\n",
        "        # The `page_content` of an image document is its path/name\n",
        "        retrieved_images = [img.page_content for img in image_hits]\n",
        "\n",
        "    return {\n",
        "        \"docs\": final_docs,\n",
        "        \"scores\": final_scores,\n",
        "        \"images\": retrieved_images,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58329084-640e-4b25-be70-ac1ec90467e9",
      "metadata": {},
      "source": [
        "## Step 5: Model Setup & Chain Creation\n",
        "\n",
        "In this section, we set up our local Large Language Model (LLM) and integrate it into a Question Answering (QA) pipeline. We're using `internvl3-8b-instruct` as our multimodal model, which can process both text and images. This setup is encapsulated within the InternVLMM class, designed for efficient and robust multimodal interactions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77fcccbf",
      "metadata": {},
      "source": [
        "### System Prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63929d26",
      "metadata": {},
      "source": [
        "### InternVLMM QA Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "910ccb5a-fb0c-4f7c-bc2a-69f464f3ffa4",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-01 04:21:49 - INFO - Loading /home/jovyan/datafabric/InternVL3-8B-Instruct ...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FlashAttention2 is not installed.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e289c88dbe541879d89701b0f660396",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-01 04:23:14 - INFO - Model loaded on cuda.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 12.4 s, sys: 10.4 s, total: 22.9 s\n",
            "Wall time: 1min 24s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "class QwenVLMM:\n",
        "    \"\"\"\n",
        "    Multimodal QA wrapper around the quantized Qwen2.5-VL model using vLLM.\n",
        "    Requires:\n",
        "      * `vllm` installed and importable.\n",
        "      * `qwen_vl_utils.process_vision_info` for multimodal image handling.\n",
        "      * HuggingFace transformers for tokenizer / image processor.\n",
        "      * External retrieval function (e.g., `retrieve_mm`) and a `SemanticCache`-like cache.\n",
        "    Expects the quantized safetensors model `RedHatAI/Qwen2.5-VL-7B-Instruct-quantized.w8a8`\n",
        "    to be accessible (vLLM will pull it from HuggingFace).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        cache,\n",
        "        text_db,\n",
        "        image_db,\n",
        "        bm25_index,\n",
        "        doc_map: dict,\n",
        "        model_name: str = \"RedHatAI/Qwen2.5-VL-7B-Instruct-quantized.w8a8\",\n",
        "        base_for_tokenizer: str = \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
        "        device: str = \"cuda\",\n",
        "    ):\n",
        "        self.cache = cache\n",
        "        self.text_db = text_db\n",
        "        self.image_db = image_db\n",
        "        self.bm25_index = bm25_index\n",
        "        self.doc_map = doc_map\n",
        "\n",
        "        self.model_name = model_name\n",
        "        self.base_for_tokenizer = base_for_tokenizer\n",
        "        self.device = device\n",
        "\n",
        "        self.tok = None\n",
        "        self.image_processor = None\n",
        "        self.llm = None  # vLLM instance\n",
        "\n",
        "        self._load()\n",
        "\n",
        "    # ---------- public function ----------\n",
        "    def generate(self, query: str, force_regenerate: bool = False, **retrieval_kwargs) -> dict:\n",
        "        \"\"\"\n",
        "        Run retrieval, prompt assembly, and model generation via vLLM.\n",
        "        \"\"\"\n",
        "        # 1. Cache check\n",
        "        if not force_regenerate:\n",
        "            cached_result = self.cache.get(query, threshold=0.92)\n",
        "            if cached_result:\n",
        "                logger.info(f\"SEMANTIC CACHE HIT for query: '{query}'\")\n",
        "                return cached_result\n",
        "        if force_regenerate:\n",
        "            logger.info(f\"Forced regeneration for query: '{query}'. Clearing old cache entry.\")\n",
        "            self.cache.delete(query)\n",
        "        logger.info(f\"CACHE MISS for query: '{query}'. Running full pipeline.\")\n",
        "\n",
        "        if self.llm is None or self.tok is None:\n",
        "            return {\"reply\": \"Error: model not initialised.\", \"used_images\": []}\n",
        "\n",
        "        # 2. Retrieval\n",
        "        hits = retrieve_mm(\n",
        "            query,\n",
        "            text_db=self.text_db,\n",
        "            image_db=self.image_db,\n",
        "            bm25_index=self.bm25_index,\n",
        "            doc_map=self.doc_map,\n",
        "            **retrieval_kwargs\n",
        "        )\n",
        "        docs = hits.get(\"docs\", [])\n",
        "        images = hits.get(\"images\", [])\n",
        "\n",
        "        if not docs and not images:\n",
        "            return {\"reply\": \"Based on the provided context, I cannot answer this question.\", \"used_images\": []}\n",
        "\n",
        "        # 3. Build prompt\n",
        "        context_str = \"\\n\\n\".join(\n",
        "            f\"<source_document name=\\\"{d.metadata.get('source', 'unknown')}\\\">\\n{d.page_content}\\n</source_document>\"\n",
        "            for d in docs\n",
        "        )\n",
        "\n",
        "        system_prompt = \"\"\"You are an AI Studio Expert Assistant. Your task is to answer the user's query based ONLY on the context provided. \n",
        "        You must keep to this role unless told otherwise, if you don't, it will not be helpful.\n",
        "        \n",
        "        **Instructions:**\n",
        "        1.  **Analyze Context:** First, analyze the user's images (if any) and the text in the `<context>` block.\n",
        "        2.  **Synthesize Answer:** Answer the user's query directly, synthesizing information from the context.\n",
        "        3.  **Cite Sources:** List all source documents you used in a `Source Documents` section.\n",
        "        4.  **Handle Missing Information:** If the answer is not in the context, respond with this exact phrase: \"Based on the provided context, I cannot answer this question.\"\n",
        "        5.  **Do not Hallucinate:** Do not hallucinate or make up factual information.\n",
        "        \n",
        "        **Output Format:**\n",
        "        Your response must follow this exact markdown structure and nothing else. Do not add any other commentary.\n",
        "        \n",
        "        ### Visual Analysis\n",
        "        (Analyze the user's images here.)\n",
        "        \n",
        "        ### Synthesized Answer\n",
        "        (Your answer to the user's query goes here.)\n",
        "        \n",
        "        ### Source Documents\n",
        "        (List the sources here, like [`source-file-name.md`].)\n",
        "        \"\"\"\n",
        "        \n",
        "        user_content = f\"\"\"<context>\n",
        "        {context_str}\n",
        "        </context>\n",
        "        \n",
        "        <user_query>\n",
        "        {query}\n",
        "        </user_query>\"\"\"\n",
        "\n",
        "        # Build chat-like string compatible with Qwen-style instruction\n",
        "        prompt_string = f\"<|system|>\\n{system_prompt}\\n<|end|>\\n<|user|>\\n{user_content}\\n<|end|>\"\n",
        "\n",
        "        # 4. Generation via vLLM\n",
        "        try:\n",
        "            self._clear_cuda()\n",
        "\n",
        "            vision_inputs = None\n",
        "            if images:\n",
        "                try:\n",
        "                    # process_vision_info should convert image paths into whatever multimodal tokens the model expects\n",
        "                    vision_inputs = process_vision_info(images)\n",
        "                except Exception as e:\n",
        "                    logger.warning(\"Vision processing failed, proceeding without visual input: %s\", e)\n",
        "                    vision_inputs = None\n",
        "\n",
        "            sampling_params = SamplingParams(\n",
        "                temperature=0.0,\n",
        "                top_p=1.0,\n",
        "                max_tokens=1024,\n",
        "            )\n",
        "\n",
        "            if vision_inputs is not None:\n",
        "                output = self.llm.generate(\n",
        "                    prompt_string,\n",
        "                    sampling_params=sampling_params,\n",
        "                    vision=vision_inputs,  # assumes vLLM wrapper accepts a `vision` kwarg for multimodal\n",
        "                )\n",
        "            else:\n",
        "                output = self.llm.generate(\n",
        "                    prompt_string,\n",
        "                    sampling_params=sampling_params,\n",
        "                )\n",
        "\n",
        "            if output.outputs:\n",
        "                reply = output.outputs[0].text.strip()\n",
        "            else:\n",
        "                reply = \"Error: no output from LLM.\"\n",
        "\n",
        "            self._clear_cuda()\n",
        "            result = {\"reply\": reply, \"used_images\": images}\n",
        "            self.cache.set(query, result)\n",
        "            return result\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            msg = str(e).lower()\n",
        "            if \"cuda\" in msg or \"out of memory\" in msg:\n",
        "                logger.warning(\"CUDA error – resetting model: %s\", e)\n",
        "                self._reset()\n",
        "                error_reply = \"I ran into a GPU memory error – please try again.\"\n",
        "            else:\n",
        "                logger.error(\"Runtime error: %s\", e)\n",
        "                error_reply = f\"Error: {e}\"\n",
        "            return {\"reply\": error_reply, \"used_images\": images}\n",
        "\n",
        "    # ---------- internal helpers ----------\n",
        "\n",
        "    def _load(self):\n",
        "        \"\"\"Load tokenizer, image_processor, & vLLM model.\"\"\"\n",
        "        logger.info(\"Loading Qwen2.5-VL via vLLM...\")\n",
        "        gc.collect()\n",
        "        self._clear_cuda()\n",
        "    \n",
        "        # Tokenizer & image processor (base model)\n",
        "        self.tok = AutoTokenizer.from_pretrained(\n",
        "            self.base_for_tokenizer, trust_remote_code=True\n",
        "        )\n",
        "        if self.tok.pad_token is None:\n",
        "            self.tok.pad_token = self.tok.eos_token\n",
        "    \n",
        "        self.image_processor = AutoImageProcessor.from_pretrained(\n",
        "            self.base_for_tokenizer, trust_remote_code=True, use_fast=True\n",
        "        )\n",
        "    \n",
        "        # Load vLLM with the quantized safetensors model (no use_mlock)\n",
        "        self.llm = LLM(\n",
        "            model=self.model_name,\n",
        "            gpu_memory_utilization=0.90,\n",
        "            tensor_parallel_size=1,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        logger.info(\"vLLM model loaded.\")\n",
        "\n",
        "\n",
        "    def _reset(self):\n",
        "        \"\"\"Free everything and reload on error.\"\"\"\n",
        "        logger.warning(\"Resetting InternQwenVLMM model …\")\n",
        "        try:\n",
        "            del self.llm, self.tok, self.image_processor\n",
        "        except Exception:\n",
        "            pass\n",
        "        self.llm = self.tok = self.image_processor = None\n",
        "        gc.collect()\n",
        "        self._clear_cuda()\n",
        "        time.sleep(1)\n",
        "        self._load()\n",
        "\n",
        "    @staticmethod\n",
        "    def _clear_cuda():\n",
        "        try:\n",
        "            import torch\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "                torch.cuda.synchronize()\n",
        "        except ImportError:\n",
        "            pass\n",
        "\n",
        "\n",
        "# Initalize mm llm\n",
        "mm = QwenVLMM(\n",
        "    cache=semantic_cache,\n",
        "    text_db=text_db,\n",
        "    image_db=image_db,\n",
        "    bm25_index=bm25,\n",
        "    doc_map=doc_map,\n",
        "    model_name=str(LOCAL_MODEL_PATH),  # quantized safetensors model\n",
        "    base_for_tokenizer=\"Qwen/Qwen2.5-VL-7B-Instruct\",  # for tokenizer / image processor\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0057f873",
      "metadata": {},
      "source": [
        "## Step 6: Test Generation and Outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c98ead24-203d-41fd-b60a-74f64445e800",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "question = \"What is the capital of paris?\"\n",
        "results = mm.generate(question, force_regenerate=True)\n",
        "\n",
        "print(\"--- MODEL RESPONSE ---\")\n",
        "display(Markdown(results[\"reply\"]))\n",
        "print(\"----------------------\\n\")\n",
        "\n",
        "display_images(results[\"used_images\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64007551-edfa-4d50-a50e-e242194433ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "question2 = \"What are some feature flags in AIStudio?\"\n",
        "results = mm.generate(question2, force_regenerate=True)\n",
        "\n",
        "print(\"--- MODEL RESPONSE ---\")\n",
        "display(Markdown(results[\"reply\"]))\n",
        "print(\"----------------------\\n\")\n",
        "\n",
        "display_images(results[\"used_images\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6014059-66ef-47a9-b37b-868c15e32039",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "question3 = \"How do i manually clean my environment without hooh?\"\n",
        "results = mm.generate(question3, force_regenerate=True)\n",
        "\n",
        "print(\"--- MODEL RESPONSE ---\")\n",
        "display(Markdown(results[\"reply\"]))\n",
        "print(\"----------------------\\n\")\n",
        "\n",
        "display_images(results[\"used_images\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81a3a494-8f3c-4bce-8494-5c8bd7e150cc",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "question4 = \"How do i sign a config file?\"\n",
        "results = mm.generate(question4, force_regenerate=True)\n",
        "\n",
        "print(\"--- MODEL RESPONSE ---\")\n",
        "display(Markdown(results[\"reply\"]))\n",
        "print(\"----------------------\\n\")\n",
        "\n",
        "display_images(results[\"used_images\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "dbca87f4",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-01 04:24:07 - INFO - ⏱️ Total execution time: 4m 42.56s\n",
            "2025-08-01 04:24:07 - INFO - ✅ Notebook execution completed successfully.\n"
          ]
        }
      ],
      "source": [
        "end_time: float = time.time()\n",
        "elapsed_time: float = end_time - start_time\n",
        "elapsed_minutes: int = int(elapsed_time // 60)\n",
        "elapsed_seconds: float = elapsed_time % 60\n",
        "\n",
        "logger.info(f\"⏱️ Total execution time: {elapsed_minutes}m {elapsed_seconds:.2f}s\")\n",
        "logger.info(\"✅ Notebook execution completed successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8147f34-6e3b-47f2-9a68-7e8077f71618",
      "metadata": {},
      "source": [
        "Built with ❤️ using Z by HP AI Studio."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
