{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9ff4be-1fee-47eb-8d09-611c29a7a83f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h1 style=\"text-align: center; font-size: 50px;\">Multimodal RAG Chatbot with Langchain and ML Flow Evaluation</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbb3e26-125f-4151-9ed3-c84a3fd9081d",
   "metadata": {},
   "source": [
    "Retrieval-Augmented Generation (RAG) is an architectural approach that can enhance the effectiveness of large language model (LLM) applications using customized data. In this example, we use LangChain, an orchestrator for language pipelines, to build an assistant capable of loading information from a web page and use it for answering user questions. We'll also use the DeepEval platform to evaluate, observe and protect the LLM responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a55c55-a1dc-4697-b920-2a81f1bb0f74",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "- Imports\n",
    "- Configurations\n",
    "- Verify Assets\n",
    "- Data Loading\n",
    "- Creation of Chunks\n",
    "- Retrieval\n",
    "- Model Setup\n",
    "- Chain Creation\n",
    "- Model Service "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f78b36-56e2-4596-8325-c75901f30c76",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7156d10f-d930-4be7-a9e8-15606d466460",
   "metadata": {},
   "source": [
    "By using our Local GenAI workspace image, many of the necessary libraries to work with RAG already come pre-installed - in our case, we just need to add the connector to work with PDF documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e700f51-9011-401c-90d3-a07ea8238955",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77853772-0239-40d0-94be-7a62fcb465c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "2025-07-16 23:21:33.459516: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-16 23:21:33.473781: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752708093.490510   10653 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752708093.495736   10653 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752708093.508879   10653 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752708093.508900   10653 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752708093.508902   10653 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752708093.508903   10653 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-16 23:21:33.513720: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# # === Standard Library Imports ===\n",
    "import base64\n",
    "import json\n",
    "import logging\n",
    "import mimetypes\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, TypedDict\n",
    "from IPython.display import Image, display\n",
    "from statistics import mean\n",
    "from PIL import Image as PILImage\n",
    "import io\n",
    "\n",
    "# === Third-Party Library Imports ===\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import torch\n",
    "from chromadb.config import Settings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.document_loaders import JSONLoader, WebBaseLoader\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document as CoreDocument\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_core.output_parsers import StrOutputParser as CoreStrOutputParser\n",
    "from langchain_core.runnables import Runnable, RunnableLambda, RunnablePassthrough\n",
    "from tqdm import tqdm\n",
    "from transformers import SiglipModel, SiglipProcessor\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from sentence_transformers import CrossEncoder\n",
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "# === ML Inference Backends ===\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Define the relative path to the 'core' directory (one level up from current working directory)\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "# === Project-Specific Imports ===\n",
    "from src.local_genai_judge import LocalGenAIJudge\n",
    "from src.utils import (\n",
    "    configure_hf_cache,\n",
    "    load_config,\n",
    "    mlflow_evaluate_setup,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c43204-6cb2-48f3-ac72-f821f12585b8",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd3b8d2c-d0e1-4409-86dc-470c8454fff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "022490c4-8d8a-4c53-92ac-c62ea70a8209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logger\n",
    "logger = logging.getLogger(\"multimodal_rag_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\") \n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75f2111d-5508-41ba-967c-256b40a9fb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"../configs/config.yaml\"\n",
    "\n",
    "CONTEXT_DIR: Path = Path(\"../data/context\")             \n",
    "CHROMA_DIR: Path = Path(\"../data/chroma_store\")     \n",
    "MEMORY_PATH: Path = Path(\"../data/memory/memory.json\")\n",
    "MANIFEST_PATH: Path = CHROMA_DIR / \"manifest.json\"\n",
    "\n",
    "IMAGE_DIR = CONTEXT_DIR / \"images\"\n",
    "WIKI_METADATA_DIR = CONTEXT_DIR / \"wiki_flat_structure.json\"\n",
    "\n",
    "MLFLOW_EXPERIMENT_NAME = \"AIStudio-Multimodal-Chatbot-Experiment\"\n",
    "MLFLOW_RUN_NAME = \"AIStudio-Multimodal-Chatbot-Run\"\n",
    "MLFLOW_MODEL_NAME = \"AIStudio-Multimodal-Model\"\n",
    "\n",
    "LOCAL_MODEL_PATH = \"/home/jovyan/datafabric/llama3.1-8b-instruct/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\"\n",
    "INTERNVL_MODEL_PATH = \"/home/jovyan/datafabric/InternVL3-8B-Instruct-Q8_0-1/InternVL3-8B-Instruct-Q8_0.gguf\"\n",
    "MM_PROJ_PATH = \"/home/jovyan/datafabric/mmproj-InternVL3-8B-Instruct-Q8_0-1/mmproj-InternVL3-8B-Instruct-Q8_0.gguf\"\n",
    "\n",
    "DEMO_FOLDER = \"../demo\"\n",
    "MLFLOW_MODEL_NAME = \"AIStudio-Multimodal-Chatbot-Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7384967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f912d15a-7aab-4620-8c89-33c8f07104f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 23:21:36 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()  \n",
    "\n",
    "logger.info('Notebook execution started.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e80960-54f9-4ec2-b062-6d49526494d1",
   "metadata": {},
   "source": [
    "\n",
    "# Verify Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa07ae5b-c46e-4c26-9f8d-2a3d732348da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If folders do not exist, create them automatically\n",
    "for _dir in (CHROMA_DIR, MEMORY_PATH.parent):\n",
    "    _dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not MEMORY_PATH.exists():\n",
    "    MEMORY_PATH.write_text(\"{}\", encoding=\"utf‑8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4417e48c-2b51-4e61-b74d-8230caf2f7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 23:21:36 - INFO - Config is properly configured. \n",
      "2025-07-16 23:21:36 - INFO - Local InternVL-8B model is properly configured. \n",
      "2025-07-16 23:21:36 - INFO - Vision projector (.gguf) is properly configured. \n",
      "2025-07-16 23:21:36 - INFO - wiki_flat_structure.json is properly configured. \n",
      "2025-07-16 23:21:36 - INFO - CONTEXT is properly configured. \n",
      "2025-07-16 23:21:36 - INFO - CHROMA is properly configured. \n",
      "2025-07-16 23:21:36 - INFO - MEMORY is properly configured. \n",
      "2025-07-16 23:21:36 - INFO - MANIFEST is properly configured. \n"
     ]
    }
   ],
   "source": [
    "def log_asset_status(asset_path: str, asset_name: str, success_message: str, failure_message: str) -> None:\n",
    "    \"\"\"\n",
    "    Logs the status of a given asset based on its existence.\n",
    "\n",
    "    Parameters:\n",
    "        asset_path (str): File or directory path to check.\n",
    "        asset_name (str): Name of the asset for logging context.\n",
    "        success_message (str): Message to log if asset exists.\n",
    "        failure_message (str): Message to log if asset does not exist.\n",
    "    \"\"\"\n",
    "    if Path(asset_path).exists():\n",
    "        logger.info(f\"{asset_name} is properly configured. {success_message}\")\n",
    "    else:\n",
    "        logger.info(f\"{asset_name} is not properly configured. {failure_message}\")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=CONFIG_PATH,\n",
    "    asset_name=\"Config\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if the configs.yaml was propely connfigured in your project on AI Studio.\"\n",
    ")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=INTERNVL_MODEL_PATH,\n",
    "    asset_name=\"Local InternVL-8B model\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please create and download the required assets in your project on AI Studio if you want to use local model.\")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=MM_PROJ_PATH,\n",
    "    asset_name=\"Vision projector (.gguf)\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Download mmproj-InternVL3-8B-Instruct-Q8_0.gguf\")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=WIKI_METADATA_DIR,\n",
    "    asset_name=\"wiki_flat_structure.json\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Place JSON Wiki Pages in data/\")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=CONTEXT_DIR,\n",
    "    asset_name=\"CONTEXT\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if CONTEXT path was downloaded correctly in your project on AI Studio.\"\n",
    ")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=CHROMA_DIR,\n",
    "    asset_name=\"CHROMA\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if CHROMA path was downloaded correctly in your project on AI Studio.\"\n",
    ")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=MEMORY_PATH,\n",
    "    asset_name=\"MEMORY\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if the MEMORY path was propely connfigured in your project on AI Studio.\"\n",
    ")\n",
    "\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=MANIFEST_PATH,\n",
    "    asset_name=\"MANIFEST\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if the MANIFEST path was propely connfigured in your project on AI Studio.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b7a32f-81e5-49bb-94c3-04a480e4be89",
   "metadata": {},
   "source": [
    "# Memory Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9473fbc-bbbb-40b2-b344-decd9322a303",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleKVMemory:\n",
    "    \"\"\"Very small persistent key‑value store (JSON on disk).\"\"\"\n",
    "    def __init__(self, file_path: Path) -> None:\n",
    "        self.file_path = file_path\n",
    "        self._store    = self._load()\n",
    "\n",
    "    # Public ------------\n",
    "    def get(self, key: str) -> Optional[str]:\n",
    "        return self._store.get(key)\n",
    "\n",
    "    def set(self, key: str, value: str) -> None:\n",
    "        self._store[key] = value\n",
    "        self._dump()\n",
    "\n",
    "    # Private -----------\n",
    "    def _load(self) -> Dict[str, str]:\n",
    "        if self.file_path.exists():\n",
    "            try:\n",
    "                with self.file_path.open(\"r\", encoding=\"utf‑8\") as f:\n",
    "                    return json.load(f)\n",
    "            except Exception as exc:\n",
    "                logger.warning(\"Memory load failed (%s). Starting fresh.\", exc)\n",
    "        return {}\n",
    "\n",
    "    def _dump(self) -> None:\n",
    "        self.file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with self.file_path.open(\"w\", encoding=\"utf‑8\") as f:\n",
    "            json.dump(self._store, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "memory = SimpleKVMemory(MEMORY_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "382208d4-aa3c-4968-bd5f-aca8510cceee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGState(TypedDict, total=False):\n",
    "    topic            : str\n",
    "    query            : str\n",
    "    answer           : Optional[str]\n",
    "    retrieved_docs   : List[Document]\n",
    "    images           : List[str]\n",
    "    from_memory      : Optional[bool]\n",
    "    messages         : List[Dict[str, Any]]   # full chat‑history if you need it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ca6361-dd3c-4778-9a9d-b11da7b20151",
   "metadata": {},
   "source": [
    "## Configuration of HuggingFace caches\n",
    "\n",
    "In the next cell, we configure HuggingFace cache, so that all the models downloaded from them are persisted locally, even after the workspace is closed. This is a future desired feature for AI Studio and the GenAI addon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "965e4ea4-084a-4f32-9f51-9ead89dd16fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure HuggingFace cache\n",
    "configure_hf_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41d47161-f197-409b-9a43-a3f59f1f695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HuggingFace Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/e5-large-v2\",\n",
    "    cache_folder=\"/tmp/hf_cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7062ee0",
   "metadata": {},
   "source": [
    "## Configuration and Secrets Loading\n",
    "\n",
    "In this section, we load configuration parameters and API keys from separate YAML files. This separation helps maintain security by keeping sensitive information (API keys) separate from configuration settings.\n",
    "\n",
    "- **config.yaml**: Contains non-sensitive configuration parameters like model sources and URLs\n",
    "- **secrets.yaml**: Contains sensitive API keys for services like Galileo and HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e0b7464-ad76-47f2-94f0-e70c4d4cc380",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d92659e-23ea-4965-bb27-58cd1dd0f3ec",
   "metadata": {},
   "source": [
    "# Data Loading & Cleaning\n",
    "\n",
    "We load wiki-pages from `wiki_flat_structure.json`, but:\n",
    "* remove any image name that  \n",
    "  – is empty / `None`  \n",
    "  – contains invalid characters (e.g. the `==image_0==` placeholders)  \n",
    "  – has an extension not in {png, jpg, jpeg, webp, gif}  \n",
    "  – points to a file that does **not** exist in `data/images/`\n",
    "* log every discarded image so we can fix the parser later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb4c1589-285f-4e72-ab8e-5ae9fbd4043d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 23:21:42 - WARNING - ⚠️ 94 broken image refs filtered out\n",
      "2025-07-16 23:21:42 - INFO - Docs loaded: 558 docs, avg_tokens=3082\n"
     ]
    }
   ],
   "source": [
    "VALID_EXTS = {\".png\", \".jpg\", \".jpeg\", \".webp\", \".gif\"}\n",
    "\n",
    "WIKI_METADATA_DIR   = Path(WIKI_METADATA_DIR)\n",
    "IMAGE_DIR = Path(IMAGE_DIR)\n",
    "\n",
    "def load_mm_docs_clean(json_path: Path, img_dir: Path) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load wiki Markdown + image references from *json_path*.\n",
    "    • Filters out images with bad extensions or missing files.\n",
    "    • Logs the first 20 broken refs.\n",
    "    • Returns a list[Document] where metadata = {source, images}\n",
    "    \"\"\"\n",
    "    bad_imgs, docs = [], []\n",
    "\n",
    "    rows = json.loads(json_path.read_text(\"utf-8\"))\n",
    "    for row in rows:\n",
    "        images_ok = []\n",
    "        for name in row.get(\"images\", []):\n",
    "            if not name:                                     # empty / placeholder\n",
    "                bad_imgs.append((row[\"path\"], name, \"empty\"))\n",
    "                continue\n",
    "            ext = Path(name).suffix.lower()\n",
    "            if ext not in VALID_EXTS:                       # unsupported ext\n",
    "                bad_imgs.append((row[\"path\"], name, f\"ext {ext}\"))\n",
    "                continue\n",
    "            img_path = img_dir / name\n",
    "            if not img_path.is_file():                      # missing on disk\n",
    "                bad_imgs.append((row[\"path\"], name, \"missing file\"))\n",
    "                continue\n",
    "            images_ok.append(name)\n",
    "\n",
    "        docs.append(\n",
    "            Document(\n",
    "                page_content=row[\"content\"],\n",
    "                metadata={\"source\": row[\"path\"], \"images\": images_ok},\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # ---- summary logging ----------------------------------------------------\n",
    "    if bad_imgs:\n",
    "        logger.warning(\"⚠️ %d broken image refs filtered out\", len(bad_imgs))\n",
    "        for src, name, reason in bad_imgs[:20]:\n",
    "            logger.debug(\"  » %s → %s (%s)\", src, name or \"<EMPTY>\", reason)\n",
    "    else:\n",
    "        logger.info(\"✅ no invalid image refs found\")\n",
    "\n",
    "    return docs\n",
    "\n",
    "mm_raw_docs = load_mm_docs_clean(WIKI_METADATA_DIR, Path(IMAGE_DIR))\n",
    "def log_stage(name: str, docs: List[Document]):\n",
    "    logger.info(f\"{name}: {len(docs)} docs, avg_tokens={sum(len(d.page_content) for d in docs)/len(docs):.0f}\")\n",
    "log_stage(\"Docs loaded\", mm_raw_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc63ed05-192e-4f59-856c-cf9bff160fc8",
   "metadata": {},
   "source": [
    "# Creation of Chunks\n",
    "Here, we split the loaded documents into chunks, so we have smaller and more specific texts to add to our vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3dbd381-58b8-4569-aa75-b26d07556b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 23:21:42 - INFO - Chunking complete: 558 docs → 2574 chunks (avg 713 chars)\n"
     ]
    }
   ],
   "source": [
    "def chunk_documents(\n",
    "    docs,\n",
    "    chunk_size: int = 1200,\n",
    "    overlap: int = 200,\n",
    ") -> list[Document]:\n",
    "    \"\"\"\n",
    "    1) Split each wiki page on Markdown headers (#, ## …) to keep logical\n",
    "       sections together.\n",
    "    2) Recursively break long sections to <= `chunk_size` chars with `overlap`.\n",
    "    3) Prefix every chunk with its page‑title and store the title in metadata.\n",
    "    \"\"\"\n",
    "    header_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=[(\"#\", \"title\"), (\"##\", \"section\")]\n",
    "    )\n",
    "    recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "    )\n",
    "\n",
    "    all_chunks: list[Document] = []\n",
    "    for doc in docs:\n",
    "        page_title = Path(doc.metadata[\"source\"]).stem.replace(\"-\", \" \")\n",
    "\n",
    "        # 1️⃣ section‑level split (returns list[Document])\n",
    "        section_docs = header_splitter.split_text(doc.page_content)\n",
    "\n",
    "        for section in section_docs:\n",
    "            # 2️⃣ size‑based split inside each section\n",
    "            tiny_texts = recursive_splitter.split_text(section.page_content)\n",
    "\n",
    "            for idx, tiny in enumerate(tiny_texts):\n",
    "                all_chunks.append(\n",
    "                    Document(\n",
    "                        page_content=f\"{page_title}\\n\\n{tiny.strip()}\",\n",
    "                        metadata={\n",
    "                            \"title\": page_title,\n",
    "                            \"source\": doc.metadata[\"source\"],\n",
    "                            \"section_header\": section.metadata.get(\"header\", \"\"),\n",
    "                            \"chunk_id\": idx,\n",
    "                        },\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    if all_chunks:\n",
    "        avg_len = int(mean(len(c.page_content) for c in all_chunks))\n",
    "        logger.info(\n",
    "            \"Chunking complete: %d docs → %d chunks (avg %d chars)\",\n",
    "            len(docs),\n",
    "            len(all_chunks),\n",
    "            avg_len,\n",
    "        )\n",
    "    else:\n",
    "        logger.warning(\"Chunking produced zero chunks for %d docs\", len(docs))\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "splits = chunk_documents(mm_raw_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33fda9b-e514-4ecd-b480-f07955e08233",
   "metadata": {},
   "source": [
    "# Setup Embeddings & Vector Store\n",
    "Here we setup Siglip for Image embeddings, and also store our cleaned text chunks embeddings into Chroma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8451784-0558-4c5b-8c30-f530ac38f4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "#  Helper: walk all docs once and gather *unique* image vectors + metadata\n",
    "# ---------------------------------------------------------------------------\n",
    "def _collect_image_vectors():\n",
    "    \"\"\"\n",
    "    Scans every wiki page for image references and returns three parallel lists:\n",
    "        img_paths : list[str]   → full file‑system paths (for SigLIP)\n",
    "        img_ids   : list[str]   → unique key per (page, image) pair\n",
    "        img_meta  : list[dict]  → {\"source\": wiki_page, \"image\": file_name}\n",
    "    Runs in < 1 s even for thousands of docs.\n",
    "    \"\"\"\n",
    "    img_paths, img_ids, img_meta = [], [], []\n",
    "    seen = set()\n",
    "\n",
    "    for doc in mm_raw_docs:                         # raw wiki pages\n",
    "        src = doc.metadata[\"source\"]\n",
    "        for name in doc.metadata.get(\"images\", []): # list[str]\n",
    "            img_id = f\"{src}::{name}\"\n",
    "            if img_id in seen:\n",
    "                continue                            # de‑dupe\n",
    "            seen.add(img_id)\n",
    "\n",
    "            img_paths.append(str(IMAGE_DIR / name))\n",
    "            img_ids.append(img_id)\n",
    "            img_meta.append({\"source\": src, \"image\": name})\n",
    "\n",
    "    return img_paths, img_ids, img_meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7511858c-b947-4420-a3f1-1f4a1ad82b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# --- Image‑embedding helper  (place this once, ABOVE the vector‑store code) --\n",
    "class SiglipEmbeddings(Embeddings):\n",
    "    def __init__(self,\n",
    "                 model_id: str = \"google/siglip2-base-patch16-224\",\n",
    "                 device: str | None = None):\n",
    "        from transformers import SiglipModel, SiglipProcessor\n",
    "        import torch, PIL.Image as PILImage\n",
    "        self.device    = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model     = SiglipModel.from_pretrained(model_id).to(self.device)\n",
    "        self.processor = SiglipProcessor.from_pretrained(model_id)\n",
    "        self.torch     = torch\n",
    "        self.PILImage  = PILImage\n",
    "\n",
    "    # ---- private helpers ---------------------------------------------------\n",
    "    def _embed_text(self, txts):\n",
    "        inp = self.processor(text=txts, return_tensors=\"pt\",\n",
    "                             padding=True, truncation=True).to(self.device)\n",
    "        with self.torch.no_grad():\n",
    "            return self.model.get_text_features(**inp).cpu().numpy()\n",
    "\n",
    "    def _embed_imgs(self, paths):\n",
    "        imgs = [self.PILImage.open(p).convert(\"RGB\") for p in paths]\n",
    "        inp  = self.processor(images=imgs, return_tensors=\"pt\").to(self.device)\n",
    "        with self.torch.no_grad():\n",
    "            return self.model.get_image_features(**inp).cpu().numpy()\n",
    "\n",
    "    # ---- LangChain API -----------------------------------------------------\n",
    "    def embed_documents(self, docs):          # list[str]  (image paths)\n",
    "        return self._embed_imgs(docs).tolist()\n",
    "\n",
    "    def embed_query(self, txt):               # single str  (textual query)\n",
    "        return self._embed_text([txt])[0].tolist()\n",
    "\n",
    "siglip_embeddings = SiglipEmbeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e83d3d3f-87a8-4209-a805-59983479d629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 23:21:48 - INFO - Loading existing Chroma index from ../data/chroma_store\n",
      "2025-07-16 23:21:48 - INFO - Loaded existing image index (738 vectors).\n"
     ]
    }
   ],
   "source": [
    "# ── 1) TEXT store ────────────────────────────────────────────────────────────\n",
    "def _current_manifest() -> List[str]:\n",
    "    \"\"\"Returns an ordered list of every Markdown/JSON context file we index.\"\"\"\n",
    "    return sorted(str(p.resolve()) for p in CONTEXT_DIR.rglob(\"*.json\"))\n",
    "\n",
    "def _needs_rebuild() -> bool:\n",
    "    if not CHROMA_DIR.exists() or not MANIFEST_PATH.exists():\n",
    "        return True\n",
    "    try:\n",
    "        old = json.loads(MANIFEST_PATH.read_text())\n",
    "    except Exception:\n",
    "        return True\n",
    "    return old != _current_manifest()\n",
    "\n",
    "def _save_manifest(manifest: List[str]) -> None:\n",
    "    CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    MANIFEST_PATH.write_text(json.dumps(manifest, indent=2))\n",
    "\n",
    "def _build_text_db() -> Chroma:\n",
    "    collection = \"mm_text\"\n",
    "    if _needs_rebuild():\n",
    "        logger.info(\"Re‑indexing text context → %s …\", CHROMA_DIR)\n",
    "        chroma = Chroma.from_documents(\n",
    "            documents        = splits,            # you already created these\n",
    "            embedding        = embeddings,\n",
    "            collection_name  = collection,\n",
    "            persist_directory= str(CHROMA_DIR),\n",
    "        )\n",
    "        _save_manifest(_current_manifest())\n",
    "        return chroma\n",
    "    logger.info(\"Loading existing Chroma index from %s\", CHROMA_DIR)\n",
    "    return Chroma(\n",
    "        collection_name   = collection,\n",
    "        persist_directory = str(CHROMA_DIR),\n",
    "        embedding_function= embeddings,\n",
    "    )\n",
    "\n",
    "text_db = _build_text_db()\n",
    "\n",
    "# ── 2) IMAGE store ───────────────────────────────────────────────────────────\n",
    "image_db = Chroma(\n",
    "    collection_name    = \"mm_image\",\n",
    "    persist_directory  = str(CHROMA_DIR),   # SAME dir as text db\n",
    "    embedding_function = siglip_embeddings, # <-- class you kept\n",
    ")\n",
    "\n",
    "# Populate vectors *only* if this looks empty -------------------------------\n",
    "if not image_db._collection.count():\n",
    "    img_paths, img_ids, img_meta = _collect_image_vectors()\n",
    "    image_db.add_texts(texts=img_paths, metadatas=img_meta, ids=img_ids)\n",
    "    image_db.persist()\n",
    "    logger.info(\"Indexed %d unique images.\", len(img_paths))\n",
    "else:\n",
    "    logger.info(\"Loaded existing image index (%d vectors).\",\n",
    "                image_db._collection.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7999ff1f-1f6a-452c-848e-e3c67adf7766",
   "metadata": {},
   "source": [
    "# Retrieval\n",
    "\n",
    "We transform the texts and images into embeddings and store them in a vector database. This allows us to perform similarity search, and proper retrieval of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf1f37f-11ff-4bf7-bae6-5e15470a8ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "\n",
    "def retrieve_mm(\n",
    "    query: str,\n",
    "    k_txt: int = 2,\n",
    "    k_img: int = 8,\n",
    "    fetch_k: int = 20,\n",
    "    hybrid_weights: Dict[str, float] = {\"init\": 0.6, \"rerank\": 0.4},\n",
    "    boost_slug: float = 0.1,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    1) Coarse recall: top `fetch_k` docs + init_scores\n",
    "    2) Cross‑encoder rerank: rerank_scores\n",
    "    3) Build hybrid_score = w1*init + w2*rerank + optional slug boost\n",
    "    4) Sort by hybrid_score, then pick top‑k_txt **without repeating sources**\n",
    "    5) Image retrieval with $in filter\n",
    "    \"\"\"\n",
    "    # 1) Coarse recall\n",
    "    docs_and_init = text_db.similarity_search_with_score(query, k=fetch_k)\n",
    "    docs, init_scores = zip(*docs_and_init)\n",
    "\n",
    "    # 2) Rerank\n",
    "    pairs = [(query, d.page_content) for d in docs]\n",
    "    rerank_scores = _cross_encoder.predict(pairs)\n",
    "\n",
    "    # 3) Compute hybrid scores (+ slug boost)\n",
    "    slug = query.lower().replace(\" \", \"-\")\n",
    "    hybrid_scores = []\n",
    "    for d, init, rerank in zip(docs, init_scores, rerank_scores):\n",
    "        score = hybrid_weights[\"init\"] * init + hybrid_weights[\"rerank\"] * rerank\n",
    "        if slug in d.metadata.get(\"source\", \"\").lower():\n",
    "            score += boost_slug\n",
    "        hybrid_scores.append(score)\n",
    "\n",
    "    # 4) Deduplicate & select top‑k_txt\n",
    "    scored_docs = list(zip(docs, hybrid_scores))\n",
    "    scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    selected: List[tuple[Document, float]] = []\n",
    "    seen_sources = set()\n",
    "    for doc, score in scored_docs:\n",
    "        src = doc.metadata.get(\"source\")\n",
    "        if src in seen_sources:\n",
    "            continue\n",
    "        seen_sources.add(src)\n",
    "        selected.append((doc, score))\n",
    "        if len(selected) >= k_txt:\n",
    "            break\n",
    "\n",
    "    if not selected:\n",
    "        return {\"docs\": [], \"images\": [], \"scores\": []}\n",
    "    selected_docs, final_scores = zip(*selected)\n",
    "\n",
    "    # 5) Image retrieval\n",
    "    sources = [d.metadata[\"source\"] for d in selected_docs]\n",
    "    q_emb = siglip_embeddings.embed_query(query)\n",
    "    img_hits = image_db.similarity_search_by_vector(\n",
    "        q_emb,\n",
    "        k=k_img * 2,\n",
    "        filter={\"source\": {\"$in\": sources}},\n",
    "    )\n",
    "    images = [img.page_content for img in img_hits[:k_img]]\n",
    "\n",
    "    return {\n",
    "        \"docs\": list(selected_docs),\n",
    "        \"images\": images,\n",
    "        \"scores\": list(final_scores),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85fbe6dc-68bc-4f3c-ac94-e98476ae4297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Doc 1  •  Development/Feature-Flags/Using-Feature-Flags-in-your-code.md\n",
      "Using Feature Flags in your code\n",
      "\n",
      "Feature flags are simple string flags that can be used to enable or disable features in the application. This is useful for rolling out features gradually, or for enabling features for specific users. We want to use it today for the following things: Azure and GCP datasets; and the integration with NVIDIA.  \n",
      "##Back End\n",
      "Feature flags location: `business/sys/featureflags/flags.go`  \n",
      "Implementation-wise, there is no need to do anything fancy. Use our old friend, `if`:  \n",
      "```go\n",
      "datasetTypes := []string{\"aws\", \"local\"}\n",
      "if featureflags.Has(\"azure\") {\n",
      "datasetTypes = append(datasetTypes, \"azure\")\n",
      "}\n",
      "if featureflags.Has(\"gcp\") {\n",
      "datasetTypes = append(datasetTypes, \"gcp …\n",
      "\n",
      "▶ Doc 2  •  Feature-Flags.md\n",
      "Feature Flags\n",
      "\n",
      "Enables shared and restricted project creating in AIS.\n",
      "**For Cloud Only:** Set Header with `{\"private-project-key\": \"e16873af-6fbf-427d-967f-b433503fccfd\"}`  \n",
      "[features.yaml](https://hp.sharepoint.com/:f:/t/HPDataSciencePlatform/Ev2kB17uPKhFg_OylIb1F88BEEyqEmAWaX5-7YJ51WcUrA?e=9aDmiC) …\n",
      "\n",
      "▶ Images\n"
     ]
    }
   ],
   "source": [
    "query = \"What are some feature flags that i can enable in AIStudio?\"\n",
    "\n",
    "results = retrieve_mm(query)\n",
    "\n",
    "# --- text context -------------------------------------------------\n",
    "for i, doc in enumerate(results[\"docs\"], 1):\n",
    "    print(f\"\\n▶ Doc {i}  •  {doc.metadata['source']}\")\n",
    "    print(doc.page_content[:700], \"…\")\n",
    "\n",
    "# --- images -------------------------------------------------------\n",
    "print(\"\\n▶ Images\")\n",
    "for p in results[\"images\"]:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58329084-640e-4b25-be70-ac1ec90467e9",
   "metadata": {},
   "source": [
    "# Model Setup\n",
    "\n",
    "In this notebook, we provide three different options for loading the model:\n",
    " * **local**: by loading the internvl3-8b-instruct-Q8_0 model from the asset downloaded on the project\n",
    " * **hugging-face-local** by downloading a DeepSeek model from Hugging Face and running locally\n",
    " * **hugging-face-cloud** by accessing the Mistral model through Hugging Face cloud API (requires HuggingFace API key saved on secrets.yaml)\n",
    "\n",
    "This choice can be set in the config.yaml file. The model deployed on the bottom cells of this notebook will load the choice from the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82c347e0-4cfd-4e6d-9e1f-a4b87598522a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llm_mm = LlamaCpp(\n",
    "    model_path=INTERNVL_MODEL_PATH,\n",
    "    n_gpu_layers=-1,\n",
    "    n_ctx=32768,\n",
    "    n_batch=256,\n",
    "    f16_kv=True,\n",
    "    verbose=False,\n",
    "    # pass any extra args down into llama-cpp-python\n",
    "    model_kwargs={\"mmproj_path\": MM_PROJ_PATH},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81f5d66-d99d-4851-a48e-248b2e81e2c9",
   "metadata": {},
   "source": [
    "# Chain Creation\n",
    "In this part, we define a pipeline that receives a question and context, formats the context documents, and uses the Qwen chat model to answer the question based on the provided context. The output is then formatted as a string for easy reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b57c199d-ee84-42cc-bc21-5ed76e40119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are **AI Studio DevOps Assistant**. Everything you know for this turn is inside\n",
    "the <context> block. Follow *all* rules below:\n",
    "1. **Answer comprehensively from the context.**\n",
    "   - Provide detailed, thorough responses using all relevant information\n",
    "   - If the answer is missing, write: \"I don't know based on the provided context.\"\n",
    "   - Never invent facts or rely on outside knowledge.\n",
    "2. **Be detailed and structured.**\n",
    "   - For procedures, provide complete numbered steps with explanations\n",
    "   - Quote file paths / commands in back‑ticks\n",
    "   - Include relevant examples and details from the context\n",
    "3. **Use all available information.**\n",
    "   - Draw from multiple documents when relevant\n",
    "   - Synthesize information to provide complete answers\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def _b64(path: str) -> str:\n",
    "    with open(path, \"rb\") as f:\n",
    "        return base64.b64encode(f.read()).decode()\n",
    "\n",
    "# FIXED: Proper multimodal input handling\n",
    "def build_multimodal_prompt(inp: dict) -> str:\n",
    "    \"\"\"Build a prompt that references images without embedding them\"\"\"\n",
    "    context = \"\\n\\n\".join(d.page_content for d in inp[\"docs\"])\n",
    "    \n",
    "    # Start with system prompt and context\n",
    "    prompt = f\"{SYSTEM_PROMPT}\\n{context}\\n\\nUser query:\\n{inp['query']}\"\n",
    "    \n",
    "    # Add image references (not the actual base64 data)\n",
    "    if inp[\"images\"]:\n",
    "        prompt += f\"\\n\\n[{len(inp['images'])} image(s) provided for analysis]\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a05478-2c07-48a7-a0f8-0dde2d49a539",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def resize_image_for_llm(img_path, max_size=256, quality=60):\n",
    "    \"\"\"Aggressively resize image to reduce tokens\"\"\"\n",
    "    img = PILImage.open(img_path)\n",
    "    \n",
    "    # Resize to much smaller size\n",
    "    img.thumbnail((max_size, max_size))\n",
    "    \n",
    "    # Convert to JPEG with heavy compression\n",
    "    buffer = io.BytesIO()\n",
    "    if img.mode in ('RGBA', 'LA', 'P'):\n",
    "        img = img.convert('RGB')\n",
    "    img.save(buffer, format=\"JPEG\", quality=quality)\n",
    "    buffer.seek(0)\n",
    "    \n",
    "    return base64.b64encode(buffer.read()).decode()\n",
    "\n",
    "def call_multimodal_llm(inp: dict) -> str:\n",
    "    \"\"\"Call InternVL with proper multimodal format - FIXED\"\"\"\n",
    "    context = \"\\n\\n\".join(d.page_content for d in inp[\"docs\"])\n",
    "    \n",
    "    # Display previews\n",
    "    for img_path in inp[\"images\"]:\n",
    "        display(Image(filename=img_path, width=350))\n",
    "    \n",
    "    if not inp[\"images\"]:\n",
    "        # Text-only fallback\n",
    "        text_prompt = f\"{SYSTEM_PROMPT}\\n{context}\\n\\nUser query:\\n{inp['query']}\"\n",
    "        return llm_mm(text_prompt)\n",
    "    \n",
    "    # FIXED: Use string content for messages\n",
    "    user_text = f\"{context}\\n\\nUser query:\\n{inp['query']}\"\n",
    "    \n",
    "    # Add image references in text (not as separate objects)\n",
    "    for i, img_path in enumerate(inp[\"images\"]):\n",
    "        user_text += f\"\\n\\n[Image {i+1} provided]\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_text}\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Try chat completion first\n",
    "        response = llm_mm.client.create_chat_completion(\n",
    "            messages=messages,\n",
    "            max_tokens=1000,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Chat completion failed: {e}\")\n",
    "        \n",
    "        try:\n",
    "            prompt = f\"{SYSTEM_PROMPT}\\n{context}\\n\\nUser query:\\n{inp['query']}\\n\\n\"\n",
    "            \n",
    "            # Add COMPRESSED images\n",
    "            for i, img_path in enumerate(inp[\"images\"]):\n",
    "                compressed_b64 = resize_image_for_llm(img_path)  # Much smaller!\n",
    "                prompt += f\"<img>{compressed_b64}</img>\\n\"\n",
    "            \n",
    "            return llm_mm(prompt)\n",
    "        except Exception as e2:\n",
    "            print(f\"Compressed image fallback failed: {e2}\")\n",
    "            # Last resort: text-only with image count\n",
    "            text_prompt = f\"{SYSTEM_PROMPT}\\n{context}\\n\\nUser query:\\n{inp['query']}\\n\\n[Note: {len(inp['images'])} images were provided but could not be processed due to size constraints]\"\n",
    "            return llm_mm(text_prompt)\n",
    "\n",
    "mm_chain = (\n",
    "    {\n",
    "      \"query\":   RunnablePassthrough(),\n",
    "      \"results\": RunnableLambda(lambda q: retrieve_mm(q)),\n",
    "    }\n",
    "    | RunnableLambda(lambda d: {\n",
    "          \"docs\":     d[\"results\"][\"docs\"],\n",
    "          \"images\":   d[\"results\"][\"images\"],\n",
    "          \"query\":    d[\"query\"],  # Keep as 'query' to match call_multimodal_llm\n",
    "      })\n",
    "    | RunnableLambda(call_multimodal_llm)\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7944376-86ec-4a0f-869b-151a209e4a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Quick Test\n",
    "\n",
    "question = \"What are the ai blueprints best practices?\"\n",
    "print(mm_chain.invoke(question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64007551-edfa-4d50-a50e-e242194433ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "question2 = \"What are some feature flags that i can enable in AIStudio?\"\n",
    "print(mm_chain.invoke(question2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6014059-66ef-47a9-b37b-868c15e32039",
   "metadata": {},
   "outputs": [],
   "source": [
    "question3 = \"How do i manually clean my environment without hooh?\"\n",
    "print(mm_chain.invoke(question3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a3a494-8f3c-4bce-8494-5c8bd7e150cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "question4 = \"How do I test a config after i sign it?\"\n",
    "print(mm_chain.invoke(question4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8147f34-6e3b-47f2-9a68-7e8077f71618",
   "metadata": {},
   "source": [
    "Built with ❤️ using Z by HP AI Studio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
