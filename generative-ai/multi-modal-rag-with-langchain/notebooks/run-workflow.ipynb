{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9ff4be-1fee-47eb-8d09-611c29a7a83f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h1 style=\"text-align: center; font-size: 50px;\">Multimodal RAG Chatbot with Langchain, Torch, Transformers</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbb3e26-125f-4151-9ed3-c84a3fd9081d",
   "metadata": {},
   "source": [
    "Retrieval-Augmented Generation (RAG) is an architectural approach that can enhance the effectiveness of large language model (LLM) applications using customized data. In this example, we use LangChain, an orchestrator for language pipelines, to build an assistant capable of loading information from a web page and use it for answering user questions. We'll leverage torch and transformers for multimodal model support in Python. We'll also use the MLFlow platform to evaluate and trace the LLM responses (in `register-workflow.ipynb`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a55c55-a1dc-4697-b920-2a81f1bb0f74",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "- Configuring the Environment\n",
    "- Data Loading & Cleaning\n",
    "- Setup Embeddings & Vector Store\n",
    "- Retrieval Function\n",
    "- Model Setup & Chain Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f78b36-56e2-4596-8325-c75901f30c76",
   "metadata": {},
   "source": [
    "## Step 0: Configuring the Environment\n",
    "\n",
    "In this step, we import all the necessary libraries and internal components required to run the RAG pipeline, including modules for notebook parsing, embedding generation, vector storage, and code generation with LLMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7156d10f-d930-4be7-a9e8-15606d466460",
   "metadata": {},
   "source": [
    "By using our Local GenAI workspace image, many of the necessary libraries to work with RAG already come pre-installed - in our case, we just need to extra support for multimodal processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "022490c4-8d8a-4c53-92ac-c62ea70a8209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os \n",
    "from pathlib import Path\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "# Create logger\n",
    "logger = logging.getLogger(\"multimodal_rag_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\") \n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f912d15a-7aab-4620-8c89-33c8f07104f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 04:16:25 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()  \n",
    "\n",
    "logger.info('Notebook execution started.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e700f51-9011-401c-90d3-a07ea8238955",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77853772-0239-40d0-94be-7a62fcb465c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 04:16:30.183631: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-28 04:16:30.191634: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753676190.201147    6834 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753676190.203771    6834 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753676190.210974    6834 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753676190.210986    6834 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753676190.210987    6834 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753676190.210987    6834 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-28 04:16:30.213654: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# === Standard Library Imports ===\n",
    "import gc\n",
    "import json\n",
    "import math\n",
    "import hashlib\n",
    "import shutil\n",
    "import warnings\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from statistics import mean\n",
    "from typing import Any, Dict, List, Optional, TypedDict\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# === Third-Party Library Imports ===\n",
    "import mlflow\n",
    "import torch\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain.schema.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from PIL import Image as PILImage\n",
    "from transformers import AutoImageProcessor, AutoModel, AutoTokenizer, BitsAndBytesConfig, SiglipModel, SiglipProcessor\n",
    "\n",
    "\n",
    "# === Project-Specific Imports ===\n",
    "from src.components import SemanticCache, SiglipEmbeddings\n",
    "from src.wiki_pages_clone import orchestrate_wiki_clone\n",
    "from src.utils import (\n",
    "    configure_hf_cache,\n",
    "    multimodal_rag_asset_status,\n",
    "    load_config,\n",
    "    load_secrets,\n",
    "    load_mm_docs_clean,\n",
    "    display_images,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd3b8d2c-d0e1-4409-86dc-470c8454fff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7384967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e80960-54f9-4ec2-b062-6d49526494d1",
   "metadata": {},
   "source": [
    "\n",
    "### Verify Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75f2111d-5508-41ba-967c-256b40a9fb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 04:16:31 - INFO - Local Model is properly configured. \n",
      "2025-07-28 04:16:31 - INFO - Config is properly configured. \n",
      "2025-07-28 04:16:31 - INFO - Secrets is properly configured. \n",
      "2025-07-28 04:16:31 - INFO - wiki_flat_structure.json is properly configured. \n",
      "2025-07-28 04:16:31 - INFO - CONTEXT is properly configured. \n",
      "2025-07-28 04:16:31 - INFO - CHROMA is properly configured. \n",
      "2025-07-28 04:16:31 - INFO - CACHE is properly configured. \n",
      "2025-07-28 04:16:31 - INFO - MANIFEST is properly configured. \n"
     ]
    }
   ],
   "source": [
    "CONFIG_PATH = \"../configs/config.yaml\"\n",
    "SECRETS_PATH = \"../configs/secrets.yaml\"\n",
    "\n",
    "LOCAL_MODEL: Path = Path(\"/home/jovyan/datafabric/InternVL3-8B-Instruct\")\n",
    "CONTEXT_DIR: Path = Path(\"../data/context\")             \n",
    "CHROMA_DIR: Path = Path(\"../data/chroma_store\")     \n",
    "CACHE_DIR: Path = CHROMA_DIR / \"semantic_cache\"\n",
    "MANIFEST_PATH: Path = CHROMA_DIR / \"manifest.json\"\n",
    "\n",
    "IMAGE_DIR = CONTEXT_DIR / \"images\"\n",
    "WIKI_METADATA_DIR = CONTEXT_DIR / \"wiki_flat_structure.json\"\n",
    "\n",
    "DEMO_FOLDER = \"../demo\"\n",
    "\n",
    "multimodal_rag_asset_status(\n",
    "    local_model_path=LOCAL_MODEL,\n",
    "    config_path=CONFIG_PATH,\n",
    "    secrets_path=SECRETS_PATH,\n",
    "    wiki_metadata_dir=WIKI_METADATA_DIR,\n",
    "    context_dir=CONTEXT_DIR,\n",
    "    chroma_dir=CHROMA_DIR,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    manifest_path=MANIFEST_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7062ee0",
   "metadata": {},
   "source": [
    "### Config Loading\n",
    "\n",
    "In this section, we load configuration parameters from the YAML file in the configs folder.\n",
    "\n",
    "- **config.yaml**: Contains non-sensitive configuration parameters like model sources and URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e0b7464-ad76-47f2-94f0-e70c4d4cc380",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ca6361-dd3c-4778-9a9d-b11da7b20151",
   "metadata": {},
   "source": [
    "### Config HuggingFace Caches\n",
    "\n",
    "In the next cell, we configure HuggingFace cache, so that all the models downloaded from them are persisted locally, even after the workspace is closed. This is a future desired feature for AI Studio and the GenAI addon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "965e4ea4-084a-4f32-9f51-9ead89dd16fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure HuggingFace cache\n",
    "configure_hf_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41d47161-f197-409b-9a43-a3f59f1f695c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.02 s, sys: 193 ms, total: 1.21 s\n",
      "Wall time: 3.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Initialize HuggingFace Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/e5-large-v2\",\n",
    "    cache_folder=\"/tmp/hf_cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d92659e-23ea-4965-bb27-58cd1dd0f3ec",
   "metadata": {},
   "source": [
    "## Step 1: Data Loading & Cleaning\n",
    "\n",
    "`wiki_flat_structure.json` is a custom json metadata for ADO Wiki data. It is flatly structured, with keys for filepath, md content, and a list of images. We also have a image folder that contains all the images for every md page. We directly scrape this data from ADO and perform any cleanup if necessary.\n",
    "\n",
    "- **secrets.yaml**: For Freemium users, use secrets.yaml to store your sensitive data like API Keys. If you are a Premium user, you can use secrets manager.\n",
    "- **AIS Secrets Manager**: For Paid users, use the secrets manager in the `Project Setup` tab to configure your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "915bdc8a-0cb9-4b17-8d8f-a56acb7ffcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 04:16:35 - INFO - Starting ADO Wiki clone process...\n",
      "2025-07-28 04:16:35 - INFO - Cloning wiki 'Phoenix-DS-Platform.wiki' to temporary directory: /tmp/tmp11pb15wy\n",
      "2025-07-28 04:16:55 - INFO - Scanning for Markdown files...\n",
      "2025-07-28 04:16:55 - INFO - → Found 567 Markdown pages.\n",
      "2025-07-28 04:16:55 - INFO - Copying referenced images to ../data/context/images...\n",
      "2025-07-28 04:17:02 - INFO - → 738 unique images copied.\n",
      "2025-07-28 04:17:02 - INFO - Assembling flat JSON structure...\n",
      "2025-07-28 04:17:02 - INFO - ✅ Wiki data successfully cloned to ../data/context\n",
      "2025-07-28 04:17:02 - INFO - Cleaned up temporary directory: /tmp/tmp11pb15wy\n",
      "2025-07-28 04:17:02 - INFO - ✅ Wiki data preparation step completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 628 ms, sys: 843 ms, total: 1.47 s\n",
      "Wall time: 27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ADO_PAT = os.getenv(\"AIS_ADO_TOKEN\")\n",
    "if not ADO_PAT:\n",
    "    logger.info(\"Environment variable not found... Secrets Manager not properly set. Falling to secrets.yaml.\")\n",
    "    try:\n",
    "        secrets = load_secrets(SECRETS_PATH)\n",
    "        ADO_PAT = secrets.get('AIS_ADO_TOKEN')\n",
    "    except NameError:\n",
    "        logger.error(\"The 'secrets' object is not defined or available.\")\n",
    "\n",
    "try:\n",
    "    orchestrate_wiki_clone(\n",
    "        pat=ADO_PAT,\n",
    "        config=config,\n",
    "        output_dir=CONTEXT_DIR\n",
    "    )\n",
    "    logger.info(\"✅ Wiki data preparation step completed successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(\"Halting notebook execution due to a critical error in the wiki preparation step.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb4c1589-285f-4e72-ab8e-5ae9fbd4043d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 04:17:03 - WARNING - ⚠️ 94 broken image refs filtered out\n",
      "2025-07-28 04:17:03 - INFO - Docs loaded: 567 docs, avg_tokens=3097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.4 ms, sys: 60.3 ms, total: 86.7 ms\n",
      "Wall time: 808 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "WIKI_METADATA_DIR   = Path(WIKI_METADATA_DIR)\n",
    "IMAGE_DIR = Path(IMAGE_DIR)\n",
    "\n",
    "mm_raw_docs = load_mm_docs_clean(WIKI_METADATA_DIR, Path(IMAGE_DIR))\n",
    "\n",
    "def log_stage(name: str, docs: List[Document]):\n",
    "    logger.info(f\"{name}: {len(docs)} docs, avg_tokens={sum(len(d.page_content) for d in docs)/len(docs):.0f}\")\n",
    "log_stage(\"Docs loaded\", mm_raw_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc63ed05-192e-4f59-856c-cf9bff160fc8",
   "metadata": {},
   "source": [
    "## Step 2: Creation of Chunks\n",
    "\n",
    "Here, we split the loaded documents into chunks, so we have smaller and more specific texts to add to our vector database. \n",
    "\n",
    "We chunk based on header style, and then within each header style we futher chunk based on the provided chunk size. Each chunk retains the page name, which preserves the relevance of each chunk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3dbd381-58b8-4569-aa75-b26d07556b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 04:17:03 - INFO - Chunking complete: 567 docs → 2614 chunks (avg 717 chars)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 69.7 ms, sys: 0 ns, total: 69.7 ms\n",
      "Wall time: 66.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def chunk_documents(\n",
    "    docs,\n",
    "    chunk_size: int = 1200,\n",
    "    overlap: int = 200,\n",
    ") -> list[Document]:\n",
    "    \"\"\"\n",
    "    1) Split each wiki page on Markdown headers (#, ## …) to keep logical\n",
    "       sections together.\n",
    "    2) Recursively break long sections to <= `chunk_size` chars with `overlap`.\n",
    "    3) Prefix every chunk with its page-title and store the title in metadata.\n",
    "    \"\"\"\n",
    "    header_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=[(\"#\", \"title\"), (\"##\", \"section\")]\n",
    "    )\n",
    "    recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "    )\n",
    "\n",
    "    all_chunks: list[Document] = []\n",
    "    for doc in docs:\n",
    "        page_title = Path(doc.metadata[\"source\"]).stem.replace(\"-\", \" \")\n",
    "\n",
    "        # 1️. section‑level split (returns list[Document])\n",
    "        section_docs = header_splitter.split_text(doc.page_content)\n",
    "\n",
    "        for section in section_docs:\n",
    "            # 2. size‑based split inside each section\n",
    "            tiny_texts = recursive_splitter.split_text(section.page_content)\n",
    "\n",
    "            for idx, tiny in enumerate(tiny_texts):\n",
    "                all_chunks.append(\n",
    "                    Document(\n",
    "                        page_content=f\"{page_title}\\n\\n{tiny.strip()}\",\n",
    "                        metadata={\n",
    "                            \"title\": page_title,\n",
    "                            \"source\": doc.metadata[\"source\"],\n",
    "                            \"section_header\": section.metadata.get(\"header\", \"\"),\n",
    "                            \"chunk_id\": idx,\n",
    "                        },\n",
    "                    )\n",
    "                )\n",
    "    if all_chunks:\n",
    "        avg_len = int(mean(len(c.page_content) for c in all_chunks))\n",
    "        logger.info(\n",
    "            \"Chunking complete: %d docs → %d chunks (avg %d chars)\",\n",
    "            len(docs),\n",
    "            len(all_chunks),\n",
    "            avg_len,\n",
    "        )\n",
    "    else:\n",
    "        logger.warning(\"Chunking produced zero chunks for %d docs\", len(docs))\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "splits = chunk_documents(mm_raw_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33fda9b-e514-4ecd-b480-f07955e08233",
   "metadata": {},
   "source": [
    "## Step 3: Setup Embeddings & Vector Store\n",
    "Here we setup Siglip for Image embeddings, and also transform our cleaned text chunks into embeddings to be stored in Chroma. We store the chroma data locally on the disk to reduce memory usage. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cd5fbc",
   "metadata": {},
   "source": [
    "### Setup Text ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e83d3d3f-87a8-4209-a805-59983479d629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 04:17:03 - INFO - Loading existing Chroma index from ../data/chroma_store\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 186 ms, sys: 20.7 ms, total: 206 ms\n",
      "Wall time: 299 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 1) TEXT store\n",
    "def _current_manifest() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Returns a dictionary mapping every context JSON file to its SHA256 content hash.\n",
    "    This allows detecting changes in file content, not just filenames.\n",
    "    \"\"\"\n",
    "    manifest = {}\n",
    "    json_files = sorted(CONTEXT_DIR.rglob(\"*.json\"))\n",
    "\n",
    "    for file_path in json_files:\n",
    "        try:\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                file_bytes = f.read()\n",
    "                file_hash = hashlib.sha256(file_bytes).hexdigest()\n",
    "                manifest[str(file_path.resolve())] = file_hash\n",
    "        except IOError as e:\n",
    "            logger.error(f\"Could not read file {file_path} for hashing: {e}\")\n",
    "    return manifest\n",
    "\n",
    "def _needs_rebuild() -> bool:\n",
    "    \"\"\"\n",
    "    Determines if the ChromaDB needs to be rebuilt.\n",
    "    A rebuild is needed if:\n",
    "    1. The Chroma directory or manifest file doesn't exist.\n",
    "    2. The manifest is unreadable.\n",
    "    3. The stored file hashes in the manifest do not match the current file hashes.\n",
    "    \"\"\"\n",
    "    if not CHROMA_DIR.exists() or not MANIFEST_PATH.exists():\n",
    "        logger.info(\"Chroma directory or manifest not found. A rebuild is required.\")\n",
    "        return True\n",
    "    try:\n",
    "        old_manifest = json.loads(MANIFEST_PATH.read_text())\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not read manifest file. A rebuild is required. Error: {e}\")\n",
    "        return True\n",
    "\n",
    "    current_manifest = _current_manifest()\n",
    "    if old_manifest != current_manifest:\n",
    "        logger.info(\"Data content has changed. A rebuild is required.\")\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def _save_manifest(manifest: Dict[str, str]) -> None:\n",
    "    \"\"\"Saves the current data manifest (mapping file paths to hashes) to disk.\"\"\"\n",
    "    CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    MANIFEST_PATH.write_text(json.dumps(manifest, indent=2))\n",
    "\n",
    "def _build_text_db() -> Chroma:\n",
    "    collection = \"mm_text\"\n",
    "    # The rebuild check is now done outside this function.\n",
    "    # We check if the directory exists. If not, we build.\n",
    "    if not CHROMA_DIR.exists() or not (CHROMA_DIR / \"chroma.sqlite3\").exists():\n",
    "        logger.info(\"Creating new text context index in %s ...\", CHROMA_DIR)\n",
    "        chroma = Chroma.from_documents(\n",
    "            documents          = splits,\n",
    "            embedding          = embeddings,\n",
    "            collection_name    = collection,\n",
    "            persist_directory  = str(CHROMA_DIR),\n",
    "        )\n",
    "        return chroma\n",
    "\n",
    "    logger.info(\"Loading existing Chroma index from %s\", CHROMA_DIR)\n",
    "    return Chroma(\n",
    "        collection_name   = collection,\n",
    "        persist_directory = str(CHROMA_DIR),\n",
    "        embedding_function= embeddings,\n",
    "    )\n",
    "    \n",
    "# Check if a rebuild is needed and wipe the old DB if so.\n",
    "# This ensures both the text and image databases are rebuilt from scratch.\n",
    "if _needs_rebuild():\n",
    "    logger.warning(\"REBUILDING: Wiping old ChromaDB store at %s\", CHROMA_DIR)\n",
    "    if CHROMA_DIR.exists():\n",
    "        shutil.rmtree(CHROMA_DIR)\n",
    "    # Save the new manifest immediately after deciding to rebuild\n",
    "    _save_manifest(_current_manifest())\n",
    "\n",
    "# Now, initialize your databases. They will be created fresh if they were just deleted.\n",
    "text_db = _build_text_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2c64bd",
   "metadata": {},
   "source": [
    "### Setup Image ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50a39492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "2025-07-28 04:17:07 - INFO - Loaded existing image index (752 vectors).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.78 s, sys: 794 ms, total: 2.58 s\n",
      "Wall time: 3.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#  Helper: walk all docs once and gather *unique* image vectors + metadata\n",
    "def _collect_image_vectors():\n",
    "    \"\"\"\n",
    "    Scans every wiki page for image references and returns three parallel lists:\n",
    "        img_paths : list[str]   → full file-system paths (for SigLIP)\n",
    "        img_ids   : list[str]   → unique key per (page, image) pair\n",
    "        img_meta  : list[dict]  → {\"source\": wiki_page, \"image\": file_name}\n",
    "    Runs in < 1s even for thousands of docs.\n",
    "    \"\"\"\n",
    "    img_paths, img_ids, img_meta = [], [], []\n",
    "    seen = set()\n",
    "\n",
    "    for doc in mm_raw_docs:                         # raw wiki pages\n",
    "        src = doc.metadata[\"source\"]\n",
    "        for name in doc.metadata.get(\"images\", []): # list[str]\n",
    "            img_id = f\"{src}::{name}\"\n",
    "            if img_id in seen:\n",
    "                continue                            # de‑dupe\n",
    "            seen.add(img_id)\n",
    "\n",
    "            img_paths.append(str(IMAGE_DIR / name))\n",
    "            img_ids.append(img_id)\n",
    "            img_meta.append({\"source\": src, \"image\": name})\n",
    "\n",
    "    return img_paths, img_ids, img_meta\n",
    "\n",
    "siglip_embeddings = SiglipEmbeddings(\"google/siglip2-base-patch16-224\", DEVICE)\n",
    "\n",
    "# 2) IMAGE store\n",
    "image_db = Chroma(\n",
    "    collection_name    = \"mm_image\",\n",
    "    persist_directory  = str(CHROMA_DIR),   # SAME dir as text db\n",
    "    embedding_function = siglip_embeddings, # <-- class you kept\n",
    ")\n",
    "\n",
    "# Populate vectors *only* if it is empty\n",
    "if not image_db._collection.count():\n",
    "    img_paths, img_ids, img_meta = _collect_image_vectors()\n",
    "    image_db.add_texts(texts=img_paths, metadatas=img_meta, ids=img_ids)\n",
    "    image_db.persist()\n",
    "    logger.info(\"Indexed %d unique images.\", len(img_paths))\n",
    "else:\n",
    "    logger.info(\"Loaded existing image index (%d vectors).\",\n",
    "                image_db._collection.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b7a32f-81e5-49bb-94c3-04a480e4be89",
   "metadata": {},
   "source": [
    "### Setup Memory Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9473fbc-bbbb-40b2-b344-decd9322a303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the semantic cache\n",
    "semantic_cache = SemanticCache(persist_directory=CACHE_DIR, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7999ff1f-1f6a-452c-848e-e3c67adf7766",
   "metadata": {},
   "source": [
    "## Step 4: Retrieval Function\n",
    "\n",
    "This code implements a multi-stage retrieval process that combines vector similarity search, cross-encoder reranking, and a hybrid scoring mechanism to select the most relevant text documents and associated images.\n",
    "\n",
    "Here, the system performs an initial similarity search against a `text_db` (likely a vector store like ChromaDB, given your imports). It uses the `query` to find the top `fetch_k` most similar text documents based on their initial embedding similarity. This step acts as a broad filter, quickly identifying a larger set of potentially relevant documents. The result includes both the documents `(docs)` and their initial similarity scores (init_scores). After the intial recall, we use a cross-encoder to rerank these `fetch_k` documents. Unlike the initial embedding similarity, a cross-encoder takes the query and each document content as a pair and provides a more nuanced relevance score by considering their interaction. We also implement Hybrid scoring and select the `top-k` documents at the end.\n",
    "\n",
    "Using the `top-k` documents, we retrieve images associated with those documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf1f37f-11ff-4bf7-bae6-5e15470a8ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_mm(\n",
    "    query: str,\n",
    "    k_txt: int = 4,\n",
    "    k_img: int = 8,\n",
    "    fetch_k: int = 20,\n",
    "    boost_slug: float = 0.1,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Performs multi-modal retrieval without a cross-encoder.\n",
    "\n",
    "    1) Coarse recall: Fetches the top `fetch_k` documents based on similarity score.\n",
    "    2) Score adjustment: Applies an optional score boost if the query slug\n",
    "       matches the document source.\n",
    "    3) Top-k selection: Sorts documents by the adjusted score and selects the top `k_txt`.\n",
    "    4) Image retrieval: Fetches relevant images for the selected top documents.\n",
    "    \"\"\"\n",
    "    # 1) Coarse recall: Fetch top `fetch_k` docs and their initial scores\n",
    "    docs_and_scores = text_db.similarity_search_with_score(query, k=fetch_k)\n",
    "\n",
    "    if not docs_and_scores:\n",
    "        return {\"docs\": [], \"images\": [], \"scores\": []}\n",
    "\n",
    "    # 2) Compute adjusted scores (+ slug boost)\n",
    "    slug = query.lower().replace(\" \", \"-\")\n",
    "    scored_docs = []\n",
    "    for doc, initial_score in docs_and_scores:\n",
    "        score = initial_score\n",
    "        # Apply boost if the formatted query slug appears in the source URL/path\n",
    "        if slug in doc.metadata.get(\"source\", \"\").lower():\n",
    "            score += boost_slug\n",
    "        scored_docs.append((doc, score))\n",
    "\n",
    "    # 3) Sort by the new score and select top-k_txt\n",
    "    scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_docs_and_scores = scored_docs[:k_txt]\n",
    "\n",
    "    if not top_docs_and_scores:\n",
    "        return {\"docs\": [], \"images\": [], \"scores\": []}\n",
    "\n",
    "    selected_docs, final_scores = zip(*top_docs_and_scores)\n",
    "\n",
    "    # 4) Image retrieval using the sources of the top text documents\n",
    "    sources = [d.metadata[\"source\"] for d in selected_docs]\n",
    "    q_emb = siglip_embeddings.embed_query(query)\n",
    "    img_hits = image_db.similarity_search_by_vector(\n",
    "        q_emb,\n",
    "        k=k_img * 2,  # Fetch more to allow for some filtering/variety\n",
    "        filter={\"source\": {\"$in\": sources}},\n",
    "    )\n",
    "    images = [img.page_content for img in img_hits[:k_img]]\n",
    "\n",
    "    return {\n",
    "        \"docs\": list(selected_docs),\n",
    "        \"images\": images,\n",
    "        \"scores\": list(final_scores),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58329084-640e-4b25-be70-ac1ec90467e9",
   "metadata": {},
   "source": [
    "## Step 5: Model Setup & Chain Creation\n",
    "\n",
    "In this section, we set up our local Large Language Model (LLM) and integrate it into a Question Answering (QA) pipeline. We're using `internvl3-8b-instruct` as our multimodal model, which can process both text and images. This setup is encapsulated within the InternVLMM class, designed for efficient and robust multimodal interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fcccbf",
   "metadata": {},
   "source": [
    "### System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffd3cdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "    You are AI Studio DevOps Assistant. Your function is to analyze images and text, then answer questions based ONLY on the provided materials.\n",
    "    \n",
    "    **PERMANENT INSTRUCTIONS:**\n",
    "    1.  **Analyze and Answer from Context**: Your entire response MUST be derived thoroughly from the provided `<context>` block or the user's image(s).\n",
    "    2.  **Follow Output Structure**: You MUST follow the multi-part response structure outlined in the user's message. Completing all sections is mandatory.\n",
    "    3.  **No External Knowledge**: You MUST NOT use any information outside the provided materials.\n",
    "    4.  **No Hallucination**: Do not invent or assume any details. If information is not present, it does not exist.\n",
    "    5.  **Handle Missing Information**: If the provided context or image(s) do not contain the answer, your ONLY response will be: \"Based on the provided context, I cannot answer this question.\" Do not add any other words or explanation.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63929d26",
   "metadata": {},
   "source": [
    "### InternVLMM QA Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "910ccb5a-fb0c-4f7c-bc2a-69f464f3ffa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 04:17:07 - INFO - Loading /home/jovyan/datafabric/InternVL3-8B-Instruct ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761de5b544ac4f45aabc58bc97cb0980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 04:18:24 - INFO - Model loaded on cuda.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.5 s, sys: 13.6 s, total: 23.1 s\n",
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "class InternVLMM:\n",
    "    \"\"\"\n",
    "    Minimal, self-contained multimodal QA wrapper around InternVL3-8B-Instruct.\n",
    "    This class:\n",
    "      • loads / resets the model\n",
    "      • builds the prompt (<context>…)\n",
    "      • returns the model's answer (based on img and text) and also the top retrieved images\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cache: SemanticCache):\n",
    "        self.tok   = None\n",
    "        self.image_processor = None\n",
    "        self.cache = cache\n",
    "        self._load()\n",
    "\n",
    "    # ---------- public function ----------\n",
    "    def generate(self, query: str, force_regenerate: bool = False, **retrieval_kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run retrieval, prompt assembly, and model generation.\n",
    "        Returns a dictionary with the text reply and a list of used image paths.\n",
    "        \"\"\"\n",
    "        \n",
    "        # === 1. CHECK SEMANTIC CACHE (or bypass if forced) ===\n",
    "        if not force_regenerate:\n",
    "            cached_result = self.cache.get(query, threshold=0.92)\n",
    "            if cached_result:\n",
    "                logger.info(f\"SEMANTIC CACHE HIT for query: '{query}'\")\n",
    "                return cached_result\n",
    "        \n",
    "        if force_regenerate:\n",
    "            logger.info(f\"Forced regeneration for query: '{query}'. Clearing old cache entry.\")\n",
    "            self.cache.delete(query)\n",
    "\n",
    "        logger.info(f\"CACHE MISS for query: '{query}'. Running full pipeline.\")\n",
    "        if self.model is None or self.tok is None:\n",
    "            return {\"reply\": \"Error: model not initialised.\", \"used_images\": []}\n",
    "    \n",
    "        # === 2. RETRIEVE (if not cached) ===\n",
    "        hits = retrieve_mm(query, **retrieval_kwargs)\n",
    "        docs: List[Any]   = hits[\"docs\"]\n",
    "        images: List[str] = hits[\"images\"] # This is the list of paths you want to return\n",
    "    \n",
    "        if not docs and not images:\n",
    "            return {\"reply\": \"I don't know based on the provided context.\", \"used_images\": []}\n",
    "\n",
    "        # === 3. BUILD PROMPT & GENERATE ===\n",
    "        context_str = \"\\n\\n\".join(\n",
    "            f\"<source_document name=\\\"{d.metadata.get('source', 'unknown')}\\\">\\n{d.page_content}\\n</source_document>\"\n",
    "            for d in docs\n",
    "        )\n",
    "        \n",
    "        # Combine the response structure and query into the user message.\n",
    "        visual_analysis_prompt = \"\"\n",
    "        if images:\n",
    "            visual_analysis_prompt = \"\"\"\n",
    "            ## **Visual Analysis**\\n\n",
    "            \"\"\"\n",
    "        \n",
    "        # Construct the final prompt\n",
    "        user_content = f\"\"\"\n",
    "            <task_instructions>\n",
    "            Your response must follow this exact structure:\n",
    "            {visual_analysis_prompt}\n",
    "            ## **Synthesized Answer**\\n\n",
    "            Next, answer the user's original query. Your answer must be synthesized from the provided text `<context>`. Use the visual analysis only if it is relevant. If the image is not relevant, rely solely on the text context to formulate your answer.\\n\n",
    "            \n",
    "            ## **Source Documents**\\n\n",
    "            At the very end of your response, cite the source from the context in brackets and backticks, like this: [`source-file-name.md`].\\n\n",
    "            \n",
    "            </task_instructions>\n",
    "    \n",
    "            <context>\n",
    "                {context_str}\n",
    "            </context>\n",
    "    \n",
    "            <user_query>\n",
    "                 {query}\n",
    "            </user_query>\n",
    "    \n",
    "            Now, generate the response following all instructions.\n",
    "            \"\"\"\n",
    "\n",
    "        # Construct the conversation history as a list of dictionaries\n",
    "        conversation = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": user_content}\n",
    "        ]\n",
    "        \n",
    "        # Apply the chat template\n",
    "        prompt = self.tok.apply_chat_template(\n",
    "            conversation,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "                \n",
    "        if images:\n",
    "            prompt += f\"\\n\\n[{len(images)} image(s) are provided for analysis]\"\n",
    "\n",
    "        # generate\n",
    "        try:\n",
    "            self._clear_cuda()\n",
    "            pixel_values = self._process_images(images) if images else None\n",
    "            reply = self.model.chat(\n",
    "                self.tok, pixel_values, prompt,\n",
    "                generation_config=dict(\n",
    "                    max_new_tokens=4096, \n",
    "                    pad_token_id=self.tok.pad_token_id, \n",
    "                    eos_token_id=self.tok.eos_token_id,\n",
    "                    repetition_penalty=1.25,\n",
    "                ),\n",
    "            )\n",
    "            self._clear_cuda()\n",
    "\n",
    "            result_dict = {\"reply\": reply, \"used_images\": images}\n",
    "            \n",
    "            # === 4. UPDATE CACHE ===\n",
    "            self.cache.set(query, result_dict)\n",
    "            \n",
    "            return result_dict\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            msg = str(e).lower()\n",
    "            if \"cuda\" in msg or \"out of memory\" in msg:\n",
    "                logger.warning(\"CUDA error – resetting model: %s\", e)\n",
    "                self._reset()\n",
    "                error_reply = \"I ran into a GPU memory error – please try again.\"\n",
    "            else:\n",
    "                logger.error(\"Runtime error: %s\", e)\n",
    "                error_reply = f\"Error: {e}\"\n",
    "            return {\"reply\": error_reply, \"used_images\": images}\n",
    "            \n",
    "    # ---------- internal helpers ----------\n",
    "    \n",
    "    def _load(self):\n",
    "        \"\"\"Load tokenizer, image_processor, & model. Handles 4-bit quant on GPUs, fp32 on CPU.\"\"\"\n",
    "        logger.info(\"Loading %s ...\", LOCAL_MODEL)\n",
    "        gc.collect()\n",
    "        self._clear_cuda()\n",
    "\n",
    "        self.tok = AutoTokenizer.from_pretrained(\n",
    "            LOCAL_MODEL, trust_remote_code=True\n",
    "        )\n",
    "        if self.tok.pad_token is None:\n",
    "            self.tok.pad_token = self.tok.eos_token\n",
    "\n",
    "        self.image_processor = AutoImageProcessor.from_pretrained(\n",
    "            LOCAL_MODEL, trust_remote_code=True, use_fast=True\n",
    "        )\n",
    "\n",
    "        q_cfg = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\", # Use the modern \"Normal Float 4\"\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16, # Speeds up computation\n",
    "            bnb_4bit_use_double_quant=True, # Minor memory improvement\n",
    "        )\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            LOCAL_MODEL,\n",
    "            quantization_config=q_cfg,\n",
    "            torch_dtype=(torch.bfloat16 if DEVICE == \"cuda\" else torch.float32),\n",
    "            low_cpu_mem_usage=True,\n",
    "            use_flash_attn=False,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\" if DEVICE == \"cuda\" else None,\n",
    "        ).eval()\n",
    "        logger.info(\"Model loaded on %s.\", DEVICE)\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Free everything and reload (called after persistent CUDA errors).\"\"\"\n",
    "        logger.warning(\"Resetting InternVL model …\")\n",
    "        del self.model, self.tok, self.image_processor\n",
    "        self.model = self.tok = self.image_processor = None\n",
    "        gc.collect()\n",
    "        self._clear_cuda()\n",
    "        time.sleep(1)\n",
    "        self._load()\n",
    "\n",
    "    def _process_images(self, image_paths: List[str]):\n",
    "        \"\"\"\n",
    "        Convert a list of image filepaths to a single batched tensor.\n",
    "        \"\"\"\n",
    "        if not image_paths:\n",
    "            return None\n",
    "        try:\n",
    "            # Open all images from their file paths\n",
    "            pil_images = [PILImage.open(p).convert(\"RGB\") for p in image_paths]\n",
    "            \n",
    "            # The processor naturally handles a list of PIL images\n",
    "            processed_data = self.image_processor(images=pil_images, return_tensors=\"pt\")\n",
    "            pixel_values = processed_data['pixel_values']\n",
    "\n",
    "            # Match model device/dtype\n",
    "            target_dtype = next(self.model.parameters()).dtype if self.model else torch.float32\n",
    "            pixel_values = pixel_values.to(device=DEVICE, dtype=target_dtype)\n",
    "            \n",
    "            return pixel_values\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(\"Image processing failed for one or more images: %s\", e)\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def _clear_cuda():\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "# Initalize mm llm\n",
    "mm = InternVLMM(semantic_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0057f873",
   "metadata": {},
   "source": [
    "## Step 6: Test Generation and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98ead24-203d-41fd-b60a-74f64445e800",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "question = \"How do i run blueprints locally?\"\n",
    "results = mm.generate(question, force_regenerate=False)\n",
    "\n",
    "print(\"--- MODEL RESPONSE ---\")\n",
    "display(Markdown(results[\"reply\"]))\n",
    "print(\"----------------------\\n\")\n",
    "\n",
    "display_images(results[\"used_images\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64007551-edfa-4d50-a50e-e242194433ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "question2 = \"What are some feature flags in AIStudio?\"\n",
    "results = mm.generate(question2, force_regenerate=False)\n",
    "\n",
    "print(\"--- MODEL RESPONSE ---\")\n",
    "display(Markdown(results[\"reply\"]))\n",
    "print(\"----------------------\\n\")\n",
    "\n",
    "display_images(results[\"used_images\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6014059-66ef-47a9-b37b-868c15e32039",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "question3 = \"How do i manually clean my environment without hooh?\"\n",
    "results = mm.generate(question3, force_regenerate=True)\n",
    "\n",
    "print(\"--- MODEL RESPONSE ---\")\n",
    "display(Markdown(results[\"reply\"]))\n",
    "print(\"----------------------\\n\")\n",
    "\n",
    "display_images(results[\"used_images\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a3a494-8f3c-4bce-8494-5c8bd7e150cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "question4 = \"How do i sign a config file?\"\n",
    "results = mm.generate(question4, force_regenerate=True)\n",
    "\n",
    "print(\"--- MODEL RESPONSE ---\")\n",
    "display(Markdown(results[\"reply\"]))\n",
    "print(\"----------------------\\n\")\n",
    "\n",
    "display_images(results[\"used_images\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dbca87f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 04:18:50 - INFO - ⏱️ Total execution time: 2m 25.10s\n",
      "2025-07-28 04:18:50 - INFO - ✅ Notebook execution completed successfully.\n"
     ]
    }
   ],
   "source": [
    "end_time: float = time.time()\n",
    "elapsed_time: float = end_time - start_time\n",
    "elapsed_minutes: int = int(elapsed_time // 60)\n",
    "elapsed_seconds: float = elapsed_time % 60\n",
    "\n",
    "logger.info(f\"⏱️ Total execution time: {elapsed_minutes}m {elapsed_seconds:.2f}s\")\n",
    "logger.info(\"✅ Notebook execution completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8147f34-6e3b-47f2-9a68-7e8077f71618",
   "metadata": {},
   "source": [
    "Built with ❤️ using Z by HP AI Studio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
