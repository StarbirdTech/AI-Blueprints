{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9ff4be-1fee-47eb-8d09-611c29a7a83f",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-size: 50px;\">Vanilla RAG Chatbot with Langchain</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbb3e26-125f-4151-9ed3-c84a3fd9081d",
   "metadata": {},
   "source": [
    "Retrieval-Augmented Generation (RAG) is an architectural approach that can enhance the effectiveness of large language model (LLM) applications using customized data. In this example, we use LangChain, an orchestrator for language pipelines, to build an assistant capable of loading information from a web page and use it for answering user questions. We'll also use the DeepEval platform to evaluate, observe and protect the LLM responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a55c55-a1dc-4697-b920-2a81f1bb0f74",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "- Imports\n",
    "- Configurations\n",
    "- Verify Assets\n",
    "- Data Loading\n",
    "- Creation of Chunks\n",
    "- Retrieval\n",
    "- Model Setup\n",
    "- Chain Creation\n",
    "- Model Service "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f78b36-56e2-4596-8325-c75901f30c76",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7156d10f-d930-4be7-a9e8-15606d466460",
   "metadata": {},
   "source": [
    "By using our Local GenAI workspace image, many of the necessary libraries to work with RAG already come pre-installed - in our case, we just need to add the connector to work with PDF documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e700f51-9011-401c-90d3-a07ea8238955",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77853772-0239-40d0-94be-7a62fcb465c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/mlflow/pyfunc/utils/data_validation.py:168: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
      "  color_warning(\n",
      "/opt/conda/lib/python3.12/site-packages/mlflow/pyfunc/utils/data_validation.py:168: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
      "  color_warning(\n",
      "2025-06-26 20:49:41,658 - WARNING - USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# === Standard Library Imports ===\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# === MLflow integration ===\n",
    "import mlflow\n",
    "\n",
    "# Define the relative path to the 'core' directory (one level up from current working directory)\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "# === Import ChatbotService from project core ===\n",
    "from core.chatbot_service.chatbot_service import ChatbotService\n",
    "\n",
    "# === Third-Party Imports ===\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.document import Document\n",
    "from langchain.document_loaders import WebBaseLoader, JSONLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import promptquality as pq\n",
    "import torch\n",
    "\n",
    "# Define the relative path to the 'src' directory (one level up from current working directory)\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "# === Project-Specific Imports (from src.utils) ===\n",
    "from src.utils import (\n",
    "    load_config_and_secrets,\n",
    "    configure_proxy,\n",
    "    initialize_llm,\n",
    "    configure_hf_cache,\n",
    "    setup_opik_environment,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c43204-6cb2-48f3-ac72-f821f12585b8",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd3b8d2c-d0e1-4409-86dc-470c8454fff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "022490c4-8d8a-4c53-92ac-c62ea70a8209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logger\n",
    "logger = logging.getLogger(\"vanilla_rag_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", \n",
    "                              datefmt=\"%Y-%m-%d %H:%M:%S\") \n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75f2111d-5508-41ba-967c-256b40a9fb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"../configs/config.yaml\"\n",
    "SECRETS_PATH = \"../configs/secrets.yaml\"\n",
    "DATA_PATH = \"../data\"\n",
    "MLFLOW_EXPERIMENT_NAME = \"AIStudio-Chatbot-Experiment\"\n",
    "MLFLOW_RUN_NAME = \"AIStudio-Chatbot-Run\"\n",
    "LOCAL_MODEL_PATH = \"/home/jovyan/datafabric/llama3.1-8b-instruct/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\"\n",
    "LOCAL_MODEL_PATHV2 = \"/home/jovyan/datafabric/Qwen3-8B-Q4_K_M/Qwen3-8B-Q4_K_M.gguf\"\n",
    "LOCAL_MODEL_PATHV3 = \"/home/jovyan/datafabric/meta-llama-3.1-8b-instruct-q4_k_m/meta-llama-3.1-8b-instruct-q4_k_m.gguf\"\n",
    "DEMO_FOLDER = \"../demo\"\n",
    "MLFLOW_MODEL_NAME = \"AIStudio-Chatbot-Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7384967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f912d15a-7aab-4620-8c89-33c8f07104f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 20:49:41 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "logger.info('Notebook execution started.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ca6361-dd3c-4778-9a9d-b11da7b20151",
   "metadata": {},
   "source": [
    "## Configuration of HuggingFace caches\n",
    "\n",
    "In the next cell, we configure HuggingFace cache, so that all the models downloaded from them are persisted locally, even after the workspace is closed. This is a future desired feature for AI Studio and the GenAI addon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "965e4ea4-084a-4f32-9f51-9ead89dd16fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure HuggingFace cache\n",
    "configure_hf_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42ff655c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 20:49:41,975 - INFO - PyTorch version 2.7.0 available.\n",
      "2025-06-26 20:49:42,077 - INFO - Use pytorch device_name: cuda\n",
      "2025-06-26 20:49:42,077 - INFO - Load pretrained SentenceTransformer: intfloat/e5-large-v2\n"
     ]
    }
   ],
   "source": [
    "# Initialize HuggingFace Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/e5-large-v2\",\n",
    "    cache_folder=\"/tmp/hf_cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7062ee0",
   "metadata": {},
   "source": [
    "## Configuration and Secrets Loading\n",
    "\n",
    "In this section, we load configuration parameters and API keys from separate YAML files. This separation helps maintain security by keeping sensitive information (API keys) separate from configuration settings.\n",
    "\n",
    "- **config.yaml**: Contains non-sensitive configuration parameters like model sources and URLs\n",
    "- **secrets.yaml**: Contains sensitive API keys for services like Galileo and HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e0b7464-ad76-47f2-94f0-e70c4d4cc380",
   "metadata": {},
   "outputs": [],
   "source": [
    "config, secrets = load_config_and_secrets(CONFIG_PATH, SECRETS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba50026-86dc-4d90-a827-55fe7882cfe8",
   "metadata": {},
   "source": [
    "## Proxy Configuration\n",
    "\n",
    "In order to connect to Galileo service, a SSH connection needs to be established. For certain enterprise networks, this might require an explicit setup of the proxy configuration. If this is your case, set up the \"proxy\" field on your config.yaml and the following cell will configure the necessary environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a76a9c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "configure_proxy(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e80960-54f9-4ec2-b062-6d49526494d1",
   "metadata": {},
   "source": [
    "# Verify Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4417e48c-2b51-4e61-b74d-8230caf2f7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 20:49:44 - INFO - Config is properly configured. \n",
      "2025-06-26 20:49:44 - INFO - Secrets is properly configured. \n",
      "2025-06-26 20:49:44 - INFO - Local Llama model is properly configured. \n"
     ]
    }
   ],
   "source": [
    "def log_asset_status(asset_path: str, asset_name: str, success_message: str, failure_message: str) -> None:\n",
    "    \"\"\"\n",
    "    Logs the status of a given asset based on its existence.\n",
    "\n",
    "    Parameters:\n",
    "        asset_path (str): File or directory path to check.\n",
    "        asset_name (str): Name of the asset for logging context.\n",
    "        success_message (str): Message to log if asset exists.\n",
    "        failure_message (str): Message to log if asset does not exist.\n",
    "    \"\"\"\n",
    "    if Path(asset_path).exists():\n",
    "        logger.info(f\"{asset_name} is properly configured. {success_message}\")\n",
    "    else:\n",
    "        logger.info(f\"{asset_name} is not properly configured. {failure_message}\")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=CONFIG_PATH,\n",
    "    asset_name=\"Config\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if the configs.yaml was propely connfigured in your project on AI Studio.\"\n",
    ")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=SECRETS_PATH,\n",
    "    asset_name=\"Secrets\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if the secrets.yaml was propely connfigured in your project on AI Studio.\"\n",
    ")\n",
    "log_asset_status(\n",
    "    asset_path=LOCAL_MODEL_PATH,\n",
    "    asset_name=\"Local Llama model\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please create and download the required assets in your project on AI Studio if you want to use local model.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d92659e-23ea-4965-bb27-58cd1dd0f3ec",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "\n",
    "In this step, we will use the Langchain framework to  extract the content from a local PDF file with the product documentation. Also, we have commented some example on how to use Web Loaders to load data from pages on the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb4c1589-285f-4e72-ab8e-5ae9fbd4043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Verify existence of the data directory ===\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    raise FileNotFoundError(f\"'data' folder not found at path: {os.path.abspath(DATA_PATH)}\")\n",
    "\n",
    "# === Wiki JSON with JSONLoader ===\n",
    "wiki_loader = JSONLoader(\n",
    "    file_path=os.path.join(DATA_PATH, \"wiki_flat_structure.json\"),\n",
    "    jq_schema=\"to_entries[] | {source: .key, text: .value.content}\",  # adapt to your schema\n",
    "    text_content=False  # keeps original formatting; set True if you want only strings\n",
    ")\n",
    "\n",
    "docs = wiki_loader.load()\n",
    "# === Optional: Load additional web-based documents ===\n",
    "# To use a different knowledge base, just change the URLs below\n",
    "\n",
    "# loader1 = WebBaseLoader(\"https://www.hp.com/us-en/workstations/ai-studio.html\")\n",
    "# data1 = loader1.load()\n",
    "\n",
    "# loader2 = WebBaseLoader(\"https://zdocs.datascience.hp.com/docs/aistudio\")\n",
    "# data2 = loader2.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc63ed05-192e-4f59-856c-cf9bff160fc8",
   "metadata": {},
   "source": [
    "# Creation of Chunks\n",
    "Here, we split the loaded documents into chunks, so we have smaller and more specific texts to add to our vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3dbd381-58b8-4569-aa75-b26d07556b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 20:49:44 - INFO - Chunks created: 3633 docs, avg_tokens=499\n"
     ]
    }
   ],
   "source": [
    "# === Initialize text splitter ===\n",
    "# - chunk_size: Maximum number of characters per text chunk.\n",
    "# - chunk_overlap: Number of overlapping characters between chunks.\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=600, chunk_overlap=100,\n",
    "    separators=[\"\\n## \", \"\\n# \", \"\\n\\n\", \".\", \"!\", \"?\"]\n",
    ")\n",
    "\n",
    "# text_splitter = SemanticChunker(\n",
    "#     embeddings=embeddings,\n",
    "# )\n",
    "\n",
    "# ─── split your loaded PDF docs ─────────────────────────────────────────────────\n",
    "splits: List[Document] = text_splitter.split_documents(docs)\n",
    "\n",
    "def log_stage(name: str, docs: List[Document]):\n",
    "    logger.info(f\"{name}: {len(docs)} docs, avg_tokens={sum(len(d.page_content) for d in docs)/len(docs):.0f}\")\n",
    "# e.g. after splits\n",
    "log_stage(\"Chunks created\", splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7999ff1f-1f6a-452c-848e-e3c67adf7766",
   "metadata": {},
   "source": [
    "# Retrieval\n",
    "\n",
    "We transform the texts into embeddings and store them in a vector database. This allows us to perform similarity search, and proper retrieval of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e43ae88-252a-4cbe-96e4-081daa3dc25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 20:49:45,026 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50 s, sys: 1.58 s, total: 51.6 s\n",
      "Wall time: 48.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# === Create a vector database from document chunks ===\n",
    "vectordb = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "# === Configure the vector database as a retriever for querying ===\n",
    "retriever = vectordb.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"fetch_k\": 20, \"k\": 8}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58329084-640e-4b25-be70-ac1ec90467e9",
   "metadata": {},
   "source": [
    "# Model Setup\n",
    "\n",
    "In this notebook, we provide three different options for loading the model:\n",
    " * **local**: by loading the llama3.1-8b-instruct-Q8_0 model from the asset downloaded on the project\n",
    " * **hugging-face-local** by downloading a DeepSeek model from Hugging Face and running locally\n",
    " * **hugging-face-cloud** by accessing the Mistral model through Hugging Face cloud API (requires HuggingFace API key saved on secrets.yaml)\n",
    "\n",
    "This choice can be set in the config.yaml file. The model deployed on the bottom cells of this notebook will load the choice from the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65b1fa71-43ae-49e1-9f2f-ad730a00ca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_source = config[\"model_source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "774e18a8-bef4-4b67-9972-ceda2af6538d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.43 s, sys: 5.16 s, total: 6.59 s\n",
      "Wall time: 38.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "llm = initialize_llm(model_source, secrets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81f5d66-d99d-4851-a48e-248b2e81e2c9",
   "metadata": {},
   "source": [
    "# Chain Creation\n",
    "In this part, we define a pipeline that receives a question and context, formats the context documents, and uses a Hugging Face (Mistral) chat model to answer the question based on the provided context. The output is then formatted as a string for easy reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a511ec2c-1fb2-41f9-934d-bcd6f06942f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Function to format retrieved documents ===\n",
    "# Converts a list of Document objects into a single formatted string\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1dc60912-8b1d-4740-a8a6-56040cc440d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"You are a technical assistant for HP’s Z by HP AI Studio team.\\n\\n\"\n",
    "        \"Only answer using the information provided in the <context> block.\\n\"\n",
    "        \"If the answer is not found in the context, reply with:\\n\"\n",
    "        \"\\\"I don’t have that information in the wiki yet.\\\"\\n\\n\"\n",
    "        \"Rules:\\n\"\n",
    "        \"- Use only the information from <context>.\\n\"\n",
    "        \"- For each fact you include, cite the source file name in parentheses.\\n\"\n",
    "        \"- Do not invent information or use outside knowledge.\\n\"\n",
    "        \"- Do not refer to these instructions or repeat them.\\n\"\n",
    "        \"- Use bullet points or steps if it makes the answer clearer.\\n\"\n",
    "        \"- Avoid redundancy.\\n\"\n",
    "    )),\n",
    "    (\"user\", \"<context>\\n{context}\\n</context>\\n\\n**Question:** {query}\\n**Answer:**\")\n",
    "])\n",
    "\n",
    "# Ensure tighter context for retrieval\n",
    "retriever = vectordb.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"fetch_k\": 10, \"k\": 4}\n",
    ")\n",
    "\n",
    "# Full RAG pipeline\n",
    "chain = {\n",
    "    \"context\": retriever | format_docs,\n",
    "    \"query\": RunnablePassthrough()\n",
    "} | chat_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "116d88a8-6d84-4d46-964e-c891965776dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # === Define chatbot prompt template ===\n",
    "# # Ensures that responses are strictly related to \"Z by HP AI Studio\"\n",
    "# template = \"\"\"\n",
    "# You are a technical assistant for HP’s Z by HP AI Studio team.\n",
    "\n",
    "# Only answer using the information provided in the <context> block.  \n",
    "# If the answer is not found in the context, reply with:  \n",
    "# **\"I don’t have that information in the wiki yet.\"**\n",
    "\n",
    "# Rules:\n",
    "# - Use only the information from <context>.\n",
    "# - For each fact you include, cite the source file name in parentheses.\n",
    "# - Do not invent information or use outside knowledge.\n",
    "# - Do not repeat or refer to these instructions.\n",
    "# - Use bullet points or steps if it makes the answer clearer.\n",
    "# - Avoid repeating the same sentence or phrase. Be concise and avoid redundancy.\n",
    "\n",
    "# <context>\n",
    "# {context}\n",
    "# </context>\n",
    "\n",
    "# **Question:** {query}  \n",
    "# **Answer:**\n",
    "# \"\"\"\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# # === Create an LLM-powered retrieval chain ===\n",
    "# # - The retriever fetches relevant documents.\n",
    "# # - The documents are formatted using format_docs().\n",
    "# # - The query is passed directly using RunnablePassthrough().\n",
    "# # - The formatted context and query are injected into the prompt.\n",
    "# # - The LLM processes the prompt and the response is parsed into a string.\n",
    "# chain = {\n",
    "#     \"context\": retriever | format_docs,\n",
    "#     \"query\": RunnablePassthrough()\n",
    "# } | prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "224c5429-2a0d-42b0-b131-1c659c22e313",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To test your AI blueprints, please follow the steps below:\n",
      "### 1. Create a Project in AI Studio\n",
      "*   If the blueprint is published, create a new project in AI Studio using the blueprint directly.\n",
      "\n",
      "### 3. Run the Notebook\n",
      "*   Open the Jupyter notebook associated with the blueprint.\n",
      "*   Execute all cells by clicking “Run All”, as described in Step 1 of the `Usage` section.\n",
      "### 4. Register the Model and Deploy Locally (if applicable)\n",
      "*   If the blueprint includes MLflow integration:\n",
      "    + Follow the next usage step to register the model in MLflow and deploy it successfully.\n",
      "\n",
      "You can verify your blueprints' functionality by checking the project output, model performance metrics, and any other relevant data or indicators. Ensure you follow the guidelines and recommendations outlined in the AI Blueprint Testing Guide for successful blueprint testing.  (Blueprint Testing Guide) (Usage section) (MLflow integration) (blueprint includes MLflow integration) (register the model in MLflow) (deploy it successfully) (AI Studio) (Jupyter notebook) (Execute all cells by clicking “Run All”) (Step 1 of the `Usage` section) (Model performance metrics) (Data or indicators) (AI Blueprint Testing Guide) (blueprint includes MLflow integration).png image.png (image-ee2b37bf-81f0-44fa-933a-e9f8b2a7dcda.png) (Project Creation and workspace startup Swimlanes)  (Project Creation and workspace startup Swimlanes) (Assumptions) (blueprint updates to update existing projects).png image.png (image-ee2b37bf-81f0-44fa-933a-e9f8b2a7dcda.png) (Notes) (blueprints are hot-updatable, AIS should probably ship with the latest available at build time) (ngcconfig): Copy of the ngcconfig section in AI Studio config.yaml. Lists which image should run the tests (ngcconfig).png image.png (image-ee2b37bf-81f0-44fa-933a-e9f8b2a7dcda.png) (ngcconfig): Copy of the ngcconfig section in AI Studio config.yaml. Lists the version of the NGC images that should be used for tests  (ngcconfig).png image.png (image-ee2b37bf-81f0-44fa-933a-e9f8b2a7dcda.png) (Usage section) (Step 1) (blueprint includes MLflow integration) (register the model in MLflow).png image.png (image-ee2b37bf-81f0-44fa-933a-e9f8b2a7dcda.png).png image.png (image-0d2a30c5-b734-4da5-a35b-9708de1515da.png) (Notes) (blueprints are hot-updatable, AIS should probably ship with the latest available at build time).png image.png (image-ee2b37bf-81f0-44fa-933a-e9f8b2a7dcda.png)  .png image.png (image-ee2b37bf-81f0-44fa-933a-e9f8b2a7dcda.png).png image.png (image-ee2b37bf-81f0-44fa-933a-e9f8b2a7dcda.png)  (Project Creation and workspace startup Swimlanes)  .png image.png (image-ee2b37bf-81f0-44fa-933a-e9f8b2a7dcda.png).png image.png (image-0d2a30c5-b734-4da5-a35b-9708de1515da.png) .png image.png (image-ee2b37bf-81f0-44fa-933a-e9f8b2a7dcda.png).png image.png (image-ee2b37bf-81f0-44fa-933a-e9f8b2a7dcda.png) .png image.png (image-0d2a30c5-b734-4da5-a35b-9708de1515da.png).png image.png (image-ee2b37bf-81f0-44fa-933a-e9f8b2a7dcda.png)  .png image.png (image-0d2a30c5-b734-4da5-a35b-9708de1515da.png).png image.png (image-ee2b37bf-81f0-44fa-933a-e9f8b2a7dcda.png)  .png image.png (image-ee2b37\n"
     ]
    }
   ],
   "source": [
    "# One-off query (synchronous)\n",
    "question = \"How do i test my ai blueprints?\"\n",
    "answer = chain.invoke(question)          # or chain(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2059383-6977-4532-886d-bdb5953dfec0",
   "metadata": {},
   "source": [
    "# Opik Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e591cf-27e2-4fc1-ba22-556e1e93b15e",
   "metadata": {},
   "source": [
    "## Configure Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146d2c8b-fb9f-4b0c-a68f-5d420068888d",
   "metadata": {},
   "source": [
    "Configure opik to either local or cloud (api). For local use `opik.configure(use_local=True)`. We also specify the metrics we want to judge here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a832a79-f7ab-4df6-8f46-30e64b3b7088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Opik is already configured. You can check the settings by viewing the config file at /home/jovyan/.opik.config\n"
     ]
    }
   ],
   "source": [
    "import opik\n",
    "from opik.evaluation import evaluate_prompt\n",
    "from opik.evaluation.metrics import (\n",
    "    Hallucination,\n",
    "    AnswerRelevance,\n",
    "    ContextPrecision,   # already used but we’ll keep it for completeness\n",
    "    ContextRecall,\n",
    "    GEval,              # generic rubric-based metric\n",
    ")\n",
    "from opik import Opik\n",
    "\n",
    "setup_opik_environment(secrets)\n",
    "\n",
    "opik.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176a68a5-ff0d-42bc-8d48-9460a08f8941",
   "metadata": {},
   "source": [
    "## Evaluate Hallucination, Answer Relevance, Context Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1356c93-e7e7-49c1-9963-cf26a0b4b3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.local_judge import LangChainJudge\n",
    "from opik.evaluation.metrics import Hallucination\n",
    "\n",
    "hallucination_judge = LangChainJudge(LOCAL_MODEL_PATH, \"hallucination\")\n",
    "answer_relevance_judge = LangChainJudge(LOCAL_MODEL_PATH, \"answer_relevance\")\n",
    "context_precision_judge = LangChainJudge(LOCAL_MODEL_PATH, \"context_precision\")\n",
    "context_recall_judge = LangChainJudge(LOCAL_MODEL_PATH, \"context_recall\")\n",
    "\n",
    "# metrics = [\n",
    "#     Hallucination(model=\"gpt-4.1-nano\"),\n",
    "#     AnswerRelevance(model=\"gpt-4.1-nano\"),\n",
    "#     ContextPrecision(model=\"gpt-4.1-nano\"),\n",
    "#     ContextRecall(model=\"gpt-4.1-nano\"),\n",
    "# ]\n",
    "\n",
    "metrics = [\n",
    "    #Hallucination(model=hallucination_judge),\n",
    "    #AnswerRelevance(model=answer_relevance_judge),\n",
    "    #ContextPrecision(model=context_precision_judge),\n",
    "    ContextRecall(model=context_recall_judge),\n",
    "]\n",
    "\n",
    "def rag_with_full_eval(\n",
    "    question: str,\n",
    "    expected_answer: str | None = None        # ← new\n",
    ") -> dict:\n",
    "    answer       = chain.invoke(question)\n",
    "    context_docs = [\n",
    "        d.page_content for d in retriever.get_relevant_documents(question)\n",
    "    ]\n",
    "\n",
    "    scores = {}\n",
    "    for m in metrics:\n",
    "        if isinstance(m, (ContextPrecision, ContextRecall)):\n",
    "            if expected_answer is None:\n",
    "                continue\n",
    "            result = m.score(\n",
    "                input=question,\n",
    "                output=answer,\n",
    "                expected_output=expected_answer,\n",
    "                context=context_docs,\n",
    "            )\n",
    "        else:\n",
    "            result = m.score(\n",
    "                input=question,\n",
    "                output=answer,\n",
    "                context=context_docs,\n",
    "            )\n",
    "        scores[m.__class__.__name__] = (\n",
    "            round(result.value, 3),\n",
    "            result.reason,\n",
    "        )\n",
    "\n",
    "    return {\"answer\": answer, \"scores\": scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84e7adb4-8794-46d0-81a6-ca9dd55e7a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 21:03:24,364 - DEBUG - Raw model output:\n",
      " . (End of additional notes) **End**. \n",
      "\n",
      "(I hope that was helpful and clear!)   . (End of response)** End**.  . \n",
      "\n",
      "(Also, I just realized that I made a mistake in my previous response by referring to the \"blueprints_testing_guide.yaml\" document as the source of information for the steps outlined in the answer)\n",
      "\n",
      "So, to make it clearer: The steps outlined in the answer are based on the standard and comprehensive process for testing blueprints in AI Studio, which is described in detail in the \"Blueprint Testing Guide\" (Source: blueprints\\_testing\\_guide.yaml). \n",
      "\n",
      "I hope this clears up any confusion, and please let me know if you have any further questions or concerns! **End.** (I hope that was helpful!)\n",
      "\n",
      "Please note that I'll be happy to help answer your questions if you would like more clarification or details about the process for testing AI Studio AI Blueprints. Just let me know how I can best assist you in getting a clear understanding of this process. \n",
      "\n",
      "(And also, please keep in mind that the \"Blueprint Testing Guide\" document is intended as a resource to help users understand and follow the standard and comprehensive steps for testing blueprint projects in the [AI-Blueprints GitHub repository](https://github.com/HPInc/AI-Blueprints)]). \n",
      "\n",
      "**The End.** (I hope that was helpful!)   . (End of additional notes) **End**.\n",
      "\n",
      "(I think I'm done now?)   . (End of answer)** End**. \n",
      "\n",
      "(I'll be happy to help you with any further questions or concerns if needed)    . (End of additional notes) **End**.  \n",
      "\n",
      "(I hope that was helpful and clear!)   . (End of response)** End**.  . \n",
      "\n",
      "(Also, I just realized that I made a mistake in my previous response by referring to the \"blueprints_testing_guide.yaml\" document as the source of information for the steps outlined in the answer)\n",
      "\n",
      "So, to make it clearer: The steps outlined in the answer are based on the standard and comprehensive process for testing blueprints in AI Studio, which is described in detail in the \"Blueprint Testing Guide\" (Source: blueprints\\_testing\\_guide.yaml). \n",
      "\n",
      "I hope this clears up any confusion, and please let me know if you have any further questions or concerns! **End.** (I hope that was helpful!)\n",
      "\n",
      "Please note that I'll be happy to help answer your questions if you would like more clarification or details about the process\n",
      "2025-06-26 21:03:24,365 - DEBUG - Raw Score:\n",
      "-1.0\n",
      "OPIK: Failed to parse model output: Context recall score must be between 0.0 and 1.0, got -1.0\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/opik/evaluation/metrics/llm_judges/context_recall/parser.py\", line 15, in parse_model_output\n",
      "    raise exceptions.MetricComputationError(\n",
      "opik.exceptions.MetricComputationError: Context recall score must be between 0.0 and 1.0, got -1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Failed to calculate context recall score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 21:03:25,217 - INFO - HTTP Request: POST https://www.comet.com/opik/api/v1/private/traces/batch \"HTTP/1.1 204 No Content\"\n",
      "2025-06-26 21:03:25,221 - INFO - HTTP Request: POST https://www.comet.com/opik/api/v1/private/spans/batch \"HTTP/1.1 204 No Content\"\n"
     ]
    }
   ],
   "source": [
    "demo_q = \"How do test the AI Studio AI Blueprints to ensure that they work?\"\n",
    "try:\n",
    "    expected_answer = \"Create the project in AI Studio, finish setup, run all notebook cells, register/deploy the model, test the UIs, push the executed notebook and interface PDFs with test results, then open a pull request\"\n",
    "    res = rag_with_full_eval(demo_q, expected_answer)\n",
    "    print(\"ASSISTANT:\", res[\"answer\"])\n",
    "    print(\"--- METRICS ---\")\n",
    "    for name, (val, why) in res[\"scores\"].items():\n",
    "        print(f\"{name:16} | {val} | {why}\")\n",
    "except Exception as e:\n",
    "    print(\"ERROR:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc6884d",
   "metadata": {},
   "source": [
    "# Model Service \n",
    "\n",
    "In this section, we demonstrate how to deploy a RAG-based chatbot service with integrated Galileo Protect and Observe capabilities. This service provides a REST API endpoint that allows users to query the knowledge base with natural language questions, upload new documents to the knowledge base, and manage conversation history, all with built-in safeguards against sensitive information and toxicity. This service encapsulates all the functionality we developed in this notebook, including the document retrieval system, RAG-based question answering capabilities, and Galileo integration for protection, observation and evaluation. It demonstrates how to use our ChatbotService from the src/service directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "867f62bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 20:55:42,371 - INFO - Use pytorch device_name: cuda\n",
      "2025-06-26 20:55:42,371 - INFO - Load pretrained SentenceTransformer: intfloat/e5-large-v2\n",
      "2025-06-26 20:55:43,971 - INFO - HTTP Request: POST https://www.comet.com/opik/api/v1/private/spans/batch \"HTTP/1.1 204 No Content\"\n",
      "2025-06-26 20:55:43,983 - INFO - HTTP Request: POST https://www.comet.com/opik/api/v1/private/traces/batch \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d931fa299794e85ab099d8a7ffbb21a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41df0541ae90475589ed88a92d1a8ea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d786c5097bd4730bcee9a608befe86b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce7175db7aea432f9f0cf3d4fdbbe461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d2c9e2e5654380842654452d4ffc7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/26 20:56:44 WARNING mlflow.utils.requirements_utils: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - PyPDF (current: uninstalled, required: PyPDF)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n",
      "2025-06-26 20:57:31,117 - INFO - Model and artifacts successfully registered in MLflow.\n",
      "Registered model 'AIStudio-Chatbot-Model' already exists. Creating a new version of this model...\n",
      "Created version '18' of model 'AIStudio-Chatbot-Model'.\n",
      "2025-06-26 20:57:31 - INFO - ✅ Model registered successfully with run ID: a14a7d61bf6f4a1883d357934fc2e32b\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_tracking_uri('/phoenix/mlflow')\n",
    "# === Set MLflow experiment context ===\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\n",
    "\n",
    "# === Validate local model file path ===\n",
    "if not os.path.exists(LOCAL_MODEL_PATH):\n",
    "    logger.info(f\"⚠️ Warning: Model file not found at {LOCAL_MODEL_PATH}. Please verify the path.\")\n",
    "\n",
    "# === Log and register model to MLflow ===\n",
    "with mlflow.start_run(run_name=MLFLOW_RUN_NAME) as run:\n",
    "    \n",
    "    # Log model artifacts using custom ChatbotService\n",
    "    ChatbotService.log_model(\n",
    "        artifact_path=MLFLOW_MODEL_NAME,\n",
    "        config_path=CONFIG_PATH,\n",
    "        secrets_path=SECRETS_PATH,\n",
    "        docs_path=DATA_PATH,\n",
    "        model_path=LOCAL_MODEL_PATH,\n",
    "        demo_folder=DEMO_FOLDER\n",
    "    )\n",
    "\n",
    "    # Construct the URI for the logged model\n",
    "    model_uri = f\"runs:/{run.info.run_id}/{MLFLOW_MODEL_NAME}\"\n",
    "\n",
    "    # Register the model into MLflow Model Registry\n",
    "    mlflow.register_model(\n",
    "        model_uri=model_uri,\n",
    "        name=MLFLOW_MODEL_NAME\n",
    "    )\n",
    "\n",
    "    logger.info(f\"✅ Model registered successfully with run ID: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "519b97dc-ba54-4256-99fd-4639c505d185",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 20:57:31 - INFO - Notebook execution completed.\n"
     ]
    }
   ],
   "source": [
    "logger.info('Notebook execution completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8147f34-6e3b-47f2-9a68-7e8077f71618",
   "metadata": {},
   "source": [
    "Built with ❤️ using Z by HP AI Studio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
