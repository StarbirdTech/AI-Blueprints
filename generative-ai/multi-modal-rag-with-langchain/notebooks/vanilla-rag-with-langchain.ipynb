{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9ff4be-1fee-47eb-8d09-611c29a7a83f",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-size: 50px;\">Vanilla RAG Chatbot with Langchain</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbb3e26-125f-4151-9ed3-c84a3fd9081d",
   "metadata": {},
   "source": [
    "Retrieval-Augmented Generation (RAG) is an architectural approach that can enhance the effectiveness of large language model (LLM) applications using customized data. In this example, we use LangChain, an orchestrator for language pipelines, to build an assistant capable of loading information from a web page and use it for answering user questions. We'll also use the DeepEval platform to evaluate, observe and protect the LLM responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a55c55-a1dc-4697-b920-2a81f1bb0f74",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "- Imports\n",
    "- Configurations\n",
    "- Verify Assets\n",
    "- Data Loading\n",
    "- Creation of Chunks\n",
    "- Retrieval\n",
    "- Model Setup\n",
    "- Chain Creation\n",
    "- Model Service "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f78b36-56e2-4596-8325-c75901f30c76",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7156d10f-d930-4be7-a9e8-15606d466460",
   "metadata": {},
   "source": [
    "By using our Local GenAI workspace image, many of the necessary libraries to work with RAG already come pre-installed - in our case, we just need to add the connector to work with PDF documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e700f51-9011-401c-90d3-a07ea8238955",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadb==0.6.3 (from -r ../requirements.txt (line 4))\n",
      "  Downloading chromadb-0.6.3-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting langchain==0.3.25 (from -r ../requirements.txt (line 5))\n",
      "  Downloading langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain_community==0.3.25 (from -r ../requirements.txt (line 6))\n",
      "  Downloading langchain_community-0.3.25-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting langchain_core==0.3.65 (from -r ../requirements.txt (line 7))\n",
      "  Downloading langchain_core-0.3.65-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain_huggingface==0.3.0 (from -r ../requirements.txt (line 8))\n",
      "  Downloading langchain_huggingface-0.3.0-py3-none-any.whl.metadata (996 bytes)\n",
      "Collecting hf-xet==1.1.4 (from -r ../requirements.txt (line 9))\n",
      "  Downloading hf_xet-1.1.4-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Collecting opencv-python-headless==4.11.0.86 (from -r ../requirements.txt (line 10))\n",
      "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting jq==1.9.1 (from -r ../requirements.txt (line 11))\n",
      "  Downloading jq-1.9.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting pymupdf==1.25.5 (from -r ../requirements.txt (line 12))\n",
      "  Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting deepeval==3.1.4 (from -r ../requirements.txt (line 13))\n",
      "  Downloading deepeval-3.1.4-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting opik==1.7.36 (from -r ../requirements.txt (line 14))\n",
      "  Downloading opik-1.7.36-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: build>=1.0.3 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.6.3->-r ../requirements.txt (line 4)) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.6.3->-r ../requirements.txt (line 4)) (2.11.4)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.6.3->-r ../requirements.txt (line 4)) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.6.3->-r ../requirements.txt (line 4)) (0.115.12)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /opt/conda/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.6.3->-r ../requirements.txt (line 4)) (0.34.2)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.6.3->-r ../requirements.txt (line 4)) (1.26.4)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.6.3->-r ../requirements.txt (line 4)) (4.0.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.6.3->-r ../requirements.txt (line 4)) (4.13.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.6.3->-r ../requirements.txt (line 4)) (1.21.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.6.3->-r ../requirements.txt (line 4)) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.6.3->-r ../requirements.txt (line 4)) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.6.3->-r ../requirements.txt (line 4)) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.6.3->-r ../requirements.txt (line 4)) (1.32.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.6.3->-r ../requirements.txt (line 4)) (0.21.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.6.3->-r ../requirements.txt (line 4)) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.6.3->-r ../requirements.txt (line 4)) (4.67.0)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.6.3->-r ../requirements.txt (line 4)) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.12/site-packages (from chromadb==0.6.3->-r ../requirements.txt (line 4)) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.6.3->-r ../requirements.txt (line 4)) (1.71.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.6.3->-r ../requirements.txt (line 4)) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.6.3->-r ../requirements.txt (line 4)) (0.15.3)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.6.3->-r ../requirements.txt (line 4)) (32.0.1)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.6.3->-r ../requirements.txt (line 4)) (9.1.2)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.6.3->-r ../requirements.txt (line 4)) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.6.3->-r ../requirements.txt (line 4)) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.6.3->-r ../requirements.txt (line 4)) (3.10.18)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.6.3->-r ../requirements.txt (line 4)) (0.27.2)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.6.3->-r ../requirements.txt (line 4)) (14.0.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /opt/conda/lib/python3.12/site-packages (from langchain==0.3.25->-r ../requirements.txt (line 5)) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /opt/conda/lib/python3.12/site-packages (from langchain==0.3.25->-r ../requirements.txt (line 5)) (0.3.39)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.12/site-packages (from langchain==0.3.25->-r ../requirements.txt (line 5)) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.12/site-packages (from langchain==0.3.25->-r ../requirements.txt (line 5)) (2.32.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.12/site-packages (from langchain_community==0.3.25->-r ../requirements.txt (line 6)) (3.11.18)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.12/site-packages (from langchain_community==0.3.25->-r ../requirements.txt (line 6)) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /opt/conda/lib/python3.12/site-packages (from langchain_community==0.3.25->-r ../requirements.txt (line 6)) (2.9.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from langchain_community==0.3.25->-r ../requirements.txt (line 6)) (0.4.0)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain==0.3.25->-r ../requirements.txt (line 5))\n",
      "  Downloading langsmith-0.3.45-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.12/site-packages (from langchain_core==0.3.65->-r ../requirements.txt (line 7)) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/conda/lib/python3.12/site-packages (from langchain_core==0.3.65->-r ../requirements.txt (line 7)) (24.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.30.2 in /opt/conda/lib/python3.12/site-packages (from langchain_huggingface==0.3.0->-r ../requirements.txt (line 8)) (0.30.2)\n",
      "Collecting anthropic (from deepeval==3.1.4->-r ../requirements.txt (line 13))\n",
      "  Downloading anthropic-0.55.0-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: click<8.2.0,>=8.0.0 in /opt/conda/lib/python3.12/site-packages (from deepeval==3.1.4->-r ../requirements.txt (line 13)) (8.1.8)\n",
      "Collecting google-genai<2.0.0,>=1.9.0 (from deepeval==3.1.4->-r ../requirements.txt (line 13))\n",
      "  Downloading google_genai-1.21.1-py3-none-any.whl.metadata (37 kB)\n",
      "Requirement already satisfied: nest_asyncio in /opt/conda/lib/python3.12/site-packages (from deepeval==3.1.4->-r ../requirements.txt (line 13)) (1.6.0)\n",
      "Collecting ollama (from deepeval==3.1.4->-r ../requirements.txt (line 13))\n",
      "  Downloading ollama-0.5.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: openai in /opt/conda/lib/python3.12/site-packages (from deepeval==3.1.4->-r ../requirements.txt (line 13)) (1.51.0)\n",
      "Collecting portalocker (from deepeval==3.1.4->-r ../requirements.txt (line 13))\n",
      "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting posthog>=2.4.0 (from chromadb==0.6.3->-r ../requirements.txt (line 4))\n",
      "  Downloading posthog-3.25.0-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting pyfiglet (from deepeval==3.1.4->-r ../requirements.txt (line 13))\n",
      "  Downloading pyfiglet-1.0.3-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting pytest (from deepeval==3.1.4->-r ../requirements.txt (line 13))\n",
      "  Downloading pytest-8.4.1-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting pytest-asyncio (from deepeval==3.1.4->-r ../requirements.txt (line 13))\n",
      "  Downloading pytest_asyncio-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting pytest-repeat (from deepeval==3.1.4->-r ../requirements.txt (line 13))\n",
      "  Downloading pytest_repeat-0.9.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting pytest-rerunfailures<13.0,>=12.0 (from deepeval==3.1.4->-r ../requirements.txt (line 13))\n",
      "  Downloading pytest_rerunfailures-12.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting pytest-xdist (from deepeval==3.1.4->-r ../requirements.txt (line 13))\n",
      "  Downloading pytest_xdist-3.7.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting rich>=10.11.0 (from chromadb==0.6.3->-r ../requirements.txt (line 4))\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting sentry-sdk (from deepeval==3.1.4->-r ../requirements.txt (line 13))\n",
      "  Downloading sentry_sdk-2.30.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from deepeval==3.1.4->-r ../requirements.txt (line 13)) (75.3.0)\n",
      "Collecting tabulate<0.10.0,>=0.9.0 (from deepeval==3.1.4->-r ../requirements.txt (line 13))\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting tenacity>=8.2.3 (from chromadb==0.6.3->-r ../requirements.txt (line 4))\n",
      "  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.12/site-packages (from deepeval==3.1.4->-r ../requirements.txt (line 13)) (0.45.0)\n",
      "Collecting boto3-stubs>=1.34.110 (from boto3-stubs[bedrock-runtime]>=1.34.110->opik==1.7.36->-r ../requirements.txt (line 14))\n",
      "  Downloading boto3_stubs-1.38.42-py3-none-any.whl.metadata (150 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=3.0.0 (from opik==1.7.36->-r ../requirements.txt (line 14))\n",
      "  Downloading rapidfuzz-3.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting litellm (from opik==1.7.36->-r ../requirements.txt (line 14))\n",
      "  Downloading litellm-1.73.0-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting uuid6 (from opik==1.7.36->-r ../requirements.txt (line 14))\n",
      "  Downloading uuid6-2025.0.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from opik==1.7.36->-r ../requirements.txt (line 14)) (3.1.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.25->-r ../requirements.txt (line 6)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.25->-r ../requirements.txt (line 6)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.25->-r ../requirements.txt (line 6)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.25->-r ../requirements.txt (line 6)) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.25->-r ../requirements.txt (line 6)) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.25->-r ../requirements.txt (line 6)) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.25->-r ../requirements.txt (line 6)) (1.20.0)\n",
      "Collecting botocore-stubs (from boto3-stubs>=1.34.110->boto3-stubs[bedrock-runtime]>=1.34.110->opik==1.7.36->-r ../requirements.txt (line 14))\n",
      "  Downloading botocore_stubs-1.38.30-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting types-s3transfer (from boto3-stubs>=1.34.110->boto3-stubs[bedrock-runtime]>=1.34.110->opik==1.7.36->-r ../requirements.txt (line 14))\n",
      "  Downloading types_s3transfer-0.13.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting mypy-boto3-bedrock-runtime<1.39.0,>=1.38.0 (from boto3-stubs[bedrock-runtime]>=1.34.110->opik==1.7.36->-r ../requirements.txt (line 14))\n",
      "  Downloading mypy_boto3_bedrock_runtime-1.38.4-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: pyproject_hooks in /opt/conda/lib/python3.12/site-packages (from build>=1.0.3->chromadb==0.6.3->-r ../requirements.txt (line 4)) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community==0.3.25->-r ../requirements.txt (line 6)) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community==0.3.25->-r ../requirements.txt (line 6)) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /opt/conda/lib/python3.12/site-packages (from fastapi>=0.95.2->chromadb==0.6.3->-r ../requirements.txt (line 4)) (0.46.2)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /opt/conda/lib/python3.12/site-packages (from google-genai<2.0.0,>=1.9.0->deepeval==3.1.4->-r ../requirements.txt (line 13)) (4.9.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /opt/conda/lib/python3.12/site-packages (from google-genai<2.0.0,>=1.9.0->deepeval==3.1.4->-r ../requirements.txt (line 13)) (2.39.0)\n",
      "Collecting httpx>=0.27.0 (from chromadb==0.6.3->-r ../requirements.txt (line 4))\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting tenacity>=8.2.3 (from chromadb==0.6.3->-r ../requirements.txt (line 4))\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /opt/conda/lib/python3.12/site-packages (from google-genai<2.0.0,>=1.9.0->deepeval==3.1.4->-r ../requirements.txt (line 13)) (15.0.1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb==0.6.3->-r ../requirements.txt (line 4)) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb==0.6.3->-r ../requirements.txt (line 4)) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb==0.6.3->-r ../requirements.txt (line 4)) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb==0.6.3->-r ../requirements.txt (line 4)) (0.16.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.30.2->langchain_huggingface==0.3.0->-r ../requirements.txt (line 8)) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.30.2->langchain_huggingface==0.3.0->-r ../requirements.txt (line 8)) (2024.6.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain_core==0.3.65->-r ../requirements.txt (line 7)) (3.0.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb==0.6.3->-r ../requirements.txt (line 4)) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb==0.6.3->-r ../requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb==0.6.3->-r ../requirements.txt (line 4)) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb==0.6.3->-r ../requirements.txt (line 4)) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /opt/conda/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb==0.6.3->-r ../requirements.txt (line 4)) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /opt/conda/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb==0.6.3->-r ../requirements.txt (line 4)) (2.2.3)\n",
      "Requirement already satisfied: durationpy>=0.7 in /opt/conda/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb==0.6.3->-r ../requirements.txt (line 4)) (0.9)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.25->-r ../requirements.txt (line 5)) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.25->-r ../requirements.txt (line 5)) (0.23.0)\n",
      "Requirement already satisfied: coloredlogs in /opt/conda/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb==0.6.3->-r ../requirements.txt (line 4)) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb==0.6.3->-r ../requirements.txt (line 4)) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb==0.6.3->-r ../requirements.txt (line 4)) (5.29.4)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb==0.6.3->-r ../requirements.txt (line 4)) (1.14.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.12/site-packages (from openai->deepeval==3.1.4->-r ../requirements.txt (line 13)) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from openai->deepeval==3.1.4->-r ../requirements.txt (line 13)) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.12/site-packages (from openai->deepeval==3.1.4->-r ../requirements.txt (line 13)) (1.3.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb==0.6.3->-r ../requirements.txt (line 4)) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb==0.6.3->-r ../requirements.txt (line 4)) (8.6.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.6.3->-r ../requirements.txt (line 4)) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.32.1 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.6.3->-r ../requirements.txt (line 4)) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.32.1 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.6.3->-r ../requirements.txt (line 4)) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.53b1 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.6.3->-r ../requirements.txt (line 4)) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.53b1 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.6.3->-r ../requirements.txt (line 4)) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.6.3->-r ../requirements.txt (line 4)) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.53b1 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.6.3->-r ../requirements.txt (line 4)) (0.53b1)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-instrumentation==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.6.3->-r ../requirements.txt (line 4)) (1.17.2)\n",
      "Requirement already satisfied: asgiref~=3.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-instrumentation-asgi==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.6.3->-r ../requirements.txt (line 4)) (3.8.1)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb==0.6.3->-r ../requirements.txt (line 4))\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /opt/conda/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb==0.6.3->-r ../requirements.txt (line 4)) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic>=1.9->chromadb==0.6.3->-r ../requirements.txt (line 4)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.12/site-packages (from pydantic>=1.9->chromadb==0.6.3->-r ../requirements.txt (line 4)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from pydantic>=1.9->chromadb==0.6.3->-r ../requirements.txt (line 4)) (0.4.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/conda/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community==0.3.25->-r ../requirements.txt (line 6)) (1.1.0)\n",
      "Collecting iniconfig>=1 (from pytest->deepeval==3.1.4->-r ../requirements.txt (line 13))\n",
      "  Downloading iniconfig-2.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /opt/conda/lib/python3.12/site-packages (from pytest->deepeval==3.1.4->-r ../requirements.txt (line 13)) (1.5.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in /opt/conda/lib/python3.12/site-packages (from pytest->deepeval==3.1.4->-r ../requirements.txt (line 13)) (2.19.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2->langchain==0.3.25->-r ../requirements.txt (line 5)) (3.4.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich>=10.11.0->chromadb==0.6.3->-r ../requirements.txt (line 4)) (3.0.0)\n",
      "Requirement already satisfied: greenlet>=1 in /opt/conda/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.25->-r ../requirements.txt (line 5)) (3.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.12/site-packages (from typer>=0.9.0->chromadb==0.6.3->-r ../requirements.txt (line 4)) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /opt/conda/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.6.3->-r ../requirements.txt (line 4)) (0.6.4)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.6.3->-r ../requirements.txt (line 4)) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.6.3->-r ../requirements.txt (line 4)) (1.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->opik==1.7.36->-r ../requirements.txt (line 14)) (3.0.2)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /opt/conda/lib/python3.12/site-packages (from litellm->opik==1.7.36->-r ../requirements.txt (line 14)) (4.23.0)\n",
      "Collecting openai (from deepeval==3.1.4->-r ../requirements.txt (line 13))\n",
      "  Downloading openai-1.91.0-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /opt/conda/lib/python3.12/site-packages (from litellm->opik==1.7.36->-r ../requirements.txt (line 14)) (0.8.0)\n",
      "Collecting execnet>=2.1 (from pytest-xdist->deepeval==3.1.4->-r ../requirements.txt (line 13))\n",
      "  Downloading execnet-2.1.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval==3.1.4->-r ../requirements.txt (line 13)) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval==3.1.4->-r ../requirements.txt (line 13)) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval==3.1.4->-r ../requirements.txt (line 13)) (4.9.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/conda/lib/python3.12/site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb==0.6.3->-r ../requirements.txt (line 4)) (3.21.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm->opik==1.7.36->-r ../requirements.txt (line 14)) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm->opik==1.7.36->-r ../requirements.txt (line 14)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm->opik==1.7.36->-r ../requirements.txt (line 14)) (0.24.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb==0.6.3->-r ../requirements.txt (line 4)) (0.1.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.12/site-packages (from tiktoken>=0.7.0->litellm->opik==1.7.36->-r ../requirements.txt (line 14)) (2024.11.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community==0.3.25->-r ../requirements.txt (line 6)) (1.1.0)\n",
      "Collecting types-awscrt (from botocore-stubs->boto3-stubs>=1.34.110->boto3-stubs[bedrock-runtime]>=1.34.110->opik==1.7.36->-r ../requirements.txt (line 14))\n",
      "  Downloading types_awscrt-0.27.2-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.12/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.6.3->-r ../requirements.txt (line 4)) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.6.3->-r ../requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /opt/conda/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval==3.1.4->-r ../requirements.txt (line 13)) (0.6.1)\n",
      "Downloading chromadb-0.6.3-py3-none-any.whl (611 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain-0.3.25-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_community-0.3.25-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.3.65-py3-none-any.whl (438 kB)\n",
      "Downloading langchain_huggingface-0.3.0-py3-none-any.whl (27 kB)\n",
      "Downloading hf_xet-1.1.4-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m131.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jq-1.9.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.0/757.0 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m122.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading deepeval-3.1.4-py3-none-any.whl (554 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m554.1/554.1 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opik-1.7.36-py3-none-any.whl (578 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m578.4/578.4 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading boto3_stubs-1.38.42-py3-none-any.whl (69 kB)\n",
      "Downloading google_genai-1.21.1-py3-none-any.whl (206 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading langsmith-0.3.45-py3-none-any.whl (363 kB)\n",
      "Downloading posthog-3.25.0-py2.py3-none-any.whl (89 kB)\n",
      "Downloading pytest_rerunfailures-12.0-py3-none-any.whl (12 kB)\n",
      "Downloading pytest-8.4.1-py3-none-any.whl (365 kB)\n",
      "Downloading rapidfuzz-3.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading sentry_sdk-2.30.0-py2.py3-none-any.whl (343 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading anthropic-0.55.0-py3-none-any.whl (289 kB)\n",
      "Downloading litellm-1.73.0-py3-none-any.whl (8.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.91.0-py3-none-any.whl (735 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m735.8/735.8 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ollama-0.5.1-py3-none-any.whl (13 kB)\n",
      "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
      "Downloading pyfiglet-1.0.3-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytest_asyncio-1.0.0-py3-none-any.whl (15 kB)\n",
      "Downloading pytest_repeat-0.9.4-py3-none-any.whl (4.2 kB)\n",
      "Downloading pytest_xdist-3.7.0-py3-none-any.whl (46 kB)\n",
      "Downloading uuid6-2025.0.0-py3-none-any.whl (7.0 kB)\n",
      "Downloading execnet-2.1.1-py3-none-any.whl (40 kB)\n",
      "Downloading iniconfig-2.1.0-py3-none-any.whl (6.0 kB)\n",
      "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Downloading mypy_boto3_bedrock_runtime-1.38.4-py3-none-any.whl (31 kB)\n",
      "Downloading botocore_stubs-1.38.30-py3-none-any.whl (65 kB)\n",
      "Downloading types_s3transfer-0.13.0-py3-none-any.whl (19 kB)\n",
      "Downloading types_awscrt-0.27.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: monotonic, uuid6, types-s3transfer, types-awscrt, tenacity, tabulate, sentry-sdk, rapidfuzz, pymupdf, pyfiglet, portalocker, opencv-python-headless, mypy-boto3-bedrock-runtime, jq, iniconfig, hf-xet, execnet, rich, pytest, posthog, httpx, botocore-stubs, pytest-xdist, pytest-rerunfailures, pytest-repeat, pytest-asyncio, openai, ollama, langsmith, google-genai, boto3-stubs, anthropic, litellm, langchain_core, opik, langchain_huggingface, langchain, deepeval, langchain_community, chromadb\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 9.1.2\n",
      "    Uninstalling tenacity-9.1.2:\n",
      "      Successfully uninstalled tenacity-9.1.2\n",
      "  Attempting uninstall: rich\n",
      "    Found existing installation: rich 14.0.0\n",
      "    Uninstalling rich-14.0.0:\n",
      "      Successfully uninstalled rich-14.0.0\n",
      "  Attempting uninstall: posthog\n",
      "    Found existing installation: posthog 4.0.1\n",
      "    Uninstalling posthog-4.0.1:\n",
      "      Successfully uninstalled posthog-4.0.1\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.27.2\n",
      "    Uninstalling httpx-0.27.2:\n",
      "      Successfully uninstalled httpx-0.27.2\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.51.0\n",
      "    Uninstalling openai-1.51.0:\n",
      "      Successfully uninstalled openai-1.51.0\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.3.39\n",
      "    Uninstalling langsmith-0.3.39:\n",
      "      Successfully uninstalled langsmith-0.3.39\n",
      "  Attempting uninstall: langchain_core\n",
      "    Found existing installation: langchain-core 0.3.55\n",
      "    Uninstalling langchain-core-0.3.55:\n",
      "      Successfully uninstalled langchain-core-0.3.55\n",
      "  Attempting uninstall: langchain_huggingface\n",
      "    Found existing installation: langchain-huggingface 0.1.0\n",
      "    Uninstalling langchain-huggingface-0.1.0:\n",
      "      Successfully uninstalled langchain-huggingface-0.1.0\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.3.24\n",
      "    Uninstalling langchain-0.3.24:\n",
      "      Successfully uninstalled langchain-0.3.24\n",
      "  Attempting uninstall: langchain_community\n",
      "    Found existing installation: langchain-community 0.3.21\n",
      "    Uninstalling langchain-community-0.3.21:\n",
      "      Successfully uninstalled langchain-community-0.3.21\n",
      "  Attempting uninstall: chromadb\n",
      "    Found existing installation: chromadb 0.5.20\n",
      "    Uninstalling chromadb-0.5.20:\n",
      "      Successfully uninstalled chromadb-0.5.20\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-google-vertexai 2.0.4 requires httpx<0.28.0,>=0.27.0, but you have httpx 0.28.1 which is incompatible.\n",
      "galileo-observe 1.13.2 requires httpx<0.28.0,>=0.27.0, but you have httpx 0.28.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed anthropic-0.55.0 boto3-stubs-1.38.42 botocore-stubs-1.38.30 chromadb-0.6.3 deepeval-3.1.4 execnet-2.1.1 google-genai-1.21.1 hf-xet-1.1.4 httpx-0.28.1 iniconfig-2.1.0 jq-1.9.1 langchain-0.3.25 langchain_community-0.3.25 langchain_core-0.3.65 langchain_huggingface-0.3.0 langsmith-0.3.45 litellm-1.73.0 monotonic-1.6 mypy-boto3-bedrock-runtime-1.38.4 ollama-0.5.1 openai-1.91.0 opencv-python-headless-4.11.0.86 opik-1.7.36 portalocker-3.2.0 posthog-3.25.0 pyfiglet-1.0.3 pymupdf-1.25.5 pytest-8.4.1 pytest-asyncio-1.0.0 pytest-repeat-0.9.4 pytest-rerunfailures-12.0 pytest-xdist-3.7.0 rapidfuzz-3.13.0 rich-13.9.4 sentry-sdk-2.30.0 tabulate-0.9.0 tenacity-8.5.0 types-awscrt-0.27.2 types-s3transfer-0.13.0 uuid6-2025.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77853772-0239-40d0-94be-7a62fcb465c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/mlflow/pyfunc/utils/data_validation.py:168: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
      "  color_warning(\n",
      "/opt/conda/lib/python3.12/site-packages/mlflow/pyfunc/utils/data_validation.py:168: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
      "  color_warning(\n",
      "2025-06-23 23:06:32,357 - WARNING - USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# === Standard Library Imports ===\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# === MLflow integration ===\n",
    "import mlflow\n",
    "\n",
    "# Define the relative path to the 'core' directory (one level up from current working directory)\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "# === Import ChatbotService from project core ===\n",
    "from core.chatbot_service.chatbot_service import ChatbotService\n",
    "\n",
    "# === Third-Party Imports ===\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.document import Document\n",
    "from langchain.document_loaders import WebBaseLoader, JSONLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import promptquality as pq\n",
    "import torch\n",
    "import opik\n",
    "\n",
    "# Define the relative path to the 'src' directory (one level up from current working directory)\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "# === Project-Specific Imports (from src.utils) ===\n",
    "from src.utils import (\n",
    "    load_config_and_secrets,\n",
    "    configure_proxy,\n",
    "    initialize_llm,\n",
    "    configure_hf_cache,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c43204-6cb2-48f3-ac72-f821f12585b8",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd3b8d2c-d0e1-4409-86dc-470c8454fff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "022490c4-8d8a-4c53-92ac-c62ea70a8209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logger\n",
    "logger = logging.getLogger(\"vanilla_rag_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", \n",
    "                              datefmt=\"%Y-%m-%d %H:%M:%S\") \n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75f2111d-5508-41ba-967c-256b40a9fb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"../configs/config.yaml\"\n",
    "SECRETS_PATH = \"../configs/secrets.yaml\"\n",
    "DATA_PATH = \"../data\"\n",
    "MLFLOW_EXPERIMENT_NAME = \"AIStudio-Chatbot-Experiment\"\n",
    "MLFLOW_RUN_NAME = \"AIStudio-Chatbot-Run\"\n",
    "LOCAL_MODEL_PATH = \"/home/jovyan/datafabric/llama3.1-8b-instruct/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\"\n",
    "DEMO_FOLDER = \"../demo\"\n",
    "MLFLOW_MODEL_NAME = \"AIStudio-Chatbot-Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7384967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f912d15a-7aab-4620-8c89-33c8f07104f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 23:06:32 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "logger.info('Notebook execution started.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ca6361-dd3c-4778-9a9d-b11da7b20151",
   "metadata": {},
   "source": [
    "## Configuration of HuggingFace caches\n",
    "\n",
    "In the next cell, we configure HuggingFace cache, so that all the models downloaded from them are persisted locally, even after the workspace is closed. This is a future desired feature for AI Studio and the GenAI addon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "965e4ea4-084a-4f32-9f51-9ead89dd16fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure HuggingFace cache\n",
    "configure_hf_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42ff655c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 23:06:33,047 - INFO - PyTorch version 2.7.0 available.\n",
      "2025-06-23 23:06:33,250 - INFO - Use pytorch device_name: cuda\n",
      "2025-06-23 23:06:33,251 - INFO - Load pretrained SentenceTransformer: intfloat/e5-large-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d74fc569234e2ab6b2f3770bc5439d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de11aff9c144cde84fa294a2c86fbc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/67.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6ce924446c4a27b4cc9f2412358a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "169512affdd146be9d14900d03c2f5e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea6c5a57f294ce1b5af6e3af50956ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e615273a3824484f9c787eff0904992f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "562b57fa369e44dcab0155f0f95e9e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94bf24ab2e249c5a270163e645d4dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc985897b9d4a8bbade882a2da650d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d14fc59781411f9c3dd05a163b6fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/201 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize HuggingFace Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/e5-large-v2\",\n",
    "    cache_folder=\"/tmp/hf_cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7062ee0",
   "metadata": {},
   "source": [
    "## Configuration and Secrets Loading\n",
    "\n",
    "In this section, we load configuration parameters and API keys from separate YAML files. This separation helps maintain security by keeping sensitive information (API keys) separate from configuration settings.\n",
    "\n",
    "- **config.yaml**: Contains non-sensitive configuration parameters like model sources and URLs\n",
    "- **secrets.yaml**: Contains sensitive API keys for services like Galileo and HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "138cf475-c60d-4634-ae41-f029f151d57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opik.configure(use_local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e0b7464-ad76-47f2-94f0-e70c4d4cc380",
   "metadata": {},
   "outputs": [],
   "source": [
    "config, secrets = load_config_and_secrets(CONFIG_PATH, SECRETS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba50026-86dc-4d90-a827-55fe7882cfe8",
   "metadata": {},
   "source": [
    "## Proxy Configuration\n",
    "\n",
    "In order to connect to Galileo service, a SSH connection needs to be established. For certain enterprise networks, this might require an explicit setup of the proxy configuration. If this is your case, set up the \"proxy\" field on your config.yaml and the following cell will configure the necessary environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a76a9c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "configure_proxy(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e80960-54f9-4ec2-b062-6d49526494d1",
   "metadata": {},
   "source": [
    "# Verify Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4417e48c-2b51-4e61-b74d-8230caf2f7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 23:06:48 - INFO - Config is properly configured. \n",
      "2025-06-23 23:06:48 - INFO - Secrets is properly configured. \n",
      "2025-06-23 23:06:48 - INFO - Local Llama model is properly configured. \n"
     ]
    }
   ],
   "source": [
    "def log_asset_status(asset_path: str, asset_name: str, success_message: str, failure_message: str) -> None:\n",
    "    \"\"\"\n",
    "    Logs the status of a given asset based on its existence.\n",
    "\n",
    "    Parameters:\n",
    "        asset_path (str): File or directory path to check.\n",
    "        asset_name (str): Name of the asset for logging context.\n",
    "        success_message (str): Message to log if asset exists.\n",
    "        failure_message (str): Message to log if asset does not exist.\n",
    "    \"\"\"\n",
    "    if Path(asset_path).exists():\n",
    "        logger.info(f\"{asset_name} is properly configured. {success_message}\")\n",
    "    else:\n",
    "        logger.info(f\"{asset_name} is not properly configured. {failure_message}\")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=CONFIG_PATH,\n",
    "    asset_name=\"Config\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if the configs.yaml was propely connfigured in your project on AI Studio.\"\n",
    ")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=SECRETS_PATH,\n",
    "    asset_name=\"Secrets\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if the secrets.yaml was propely connfigured in your project on AI Studio.\"\n",
    ")\n",
    "log_asset_status(\n",
    "    asset_path=LOCAL_MODEL_PATH,\n",
    "    asset_name=\"Local Llama model\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please create and download the required assets in your project on AI Studio if you want to use local model.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d92659e-23ea-4965-bb27-58cd1dd0f3ec",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "\n",
    "In this step, we will use the Langchain framework to  extract the content from a local PDF file with the product documentation. Also, we have commented some example on how to use Web Loaders to load data from pages on the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb4c1589-285f-4e72-ab8e-5ae9fbd4043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Verify existence of the data directory ===\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    raise FileNotFoundError(f\"'data' folder not found at path: {os.path.abspath(DATA_PATH)}\")\n",
    "\n",
    "# === Wiki JSON with JSONLoader ===\n",
    "wiki_loader = JSONLoader(\n",
    "    file_path=os.path.join(DATA_PATH, \"wiki_flat_structure.json\"),\n",
    "    jq_schema=\"to_entries[] | {source: .key, text: .value.content}\",  # adapt to your schema\n",
    "    text_content=False  # keeps original formatting; set True if you want only strings\n",
    ")\n",
    "\n",
    "docs = wiki_loader.load()\n",
    "# === Optional: Load additional web-based documents ===\n",
    "# To use a different knowledge base, just change the URLs below\n",
    "\n",
    "# loader1 = WebBaseLoader(\"https://www.hp.com/us-en/workstations/ai-studio.html\")\n",
    "# data1 = loader1.load()\n",
    "\n",
    "# loader2 = WebBaseLoader(\"https://zdocs.datascience.hp.com/docs/aistudio\")\n",
    "# data2 = loader2.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc63ed05-192e-4f59-856c-cf9bff160fc8",
   "metadata": {},
   "source": [
    "# Creation of Chunks\n",
    "Here, we split the loaded documents into chunks, so we have smaller and more specific texts to add to our vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3dbd381-58b8-4569-aa75-b26d07556b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Initialize text splitter ===\n",
    "# - chunk_size: Maximum number of characters per text chunk.\n",
    "# - chunk_overlap: Number of overlapping characters between chunks.\n",
    "text_splitter = MarkdownTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "# === Split the loaded PDF data into smaller text chunks ===\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7999ff1f-1f6a-452c-848e-e3c67adf7766",
   "metadata": {},
   "source": [
    "# Retrieval\n",
    "\n",
    "We transform the texts into embeddings and store them in a vector database. This allows us to perform similarity search, and proper retrieval of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e43ae88-252a-4cbe-96e4-081daa3dc25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 23:06:48,502 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.1 s, sys: 1.6 s, total: 53.7 s\n",
      "Wall time: 49.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# === Create a vector database from document chunks ===\n",
    "vectordb = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "# === Configure the vector database as a retriever for querying ===\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58329084-640e-4b25-be70-ac1ec90467e9",
   "metadata": {},
   "source": [
    "# Model Setup\n",
    "\n",
    "In this notebook, we provide three different options for loading the model:\n",
    " * **local**: by loading the llama3.1-8b-instruct-Q8_0 model from the asset downloaded on the project\n",
    " * **hugging-face-local** by downloading a DeepSeek model from Hugging Face and running locally\n",
    " * **hugging-face-cloud** by accessing the Mistral model through Hugging Face cloud API (requires HuggingFace API key saved on secrets.yaml)\n",
    "\n",
    "This choice can be set in the config.yaml file. The model deployed on the bottom cells of this notebook will load the choice from the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65b1fa71-43ae-49e1-9f2f-ad730a00ca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_source = config[\"model_source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "774e18a8-bef4-4b67-9972-ceda2af6538d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 259 ms, sys: 5.39 s, total: 5.65 s\n",
      "Wall time: 39.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "llm = initialize_llm(model_source, secrets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81f5d66-d99d-4851-a48e-248b2e81e2c9",
   "metadata": {},
   "source": [
    "# Chain Creation\n",
    "In this part, we define a pipeline that receives a question and context, formats the context documents, and uses a Hugging Face (Mistral) chat model to answer the question based on the provided context. The output is then formatted as a string for easy reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a511ec2c-1fb2-41f9-934d-bcd6f06942f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Function to format retrieved documents ===\n",
    "# Converts a list of Document objects into a single formatted string\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "116d88a8-6d84-4d46-964e-c891965776dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Define chatbot prompt template ===\n",
    "# Ensures that responses are strictly related to \"Z by HP AI Studio\"\n",
    "template = \"\"\"\n",
    "You are a technical assistant for HP’s Z by HP AI Studio team.\n",
    "\n",
    "Only answer using the information provided in the <context> block.  \n",
    "If the answer is not found in the context, reply with:  \n",
    "**\"I don’t have that information in the wiki yet.\"**\n",
    "\n",
    "Rules:\n",
    "- Use only the information from <context>.\n",
    "- For each fact you include, cite the source file name in parentheses.\n",
    "- Do not invent information or use outside knowledge.\n",
    "- Do not repeat or refer to these instructions.\n",
    "- Use bullet points or steps if it makes the answer clearer.\n",
    "- Avoid repeating the same sentence or phrase. Be concise and avoid redundancy.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "**Question:** {query}  \n",
    "**Answer (with citations):**\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# === Create an LLM-powered retrieval chain ===\n",
    "# - The retriever fetches relevant documents.\n",
    "# - The documents are formatted using format_docs().\n",
    "# - The query is passed directly using RunnablePassthrough().\n",
    "# - The formatted context and query are injected into the prompt.\n",
    "# - The LLM processes the prompt and the response is parsed into a string.\n",
    "chain = {\n",
    "    \"context\": retriever | format_docs,\n",
    "    \"query\": RunnablePassthrough()\n",
    "} | prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "224c5429-2a0d-42b0-b131-1c659c22e313",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To test your AI blueprints, please follow these steps:\n",
      "\n",
      "1.  Create a project in AI Studio:\n",
      "    *   If the blueprint is published, create a new project in AI Studio using the blueprint directly.\n",
      "        ([blueprint-testing-guide.md](https://github.com/HPInc/AI-Blueprints/blob/master/docs/blueprint-testing-guide.md#11-create-a-project-in-ai-studio))).\n",
      "    *   If not published, manually create the project by navigating to the related blueprint folder in the repo and following the setup instructions in the README to configure the environment in AI Studio. ([blueprint-testing-guide.md](https://github.com/HPInc/AI-Blueprints/blob/master/docs/blueprint-testing-guide.md#12-complete-the-setup)).\n",
      "2.  Complete the setup:\n",
      "    *   Ensure **all steps under the `Setup` section** in the README are completed. ([blueprint-testing-guide.md](https://github.com/HPInc/AI-Blueprints/blob/master/docs/blueprint-testing-guide.md#23-complete-the-setup)).\n",
      "3.  Run your tests using VS Code and AI Studio:\n",
      "    *   Open the testing menu (vial icon) from the left panel in AI Studio.\n",
      "        ([ai-studio-vscodeworkspace.md](https://github.com/HPInc/AI-Blueprints/blob/master/docs/ai-studio-vscode-workspace.md)).\n",
      "    *   Let the tests populate; it may take a while. ([blueprint-testing-guide.md](https://github.com/HPInc/AI-Blueprints/blob/master/docs/blueprint-testing-guide.md#3-run-your-tests-using-vs-code-and-ai-studio)).\n",
      "\n",
      "Please refer to the [AI Blueprints Testing Guide](https://github.com/HPInc/AI-Blueprints/blob/master/docs/blueprint-testing-guide.md) for detailed steps and instructions. ([blueprint-testing-guide.md](https://github.com/HPInc/AI-Blueprints/blob/master/docs/blueprint-testing-guide.md)). \n",
      "\n",
      "**Step 9: Add Test Cases and Results**\n",
      "*   If not already present, create a `test-cases` folder under the `docs` directory.\n",
      "    *   Add an Excel file documenting:\n",
      "        -   **Test configurations used**\n",
      "            -   Different model backends like `local`, `huggingface-cloud`, `huggingface-local`\n",
      "        -   **Results of each test** (`Pass`/`Fail`)\n",
      "    *   Save and upload your changes to the GitHub repository. ([blueprint-testing-guide.md](https://github.com/HPInc/AI-Blueprints/blob/master/docs/blueprint-testing-guide.md#9-add-test-cases-and-results)).\n",
      "*   Open a PR with your testing documentation and results for review and approval from the team. ([blueprint-testing-guide.md](https://github.com/HPInc/AI-Blueprints/blob/master/docs/blueprint-testing-guide.md#10-submit-a-pull-request)).  I don’t have that information in the wiki yet.  \n",
      "**Step 10: Submit a Pull Request**\n",
      "*   Open a PR with your testing documentation and results for review and approval from the team.\n",
      "    *   Make sure to include any relevant details or feedback from the reviewers in the pull request.\n",
      "    *   Once approved, merge the pull request to integrate the updated testing documentation and results into the main repository. ([blueprint-testing-guide.md](https://github.com/HPInc/AI-Blueprints/blob/master/docs/blueprint-testing-guide.md#10-submit-a-pull-request)). I don’t have that information in the wiki yet.  \n",
      "**Step 2: Complete the Setup**\n",
      "*   Ensure **all steps under the `Setup` section** in the README are completed.\n",
      "    *   Follow the setup instructions to manually configure the environment in AI Studio, including setting up the necessary tools and dependencies for your blueprint project.\n",
      "\n",
      "To summarize, to test your AI blueprints, you need to create a project in AI Studio, complete the setup according to the instructions in the README, run your tests using VS Code and AI Studio, document your test cases and results, and submit a pull request with your updated testing documentation and results for review and approval. I don’t have that information in the wiki yet.    I don’t have that information in the wiki yet.    I don’t have that information in the wiki yet.   I don’t have that information in the wiki yet.   I don’t have that information in the wiki yet.  I don’t have that information in the wiki yet.  I don’t have that information in the wiki yet.     I don’t have that information in the wiki yet.    I don’t have that information in the wiki yet.    I don’t have that information in the wiki yet.   I don’t have that information in the wiki yet.   I don’t have that information in the wiki yet.  I don’t have that information in the wiki yet.  I don’t have that information in the wiki yet.     I don’t have\n"
     ]
    }
   ],
   "source": [
    "# One-off query (synchronous)\n",
    "question = \"How do i test my ai blueprints?\"\n",
    "answer = chain.invoke(question)          # or chain(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2059383-6977-4532-886d-bdb5953dfec0",
   "metadata": {},
   "source": [
    "# DeepEval Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9c36c2-67d7-43ea-8783-6fd97af2520d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cc6884d",
   "metadata": {},
   "source": [
    "# Model Service \n",
    "\n",
    "In this section, we demonstrate how to deploy a RAG-based chatbot service with integrated Galileo Protect and Observe capabilities. This service provides a REST API endpoint that allows users to query the knowledge base with natural language questions, upload new documents to the knowledge base, and manage conversation history, all with built-in safeguards against sensitive information and toxicity. This service encapsulates all the functionality we developed in this notebook, including the document retrieval system, RAG-based question answering capabilities, and Galileo integration for protection, observation and evaluation. It demonstrates how to use our ChatbotService from the src/service directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "867f62bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 23:08:54,146 - INFO - Use pytorch device_name: cuda\n",
      "2025-06-23 23:08:54,146 - INFO - Load pretrained SentenceTransformer: intfloat/e5-large-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03572b450b814b46988b5566dc937b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c21b8c767a494f9611987e05083789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "537837b36623431293802592f748f27e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38fa61f01cce49b4902eaab99ee1a568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652108f42f1f46b09018bbd3eb4672d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/23 23:09:49 WARNING mlflow.utils.requirements_utils: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - PyPDF (current: uninstalled, required: PyPDF)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n",
      "2025-06-23 23:10:32,350 - INFO - Model and artifacts successfully registered in MLflow.\n",
      "Registered model 'AIStudio-Chatbot-Model' already exists. Creating a new version of this model...\n",
      "Created version '3' of model 'AIStudio-Chatbot-Model'.\n",
      "2025-06-23 23:10:32 - INFO - ✅ Model registered successfully with run ID: c000592805cd49869051d4686363cd8a\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_tracking_uri('/phoenix/mlflow')\n",
    "# === Set MLflow experiment context ===\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\n",
    "\n",
    "# === Validate local model file path ===\n",
    "if not os.path.exists(LOCAL_MODEL_PATH):\n",
    "    logger.info(f\"⚠️ Warning: Model file not found at {LOCAL_MODEL_PATH}. Please verify the path.\")\n",
    "\n",
    "# === Log and register model to MLflow ===\n",
    "with mlflow.start_run(run_name=MLFLOW_RUN_NAME) as run:\n",
    "    \n",
    "    # Log model artifacts using custom ChatbotService\n",
    "    ChatbotService.log_model(\n",
    "        artifact_path=MLFLOW_MODEL_NAME,\n",
    "        config_path=CONFIG_PATH,\n",
    "        secrets_path=SECRETS_PATH,\n",
    "        docs_path=DATA_PATH,\n",
    "        model_path=LOCAL_MODEL_PATH,\n",
    "        demo_folder=DEMO_FOLDER\n",
    "    )\n",
    "\n",
    "    # Construct the URI for the logged model\n",
    "    model_uri = f\"runs:/{run.info.run_id}/{MLFLOW_MODEL_NAME}\"\n",
    "\n",
    "    # Register the model into MLflow Model Registry\n",
    "    mlflow.register_model(\n",
    "        model_uri=model_uri,\n",
    "        name=MLFLOW_MODEL_NAME\n",
    "    )\n",
    "\n",
    "    logger.info(f\"✅ Model registered successfully with run ID: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "519b97dc-ba54-4256-99fd-4639c505d185",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 23:10:32 - INFO - Notebook execution completed.\n"
     ]
    }
   ],
   "source": [
    "logger.info('Notebook execution completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8147f34-6e3b-47f2-9a68-7e8077f71618",
   "metadata": {},
   "source": [
    "Built with ❤️ using Z by HP AI Studio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
