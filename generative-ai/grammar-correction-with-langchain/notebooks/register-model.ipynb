{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7c007e5",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-size: 50px;\"> Register Model </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdcb43f",
   "metadata": {},
   "source": [
    "# Notebook Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756db15a",
   "metadata": {},
   "source": [
    "- Configure the Environment\n",
    "- Define Constants and Paths\n",
    "- Define Necessary Modules\n",
    "- Define MLflow Model Class\n",
    "- Log and Register Model\n",
    "- Prepare Eval Data\n",
    "- Evaluate Model\n",
    "- Log Execution Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5423f7b3",
   "metadata": {},
   "source": [
    "## Configure the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67a1d3d1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "# Configure logger\n",
    "logger: logging.Logger = logging.getLogger(\"register_model_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.propagate = False  # Prevent duplicate logs\n",
    "\n",
    "# Set formatter\n",
    "formatter: logging.Formatter = logging.Formatter(\n",
    "    fmt=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "# Configure and attach stream handler\n",
    "stream_handler: logging.StreamHandler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a607d95",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 00:18:52 - INFO - ✅ Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "logger.info(\"✅ Notebook execution started.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3c1f447",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 38.4 s, sys: 17.2 s, total: 55.6 s\n",
      "Wall time: 30min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "%pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be896c9a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Standard & Third-Party Libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from mlflow.models import ModelSignature, evaluate\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "from typing import List\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "# Add src directory to system path\n",
    "sys.path.append(str(Path(\"..\").resolve() / \"src\"))\n",
    "\n",
    "# Import standard MLflow metrics\n",
    "from mlflow.metrics import exact_match, rouge1, rougeL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "163acd90",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964f2b67",
   "metadata": {},
   "source": [
    "## Define Constants and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49769695",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration Paths\n",
    "CONFIG_PATH = Path(\"../configs/configs.yaml\")\n",
    "REQUIREMENTS_PATH = Path(\"../requirements.txt\")\n",
    "LOCAL_MODEL_PATH = Path(\"/home/jovyan/datafabric/meta-llama3.1-8b-Q8/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\")\n",
    "SECRETS_PATH = Path(\"../configs/secrets.yaml\")\n",
    "\n",
    "# MLflow Configuration\n",
    "MLFLOW_EXPERIMENT_NAME = \"Markdown_Correction_Service\"\n",
    "MLFLOW_MODEL_NAME = \"markdown-corrector\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69633861",
   "metadata": {},
   "source": [
    "## Define Necessary Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c842bde",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "###--------GITHUB EXTRACTOR START--------###\n",
    "############################################\n",
    "\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import sys\n",
    "import base64 \n",
    "import re \n",
    "from markdown_it import MarkdownIt\n",
    "from markdown_it.token import Token \n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Optional, Tuple, Dict, Union\n",
    "\n",
    "class GitHubMarkdownProcessor:\n",
    "    \"\"\"\n",
    "    Processor for extracting and parsing Markdown files from GitHub repositories.\n",
    "\n",
    "    This class fetches `.md` files from a public or private GitHub repository,\n",
    "    replaces structural and inline components with labeled placeholders, and \n",
    "    saves the resulting structure locally.\n",
    "\n",
    "    Attributes:\n",
    "        repo_url (str): GitHub repository URL.\n",
    "        repo_owner (str): Owner of the GitHub repository.\n",
    "        repo_name (str): Name of the GitHub repository.\n",
    "        access_token (Optional[str]): GitHub Personal Access Token (if needed).\n",
    "        save_dir (str): Directory to save the processed files.\n",
    "        api_base_url (str): Base GitHub API URL for the repository.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, repo_url: str, access_token: Optional[str] = None, save_dir: str = './parsed_repo'):\n",
    "        \"\"\"\n",
    "        Initializes the Markdown processor with a GitHub repo URL.\n",
    "\n",
    "        Args:\n",
    "            repo_url (str): Full URL to the GitHub repository.\n",
    "            access_token (Optional[str]): GitHub token for private repo access.\n",
    "            save_dir (str): Output directory to store processed Markdown files.\n",
    "        \"\"\"\n",
    "        self.repo_url = repo_url\n",
    "        self.access_token = access_token\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        owner, repo, error = self.parse_url()\n",
    "        if error:\n",
    "            raise ValueError(error)\n",
    "        \n",
    "        self.repo_owner = owner\n",
    "        self.repo_name = repo\n",
    "        self.api_base_url = f\"https://api.github.com/repos/{self.repo_owner}/{self.repo_name}\"\n",
    "\n",
    "    def parse_url(self) -> Tuple[Optional[str], Optional[str], Optional[str]] :\n",
    "        \"\"\"\n",
    "        Parses a GitHub URL and extracts the repository owner and name.\n",
    "    \n",
    "        Args:\n",
    "            github_url (str): The full GitHub URL to parse.\n",
    "    \n",
    "        Returns:\n",
    "            Tuple[Optional[str], Optional[str], Optional[str]]:\n",
    "                - The repository owner (e.g., 'openai')\n",
    "                - The repository name (e.g., 'whisper')\n",
    "                - An error message string if parsing fails; otherwise, None\n",
    "        \"\"\"\n",
    "        parsed_url = urlparse(self.repo_url)\n",
    "        path_parts = parsed_url.path.strip(\"/\").split(\"/\")\n",
    "    \n",
    "        # Validate URL format\n",
    "        if len(path_parts) < 2:\n",
    "            return None, None, \"Invalid GitHub URL format.\"\n",
    "        \n",
    "        # Return owner and repo\n",
    "        return path_parts[0], path_parts[1], None\n",
    "    \n",
    "    def check_repo(self) -> str:\n",
    "        \"\"\"\n",
    "        Determines the visibility (public or private) of a GitHub repository.\n",
    "    \n",
    "        Parses the provided GitHub URL, queries the GitHub API to fetch repository details,\n",
    "        and returns a string indicating its visibility or an error message if access fails.\n",
    "    \n",
    "        Args:\n",
    "            github_url (str): The URL of the GitHub repository to check.\n",
    "            access_token (Optional[str], optional): GitHub personal access token for authenticated requests. Defaults to None.\n",
    "    \n",
    "        Returns:\n",
    "            str: \"private\" or \"public\" if the repository is accessible;\n",
    "                 otherwise, a descriptive error message.\n",
    "        \"\"\"\n",
    "        # Parse url into components\n",
    "        owner, name, error = self.parse_url()\n",
    "        if error:\n",
    "            return error\n",
    "        \n",
    "        # Build GitHub URL\n",
    "        url = f\"https://api.github.com/repos/{owner}/{name}\"\n",
    "    \n",
    "        # Build authentication header\n",
    "        headers = {}\n",
    "        if self.access_token:\n",
    "            headers[\"Authorization\"] = f\"Bearer {self.access_token}\"\n",
    "    \n",
    "        response = requests.get(url, headers=headers)\n",
    "    \n",
    "        # Determine privacy of repo\n",
    "        if response.status_code == 200:\n",
    "            repo_data = response.json()\n",
    "            return \"private\" if repo_data.get(\"private\") else \"public\"\n",
    "        elif response.status_code == 404:\n",
    "            return \"Repository is inaccessible. Please authenticate.\"\n",
    "        else:\n",
    "            return f\"Error: {response.status_code}, {response.text}\"\n",
    "        \n",
    "    def extract_md_files(self) -> Tuple[Optional[Dict], Optional[str]]:\n",
    "        \"\"\"\n",
    "        Traverses a GitHub repository to extract all Markdown (.md) files and organize them in a nested directory structure.\n",
    "    \n",
    "        Connects to the GitHub API, retrieves the file tree of the default branch, downloads any Markdown files,\n",
    "        and reconstructs their paths locally in a dictionary format. Supports optional authentication with a personal access token.\n",
    "    \n",
    "        Args:\n",
    "            github_url (str): The GitHub repository URL (e.g., \"https://github.com/user/repo\").\n",
    "            access_token (Optional[str], optional): GitHub personal access token for authenticated requests. Defaults to None.\n",
    "    \n",
    "        Returns:\n",
    "            Tuple[Optional[Dict], Optional[str]]:\n",
    "                - A nested dictionary representing the directory structure and Markdown file contents.\n",
    "                - An error message string if any step fails; otherwise, None.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Parse url into components\n",
    "        owner, name, error = self.parse_url()\n",
    "        if error:\n",
    "            return None, error\n",
    "        \n",
    "        # Build GitHub URL\n",
    "        url = f\"https://api.github.com/repos/{owner}/{name}\"\n",
    "    \n",
    "        # Build authentication header\n",
    "        headers = {}\n",
    "        if self.access_token:\n",
    "            headers[\"Authorization\"] = f\"Bearer {self.access_token}\"\n",
    "    \n",
    "        response = requests.get(url, headers=headers)\n",
    "        if not response.ok:\n",
    "            return None, f\"Error: {response.status_code}, {response.text}\"\n",
    "    \n",
    "        # Get default branch\n",
    "        default_branch = response.json().get(\"default_branch\", \"main\")\n",
    "    \n",
    "        # Build GitHub URL for file tree\n",
    "        tree_url = f\"https://api.github.com/repos/{owner}/{name}/git/trees/{default_branch}?recursive=1\"\n",
    "        tree_response = requests.get(tree_url, headers=headers)\n",
    "        if not tree_response.ok:\n",
    "            return None, f\"Error: {tree_response.status_code}, {tree_response.text}\"\n",
    "        \n",
    "        # Dictionary to hold directory structure\n",
    "        dir_structure = {}\n",
    "    \n",
    "        # Iterate through repo tree structure\n",
    "        for item in tree_response.json().get(\"tree\", []):\n",
    "            path=item[\"path\"]\n",
    "        \n",
    "            # Skip all non md files\n",
    "            if item[\"type\"] != \"blob\" or not path.endswith(\".md\"):\n",
    "                continue\n",
    "    \n",
    "            # Fetch md file content\n",
    "            content_url = f\"https://api.github.com/repos/{owner}/{name}/contents/{path}\"\n",
    "            content_response = requests.get(content_url, headers=headers)\n",
    "            if not content_response.ok:\n",
    "                continue\n",
    "    \n",
    "            # Decode content response\n",
    "            file_data = content_response.json()\n",
    "            try:\n",
    "                content = base64.b64decode(file_data[\"content\"]).decode(\"utf-8\")\n",
    "            except Exception as e:\n",
    "                content = f\"Error decoding content: {e}\"\n",
    "    \n",
    "            # Build directory structure\n",
    "            parts = path.split(\"/\")\n",
    "            current = dir_structure \n",
    "            for part in parts[:-1]:\n",
    "                current = current.setdefault(part, {})\n",
    "            current[parts[-1]] = content \n",
    "    \n",
    "        return dir_structure, None\n",
    "    \n",
    "    def run(self) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        High-level method to:\n",
    "        1. Check repository access\n",
    "        2. Extract markdown files\n",
    "        3. Return raw markdown content by file path\n",
    "    \n",
    "        Returns:\n",
    "            Dict[str, str]: Mapping from file paths to raw markdown content.\n",
    "        \"\"\"\n",
    "        visibility = self.check_repo()\n",
    "        logger.info(f\"Repository visibility: {visibility}\")\n",
    "    \n",
    "        if visibility == \"Repository is inaccessible. Please authenticate.\":\n",
    "            raise PermissionError(\"Cannot access repository. Check your access token.\")\n",
    "    \n",
    "        structure, error = self.extract_md_files()\n",
    "        if error:\n",
    "            raise RuntimeError(f\"Markdown extraction failed: {error}\")\n",
    "    \n",
    "        raw_data = {}\n",
    "    \n",
    "        def process_structure(structure: Dict[str, Union[str, dict]], path: str = \"\") -> None:\n",
    "            \"\"\"\n",
    "            Recursively flattens a nested directory structure of markdown files.\n",
    "        \n",
    "            Args:\n",
    "                structure (Dict[str, Union[str, dict]]): Nested dictionary representing directories and markdown file contents.\n",
    "                path (str, optional): Current path used to build the full file path during traversal. Defaults to \"\".\n",
    "        \n",
    "            Returns:\n",
    "                None: Updates the outer `raw_data` dictionary in-place with path-to-content mappings.\n",
    "            \"\"\"\n",
    "            for name, content in structure.items():\n",
    "                current_path = os.path.join(path, name)\n",
    "                if isinstance(content, dict):\n",
    "                    process_structure(content, current_path)\n",
    "                else:\n",
    "                    raw_data[current_path] = content\n",
    "    \n",
    "        process_structure(structure)\n",
    "        logger.info(\"Raw markdown extraction complete.\")\n",
    "        return raw_data\n",
    "\n",
    "############################################\n",
    "###---------GITHUB EXTRACTOR END---------###\n",
    "############################################\n",
    "\n",
    "############################################\n",
    "###-------------PARSER START-------------###\n",
    "############################################\n",
    "\n",
    "import re\n",
    "from typing import Dict, Tuple\n",
    "from markdown_it import MarkdownIt\n",
    "\n",
    "\n",
    "def parse_md_for_grammar_correction(md_content: str) -> Tuple[Dict[str, str], str]:\n",
    "    \"\"\"\n",
    "    Parses Markdown content by replacing non-prose elements with placeholders.\n",
    "\n",
    "    Args:\n",
    "        md_content (str): Raw markdown content to process.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[str, str], str]:\n",
    "            - A dictionary mapping placeholder keys to original markdown blocks.\n",
    "            - The transformed markdown with placeholders in place of structure.\n",
    "    \"\"\"\n",
    "\n",
    "    md = MarkdownIt()\n",
    "\n",
    "    placeholder_map = {}\n",
    "    counter = 0\n",
    "\n",
    "    def get_next_placeholder(value: str, prefix=\"PH\") -> str:\n",
    "        \"\"\"\n",
    "        Generates a unique placeholder token for the given value and stores the mapping.\n",
    "\n",
    "        If the input contains any previously assigned placeholders, they are unwrapped and reassigned\n",
    "        as a new single placeholder. This helps deduplicate and simplify complex inline structures.\n",
    "\n",
    "        Args:\n",
    "            value (str): The text content to be replaced by a placeholder.\n",
    "            prefix (str): The placeholder prefix (e.g., \"PH\", \"BULLET\").\n",
    "\n",
    "        Returns:\n",
    "            str: The generated placeholder token (e.g., \"<<PH3>>\" or \"[[BULLET2]]\").\n",
    "        \"\"\"\n",
    "        nonlocal counter\n",
    "        value = re.sub(\n",
    "            r\"<<(PH|BULLET|SEP)\\d*>>|\\[\\[BULLET\\d+\\]\\]\",\n",
    "            lambda m: placeholder_map.get(m.group(0).strip(\"<>[]\"), m.group(0)),\n",
    "            value,\n",
    "        )\n",
    "        counter += 1\n",
    "        key = f\"{prefix}{counter}\"\n",
    "        placeholder_map[key] = value\n",
    "\n",
    "        if prefix == \"BULLET\":\n",
    "            return f\"[[{key}]]\"\n",
    "        else:\n",
    "            return f\"<<{key}>>\"\n",
    "\n",
    "    def protect_front_matter(content: str) -> str:\n",
    "        \"\"\"\n",
    "        Detects and replaces a YAML front matter block with a single placeholder.\n",
    "        The front matter must be at the very beginning of the string.\n",
    "\n",
    "        Args:\n",
    "            content (str): Input markdown content.\n",
    "\n",
    "        Returns:\n",
    "            str: Markdown with front matter replaced by a placeholder.\n",
    "        \"\"\"\n",
    "        front_matter_pattern = re.compile(r'\\A---\\s*\\n.*?\\n---\\s*\\n?', re.DOTALL)\n",
    "        \n",
    "        match = front_matter_pattern.search(content)\n",
    "        if match:\n",
    "            front_matter_block = match.group(0)\n",
    "            placeholder = get_next_placeholder(front_matter_block)\n",
    "            return front_matter_pattern.sub(placeholder, content, count=1)\n",
    "        \n",
    "        return content\n",
    "\n",
    "    def protect_tables(content: str) -> str:\n",
    "        \"\"\"\n",
    "        Detects Markdown tables and replaces them with placeholders.\n",
    "\n",
    "        Args:\n",
    "            content (str): Input markdown content.\n",
    "\n",
    "        Returns:\n",
    "            str: Markdown with tables replaced by placeholders.\n",
    "        \"\"\"\n",
    "        lines = content.splitlines()\n",
    "        protected_lines = []\n",
    "        in_table = False\n",
    "        table_buffer = []\n",
    "\n",
    "        for line in lines:\n",
    "            if (\n",
    "                \"|\" in line\n",
    "                and line.strip().startswith(\"|\")\n",
    "                and line.strip().endswith(\"|\")\n",
    "            ):\n",
    "                if not in_table:\n",
    "                    in_table = True\n",
    "                    table_buffer = [line]\n",
    "                else:\n",
    "                    table_buffer.append(line)\n",
    "            elif in_table and re.match(r\"^\\s*\\|[\\s\\-\\|:]+\\|\\s*$\", line):\n",
    "                table_buffer.append(line)\n",
    "            elif in_table:\n",
    "                if len(table_buffer) >= 2:\n",
    "                    table_content = \"\\n\".join(table_buffer)\n",
    "                    placeholder = get_next_placeholder(table_content)\n",
    "                    protected_lines.append(placeholder)\n",
    "                else:\n",
    "                    protected_lines.extend(table_buffer)\n",
    "\n",
    "                table_buffer = []\n",
    "                in_table = False\n",
    "                protected_lines.append(line)\n",
    "            else:\n",
    "                protected_lines.append(line)\n",
    "\n",
    "        # Handle final table\n",
    "        if in_table and len(table_buffer) >= 2:\n",
    "            table_content = \"\\n\".join(table_buffer)\n",
    "            placeholder = get_next_placeholder(table_content)\n",
    "            protected_lines.append(placeholder)\n",
    "        elif in_table:\n",
    "            protected_lines.extend(table_buffer)\n",
    "\n",
    "        return \"\\n\".join(protected_lines)\n",
    "\n",
    "    def is_low_prose_line(line: str, threshold: float = 0.1) -> bool:\n",
    "        \"\"\"\n",
    "        Determines if a line contains mostly structure and little natural language prose.\n",
    "\n",
    "        Args:\n",
    "            line (str): A single markdown line.\n",
    "            threshold (float): Minimum proportion of prose content.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if it's mostly non-prose.\n",
    "        \"\"\"\n",
    "        if not line.strip():\n",
    "            return True\n",
    "        no_placeholders = re.sub(r\"<<(PH|BULLET|SEP)\\d*>>|\\[\\[BULLET\\d+\\]\\]\", \"\", line)\n",
    "        no_markdown = re.sub(r\"[*_`[\\]()#|-]\", \"\", no_placeholders)\n",
    "        prose_content = no_markdown.strip()\n",
    "        if len(line) > 0 and (len(prose_content) / len(line)) < threshold:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    md_content = protect_front_matter(md_content)\n",
    "    md_content = protect_tables(md_content)\n",
    "\n",
    "    tokens = md.parse(md_content)\n",
    "    lines = md_content.splitlines()\n",
    "    block_replacements = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        token = tokens[i]\n",
    "\n",
    "        if (\n",
    "            token.type == \"html_block\"\n",
    "            and i + 3 < len(tokens)\n",
    "            and tokens[i + 1].type == \"heading_open\"\n",
    "        ):\n",
    "            html_start, _ = token.map\n",
    "            _, heading_end = tokens[i + 1].map\n",
    "            raw_block = \"\\n\".join(lines[html_start:heading_end])\n",
    "            placeholder = get_next_placeholder(raw_block)\n",
    "            block_replacements.append((html_start, heading_end, placeholder))\n",
    "            i += 4\n",
    "            continue\n",
    "        elif token.type == \"fence\" or token.type == \"html_block\":\n",
    "            start, end = token.map\n",
    "            raw_block = \"\\n\".join(lines[start:end])\n",
    "            placeholder = get_next_placeholder(raw_block)\n",
    "            block_replacements.append((start, end, placeholder))\n",
    "            i += 1\n",
    "            continue\n",
    "        elif token.type == \"heading_open\":\n",
    "            level = int(token.tag[1])\n",
    "            inline_token = tokens[i + 1]\n",
    "            header_text = inline_token.content.strip()\n",
    "            placeholder = get_next_placeholder(header_text)\n",
    "            start, end = token.map\n",
    "            markdown_prefix = \"#\" * level\n",
    "            block_replacements.append((start, end, f\"{markdown_prefix} {placeholder}\"))\n",
    "            i += 3\n",
    "            continue\n",
    "        elif token.type == \"blockquote_open\":\n",
    "            start = token.map[0] if token.map else i\n",
    "            j = i + 1\n",
    "            blockquote_content = []\n",
    "            while j < len(tokens) and tokens[j].type != \"blockquote_close\":\n",
    "                if (\n",
    "                    tokens[j].type == \"paragraph_open\"\n",
    "                    and j + 1 < len(tokens)\n",
    "                    and tokens[j + 1].type == \"inline\"\n",
    "                ):\n",
    "                    blockquote_content.append(tokens[j + 1].content)\n",
    "                j += 1\n",
    "            if j < len(tokens):\n",
    "                end = tokens[j].map[1] if tokens[j].map else start + 1\n",
    "                if blockquote_content:\n",
    "                    text_content = \" \".join(blockquote_content)\n",
    "                    placeholder = get_next_placeholder(text_content)\n",
    "                    block_replacements.append((start, end, f\"> {placeholder}\"))\n",
    "                i = j + 1\n",
    "            else:\n",
    "                i += 1\n",
    "            continue\n",
    "        i += 1\n",
    "\n",
    "    for start, end, replacement in sorted(block_replacements, reverse=True):\n",
    "        lines[start:end] = [replacement]\n",
    "\n",
    "    def replace_md_links(match):\n",
    "        \"\"\"\n",
    "        Replaces standard Markdown links with placeholders for the URL target.\n",
    "        \"\"\"\n",
    "        text, url = match.group(1), match.group(2)\n",
    "        if re.match(r\"^<<PH\\d+>>$\", url):\n",
    "            return match.group(0)\n",
    "        return f\"[{text}]({get_next_placeholder(url)})\"\n",
    "\n",
    "    def replace_internal_links(match):\n",
    "        \"\"\"\n",
    "        Replaces internal anchor links with placeholders for the anchor target.\n",
    "        \"\"\"\n",
    "        text, anchor = match.group(1), f\"#{match.group(2)}\"\n",
    "        if re.match(r\"^<<PH\\d+>>$\", anchor):\n",
    "            return match.group(0)\n",
    "        return f\"[{text}]({get_next_placeholder(anchor)})\"\n",
    "\n",
    "    processed_lines = []\n",
    "    for line in lines:\n",
    "        # Protect inline code first, as it can contain any character.\n",
    "        line = re.sub(\n",
    "            r\"`([^`]+)`\", lambda m: f\"`{get_next_placeholder(m.group(1))}`\", line\n",
    "        )\n",
    "        \n",
    "        # Protect Markdown images \n",
    "        line = re.sub(\n",
    "            r'!\\[([^\\]]*)\\]\\(([^)]+)\\)',\n",
    "            lambda m: get_next_placeholder(m.group(0)),\n",
    "            line\n",
    "        )\n",
    "\n",
    "        # Protect standard Markdown links []() and internal links [](#).\n",
    "        line = re.sub(r\"\\[([^\\]]+)]\\(([^)]+)\\)\", replace_md_links, line)\n",
    "        line = re.sub(r\"\\[([^\\]]+)]\\(#([^)]+)\\)\", replace_internal_links, line)\n",
    "        \n",
    "        # Protect raw URLs last\n",
    "        line = re.sub(\n",
    "            r\"https?://[^\\s)\\]}]+\", lambda m: get_next_placeholder(m.group(0)), line\n",
    "        )\n",
    "        \n",
    "        processed_lines.append(line)\n",
    "\n",
    "    def is_title_line(content: str) -> bool:\n",
    "        \"\"\"Heuristic check to determine if a line is a title-style phrase.\"\"\"\n",
    "        clean = re.sub(r\"<<[^>]+>>\", \"\", content)\n",
    "        clean = re.sub(r\"[*_`[\\]\\(\\)]\", \"\", clean).strip()\n",
    "        words = re.findall(r\"[A-Za-z]+(?:-[A-Za-z]+)*\", clean)\n",
    "        if len(words) < 2:\n",
    "            return False\n",
    "        alpha = re.sub(r\"[^A-Za-z]\", \"\", clean)\n",
    "        if alpha.isupper():\n",
    "            return True\n",
    "        upper_words = [w for w in words if w[0].isupper()]\n",
    "        if len(words) > 0 and len(upper_words) / len(words) >= 0.75:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    bullet_placeholder_lines = []\n",
    "    for line in processed_lines:\n",
    "        if is_low_prose_line(line):\n",
    "            placeholder = get_next_placeholder(line)\n",
    "            bullet_placeholder_lines.append(placeholder)\n",
    "            continue\n",
    "\n",
    "        m = re.match(r\"^(\\s*)([-*+]|\\d+\\.)\\s+(.*)$\", line)\n",
    "        if m:\n",
    "            indent, bullet, content = m.groups()\n",
    "            if is_title_line(content):\n",
    "                ph = get_next_placeholder(content)\n",
    "                bullet_placeholder_lines.append(f\"{indent}{bullet} {ph}\")\n",
    "            else:\n",
    "                bph = get_next_placeholder(bullet, prefix=\"BULLET\")\n",
    "                bullet_placeholder_lines.append(f\"{indent}{bph} {content}\")\n",
    "        else:\n",
    "            bullet_placeholder_lines.append(line)\n",
    "\n",
    "    raw_processed = \"\\n\".join(bullet_placeholder_lines)\n",
    "    raw_processed = re.sub(\n",
    "        r\"^\\s*---\\s*$\",\n",
    "        lambda m: get_next_placeholder(m.group(0)),\n",
    "        raw_processed,\n",
    "        flags=re.MULTILINE,\n",
    "    )\n",
    "\n",
    "    final_lines = []\n",
    "    for line in raw_processed.splitlines(keepends=True):\n",
    "        if line.endswith(\"\\n\"):\n",
    "            content = line.rstrip(\"\\n\")\n",
    "            trailing_newline = True\n",
    "        else:\n",
    "            content = line\n",
    "            trailing_newline = False\n",
    "        newline_placeholder = get_next_placeholder(\"\\n\", prefix=\"PH\")\n",
    "        final_lines.append(content + (newline_placeholder if trailing_newline else \"\"))\n",
    "\n",
    "    processed_content = \"\".join(final_lines)\n",
    "\n",
    "    merged_placeholder_map = {}\n",
    "    pattern = re.compile(r\"(?:<<PH\\d+>>){2,}\")\n",
    "\n",
    "    while True:\n",
    "        match = pattern.search(processed_content)\n",
    "        if not match:\n",
    "            break\n",
    "        ph_sequence = re.findall(r\"<<PH\\d+>>\", match.group(0))\n",
    "        keys = [ph.strip(\"<>\") for ph in ph_sequence]\n",
    "        merged_value = \"\".join(placeholder_map.get(k, \"\") for k in keys)\n",
    "        counter += 1\n",
    "        new_key = f\"PH{counter}\"\n",
    "        new_ph = f\"<<{new_key}>>\"\n",
    "        placeholder_map[new_key] = merged_value\n",
    "        processed_content = (\n",
    "            processed_content[: match.start()]\n",
    "            + new_ph\n",
    "            + processed_content[match.end() :]\n",
    "        )\n",
    "        merged_placeholder_map[new_key] = keys\n",
    "\n",
    "    processed_content = re.sub(\n",
    "        r\"(<<PH\\d+>>)(?!<<)(?=\\w)\", r\"\\1<<SEP>>\", processed_content\n",
    "    )\n",
    "    placeholder_map[\"SEP\"] = \"\"\n",
    "\n",
    "    return placeholder_map, processed_content\n",
    "\n",
    "def restore_placeholders(corrected_text: str, placeholder_map: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Replaces placeholders in the corrected markdown content with their original values.\n",
    "\n",
    "    Args:\n",
    "        corrected_text (str): Markdown content containing placeholders.\n",
    "        placeholder_map (Dict[str, str]): Map of placeholders to original content.\n",
    "\n",
    "    Returns:\n",
    "        str: Fully restored markdown with original formatting.\n",
    "    \"\"\"\n",
    "    restored_text = corrected_text\n",
    "\n",
    "    # Replace placeholder tokens with original values\n",
    "    # Sort by length descending to handle nested placeholders correctly\n",
    "    for placeholder, original in sorted(\n",
    "        placeholder_map.items(), key=lambda x: -len(x[0])\n",
    "    ):\n",
    "        if placeholder.startswith(\"BULLET\"):\n",
    "            restored_text = restored_text.replace(f\"[[{placeholder}]]\", original)\n",
    "        else:\n",
    "            restored_text = restored_text.replace(f\"<<{placeholder}>>\", original)\n",
    "\n",
    "    # Remove SEP markers\n",
    "    restored_text = restored_text.replace(\"<<SEP>>\", \"\")\n",
    "\n",
    "    return restored_text\n",
    "    \n",
    "############################################\n",
    "###--------------PARSER END--------------###\n",
    "############################################\n",
    "\n",
    "############################################\n",
    "###------------CHUNKER  START------------###\n",
    "############################################\n",
    "\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "def estimate_token_count(text: str) -> int:\n",
    "    '''\n",
    "    Estimate the number of tokens in a given string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "        int: Approximate number of tokens, assuming 4 characters per token\n",
    "    '''\n",
    "    return len(text) // 4 \n",
    "\n",
    "def split_by_top_level_headers(markdown: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split markdown into sections using top-level headers (e.g., #, ##, ..., ######).\n",
    "\n",
    "    Args:\n",
    "        markdown (str): The input markdown text.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of sections split by top-level headers.\n",
    "    \"\"\"\n",
    "    # Find all headers from # to ###### at the beginning of a line\n",
    "    matches = list(re.finditer(r'^\\s*#{1,6}\\s.*', markdown, flags=re.MULTILINE))\n",
    "    if not matches:\n",
    "        # No headers found, return whole content\n",
    "        return [markdown]\n",
    "\n",
    "    sections = []\n",
    "    for i, match in enumerate(matches):\n",
    "        start = match.start()\n",
    "        end = matches[i + 1].start() if i + 1 < len(matches) else len(markdown)\n",
    "        sections.append(markdown[start:end])\n",
    "\n",
    "    return sections\n",
    "\n",
    "def smart_sentence_split(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text at sentence boundaries or placeholder boundaries.\n",
    "\n",
    "    Args: \n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of sentence-level parts, preserving placeholder boundaries.\n",
    "    \"\"\"\n",
    "    # Pattern to capture sentence boundaries and placeholder boundaries\n",
    "    pattern = r'([.!?]\\s+(?=[A-Z]))|(__PLACEHOLDER\\d+__)'\n",
    "    matches = re.split(pattern, text)\n",
    "\n",
    "    # Reconstruct complete sentence or placeholder segments\n",
    "    parts = []\n",
    "    buffer = ''\n",
    "    for chunk in matches:\n",
    "        if chunk is None:\n",
    "            continue\n",
    "        buffer += chunk\n",
    "        if re.match(r'[.!?]\\s+$', chunk) or re.match(r'__PLACEHOLDER\\d+__', chunk.strip()):\n",
    "            parts.append(buffer.strip())\n",
    "            buffer = ''\n",
    "    if buffer.strip():\n",
    "        parts.append(buffer.strip())\n",
    "\n",
    "    return parts\n",
    "\n",
    "def chunk_large_section(section: str, max_tokens: int = 1000) -> List[str]:\n",
    "    \"\"\"\n",
    "    Chunk a section while avoiding breaks mid-sentence or mid-placeholder.\n",
    "\n",
    "    Args:\n",
    "        section (str): A section of text to be chunked.\n",
    "        max_tokens (int): Maximum token count per chunk.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of token-limited chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_token_count = 0\n",
    "\n",
    "    # Split section into sentence-safe pieces\n",
    "    parts = smart_sentence_split(section)\n",
    "\n",
    "    for part in parts:\n",
    "        part = part.strip()\n",
    "        if not part:\n",
    "            continue\n",
    "\n",
    "        part_token_count = estimate_token_count(part)\n",
    "\n",
    "        # Finalize current chunk if adding this part would exceed max tokens\n",
    "        if current_token_count + part_token_count > max_tokens:\n",
    "            if current_chunk:\n",
    "                chunks.append(' '.join(current_chunk).strip())\n",
    "                current_chunk = []\n",
    "                current_token_count = 0\n",
    "\n",
    "        current_chunk.append(part)\n",
    "        current_token_count += part_token_count\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk).strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def chunk_markdown(markdown: str, max_tokens: int = 100) -> List[str]:\n",
    "    '''\n",
    "    Chunk a full markdown document into smaller parts based on headers and token limits.\n",
    "\n",
    "    Args:\n",
    "        markdown (str): The complete markdown content.\n",
    "        max_tokens (int): Maximum allowed tokens per chunk.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of markdown chunks that are token-limited and structured.\n",
    "    '''\n",
    "    # Split by top level headers\n",
    "    sections = split_by_top_level_headers(markdown)\n",
    "    final_chunks = []\n",
    "\n",
    "    for section in sections:\n",
    "        # If the section is small enough, keep it as is\n",
    "        if estimate_token_count(section) <= max_tokens:\n",
    "            final_chunks.append(section.strip())\n",
    "        else:\n",
    "            # Otherwise, chunk it further based on sentence boundaries\n",
    "            final_chunks.extend(chunk_large_section(section, max_tokens=max_tokens))\n",
    "\n",
    "    def split_long_chunk(chunk: str, max_chars: int = 3500) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split a long chunk into smaller character-limited subchunks, preferring newline or sentence boundaries.\n",
    "\n",
    "        Args:\n",
    "            chunk (str): A markdown chunk that may be too long.\n",
    "            max_chars (int): Maximum number of characters per subchunk.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: Subchunks of the input chunk that fit the character limit.\n",
    "        \"\"\"\n",
    "        if len(chunk) <= max_chars:\n",
    "            return [chunk]\n",
    "\n",
    "        # Try splitting on newlines first\n",
    "        parts = re.split(r'(?<=\\n)', chunk)\n",
    "        subchunks = []\n",
    "        buffer = \"\"\n",
    "\n",
    "        for part in parts:\n",
    "            if len(buffer) + len(part) > max_chars:\n",
    "                if buffer:\n",
    "                    subchunks.append(buffer.strip())\n",
    "                buffer = part\n",
    "            else:\n",
    "                buffer += part\n",
    "\n",
    "        if buffer.strip():\n",
    "            subchunks.append(buffer.strip())\n",
    "\n",
    "        # If subchunks are still too long, split on sentence boundary\n",
    "        final_subchunks = []\n",
    "        for sub in subchunks:\n",
    "            if len(sub) <= max_chars:\n",
    "                final_subchunks.append(sub)\n",
    "            else:\n",
    "                sentences = re.split(r'(?<=[.!?])\\s+', sub)\n",
    "                sentence_buffer = \"\"\n",
    "                for s in sentences:\n",
    "                    if len(sentence_buffer) + len(s) > max_chars:\n",
    "                        final_subchunks.append(sentence_buffer.strip())\n",
    "                        sentence_buffer = s\n",
    "                    else:\n",
    "                        sentence_buffer += (\" \" if sentence_buffer else \"\") + s\n",
    "                if sentence_buffer.strip():\n",
    "                    final_subchunks.append(sentence_buffer.strip())\n",
    "\n",
    "        return final_subchunks\n",
    "\n",
    "    # Apply post-splitting to each chunk to enforce character limits\n",
    "    adjusted_chunks = []\n",
    "    for chunk in final_chunks:\n",
    "        adjusted_chunks.extend(split_long_chunk(chunk))\n",
    "\n",
    "    return adjusted_chunks\n",
    "\n",
    "############################################\n",
    "###-------------CHUNKER  END-------------###\n",
    "############################################\n",
    "\n",
    "############################################\n",
    "###-------------PROMPT START-------------###\n",
    "############################################\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Template for llama3-instruct format\n",
    "MARKDOWN_CORRECTION_TEMPLATE_LLAMA3 = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a markdown grammar correction assistant. Your job is to correct only grammatical errors in the user's Markdown content.\n",
    "\n",
    "Strictly follow these rules:\n",
    "- Do **NOT** modify any placeholders (e.g., <<PH1>>, <<PH93>>, [[BULLET3]], <<SEP>>). Leave them **EXACTLY as they appear**, including spacing and underscores.\n",
    "- Do **NOT** remove, reword, rename, reformat, or relocate any placeholder.\n",
    "- Do **NOT** add any extra markdown or other symbols (e.g., #, >)\n",
    "- Do **NOT** add or remove any extra space around placeholders.\n",
    "- Do **NOT** remove Markdown styling characters (e.g., **, *, _, __, `, [, ]).\n",
    "- Do **NOT** add or remove extra content from the original text.\n",
    "- Do **NOT** change or swap markdown syntax ([], (), *, `). They should remain **EXACTLY** as they are in the original text. \n",
    "- Only correct grammar **within natural language sentences**, leaving structure unchanged.\n",
    "- **ALWAYS** maintain title case wherever it is is present in the original text. \n",
    "\n",
    "Example:\n",
    "- Original: \"<SEP>We use <<PH4>> to **builds** 2 model **likke** this:<<PH17>><<PH18>>\"\n",
    "- Corrected: \"<SEP>We use <<PH4>> to **build** 2 models **like** this:<<PH17>><<PH18_>>\"\n",
    "\n",
    "Example:\n",
    "- Original: \"[[BULLET1]] **It Will Be More Profitablr<PH12>>**\"\n",
    "- Corrected: \"[[BULLET1]] **It Will Be More Profitable<PH12>>**\"\n",
    "\n",
    "Example:\n",
    "- Original: \"<<PH27>><<PH28>><<SEP>>This methd is **not necessary** the way ti build *AI* agents <<PH32>>\"\n",
    "- Corrected: \"<<PH27>><<PH28>><<SEP>>This method is **not necessary** the way to build *AI* agents <<PH32>>\"\n",
    "\n",
    "All placeholders are present and stay exactly the same with no additional spaces — only grammar is corrected.\n",
    "\n",
    "Respond only with the corrected Markdown content. Do not explain anything.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "Original markdown:\n",
    "{markdown}\n",
    "\n",
    "Corrected markdown:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_markdown_correction_prompt() -> PromptTemplate:\n",
    "    \"\"\"\n",
    "    Get the markdown correction prompt formatted for LLaMA 3 instruct.\n",
    "\n",
    "    Returns:\n",
    "        PromptTemplate: Ready to use in LangChain with LLaMA 3 format.\n",
    "    \"\"\"\n",
    "    return PromptTemplate.from_template(MARKDOWN_CORRECTION_TEMPLATE_LLAMA3)\n",
    "\n",
    "############################################\n",
    "###--------------PROMPT END--------------###\n",
    "############################################\n",
    "\n",
    "############################################\n",
    "###-------------UTILS  START-------------###\n",
    "############################################\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import importlib.util\n",
    "import multiprocessing\n",
    "from typing import Dict, Any, Optional, Union, List, Tuple\n",
    "\n",
    "#Default models to be loaded in our examples:\n",
    "DEFAULT_MODELS = {\n",
    "    \"local\": \"/home/jovyan/datafabric/llama2-7b/ggml-model-f16-Q5_K_M.gguf\",\n",
    "    \"tensorrt\": \"\",\n",
    "    \"hugging-face-local\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    \"hugging-face-cloud\": \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "}\n",
    "\n",
    "# Context window sizes for various models\n",
    "MODEL_CONTEXT_WINDOWS = {\n",
    "    # LlamaCpp models\n",
    "    'ggml-model-f16-Q5_K_M.gguf': 4096,\n",
    "    'ggml-model-7b-q4_0.bin': 4096,\n",
    "    'gguf-model-7b-4bit.bin': 4096,\n",
    "\n",
    "    # HuggingFace models\n",
    "    'mistralai/Mistral-7B-Instruct-v0.3': 8192,\n",
    "    'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B': 4096,\n",
    "    'meta-llama/Llama-2-7b-chat-hf': 4096,\n",
    "    'meta-llama/Llama-3-8b-chat-hf': 8192,\n",
    "    'google/flan-t5-base': 512,\n",
    "    'google/flan-t5-large': 512,\n",
    "    'TheBloke/WizardCoder-Python-7B-V1.0-GGUF': 4096,\n",
    "\n",
    "    # OpenAI models\n",
    "    'gpt-3.5-turbo': 16385,\n",
    "    'gpt-4': 8192,\n",
    "    'gpt-4-32k': 32768,\n",
    "    'gpt-4-turbo': 128000,\n",
    "    'gpt-4o': 128000,\n",
    "\n",
    "    # Anthropic models\n",
    "    'claude-3-opus-20240229': 200000,\n",
    "    'claude-3-sonnet-20240229': 180000,\n",
    "    'claude-3-haiku-20240307': 48000,\n",
    "\n",
    "    # Other models\n",
    "    'qwen/Qwen-7B': 8192,\n",
    "    'microsoft/phi-2': 2048,\n",
    "    'tiiuae/falcon-7b': 4096,\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\": 128000,\n",
    "}\n",
    "\n",
    "def load_config_and_secrets(\n",
    "    config_path: str = \"../../configs/config.yaml\",\n",
    "    secrets_path: str = \"../../configs/secrets.yaml\"\n",
    ") -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load configuration and secrets from YAML files.\n",
    "\n",
    "    Args:\n",
    "        config_path: Path to the configuration YAML file.\n",
    "        secrets_path: Path to the secrets YAML file.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing (config, secrets) as dictionaries.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If either the config or secrets file is not found.\n",
    "    \"\"\"\n",
    "    # Convert to absolute paths if needed\n",
    "    config_path = os.path.abspath(config_path)\n",
    "    secrets_path = os.path.abspath(secrets_path)\n",
    "\n",
    "    if not os.path.exists(secrets_path):\n",
    "        raise FileNotFoundError(f\"secrets.yaml file not found in path: {secrets_path}\")\n",
    "\n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f\"config.yaml file not found in path: {config_path}\")\n",
    "\n",
    "    with open(config_path) as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "    with open(secrets_path) as file:\n",
    "        secrets = yaml.safe_load(file)\n",
    "\n",
    "    return config, secrets\n",
    "\n",
    "def initialize_llm(\n",
    "    model_source: str = \"local\",\n",
    "    secrets: Optional[Dict[str, Any]] = None,\n",
    "    local_model_path: str = DEFAULT_MODELS[\"local\"],\n",
    "    hf_repo_id: str = \"\"\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Initialize a language model based on specified source.\n",
    "\n",
    "    Args:\n",
    "        model_source: Source of the model. Options are \"local\", \"hugging-face-local\", or \"hugging-face-cloud\".\n",
    "        secrets: Dictionary containing API keys for cloud services.\n",
    "        local_model_path: Path to local model file.\n",
    "\n",
    "    Returns:\n",
    "        Initialized language model object.\n",
    "\n",
    "    Raises:\n",
    "        ImportError: If required libraries are not installed.\n",
    "        ValueError: If an unsupported model_source is provided.\n",
    "    \"\"\"\n",
    "    # Check dependencies\n",
    "    missing_deps = []\n",
    "    for module in [\"langchain_huggingface\", \"langchain_core.callbacks\", \"langchain_community.llms\"]:\n",
    "        if not importlib.util.find_spec(module):\n",
    "            missing_deps.append(module)\n",
    "    \n",
    "    if missing_deps:\n",
    "        raise ImportError(f\"Missing required dependencies: {', '.join(missing_deps)}\")\n",
    "    \n",
    "    # Import required libraries\n",
    "    from langchain_huggingface import HuggingFacePipeline, HuggingFaceEndpoint\n",
    "    from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "    from langchain_community.llms import LlamaCpp\n",
    "\n",
    "    model = None\n",
    "    context_window = None\n",
    "    \n",
    "    # Initialize based on model source\n",
    "    if model_source == \"hugging-face-cloud\":\n",
    "        if hf_repo_id == \"\":\n",
    "            repo_id = DEFAULT_MODELS[\"hugging-face-cloud\"]\n",
    "        else:\n",
    "            repo_id = hf_repo_id  \n",
    "        if not secrets or \"HUGGINGFACE_API_KEY\" not in secrets:\n",
    "            raise ValueError(\"HuggingFace API key is required for cloud model access\")\n",
    "            \n",
    "        huggingfacehub_api_token = secrets[\"HUGGINGFACE_API_KEY\"]\n",
    "        # Get context window from our lookup table\n",
    "        if repo_id in MODEL_CONTEXT_WINDOWS:\n",
    "            context_window = MODEL_CONTEXT_WINDOWS[repo_id]\n",
    "\n",
    "        model = HuggingFaceEndpoint(\n",
    "            huggingfacehub_api_token=huggingfacehub_api_token,\n",
    "            repo_id=repo_id,\n",
    "        )\n",
    "\n",
    "    elif model_source == \"hugging-face-local\":\n",
    "        from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "        if \"HUGGINGFACE_API_KEY\" in secrets:\n",
    "            os.environ[\"HF_TOKEN\"] = secrets[\"HUGGINGFACE_API_KEY\"]\n",
    "        if hf_repo_id == \"\":\n",
    "            model_id = DEFAULT_MODELS[\"hugging-face-local\"]\n",
    "        else:\n",
    "            model_id = hf_repo_id        \n",
    "        # Get context window from our lookup table\n",
    "        if model_id in MODEL_CONTEXT_WINDOWS:\n",
    "            context_window = MODEL_CONTEXT_WINDOWS[model_id]\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        hf_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "        # If tokenizer has model_max_length, that's our context window\n",
    "        if hasattr(tokenizer, 'model_max_length') and tokenizer.model_max_length not in (None, -1):\n",
    "            context_window = tokenizer.model_max_length\n",
    "\n",
    "        pipe = pipeline(\"text-generation\", model=hf_model, tokenizer=tokenizer, max_new_tokens=100, device=0)\n",
    "        model = HuggingFacePipeline(pipeline=pipe)\n",
    "        \n",
    "    elif model_source == \"tensorrt\":\n",
    "        #If a Hugging Face model is specified, it will be used - otherwise, it will try loading the model from local_path\n",
    "        try:\n",
    "            import tensorrt_llm\n",
    "            sampling_params = tensorrt_llm.SamplingParams(temperature=0.1, top_p=0.95, max_tokens=512) \n",
    "            if hf_repo_id != \"\":\n",
    "                return TensorRTLangchain(model_path = hf_repo_id, sampling_params = sampling_params)\n",
    "            else:\n",
    "                model_config = os.path.join(local_model_path, config.json)\n",
    "                if os.path.isdir(local_model_path) and os.path.isfile(model_config):\n",
    "                    return TensorRTLangchain(model_path = local_model_path, sampling_params = sampling_params)\n",
    "                else:\n",
    "                    raise Exception(\"Model format incompatible with TensorRT LLM\")\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"Could not import tensorrt-llm library. \"\n",
    "                \"Please make sure tensorrt-llm is installed properly, or \"\n",
    "                \"consider using workspaces based on the NeMo Framework\"\n",
    "            )\n",
    "    elif model_source == \"local\":\n",
    "        callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "        # For LlamaCpp, get the context window from the filename\n",
    "        model_filename = os.path.basename(local_model_path)\n",
    "        if model_filename in MODEL_CONTEXT_WINDOWS:\n",
    "            context_window = MODEL_CONTEXT_WINDOWS[model_filename]\n",
    "        else:  \n",
    "            # Default context window for LlamaCpp models (explicitly set)\n",
    "            context_window = 4096\n",
    "\n",
    "        model = LlamaCpp(\n",
    "            model_path=local_model_path,\n",
    "            n_gpu_layers=-1,                             \n",
    "            n_batch=512,                                 \n",
    "            n_ctx=32000,\n",
    "            max_tokens=1024,\n",
    "            f16_kv=True,\n",
    "            use_mmap=False,                             \n",
    "            low_vram=False,                            \n",
    "            rope_scaling=None,\n",
    "            temperature=0.0,\n",
    "            repeat_penalty=1.0,\n",
    "            streaming=False,\n",
    "            stop=None,\n",
    "            seed=42,\n",
    "            num_threads=multiprocessing.cpu_count(),\n",
    "            verbose=False #\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model source: {model_source}\")\n",
    "\n",
    "    # Store context window as model attribute for easy access\n",
    "    if model and hasattr(model, '__dict__'):\n",
    "        model.__dict__['_context_window'] = context_window\n",
    "\n",
    "    return model\n",
    "\n",
    "############################################\n",
    "###--------------UTILS  END--------------###\n",
    "############################################\n",
    "\n",
    "############################################\n",
    "###-----------LLM METRIC START-----------###\n",
    "############################################\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from typing import List, Optional\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import multiprocessing\n",
    "\n",
    "# Initialize TF-IDF vertorizer for semantic similarity\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000) \n",
    "\n",
    "def semantic_similarity_eval_fn(predictions: List[str], targets: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Compute semantic similarity between predictions and targets using TF-IDF and cosine similarity.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): Model-generated texts.\n",
    "        targets (List[str]): Ground truth references.\n",
    "\n",
    "    Returns:\n",
    "        float: Mean cosine similarity between matched pairs.\n",
    "    \"\"\"\n",
    "    all_texts = list(targets) + list(predictions)\n",
    "    \n",
    "    all_texts = [str(text) if text else \"\" for text in all_texts]\n",
    "    \n",
    "    if len(set(all_texts)) < 2:  \n",
    "        return 10.0\n",
    "    \n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(all_texts)\n",
    "    \n",
    "    n_targets = len(targets)\n",
    "    target_vectors = tfidf_matrix[:n_targets]\n",
    "    pred_vectors = tfidf_matrix[n_targets:]\n",
    "    \n",
    "    similarities = []\n",
    "    for i in range(len(targets)):\n",
    "        similarity = cosine_similarity(target_vectors[i:i+1], pred_vectors[i:i+1])[0][0]\n",
    "        similarities.append(similarity)\n",
    "    \n",
    "    return np.mean(similarities)*10\n",
    "\n",
    "def _count_syllables(word: str) -> int:\n",
    "    \"\"\"Rudimentary English syllable counter.\"\"\"\n",
    "    word = word.lower()\n",
    "    groups = re.findall(r'[aeiouy]+', word)\n",
    "    count = len(groups)\n",
    "    if word.endswith('e'):\n",
    "        count = max(1, count - 1)\n",
    "    return max(count, 1)\n",
    "\n",
    "def flesch_reading_ease(text: str) -> float:\n",
    "    \"\"\"Calculates Flesch Reading Ease score.\"\"\"\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    sentences = [s for s in sentences if s.strip()]\n",
    "    words = re.findall(r'\\w+', text)\n",
    "    if not sentences or not words:\n",
    "        return 0.0\n",
    "    syllables = sum(_count_syllables(w) for w in words)\n",
    "    W = len(words)\n",
    "    S = len(sentences)\n",
    "    score = 206.835 - 1.015 * (W / S) - 84.6 * (syllables / W)\n",
    "    return score\n",
    "\n",
    "def readability_improvement_eval_fn(predictions: List[str], targets: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the average absolute change in Flesch Reading Ease.\n",
    "    A score of 0.0 means no change in readability on average.\n",
    "    Positive values indicate improvement, negative values indicate a decrease.\n",
    "    \"\"\"\n",
    "    deltas = []\n",
    "    for pred, target in zip(predictions, targets):\n",
    "        # Ensure inputs are strings and handle if they are empty\n",
    "        orig_text = str(target)\n",
    "        pred_text = str(pred)\n",
    "\n",
    "        if not orig_text.strip():\n",
    "            orig_score = 0.0\n",
    "        else:\n",
    "            orig_score = flesch_reading_ease(orig_text)\n",
    "\n",
    "        if not pred_text.strip():\n",
    "            new_score = 0.0\n",
    "        else:\n",
    "            new_score = flesch_reading_ease(pred_text)\n",
    "        \n",
    "        # Calculate the simple difference (delta)\n",
    "        deltas.append(new_score - orig_score)\n",
    "        \n",
    "    # If the list is empty for any reason, return 0\n",
    "    if not deltas:\n",
    "        return 0.0\n",
    "\n",
    "    # Return the average of all the deltas directly.\n",
    "    return float(np.mean(deltas))\n",
    "\n",
    "def llm_judge_eval_fn_local(predictions: pd.Series, targets: pd.Series, llm) -> float:\n",
    "    \"\"\"\n",
    "    Use the main Llama 3 model to rate the grammar of predictions from 1 to 10.\n",
    "    This function correctly handles Pandas Series as input from MLflow.\n",
    "    \"\"\"\n",
    "    # Use the correct method to check if a Pandas Series is empty\n",
    "    if predictions.empty:\n",
    "        return 0.0\n",
    "\n",
    "    scores = []\n",
    "    # Iterating over a Series works as expected\n",
    "    for pred in predictions:\n",
    "        # This prompt is formatted for Llama 3 Instruct\n",
    "        prompt = f\"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "You are a grammar judge. Rate the grammar of the provided text on a scale from 1 (very poor) to 10 (perfect).\n",
    "Respond with a single integer number only. Do not add any explanation or punctuation.\n",
    "\n",
    "Keep in mind the text is markdown text, so do not penalize the use of markdown syntax such as *, #, `, [].\n",
    "\n",
    "Example:\n",
    "Text: I has a apple.\n",
    "Answer: 7\n",
    "\n",
    "Text: The dog chased **the ball** across the yard.\n",
    "Answer: 10<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "Text: {pred}\n",
    "Answer:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "        try:\n",
    "            result = llm.invoke(prompt)\n",
    "            text = result.strip()\n",
    "            match = re.search(r\"\\d+\", text)\n",
    "            score = float(match.group(0)) if match else 0.0\n",
    "            scores.append(score)\n",
    "        except Exception as e:\n",
    "            print(f\"[LLM judge runtime error]: {e}\")\n",
    "            scores.append(5.0)\n",
    "\n",
    "    return float(np.mean(scores)) if scores else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4287a10d",
   "metadata": {},
   "source": [
    "## Define MLflow Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96297375",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class MarkdownCorrectorModel(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"An MLflow model that encapsulates the entire markdown correction pipeline.\"\"\"\n",
    "\n",
    "    def load_context(self, context: mlflow.pyfunc.PythonModelContext) -> None:\n",
    "        \"\"\"Initializes the model, loading configurations, secrets, and the LLM.\"\"\"\n",
    "        config_path = context.artifacts[\"config\"]\n",
    "        with open(config_path, \"r\") as f:\n",
    "            self.config = yaml.safe_load(f)\n",
    "        \n",
    "        github_token = os.getenv(\"AIS_GITHUB_ACCESS_TOKEN\")\n",
    "        if github_token:\n",
    "            self.secrets = {\"AIS_GITHUB_ACCESS_TOKEN\": github_token}\n",
    "            logger.info(\"Loaded GITHUB_ACCESS_TOKEN from environment variable.\")\n",
    "        else:\n",
    "            secrets_path = context.artifacts.get(\"secrets\")\n",
    "            if secrets_path and os.path.exists(secrets_path):\n",
    "                with open(secrets_path, \"r\") as f:\n",
    "                    self.secrets = yaml.safe_load(f)\n",
    "                logger.info(f\"Loaded secrets from {secrets_path}.\")\n",
    "            else:\n",
    "                # If no token is found anywhere, initialize with an empty dict or handle as an error\n",
    "                self.secrets = {}\n",
    "                logger.warning(\"No GITHUB_ACCESS_TOKEN found in environment or secrets.yaml.\")\n",
    "        \n",
    "        model_source = self.config.get(\"model_source\", \"local\")\n",
    "        local_model_path = context.artifacts.get(\"local_model\")\n",
    "        self.llm = initialize_llm(model_source, self.secrets, local_model_path)\n",
    "        correction_prompt = get_markdown_correction_prompt()\n",
    "        self.llm_chain = correction_prompt | self.llm\n",
    "        \n",
    "        # Define which metrics will be calculated\n",
    "        self.metric_functions = {\n",
    "            \"semantic_similarity\": semantic_similarity_eval_fn,\n",
    "            \"readability_improvement\": readability_improvement_eval_fn,\n",
    "            \"grammar_quality_score\": lambda predictions, targets: llm_judge_eval_fn_local(\n",
    "                predictions, targets, llm=self.llm\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        logger.info(\"✅ Model context loaded with only instantaneous evaluation metrics.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _safe_join_chunks(chunks: List[str]) -> str:\n",
    "        joined = \"\"\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            if i == 0: joined += chunk\n",
    "            else:\n",
    "                prev = chunks[i - 1].rstrip()\n",
    "                curr = chunk\n",
    "                if prev.endswith('.') and re.match(r'^[A-Z\\\"]', curr.lstrip()):\n",
    "                    joined += ' ' + curr.lstrip()\n",
    "                else:\n",
    "                    joined += curr\n",
    "        return joined\n",
    "\n",
    "    def predict(self, context, model_input: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Runs correction and evaluates the output using only the fastest metrics.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Get the file content string directly from the input dataframe.\n",
    "        file_content = model_input[\"files\"].iloc[0]\n",
    "        \n",
    "        # Create a dictionary for the single file. \n",
    "        markdowns = {\"corrected_file.md\": file_content}\n",
    "            \n",
    "        original_files = markdowns.copy()\n",
    "\n",
    "        # Correction logic\n",
    "        parsed_markdowns, placeholder_maps, all_chunks = {}, {}, {}\n",
    "        for fn, content in markdowns.items():\n",
    "            placeholder_map, proc = parse_md_for_grammar_correction(content)\n",
    "            parsed_markdowns[fn], placeholder_maps[fn] = proc, placeholder_map\n",
    "            all_chunks[fn] = chunk_markdown(proc)\n",
    "        \n",
    "        corrected_chunks_by_file = defaultdict(list)\n",
    "        for fn, chunks in all_chunks.items():\n",
    "            for chunk in chunks:\n",
    "                resp = self.llm_chain.invoke({\"markdown\": chunk})\n",
    "                content_to_append = resp.content if hasattr(resp, 'content') else resp\n",
    "                corrected_chunks_by_file[fn].append(content_to_append)\n",
    "        \n",
    "        final_corrected = {}\n",
    "        for fn, chunk_list in corrected_chunks_by_file.items():\n",
    "            joined = self._safe_join_chunks(chunk_list)\n",
    "            final_corrected[fn] = restore_placeholders(joined, placeholder_maps[fn])\n",
    "            \n",
    "        # Metric calculation loop\n",
    "        evaluation_metrics = {}\n",
    "        eval_data = [{\"predictions\": final_corrected.get(fn, \"\"), \"targets\": content} for fn, content in original_files.items()]\n",
    "        eval_df = pd.DataFrame(eval_data)\n",
    "        \n",
    "        for name, metric_func in self.metric_functions.items():\n",
    "            try:\n",
    "                raw_score = metric_func(\n",
    "                    predictions=eval_df[\"predictions\"],\n",
    "                    targets=eval_df[\"targets\"]\n",
    "                )\n",
    "                evaluation_metrics[name] = float(raw_score)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not calculate metric '{name}'. Error: {e}\")\n",
    "                evaluation_metrics[name] = \"N/A\"\n",
    "\n",
    "                \n",
    "        response_time = time.time() - start_time\n",
    "\n",
    "        row = {\n",
    "            \"corrected\": final_corrected,\n",
    "            \"originals\": original_files,\n",
    "            \"response_time\": response_time,\n",
    "            \"evaluation_metrics\": evaluation_metrics,\n",
    "        }\n",
    "        return pd.DataFrame([row])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21960a9",
   "metadata": {},
   "source": [
    "## Log and Register Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64c3b585",
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 00:49:56 - INFO - Starting MLflow run with ID: 8a7223d2332e4a7eb2fb889b18e432d7\n",
      "2025/08/08 00:49:56 INFO mlflow.pyfunc: Validating input example against model signature\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0381f3e1be7e49b08c2ae0d519d40783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ad139deec54456ad2d3208ea91932b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 00:53:07 - WARNING - No GITHUB_ACCESS_TOKEN found in environment or secrets.yaml.\n",
      "/opt/conda/lib/python3.12/site-packages/mlflow/pyfunc/model.py:1034: UserWarning: WARNING! low_vram is not default parameter.\n",
      "                low_vram was transferred to model_kwargs.\n",
      "                Please confirm that low_vram is what you intended.\n",
      "  python_model.load_context(context=context)\n",
      "/opt/conda/lib/python3.12/site-packages/mlflow/pyfunc/model.py:1034: UserWarning: WARNING! rope_scaling is not default parameter.\n",
      "                rope_scaling was transferred to model_kwargs.\n",
      "                Please confirm that rope_scaling is what you intended.\n",
      "  python_model.load_context(context=context)\n",
      "/opt/conda/lib/python3.12/site-packages/mlflow/pyfunc/model.py:1034: UserWarning: WARNING! num_threads is not default parameter.\n",
      "                num_threads was transferred to model_kwargs.\n",
      "                Please confirm that num_threads is what you intended.\n",
      "  python_model.load_context(context=context)\n",
      "2025-08-08 00:53:12 - INFO - ✅ Model context loaded with only instantaneous evaluation metrics.\n",
      "Registered model 'markdown-corrector' already exists. Creating a new version of this model...\n",
      "Created version '37' of model 'markdown-corrector'.\n",
      "2025-08-08 00:55:54 - INFO - ✅ Model 'markdown-corrector' logged successfully (fast eval version).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18 s, sys: 39.2 s, total: 57.2 s\n",
      "Wall time: 5min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define artifacts to be packaged with the model\n",
    "artifacts = {\n",
    "    \"config\": str(CONFIG_PATH),\n",
    "    \"local_model\": str(LOCAL_MODEL_PATH),\n",
    "}\n",
    "\n",
    "if SECRETS_PATH.exists():\n",
    "    artifacts[\"secrets\"] = str(SECRETS_PATH)\n",
    "\n",
    "# Define the model's signature\n",
    "input_schema = Schema([\n",
    "    ColSpec(\"string\", \"repo_url\"),\n",
    "    ColSpec(\"string\", \"files\")\n",
    "])\n",
    "output_schema = Schema([\n",
    "    ColSpec(\"string\", \"corrected\"),\n",
    "    ColSpec(\"string\", \"originals\"),\n",
    "    ColSpec(\"double\", \"response_time\"),\n",
    "    ColSpec(\"string\", \"evaluation_metrics\")\n",
    "])\n",
    "signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "\n",
    "# Set up MLflow experiment\n",
    "mlflow.set_tracking_uri('/phoenix/mlflow')\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\n",
    "\n",
    "with mlflow.start_run(run_name=\"MarkdownCorrector_FastEval\") as run:\n",
    "    run_id = run.info.run_id\n",
    "    logger.info(f\"Starting MLflow run with ID: {run_id}\")\n",
    "    \n",
    "    # Log the model to MLflow\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=MLFLOW_MODEL_NAME,\n",
    "        python_model=MarkdownCorrectorModel(),\n",
    "        artifacts=artifacts,\n",
    "        pip_requirements=\"../requirements.txt\",\n",
    "        signature=signature,\n",
    "        registered_model_name=MLFLOW_MODEL_NAME,\n",
    "        input_example=pd.DataFrame([{\"repo_url\": None, \"files\": \"This is a testt.\"}])\n",
    "    )\n",
    "    logger.info(f\"✅ Model '{MLFLOW_MODEL_NAME}' logged successfully (fast eval version).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b506c45",
   "metadata": {},
   "source": [
    "## Prepare Eval Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdbe3a09",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 00:55:54 - INFO - Preparing data for evaluation…\n",
      "2025-08-08 00:55:56 - WARNING - No GITHUB_ACCESS_TOKEN found in environment or secrets.yaml.\n",
      "/opt/conda/lib/python3.12/site-packages/mlflow/pyfunc/model.py:1034: UserWarning: WARNING! low_vram is not default parameter.\n",
      "                low_vram was transferred to model_kwargs.\n",
      "                Please confirm that low_vram is what you intended.\n",
      "  python_model.load_context(context=context)\n",
      "/opt/conda/lib/python3.12/site-packages/mlflow/pyfunc/model.py:1034: UserWarning: WARNING! rope_scaling is not default parameter.\n",
      "                rope_scaling was transferred to model_kwargs.\n",
      "                Please confirm that rope_scaling is what you intended.\n",
      "  python_model.load_context(context=context)\n",
      "/opt/conda/lib/python3.12/site-packages/mlflow/pyfunc/model.py:1034: UserWarning: WARNING! num_threads is not default parameter.\n",
      "                num_threads was transferred to model_kwargs.\n",
      "                Please confirm that num_threads is what you intended.\n",
      "  python_model.load_context(context=context)\n",
      "2025-08-08 00:57:15 - INFO - ✅ Model context loaded with only instantaneous evaluation metrics.\n",
      "2025-08-08 00:57:15 - WARNING - No GITHUB_ACCESS_TOKEN found in environment or secrets.yaml.\n",
      "2025-08-08 00:57:15 - INFO - Repository visibility: public\n",
      "2025-08-08 00:57:16 - INFO - Raw markdown extraction complete.\n",
      "2025-08-08 00:57:16 - INFO - Starting prediction for 2 files...\n",
      "2025-08-08 00:57:16 - INFO -   - Predicting for: README1.md\n",
      "2025-08-08 01:02:13 - INFO -   - Predicting for: README2.md\n",
      "2025-08-08 01:16:23 - INFO - ✅ Prediction complete for all files.\n",
      "2025-08-08 01:16:23 - INFO - ✅ Created evaluation DataFrame with 2 file(s).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>corrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;h1 style=\"text-align: center; font-size: 40px...</td>\n",
       "      <td>&lt;h1 style=\"text-align: center; font-size: 40px...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td># 📜💬 Shakespeare text generation with RNN\\n\\n&lt;...</td>\n",
       "      <td># 📜💬 Shakespeare text generation with RNN\\n\\n&lt;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            original  \\\n",
       "0  <h1 style=\"text-align: center; font-size: 40px...   \n",
       "1  # 📜💬 Shakespeare text generation with RNN\\n\\n<...   \n",
       "\n",
       "                                           corrected  \n",
       "0  <h1 style=\"text-align: center; font-size: 40px...  \n",
       "1  # 📜💬 Shakespeare text generation with RNN\\n\\n<...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18min 58s, sys: 20.8 s, total: 19min 18s\n",
      "Wall time: 20min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "logger.info(\"Preparing data for evaluation…\")\n",
    "\n",
    "# Load the newly registered model\n",
    "model_uri    = f\"models:/{MLFLOW_MODEL_NAME}/latest\"\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "# Get original content from the test repo\n",
    "test_repo_url = \"https://github.com/hp-david/grammarbptest\"\n",
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Load secrets\n",
    "github_token = os.getenv(\"AIS_GITHUB_ACCESS_TOKEN\")\n",
    "if github_token:\n",
    "    secrets = {\"AIS_GITHUB_ACCESS_TOKEN\": github_token}\n",
    "    logger.info(\"Loaded GITHUB_ACCESS_TOKEN from environment variable.\")\n",
    "else:\n",
    "    try:\n",
    "        secrets_path = context.artifacts.get(\"secrets\")\n",
    "    except NameError:\n",
    "        secrets_path = \"../configs/secrets.yaml\" # Fallback for standalone execution\n",
    "\n",
    "    if secrets_path and os.path.exists(secrets_path):\n",
    "        with open(secrets_path, \"r\") as f:\n",
    "            secrets = yaml.safe_load(f)\n",
    "        logger.info(f\"Loaded secrets from {secrets_path}.\")\n",
    "    else:\n",
    "        secrets = {}\n",
    "        logger.warning(\"No GITHUB_ACCESS_TOKEN found in environment or secrets.yaml.\")\n",
    "\n",
    "# Instantiate and fetch raw markdowns\n",
    "processor = GitHubMarkdownProcessor(\n",
    "    repo_url=     test_repo_url,\n",
    "    access_token= secrets.get(\"AIS_GITHUB_ACCESS_TOKEN\") # Use .get() for safety\n",
    ")\n",
    "original_markdowns = processor.run()\n",
    "\n",
    "# Initialize a dictionary to store all corrected results\n",
    "all_corrected = {}\n",
    "logger.info(f\"Starting prediction for {len(original_markdowns)} files...\")\n",
    "\n",
    "# Loop through each file and call predict, just like the frontend\n",
    "for filename, content in original_markdowns.items():\n",
    "    logger.info(f\"  - Predicting for: {filename}\")\n",
    "    \n",
    "    # Create a dataframe with a SINGLE file's content string\n",
    "    test_input = pd.DataFrame([{\"repo_url\": None, \"files\": content}])\n",
    "    \n",
    "    # Run prediction for this single file\n",
    "    prediction_df = loaded_model.predict(test_input)\n",
    "    \n",
    "    # Extract the corrected content dictionary from the prediction\n",
    "    corrected_dict = prediction_df.loc[0, \"corrected\"]\n",
    "    \n",
    "    # Store the corrected text using the original filename as the key\n",
    "    all_corrected[filename] = corrected_dict.get(\"corrected_file.md\", content)\n",
    "\n",
    "logger.info(\"✅ Prediction complete for all files.\")\n",
    "\n",
    "eval_data = []\n",
    "for fn, orig in original_markdowns.items():\n",
    "    if fn in all_corrected:\n",
    "        eval_data.append({\n",
    "            \"original\":  orig,\n",
    "            \"corrected\": all_corrected[fn]\n",
    "        })\n",
    "\n",
    "evaluation_df = pd.DataFrame(eval_data)\n",
    "logger.info(f\"✅ Created evaluation DataFrame with {len(evaluation_df)} file(s).\")\n",
    "display(evaluation_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966b18e7",
   "metadata": {},
   "source": [
    "## Log Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0118e140",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 01:16:24 - INFO - Logging metrics for run ID: 8a7223d2332e4a7eb2fb889b18e432d7\n",
      "2025-08-08 01:16:24 - INFO - ✅ Metrics logged successfully.\n",
      "2025-08-08 01:16:24 - INFO - {\n",
      "  \"semantic_similarity\": 9.96603608439988,\n",
      "  \"readability_improvement\": -0.3435385604734904,\n",
      "  \"grammar_quality_score\": 10.0\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.6 ms, sys: 18.1 ms, total: 48.7 ms\n",
      "Wall time: 413 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with mlflow.start_run(run_id=run_id):\n",
    "    logger.info(f\"Logging metrics for run ID: {run_id}\")\n",
    "\n",
    "    metrics_to_log = prediction_df.loc[0, \"evaluation_metrics\"]\n",
    "\n",
    "    # Log the dictionary of metrics to MLflow\n",
    "    mlflow.log_metrics(metrics_to_log)\n",
    "\n",
    "    logger.info(\"✅ Metrics logged successfully.\")\n",
    "    logger.info(json.dumps(metrics_to_log, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624dad49",
   "metadata": {},
   "source": [
    "## Log Execution Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "299e0a3d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 01:16:24 - INFO - ⏱️ Total execution time: 57m 31.53s\n",
      "2025-08-08 01:16:24 - INFO - ✅ Notebook execution completed successfully.\n"
     ]
    }
   ],
   "source": [
    "end_time: float = time.time()\n",
    "elapsed_time: float = end_time - start_time\n",
    "elapsed_minutes: int = int(elapsed_time // 60)\n",
    "elapsed_seconds: float = elapsed_time % 60\n",
    "\n",
    "logger.info(f\"⏱️ Total execution time: {elapsed_minutes}m {elapsed_seconds:.2f}s\")\n",
    "logger.info(\"✅ Notebook execution completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4f4304-6084-4053-8155-e189715210fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
