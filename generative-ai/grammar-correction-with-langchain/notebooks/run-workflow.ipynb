{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fdb4bbf",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-size: 50px;\"> Run Workflow </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149608e8",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "- Configure the Environment\n",
    "- Define Constants and Paths\n",
    "- Load Configuratons and Secrets\n",
    "- Extract Markdown Files with Placeholders\n",
    "- Parse Markdown Files\n",
    "- Chunk Markdown Content\n",
    "- Initialize Model\n",
    "- Invoke Model on Each Chunk\n",
    "- Save Results\n",
    "- Log Execution Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115eb488",
   "metadata": {},
   "source": [
    "## Step 0: Configure the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6591e9f3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "# Configure logger\n",
    "logger: logging.Logger = logging.getLogger(\"run_workflow_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.propagate = False  # Prevent duplicate logs from parent loggers\n",
    "\n",
    "# Set formatter\n",
    "formatter: logging.Formatter = logging.Formatter(\n",
    "    fmt=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "# Configure and attach stream handler\n",
    "stream_handler: logging.StreamHandler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97410cf1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 08:47:35 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()  \n",
    "\n",
    "logger.info(\"Notebook execution started.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f73afb67",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 58 ms, sys: 31.6 ms, total: 89.5 ms\n",
      "Wall time: 2.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Required Libraries\n",
    "%pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "800f576c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import sys\n",
    "import difflib\n",
    "import json\n",
    "import re\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src directory to system path\n",
    "sys.path.append(str(Path(\"..\").resolve() / \"src\"))\n",
    "\n",
    "# Internal Modules\n",
    "from github_extractor import GitHubMarkdownProcessor\n",
    "from utils import load_config_and_secrets, initialize_llm\n",
    "from parser import parse_md_for_grammar_correction, restore_placeholders\n",
    "from chunker import chunk_markdown\n",
    "from prompt_templates import get_markdown_correction_prompt\n",
    "from markdown_correction_service import MarkdownCorrectionService\n",
    "\n",
    "# Other modules\n",
    "import mlflow\n",
    "from mlflow.models import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d8ce4f4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d56403",
   "metadata": {},
   "source": [
    "### Define Constants and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c21ae9c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "CONFIG_PATH = Path(\"../configs/configs.yaml\")\n",
    "SECRETS_PATH = Path(\"../configs/secrets.yaml\")\n",
    "LOCAL_MODEL_PATH = Path(\"/home/jovyan/datafabric/meta-llama3.1-8b-Q8/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84de48c5",
   "metadata": {},
   "source": [
    "### Load Configurations and Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cca78569",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 08:47:43 - WARNING - No GITHUB_ACCESS_TOKEN found in environment or secrets.yaml.\n",
      "2025-08-08 08:47:43 - WARNING - No GITHUB_ACCESS_TOKEN found in environment or secrets.yaml.\n"
     ]
    }
   ],
   "source": [
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Load secrets \n",
    "github_token = os.getenv(\"AIS_GITHUB_ACCESS_TOKEN\")\n",
    "if github_token:\n",
    "    secrets = {\"AIS_GITHUB_ACCESS_TOKEN\": github_token}\n",
    "    logger.info(\"Loaded GITHUB_ACCESS_TOKEN from environment variable.\")\n",
    "else:\n",
    "    secrets_path = SECRETS_PATH\n",
    "    if os.path.exists(secrets_path):\n",
    "        with open(secrets_path, \"r\") as f:\n",
    "            secrets = yaml.safe_load(f)\n",
    "        logger.info(f\"Loaded secrets from {secrets_path}.\")\n",
    "    else:\n",
    "        # If no token is found anywhere, initialize with an empty dict or handle as an error\n",
    "        secrets = {}\n",
    "        logger.warning(\"No GITHUB_ACCESS_TOKEN found in environment or secrets.yaml.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9594ff16",
   "metadata": {},
   "source": [
    "## Step 1: Extracting and Parsing Markdown Files From GitHub Repositories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1634fe",
   "metadata": {},
   "source": [
    "### Extract Markdown Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2012bfaf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 08:47:43 - INFO - Repository visibility: public\n",
      "2025-08-08 08:47:43 - INFO - Repository visibility: public\n",
      "2025-08-08 08:47:45 - INFO - Raw markdown extraction complete.\n",
      "2025-08-08 08:47:45 - INFO - Raw markdown extraction complete.\n"
     ]
    }
   ],
   "source": [
    "# Define repo URL and GitHub access token\n",
    "repo_url = \"https://github.com/hp-david/test/tree/main\" #\"https://github.com/HPInc/AI-Blueprints\" \n",
    "access_token = secrets.get(\"AIS_GITHUB_ACCESS_TOKEN\")\n",
    "\n",
    "# Create processor instance\n",
    "processor = GitHubMarkdownProcessor(repo_url=repo_url, access_token=access_token)\n",
    "\n",
    "# Run preprocessing workflow\n",
    "markdowns = processor.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc969753",
   "metadata": {},
   "source": [
    "### Parse Markdown Files with Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b6c5a8c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 08:47:45 - INFO - Parsed 5 files successfully\n",
      "2025-08-08 08:47:45 - INFO - Parsed 5 files successfully\n"
     ]
    }
   ],
   "source": [
    "parsed_markdowns = {}\n",
    "placeholder_maps = {}\n",
    "\n",
    "for filename, content in markdowns.items():\n",
    "    # Parse the content and get placeholder map\n",
    "    placeholder_map, processed_content = parse_md_for_grammar_correction(content)\n",
    "    \n",
    "    # Store the processed content (maintains dictionary structure for chunker)\n",
    "    parsed_markdowns[filename] = processed_content\n",
    "    \n",
    "    # Store the placeholder map for restoration\n",
    "    placeholder_maps[filename] = placeholder_map\n",
    "\n",
    "logger.info(f\"Parsed {len(parsed_markdowns)} files successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6342a312",
   "metadata": {},
   "source": [
    "### Chunk Markdown Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "545c7a75",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "all_chunks = {}  \n",
    "\n",
    "# Chunk each file's content and store the results in a dictionary\n",
    "for file_name, content in parsed_markdowns.items():\n",
    "    chunks = chunk_markdown(content)\n",
    "    all_chunks[file_name] = chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527b5a1c",
   "metadata": {},
   "source": [
    "### Display Chunks (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b4a37ce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor file_name, chunks in all_chunks.items():\\n    logger.info(f\"\\n===== {file_name} =====\\n\")\\n    for i, chunk in enumerate(chunks):\\n        logger.info(f\"\\n--- Chunk {i+1} ---\\n\")\\n        logger.info(chunk)\\n        logger.info(\"\\n\" + \"-\" * 40 + \"\\n\")\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for file_name, chunks in all_chunks.items():\n",
    "    logger.info(f\"\\n===== {file_name} =====\\n\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        logger.info(f\"\\n--- Chunk {i+1} ---\\n\")\n",
    "        logger.info(chunk)\n",
    "        logger.info(\"\\n\" + \"-\" * 40 + \"\\n\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cfa1f8",
   "metadata": {},
   "source": [
    "## Step 2: Correct Markdown Files with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89a5b195",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Get markdown correction prompt from prompt_templates module\n",
    "correction_prompt = get_markdown_correction_prompt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65823c9d",
   "metadata": {},
   "source": [
    "### Initialize Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6470bbf6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/IPython/core/magics/execution.py:1390: UserWarning: WARNING! low_vram is not default parameter.\n",
      "                low_vram was transferred to model_kwargs.\n",
      "                Please confirm that low_vram is what you intended.\n",
      "  exec(code, glob, local_ns)\n",
      "/opt/conda/lib/python3.12/site-packages/IPython/core/magics/execution.py:1390: UserWarning: WARNING! rope_scaling is not default parameter.\n",
      "                rope_scaling was transferred to model_kwargs.\n",
      "                Please confirm that rope_scaling is what you intended.\n",
      "  exec(code, glob, local_ns)\n",
      "/opt/conda/lib/python3.12/site-packages/IPython/core/magics/execution.py:1390: UserWarning: WARNING! num_threads is not default parameter.\n",
      "                num_threads was transferred to model_kwargs.\n",
      "                Please confirm that num_threads is what you intended.\n",
      "  exec(code, glob, local_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.44 s, sys: 3.65 s, total: 7.09 s\n",
      "Wall time: 1min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set the model source from the configs\n",
    "if \"model_source\" in config:\n",
    "    model_source = config[\"model_source\"]\n",
    "\n",
    "# Initialize llm \n",
    "llm = initialize_llm(model_source, secrets, str(LOCAL_MODEL_PATH))\n",
    "\n",
    "# Create the LLM chain with the correction prompt\n",
    "llm_chain = correction_prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7b3fb3",
   "metadata": {},
   "source": [
    "### Invoke Model on Each Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2eb6cc26",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 08:49:33 - INFO - chunk 0 done\n",
      "2025-08-08 08:49:33 - INFO - chunk 0 done\n",
      "2025-08-08 08:49:36 - INFO - chunk 1 done\n",
      "2025-08-08 08:49:36 - INFO - chunk 1 done\n",
      "2025-08-08 08:49:38 - INFO - chunk 2 done\n",
      "2025-08-08 08:49:38 - INFO - chunk 2 done\n",
      "2025-08-08 08:49:42 - INFO - chunk 3 done\n",
      "2025-08-08 08:49:42 - INFO - chunk 3 done\n",
      "2025-08-08 08:49:46 - INFO - chunk 4 done\n",
      "2025-08-08 08:49:46 - INFO - chunk 4 done\n",
      "2025-08-08 08:49:48 - INFO - chunk 5 done\n",
      "2025-08-08 08:49:48 - INFO - chunk 5 done\n",
      "2025-08-08 08:49:50 - INFO - chunk 6 done\n",
      "2025-08-08 08:49:50 - INFO - chunk 6 done\n",
      "2025-08-08 08:49:57 - INFO - chunk 7 done\n",
      "2025-08-08 08:49:57 - INFO - chunk 7 done\n",
      "2025-08-08 08:50:01 - INFO - chunk 8 done\n",
      "2025-08-08 08:50:01 - INFO - chunk 8 done\n",
      "2025-08-08 08:50:17 - INFO - chunk 9 done\n",
      "2025-08-08 08:50:17 - INFO - chunk 9 done\n",
      "2025-08-08 08:50:19 - INFO - chunk 10 done\n",
      "2025-08-08 08:50:19 - INFO - chunk 10 done\n",
      "2025-08-08 08:50:21 - INFO - chunk 11 done\n",
      "2025-08-08 08:50:21 - INFO - chunk 11 done\n",
      "2025-08-08 08:50:27 - INFO - chunk 12 done\n",
      "2025-08-08 08:50:27 - INFO - chunk 12 done\n",
      "2025-08-08 08:50:28 - INFO - chunk 13 done\n",
      "2025-08-08 08:50:28 - INFO - chunk 13 done\n",
      "2025-08-08 08:50:29 - INFO - chunk 14 done\n",
      "2025-08-08 08:50:29 - INFO - chunk 14 done\n",
      "2025-08-08 08:50:31 - INFO - chunk 15 done\n",
      "2025-08-08 08:50:31 - INFO - chunk 15 done\n",
      "2025-08-08 08:50:33 - INFO - chunk 16 done\n",
      "2025-08-08 08:50:33 - INFO - chunk 16 done\n",
      "2025-08-08 08:50:36 - INFO - chunk 17 done\n",
      "2025-08-08 08:50:36 - INFO - chunk 17 done\n",
      "2025-08-08 08:50:45 - INFO - chunk 18 done\n",
      "2025-08-08 08:50:45 - INFO - chunk 18 done\n",
      "2025-08-08 08:50:50 - INFO - chunk 19 done\n",
      "2025-08-08 08:50:50 - INFO - chunk 19 done\n",
      "2025-08-08 08:50:51 - INFO - chunk 20 done\n",
      "2025-08-08 08:50:51 - INFO - chunk 20 done\n",
      "2025-08-08 08:50:53 - INFO - chunk 21 done\n",
      "2025-08-08 08:50:53 - INFO - chunk 21 done\n",
      "2025-08-08 08:50:55 - INFO - chunk 22 done\n",
      "2025-08-08 08:50:55 - INFO - chunk 22 done\n",
      "2025-08-08 08:51:00 - INFO - chunk 23 done\n",
      "2025-08-08 08:51:00 - INFO - chunk 23 done\n",
      "2025-08-08 08:51:02 - INFO - chunk 24 done\n",
      "2025-08-08 08:51:02 - INFO - chunk 24 done\n",
      "2025-08-08 08:51:04 - INFO - chunk 25 done\n",
      "2025-08-08 08:51:04 - INFO - chunk 25 done\n",
      "2025-08-08 08:51:05 - INFO - chunk 26 done\n",
      "2025-08-08 08:51:05 - INFO - chunk 26 done\n",
      "2025-08-08 08:51:07 - INFO - chunk 27 done\n",
      "2025-08-08 08:51:07 - INFO - chunk 27 done\n",
      "2025-08-08 08:51:08 - INFO - chunk 28 done\n",
      "2025-08-08 08:51:08 - INFO - chunk 28 done\n",
      "2025-08-08 08:51:09 - INFO - chunk 29 done\n",
      "2025-08-08 08:51:09 - INFO - chunk 29 done\n",
      "2025-08-08 08:51:13 - INFO - chunk 30 done\n",
      "2025-08-08 08:51:13 - INFO - chunk 30 done\n",
      "2025-08-08 08:51:20 - INFO - chunk 31 done\n",
      "2025-08-08 08:51:20 - INFO - chunk 31 done\n",
      "2025-08-08 08:51:23 - INFO - chunk 32 done\n",
      "2025-08-08 08:51:23 - INFO - chunk 32 done\n",
      "2025-08-08 08:51:25 - INFO - chunk 33 done\n",
      "2025-08-08 08:51:25 - INFO - chunk 33 done\n",
      "2025-08-08 08:51:30 - INFO - chunk 34 done\n",
      "2025-08-08 08:51:30 - INFO - chunk 34 done\n",
      "2025-08-08 08:51:42 - INFO - chunk 35 done\n",
      "2025-08-08 08:51:42 - INFO - chunk 35 done\n",
      "2025-08-08 08:51:45 - INFO - chunk 36 done\n",
      "2025-08-08 08:51:45 - INFO - chunk 36 done\n",
      "2025-08-08 08:52:02 - INFO - chunk 37 done\n",
      "2025-08-08 08:52:02 - INFO - chunk 37 done\n",
      "2025-08-08 08:52:03 - INFO - chunk 38 done\n",
      "2025-08-08 08:52:03 - INFO - chunk 38 done\n",
      "2025-08-08 08:52:06 - INFO - chunk 39 done\n",
      "2025-08-08 08:52:06 - INFO - chunk 39 done\n",
      "2025-08-08 08:52:09 - INFO - chunk 40 done\n",
      "2025-08-08 08:52:09 - INFO - chunk 40 done\n",
      "2025-08-08 08:52:27 - INFO - chunk 41 done\n",
      "2025-08-08 08:52:27 - INFO - chunk 41 done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 54s, sys: 1.35 s, total: 2min 56s\n",
      "Wall time: 2min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "results = []\n",
    "count = 0\n",
    "\n",
    "# Process each chunk through the language model and store the results\n",
    "for file_name, chunks in all_chunks.items():  \n",
    "    for chunk in chunks:\n",
    "        # Send the chunks to the llm for correction\n",
    "        response = llm_chain.invoke({\"markdown\": chunk})\n",
    "\n",
    "        # Store the file name, original text, and corrected text\n",
    "        results.append({\n",
    "            \"file\": file_name,\n",
    "            \"original\": chunk,\n",
    "            \"corrected\": response\n",
    "        })\n",
    "\n",
    "        # Log progress (optional)\n",
    "        logger.info(f\"chunk {count} done\")\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4fa12",
   "metadata": {},
   "source": [
    "### Display Corrected Chunks (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b4223c3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor result in results:\\n    original_text = result[\"original\"]\\n    corrected_text = result[\"corrected\"]\\n\\n    original_tokens = len(llm.client.tokenize(original_text.encode(\"utf-8\")))\\n    corrected_tokens = len(llm.client.tokenize(corrected_text.encode(\"utf-8\")))\\n\\n    logger.info(f\"\\n===== {result[\\'file\\']} =====\\n\")\\n    logger.info(f\"--- Original ({original_tokens} tokens) ---\\n\")\\n    logger.info(original_text)\\n    logger.info(f\"\\n--- Corrected ({corrected_tokens} tokens) ---\\n\")\\n    logger.info(corrected_text)\\n    logger.info(\"\\n\" + \"=\" * 60 + \"\\n\")\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for result in results:\n",
    "    original_text = result[\"original\"]\n",
    "    corrected_text = result[\"corrected\"]\n",
    "    \n",
    "    original_tokens = len(llm.client.tokenize(original_text.encode(\"utf-8\")))\n",
    "    corrected_tokens = len(llm.client.tokenize(corrected_text.encode(\"utf-8\")))\n",
    "\n",
    "    logger.info(f\"\\n===== {result['file']} =====\\n\")\n",
    "    logger.info(f\"--- Original ({original_tokens} tokens) ---\\n\")\n",
    "    logger.info(original_text)\n",
    "    logger.info(f\"\\n--- Corrected ({corrected_tokens} tokens) ---\\n\")\n",
    "    logger.info(corrected_text)\n",
    "    logger.info(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3616c50",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5503d8fa",
   "metadata": {},
   "source": [
    "#### Save Raw Corrrected Markdowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c1f2da8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Helper: Safe chunk joiner.\n",
    "def safe_join_chunks(chunks: List[str]) -> str:\n",
    "    \"\"\"Rejoins a list of text chunks into a single string, preserving formatting and sentence boundaries.\n",
    "\n",
    "    Ensures that chunks split mid-sentence get a space inserted appropriately.\n",
    "\n",
    "    Args:\n",
    "        chunks (List[str]): A list of processed text segments.\n",
    "\n",
    "    Returns:\n",
    "        str: The reassembled markdown text.\n",
    "    \"\"\"\n",
    "    joined = \"\"\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if i == 0:\n",
    "            joined += chunk\n",
    "        else:\n",
    "            prev = chunks[i - 1].rstrip()\n",
    "            curr = chunk\n",
    "\n",
    "            # Heuristic: Detect if a sentence was split across two chunks.\n",
    "            if prev.endswith('.') and re.match(r'^[A-Z\\\"]', curr.lstrip()):\n",
    "                # If it's a sentence break, add a single space to separate them.\n",
    "                joined += ' ' + curr.lstrip()\n",
    "            else:\n",
    "                # Otherwise, join the chunk directly.\n",
    "                joined += curr  \n",
    "    return joined\n",
    "\n",
    "\n",
    "# Group corrected chunks by file\n",
    "corrected_chunks_by_file = defaultdict(list)\n",
    "for result in results:\n",
    "    corrected_chunks_by_file[result[\"file\"]].append(result[\"corrected\"])\n",
    "\n",
    "# Rebuild each file from its corrected chunks with smart joining\n",
    "rebuilt_corrected_files = {\n",
    "    file_name: safe_join_chunks(chunks)\n",
    "    for file_name, chunks in corrected_chunks_by_file.items()\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(\"corrected\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Restore placeholders and write final output\n",
    "for file_name, corrected_content in rebuilt_corrected_files.items():\n",
    "    placeholder_map = placeholder_maps.get(file_name, {})\n",
    "    restored_content = restore_placeholders(corrected_content, placeholder_map)\n",
    "\n",
    "    file_path = Path(file_name)\n",
    "\n",
    "    # Construct the output path while preserving the original directory structure.\n",
    "    output_path = output_dir / file_path.parent / f\"{file_path.stem}_{timestamp}{file_path.suffix}\"\n",
    "\n",
    "    # This line is now crucial for creating the subdirectories.\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(restored_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebcf303",
   "metadata": {},
   "source": [
    "#### Save Corrected Markdowns with Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d272f2d0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 2 (Corrected)\n",
    "\n",
    "diff_output_dir = Path(\"corrected-diffs\")\n",
    "diff_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for file_name, corrected_content in rebuilt_corrected_files.items():\n",
    "    placeholder_map = placeholder_maps.get(file_name, {})\n",
    "    restored_content = restore_placeholders(corrected_content, placeholder_map)\n",
    "\n",
    "    # Get original content from markdowns dict\n",
    "    original_content = markdowns.get(file_name)\n",
    "    if original_content is None:\n",
    "        logger.info(f\"Warning: No original content for file {file_name}\")\n",
    "        continue\n",
    "\n",
    "    # Create unified diff view (HTML side-by-side)\n",
    "    differ = difflib.HtmlDiff(tabsize=4, wrapcolumn=80)\n",
    "    diff_html = differ.make_file(\n",
    "        original_content.splitlines(),\n",
    "        restored_content.splitlines(),\n",
    "        fromdesc=f\"Original: {file_name}\",\n",
    "        todesc=f\"Corrected: {file_name}\",\n",
    "        context=True,\n",
    "        numlines=3\n",
    "    )\n",
    "\n",
    "    file_path = Path(file_name)\n",
    "\n",
    "    # Construct the diff path while preserving the original directory structure.\n",
    "    diff_path = diff_output_dir / file_path.parent / f\"{file_path.stem}_{timestamp}{file_path.suffix}.html\"\n",
    "    \n",
    "    # Ensure the subdirectories exist in the diffs folder too.\n",
    "    diff_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(diff_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(diff_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7589c5ca",
   "metadata": {},
   "source": [
    "#### Save Chunks Into a JSON for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "932307d3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "results_path = Path(\"results.json\")\n",
    "with results_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef870f3",
   "metadata": {},
   "source": [
    "### Log Execution Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66633023",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 08:52:27 - INFO - ⏱️ Total execution time: 4m 52.07s\n",
      "2025-08-08 08:52:27 - INFO - ⏱️ Total execution time: 4m 52.07s\n",
      "2025-08-08 08:52:27 - INFO - ✅ Notebook execution completed successfully.\n",
      "2025-08-08 08:52:27 - INFO - ✅ Notebook execution completed successfully.\n"
     ]
    }
   ],
   "source": [
    "end_time: float = time.time()\n",
    "elapsed_time: float = end_time - start_time\n",
    "elapsed_minutes: int = int(elapsed_time // 60)\n",
    "elapsed_seconds: float = elapsed_time % 60\n",
    "\n",
    "logger.info(f\"⏱️ Total execution time: {elapsed_minutes}m {elapsed_seconds:.2f}s\")\n",
    "logger.info(\"✅ Notebook execution completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
