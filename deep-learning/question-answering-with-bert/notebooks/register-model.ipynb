{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c24de73e",
   "metadata": {},
   "source": [
    "<h1 style=\\\"text-align: center; font-size: 50px;\\\"> Pre-trained BERT for building a Q&A system MLflow integration </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5552d71",
   "metadata": {},
   "source": [
    "Notebook Overview\n",
    "- Start Execution\n",
    "- User Constants\n",
    "- Install and Import Libraries\n",
    "- Configure Settings\n",
    "- Logging Model to MLflow\n",
    "- Fetching the Latest Model Version from MLflow\n",
    "- Loading the Model and Running Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba030528",
   "metadata": {},
   "source": [
    "## Start Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56ddaf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "# Configure logger\n",
    "logger: logging.Logger = logging.getLogger(\"register_model_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.propagate = False  # Prevent duplicate logs from parent loggers\n",
    "\n",
    "# Set formatter\n",
    "formatter: logging.Formatter = logging.Formatter(\n",
    "    fmt=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "# Configure and attach stream handler\n",
    "stream_handler: logging.StreamHandler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "003ca508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-18 16:20:40 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()  \n",
    "\n",
    "logger.info(\"Notebook execution started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68d7c5f",
   "metadata": {},
   "source": [
    "## User Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80a52be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT = \"Marta is mother of John and Amanda\"\n",
    "QUESTION = \"what is the name of Marta's daugther?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1772107f",
   "metadata": {},
   "source": [
    "## Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce5606bf-ca4a-4c4d-8244-1dc1d8eb0f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 40 ms, sys: 1.21 ms, total: 41.2 ms\n",
      "Wall time: 1.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "%pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5910e323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-18 16:20:46.891843: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-18 16:20:46.904942: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752855646.922525    5843 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752855646.927906    5843 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752855646.941729    5843 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752855646.941764    5843 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752855646.941765    5843 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752855646.941766    5843 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-18 16:20:46.946686: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------ Standard Library Imports ------------------------\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# ------------------------ MLflow for Experiment Tracking and Model Management ------------------------\n",
    "import mlflow\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "from mlflow.types import ParamSchema, ParamSpec\n",
    "from mlflow.models import ModelSignature\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "# ------------------------ Third-Party Libraries ------------------------\n",
    "from transformers import  pipeline\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4301e6d1",
   "metadata": {},
   "source": [
    "## Configure Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8797dd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bf4a5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- Define global experiment and run names to be used throughout the notebook -------------------------\n",
    "\n",
    "MODEL_CHECKPOINT = \"distilbert-base-cased\"\n",
    "EXPERIMENT_SET = \"BERT Q&A - distilbert-base-cased\"\n",
    "SAVE_MODEL_NAME = \"distilbert_bertqa\"\n",
    "EXPERIMENT_NAME = \"BERT model for Q&A\"\n",
    "MODEL_NAME = \"BERT_QA\"\n",
    "RUN_NAME = 'BERT_QA'\n",
    "NAME = 'BERT_QA'\n",
    "\n",
    "# ------------------------- Set up the chunk separator for text processing -------------------------\n",
    "CHUNK_SEPARATOR = \"\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c2b1cf",
   "metadata": {},
   "source": [
    "## Logging Model to MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d56edc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/opt/conda/lib/python3.12/site-packages/mlflow/pyfunc/utils/data_validation.py:168: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
      "  color_warning(\n"
     ]
    }
   ],
   "source": [
    "model_name = SAVE_MODEL_NAME\n",
    "qa_pipeline = pipeline(\n",
    "    'question-answering',\n",
    "    model = model_name,\n",
    "    device=0 if torch.cuda.is_available() else -1  #GPU if available, otherwise CPU\n",
    ")\n",
    "\n",
    "class DistilBERTModel(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the model wrapper\"\"\"\n",
    "        self.model = None\n",
    "        \n",
    "    def _preprocess(self, inputs):\n",
    "        \"\"\"\n",
    "        Preprocesses the input data.\n",
    "\n",
    "        Args:\n",
    "            inputs: A dictionary containing two keys:\n",
    "                - 'context': A list with the context text.\n",
    "                - 'question': A list with the question to be answered.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the context (str) and the question (str).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            context = inputs['context'][0]\n",
    "            question = inputs['question'][0]\n",
    "            logger.info(f\"Preprocessing - Context: {context[:50]}..., Question: {question}\")\n",
    "            return context, question\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error preprocessing the input data: {str(e)}\")  \n",
    "            raise\n",
    "\n",
    "    def load_context(self, context):\n",
    "        \"\"\"\n",
    "        Loads the question-answering pipeline using the saved model artifact.\n",
    "\n",
    "        Args:\n",
    "            context: The MLflow context object \n",
    "                containing the loaded artifacts.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from transformers import pipeline\n",
    "            \n",
    "            # Initialize the pipeline with the model artifacts\n",
    "            self.model = pipeline(\n",
    "                'question-answering',\n",
    "                model=context.artifacts[\"model\"],\n",
    "                device=0 if torch.cuda.is_available() else -1 \n",
    "            )\n",
    "            logger.info(\"Question-answering pipeline loaded successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading the question-answering pipeline: {str(e)}\")\n",
    "            # Try loading without device specification for compatibility\n",
    "            try:\n",
    "                self.model = pipeline(\n",
    "                    'question-answering',\n",
    "                    model=context.artifacts[\"model\"]\n",
    "                )\n",
    "                logger.info(\"Question-answering pipeline loaded successfully (CPU fallback)\")\n",
    "            except Exception as fallback_error:\n",
    "                logger.error(f\"Failed to load pipeline even with fallback: {str(fallback_error)}\")\n",
    "                raise\n",
    "\n",
    "    def predict(self, context, model_input, params=None):\n",
    "        \"\"\"\n",
    "        Runs inference using the loaded model and input data.\n",
    "\n",
    "        Args:\n",
    "            context: The MLflow context object \n",
    "                with access to artifacts.\n",
    "            model_input: A dictionary containing 'context' and 'question' keys.\n",
    "            params: Optional parameters for inference.\n",
    "\n",
    "        Returns:\n",
    "            The output from the model containing the predicted answer and optionally the score.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check if model is loaded\n",
    "            if self.model is None:\n",
    "                raise ValueError(\"Model not loaded. Please ensure load_context was called successfully.\")\n",
    "                \n",
    "            in_ctx, question = self._preprocess(model_input)\n",
    "            output = self.model(context=in_ctx, question=question)\n",
    "            \n",
    "            # Handle params if provided\n",
    "            if params and params.get('show_score', False):\n",
    "                return output\n",
    "            else:\n",
    "                # Return only essential fields for backward compatibility\n",
    "                return {\n",
    "                    'answer': output.get('answer', ''),\n",
    "                    'score': output.get('score', 0.0),\n",
    "                    'start': output.get('start', 0),\n",
    "                    'end': output.get('end', 0)\n",
    "                }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error running inference: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    @classmethod\n",
    "    def log_model(cls, model_name, source_trainer = None, source_pipeline = None, demo_folder=\"../demo\", config_path=\"../configs/config.yaml\"):\n",
    "        \"\"\"\n",
    "        Logs the model to MLflow, including artifacts, dependencies, and input/output signatures.\n",
    "\n",
    "        Args:\n",
    "            model_name: Path where the model will be temporarily saved before logging.\n",
    "            source_trainer: A trainer object with a `.save_model()` method. Defaults to None.\n",
    "            source_pipeline: A pipeline object with a `.save_pretrained()` method. Defaults to None.\n",
    "            demo_folder: Path to the folder containing the compiled demo UI. Defaults to \"demo\".\n",
    "            config_path: Path to configuration file. Defaults to \"../configs/config.yaml\".\n",
    "        \"\"\"\n",
    "        try:\n",
    "            input_schema = Schema(\n",
    "            [\n",
    "                ColSpec(\"string\", \"context\"),\n",
    "                ColSpec(\"string\", \"question\"),\n",
    "            ]\n",
    "            )\n",
    "            output_schema = Schema(\n",
    "                [\n",
    "                    ColSpec(\"string\", \"answer\")\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            params_schema = ParamSchema(\n",
    "                [\n",
    "                    ParamSpec(\"show_score\", \"boolean\", False)\n",
    "                ]\n",
    "            )\n",
    "          \n",
    "            signature = ModelSignature(inputs=input_schema, outputs=output_schema, params=params_schema)\n",
    "            if source_trainer is not None:\n",
    "                source_trainer.save_model(model_name)\n",
    "            elif source_pipeline is not None:\n",
    "                source_pipeline.save_pretrained(model_name)\n",
    "\n",
    "            requirements = [\n",
    "                \"transformers==4.53.2\",\n",
    "                \"tf_keras==2.19.0\",\n",
    "                \"torch==2.4.1\"\n",
    "            ]\n",
    "            mlflow.pyfunc.log_model(\n",
    "                model_name,\n",
    "                python_model=cls(),\n",
    "                artifacts={\"model\": model_name, \"demo\": demo_folder, \"config\": config_path},\n",
    "                signature=signature,\n",
    "                pip_requirements=\n",
    "                requirements\n",
    "            \n",
    "            )\n",
    "            shutil.rmtree(model_name)\n",
    "            logger.info(\"Logging model to MLflow done successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error logging model to MLflow: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d659ac2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-18 16:21:11 - INFO - Run's Artifact URI: /phoenix/mlflow/778705262323411690/1823e802c55447b8801189f84a5fa407/artifacts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e8dae3fbd944c9bd272dd76b2a4109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0b07a8b17248eebeeecfd2b0363a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-18 16:21:24 - INFO - Logging model to MLflow done successfully\n",
      "Registered model 'BERT_QA' already exists. Creating a new version of this model...\n",
      "Created version '11' of model 'BERT_QA'.\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_tracking_uri('/phoenix/mlflow')\n",
    "mlflow.set_experiment(experiment_name = EXPERIMENT_NAME)\n",
    "with mlflow.start_run(run_name= RUN_NAME) as run:\n",
    "    logger.info(f\"Run's Artifact URI: {run.info.artifact_uri}\")\n",
    "    DistilBERTModel.log_model(model_name = MODEL_NAME, source_pipeline=qa_pipeline)\n",
    "    mlflow.register_model(model_uri = f\"runs:/{run.info.run_id}/{MODEL_NAME}\", name = NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec2f617",
   "metadata": {},
   "source": [
    "## Fetching the Latest Model Version from MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "629ad3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 inputs: \n",
      "  ['context': string (required), 'question': string (required)]\n",
      "outputs: \n",
      "  ['answer': string (required)]\n",
      "params: \n",
      "  ['show_score': boolean (default: False)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client = mlflow.MlflowClient()\n",
    "model_metadata = client.get_latest_versions(MODEL_NAME, stages=[\"None\"])\n",
    "latest_model_version = model_metadata[0].version\n",
    "print(latest_model_version, mlflow.models.get_model_info(f\"models:/BERT_QA/{latest_model_version}\").signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe875f6",
   "metadata": {},
   "source": [
    "## Loading the Model and Running Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "793510ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "2025-07-18 16:21:47 - INFO - Question-answering pipeline loaded successfully\n",
      "2025-07-18 16:21:47 - INFO - Preprocessing - Context: ['Marta is mother of John and Amanda']..., Question: [\"what is the name of Marta's daugther?\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': 'John and Amanda',\n",
       " 'score': 0.3556230068206787,\n",
       " 'start': 19,\n",
       " 'end': 34}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = mlflow.pyfunc.load_model(model_uri=f\"models:/BERT_QA/{latest_model_version}\")\n",
    "context = CONTEXT\n",
    "question = QUESTION\n",
    "model.predict({\"context\": [context], \"question\":[question]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49bc16ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-18 16:21:47 - INFO - ⏱️ Total execution time: 1m 6.98s\n",
      "2025-07-18 16:21:47 - INFO - ✅ Notebook execution completed successfully.\n"
     ]
    }
   ],
   "source": [
    "end_time: float = time.time()\n",
    "elapsed_time: float = end_time - start_time\n",
    "elapsed_minutes: int = int(elapsed_time // 60)\n",
    "elapsed_seconds: float = elapsed_time % 60\n",
    "\n",
    "logger.info(f\"⏱️ Total execution time: {elapsed_minutes}m {elapsed_seconds:.2f}s\")\n",
    "logger.info(\"✅ Notebook execution completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4b56a8",
   "metadata": {},
   "source": [
    "Built with ❤️ using Z by HP AI Studio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
