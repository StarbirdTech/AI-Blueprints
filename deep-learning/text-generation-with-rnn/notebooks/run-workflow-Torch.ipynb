{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\\\"text-align: center; font-size: 50px;\\\"> üìú Text Generation with Neural Networks and Torch</h1>\n",
    "\n",
    "In this notebook our objective is to demonstrate how to generate text using a character-based RNN and Torch working with a dataset of Shakespeare's writing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "- Start Execution\n",
    "- Install and Import Libraries\n",
    "- Configure Settings\n",
    "- Verify Assets\n",
    "- Get Text Data\n",
    "- Preparing textual data\n",
    "- One Hot Encoding\n",
    "- Creating Training Batches\n",
    "- Creating the LSTM Model\n",
    "- Training the Network\n",
    "- Generating Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "# Configure logger\n",
    "logger: logging.Logger = logging.getLogger(\"run_workflow_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.propagate = False  # Prevent duplicate logs from parent loggers\n",
    "\n",
    "# Set formatter\n",
    "formatter: logging.Formatter = logging.Formatter(\n",
    "    fmt=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "# Configure and attach stream handler\n",
    "stream_handler: logging.StreamHandler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()  \n",
    "\n",
    "logger.info(\"Notebook execution started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc3f805bc70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard Library Imports\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Third-Party Libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ Define global experiment and run names to be used throughout the notebook ------------------------\n",
    "EXPERIMENT_SET = \"RNN text generation\"\n",
    "RUN_NAME = \"RNN Text Generation\"\n",
    "MODEL_NAME = \"dict_torch_rnn_model\"\n",
    "TORCH_MODEL = \"dict_torch_rnn_model.pt\"\n",
    "REGISTER_NAME = \"Shakespeare_Model\"\n",
    "EXPERIMENT_NAME = \"Shakespeare Text Generation\"\n",
    "\n",
    "# ------------------------ Paths ------------------------\n",
    "DATA_PATH = \"../data/shakespeare.txt\"\n",
    "MODEL_DECODER_PATH = \"models/decoder.pt\"\n",
    "MODEL_ENCODER_PATH = \"models/encoder.pt\"\n",
    "MODEL_PATH = 'models/dict_torch_rnn_model.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 22:04:50 - INFO - Shakespeare text is properly configured. \n",
      "2025-06-24 22:04:50 - INFO - Decoder model is properly configured. \n",
      "2025-06-24 22:04:50 - INFO - Encoder model is properly configured. \n",
      "2025-06-24 22:04:50 - INFO - Rnn model is properly configured. \n"
     ]
    }
   ],
   "source": [
    "def log_asset_status(asset_path: str, asset_name: str, success_message: str, failure_message: str) -> None:\n",
    "    \"\"\"\n",
    "    Logs the status of a given asset based on its existence.\n",
    "\n",
    "    Parameters:\n",
    "        asset_path (str): File or directory path to check.\n",
    "        asset_name (str): Name of the asset for logging context.\n",
    "        success_message (str): Message to log if asset exists.\n",
    "        failure_message (str): Message to log if asset does not exist.\n",
    "    \"\"\"\n",
    "    if Path(asset_path).exists():\n",
    "        logger.info(f\"{asset_name} is properly configured. {success_message}\")\n",
    "    else:\n",
    "        logger.info(f\"{asset_name} is not properly configured. {failure_message}\")\n",
    "        \n",
    "log_asset_status(\n",
    "    asset_path=DATA_PATH,\n",
    "    asset_name=\"Shakespeare text\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please create and download the required assets in your project on AI Studio.\"\n",
    ")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=MODEL_DECODER_PATH ,\n",
    "    asset_name=\"Decoder model\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if model folder was properly downloaded in your project on AI Studio.\"\n",
    ")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=MODEL_ENCODER_PATH,\n",
    "    asset_name=\"Encoder model\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if model folder was properly downloaded in your project on AI Studio.\"\n",
    ")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=MODEL_PATH,\n",
    "    asset_name=\"Rnn model\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if model folder was properly downloaded in your project on AI Studio.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the text we'll use as a basis for our generations: let's try to generate 'Shakespearean' texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This text is from Shakespeare's Sonnet 1. It's one of the 154 sonnets written by William Shakespeare that were first published in 1609. This particular sonnet, like many others, discusses themes of beauty, procreation, and the transient nature of life, urging the beautiful to reproduce so their beauty can live on through their offspring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH,'r',encoding='utf8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 22:04:50 - INFO - First 600 chars: \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bud buriest thy content,\n",
      "  And tender churl mak'st waste in niggarding:\n",
      "    Pity the world, or else th\n"
     ]
    }
   ],
   "source": [
    "logger.info('First 600 chars: \\n')\n",
    "print(text[:600])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing textual data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to encode our data to give the model a proper numerical representation of our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_characters = set(text) # creates a set of unique characters found in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = dict(enumerate(all_characters))\n",
    "# assigns a unique integer to each character in a dictionary format, \n",
    "# creating a mapping that can later be used to transform encoded predictions back into characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = {char: ind for ind, char in decoder.items()} \n",
    "# reverses the decoder dictionary, providing a mapping from characters to their respective assigned integers, which is used to encode the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(decoder, MODEL_DECODER_PATH)\n",
    "torch.save(encoder, MODEL_ENCODER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = np.array([encoder[char] for char in text])\n",
    "# encodes the entire text as an array of integers, with each integer representing the character at that position\n",
    "#in the text according to the encoder dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding is a way to convert categorical data into a fixed-size vector of numerical values.\n",
    "\n",
    "This encoding allows the model to treat input data uniformly and is particularly important for models that need to determine the presence or absence of a feature, such as a particular character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(encoded_text, num_uni_chars):\n",
    "    \"\"\"\n",
    "        Convert categorical data into a fixed-size vector of numerical values.\n",
    "\n",
    "        Args:\n",
    "            encoded_text: Batch of encoded text.\n",
    "            num_uni_chars: Number of unique characters\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a placeholder for zeros\n",
    "        one_hot = np.zeros((encoded_text.size, num_uni_chars))\n",
    "        \n",
    "        # Convert data type for later use with pytorch\n",
    "        one_hot = one_hot.astype(np.float32)\n",
    "\n",
    "        # Using indexing fill in the 1s at the correct index locations\n",
    "        one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0\n",
    "        \n",
    "        # Reshape it so it matches the batch shape\n",
    "        one_hot = one_hot.reshape((*encoded_text.shape, num_uni_chars))\n",
    "        \n",
    "        return one_hot\n",
    "    except Exception as e:\n",
    "            logger.error(f\"Error converting categorical data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Training Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training batches are a way of dividing the dataset into smaller, manageable groups of data points that are fed into a machine learning model during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(encoded_text, samp_per_batch=10, seq_len=50):\n",
    "    \n",
    "    '''\n",
    "    Generate (using yield) batches for training.\n",
    "    \n",
    "    X: Encoded Text of length seq_len\n",
    "    Y: Encoded Text shifted by one\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "    X:\n",
    "    \n",
    "    [[1 2 3]]\n",
    "    \n",
    "    Y:\n",
    "    \n",
    "    [[ 2 3 4]]\n",
    "    \n",
    "    encoded_text : Complete Encoded Text to make batches from\n",
    "    batch_size : Number of samples per batch\n",
    "    seq_len : Length of character sequence\n",
    "       \n",
    "    '''\n",
    "    try:\n",
    "        # Total number of characters per batch\n",
    "        # Example: If samp_per_batch is 2 and seq_len is 50, then 100\n",
    "        # characters come out per batch.\n",
    "        char_per_batch = samp_per_batch * seq_len\n",
    "        \n",
    "        # Number of batches available to make\n",
    "        # Use int() to roun to nearest integer\n",
    "        num_batches_avail = int(len(encoded_text)/char_per_batch)\n",
    "        \n",
    "        # Cut off end of encoded_text that\n",
    "        # won't fit evenly into a batch\n",
    "        encoded_text = encoded_text[:num_batches_avail * char_per_batch]\n",
    "        \n",
    "        # Reshape text into rows the size of a batch\n",
    "        encoded_text = encoded_text.reshape((samp_per_batch, -1))\n",
    "\n",
    "        # Go through each row in array.\n",
    "        for n in range(0, encoded_text.shape[1], seq_len):\n",
    "            # Grab feature characters\n",
    "            x = encoded_text[:, n:n+seq_len]\n",
    "            # y is the target shifted over by 1\n",
    "            y = np.zeros_like(x)\n",
    "            try:\n",
    "                y[:, :-1] = x[:, 1:]\n",
    "                y[:, -1]  = encoded_text[:, n+seq_len]\n",
    "            except:\n",
    "                y[:, :-1] = x[:, 1:]\n",
    "                y[:, -1] = encoded_text[:, 0]\n",
    "                \n",
    "            yield x, y\n",
    "    except Exception as e:\n",
    "            logger.error(f\"Error Generating batches: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharModel(nn.Module):\n",
    "    def __init__(self, decoder, encoder, all_chars, num_hidden=256, num_layers=4,drop_prob=0.5, use_gpu=False):\n",
    "        \"\"\"Initializes CharModel\n",
    "\n",
    "        Args:\n",
    "            decoder: Assigns a unique integer to each character in a dictionary format\n",
    "            encoder : Reverses the decoder dictionary, providing a mapping from characters to their respective assigned integers.\n",
    "            all_chars: Set of unique characters found in the text.\n",
    "            num_hidden: Number of hidden layers. Defaults to 256.\n",
    "            num_layers: Number of layers. Defaults to 4.\n",
    "            drop_prob: Regularization technique to prevent overfitting. Defaults to 0.5.\n",
    "            use_gpu: If the model uses GPU. Defaults to False.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            super().__init__()\n",
    "            self.drop_prob = drop_prob\n",
    "            self.num_layers = num_layers\n",
    "            self.num_hidden = num_hidden\n",
    "            self.use_gpu = use_gpu\n",
    "            \n",
    "            self.all_chars = all_chars\n",
    "            self.decoder = torch.load(decoder)\n",
    "            self.encoder = torch.load(encoder)\n",
    "            \n",
    "            self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n",
    "            self.dropout = nn.Dropout(drop_prob)\n",
    "            self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
    "            logger.info(\"CharModel initialized successfully\")\n",
    "    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing CharModel: {str(e)}\")\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"Implementation of the CharModel logic, in which, the input passes through every step of the arquiteture\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor with shape (batch size and senquency length) containing character indices.\n",
    "            hidden: Tuple containing the inicial hidden states of the CharModel each with shape (batch size and senquency length).\n",
    "\n",
    "        Returns:\n",
    "            final_out: Output tensor representing the predicted logits for each character in the sequence.\n",
    "            hidden: Tuple containing the final hidden states of the CharModel.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            lstm_output, hidden = self.lstm(x, hidden)       \n",
    "            drop_output = self.dropout(lstm_output)\n",
    "            drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n",
    "            final_out = self.fc_linear(drop_output)\n",
    "            \n",
    "            return final_out, hidden\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error implementing CharModel logic: {str(e)}\")\n",
    "    \n",
    "    \n",
    "    def hidden_state(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initializes and returns the initial hidden state for a recurrent neural network (e.g., LSTM).\n",
    "\n",
    "        This method creates zero-filled tensors for the hidden state (h_0) and cell state (c_0), \n",
    "        supporting GPU execution if `self.use_gpu` is set to True.\n",
    "\n",
    "        Args:\n",
    "            batch_size: The number of sequences in the input batch, used to determine the tensor dimensions.\n",
    "\n",
    "        Returns:\n",
    "            Tuple: A tuple containing the hidden state and cell state tensors \n",
    "            with shape (num_layers, batch_size, num_hidden). Returns None if an exception occurs, and logs the error.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.use_gpu:\n",
    "                hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden).to(device),\n",
    "                        torch.zeros(self.num_layers,batch_size,self.num_hidden).to(device))\n",
    "            else:\n",
    "                hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden),\n",
    "                        torch.zeros(self.num_layers,batch_size,self.num_hidden))\n",
    "            \n",
    "            return hidden\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error Initializing and returning the initial hidden state: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 22:04:50 - INFO - CharModel initialized successfully\n"
     ]
    }
   ],
   "source": [
    "model = CharModel(\n",
    "    all_chars=all_characters,\n",
    "    num_hidden=512,\n",
    "    num_layers=3,\n",
    "    drop_prob=0.5,\n",
    "    use_gpu=True,\n",
    "    encoder= MODEL_ENCODER_PATH,\n",
    "    decoder= MODEL_DECODER_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of data to be used for training\n",
    "train_percent = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2722804"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(len(encoded_text) * (train_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ind = int(len(encoded_text) * (train_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = encoded_text[:train_ind]\n",
    "val_data = encoded_text[train_ind:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs to train for\n",
    "epochs = 30\n",
    "# batch size \n",
    "batch_size = 128\n",
    "# Length of sequence\n",
    "seq_len = 100\n",
    "# for printing report purposes\n",
    "# always start at 0\n",
    "tracker = 0\n",
    "# number of characters in text\n",
    "num_char = max(encoded_text)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/24 22:04:52 INFO mlflow.tracking.fluent: Experiment with name 'Shakespeare Text Generation' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/phoenix/mlflow/201817814577060446', creation_time=1750802692086, experiment_id='201817814577060446', last_update_time=1750802692086, lifecycle_stage='active', name='Shakespeare Text Generation', tags={}>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri('/phoenix/mlflow')\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Step: 212 Val Loss: 2.6792349815368652\n",
      "Epoch: 1 Step: 424 Val Loss: 2.0928144454956055\n",
      "Epoch: 2 Step: 636 Val Loss: 1.863274335861206\n",
      "Epoch: 3 Step: 848 Val Loss: 1.7218742370605469\n",
      "Epoch: 4 Step: 1060 Val Loss: 1.6229908466339111\n",
      "Epoch: 5 Step: 1272 Val Loss: 1.5663131475448608\n",
      "Epoch: 6 Step: 1484 Val Loss: 1.5238914489746094\n",
      "Epoch: 7 Step: 1696 Val Loss: 1.485133409500122\n",
      "Epoch: 8 Step: 1908 Val Loss: 1.4531649351119995\n",
      "Epoch: 9 Step: 2120 Val Loss: 1.4358128309249878\n",
      "Epoch: 10 Step: 2332 Val Loss: 1.4237920045852661\n",
      "Epoch: 11 Step: 2544 Val Loss: 1.4111253023147583\n",
      "Epoch: 12 Step: 2756 Val Loss: 1.404954195022583\n",
      "Epoch: 13 Step: 2968 Val Loss: 1.3983490467071533\n",
      "Epoch: 14 Step: 3180 Val Loss: 1.3930407762527466\n",
      "Epoch: 15 Step: 3392 Val Loss: 1.384071946144104\n",
      "Epoch: 16 Step: 3604 Val Loss: 1.3750123977661133\n",
      "Epoch: 17 Step: 3816 Val Loss: 1.3771308660507202\n",
      "Epoch: 18 Step: 4028 Val Loss: 1.3724491596221924\n",
      "Epoch: 19 Step: 4240 Val Loss: 1.370200514793396\n",
      "Epoch: 20 Step: 4452 Val Loss: 1.3689918518066406\n",
      "Epoch: 21 Step: 4664 Val Loss: 1.3691165447235107\n",
      "Epoch: 22 Step: 4876 Val Loss: 1.3669312000274658\n",
      "Epoch: 23 Step: 5088 Val Loss: 1.366728663444519\n",
      "Epoch: 24 Step: 5300 Val Loss: 1.3685728311538696\n",
      "Epoch: 25 Step: 5512 Val Loss: 1.3626692295074463\n",
      "Epoch: 26 Step: 5724 Val Loss: 1.3618073463439941\n",
      "Epoch: 27 Step: 5936 Val Loss: 1.3585015535354614\n",
      "Epoch: 28 Step: 6148 Val Loss: 1.355994462966919\n",
      "Epoch: 29 Step: 6360 Val Loss: 1.3563776016235352\n"
     ]
    }
   ],
   "source": [
    "mlflow.start_run(run_name = RUN_NAME)\n",
    "\n",
    "mlflow.log_param(\"epochs\", epochs)\n",
    "mlflow.log_param(\"batch_size\", batch_size)\n",
    "\n",
    "# Set model to train\n",
    "model.train()\n",
    "\n",
    "# Check to see if using GPU\n",
    "if model.use_gpu:\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "    model.cuda()\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    hidden = model.hidden_state(batch_size)\n",
    "    \n",
    "    \n",
    "    for x,y in generate_batches(train_data, batch_size, seq_len):\n",
    "        \n",
    "        tracker += 1\n",
    "        \n",
    "        # One Hot Encode incoming data\n",
    "        x = one_hot_encoder(x, num_char)\n",
    "        \n",
    "        # Convert Numpy Arrays to Tensor\n",
    "        inputs = torch.from_numpy(x)\n",
    "        targets = torch.from_numpy(y)\n",
    "        \n",
    "        # Adjust for GPU if necessary\n",
    "        if model.use_gpu:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "        # Reset Hidden State\n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        lstm_output, hidden = model.forward(inputs, hidden)\n",
    "        loss = criterion(lstm_output, targets.view(batch_size*seq_len).long())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Clipping gradients to avoid explosion\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if tracker % 100 == 0:\n",
    "            val_hidden = model.hidden_state(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            \n",
    "            for x,y in generate_batches(val_data, batch_size, seq_len):\n",
    "                x = one_hot_encoder(x, num_char)\n",
    "                inputs = torch.from_numpy(x)\n",
    "                targets = torch.from_numpy(y)\n",
    "                \n",
    "                if model.use_gpu:\n",
    "                    inputs = inputs.to(device)\n",
    "                    targets = targets.to(device)\n",
    "                \n",
    "                val_hidden = tuple([state.data for state in val_hidden])\n",
    "                \n",
    "                lstm_output, val_hidden = model.forward(inputs, val_hidden)\n",
    "                val_loss = criterion(lstm_output, targets.view(batch_size*seq_len).long())\n",
    "        \n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "  \n",
    "            mlflow.log_metric(\"Val Loss\", val_loss.item(), step=tracker)\n",
    "        \n",
    "            model.train()\n",
    "            \n",
    "    print(f\"Epoch: {i} Step: {tracker} Val Loss: {val_loss.item()}\")\n",
    "\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = TORCH_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 23:20:53 - INFO - Model saved\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), f'models/{TORCH_MODEL}')\n",
    "logger.info(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharModel(\n",
       "  (lstm): LSTM(84, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc_linear): Linear(in_features=512, out_features=84, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f'models/{TORCH_MODEL}'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_char(model, char, hidden=None, k=1):\n",
    "    \"\"\"\n",
    "    Predicts the next character given an input character and the current hidden state.\n",
    "\n",
    "    This method encodes the input character, feeds it through the trained character-level \n",
    "    language model (e.g., LSTM), and samples from the top-k most probable characters \n",
    "    to determine the next one. It also returns the updated hidden state for sequential prediction.\n",
    "\n",
    "    Args:\n",
    "        char: The input character to start prediction from.\n",
    "        hidden: Current hidden state of the model. Each tensor has shape (num_layers, batch_size, num_hidden).\n",
    "            If None, a new hidden state should be initialized before calling this method.\n",
    "        k: Number of top predictions to sample from.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the predicted next character and the updated hidden state.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        encoded_text = model.encoder[char]\n",
    "        encoded_text = np.array([[encoded_text]])\n",
    "        encoded_text = one_hot_encoder(encoded_text, len(model.all_chars))\n",
    "        inputs = torch.from_numpy(encoded_text)\n",
    "    \n",
    "        if(model.use_gpu):\n",
    "            inputs = inputs.to(device)  \n",
    "\n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        lstm_out, hidden = model(inputs, hidden)        \n",
    "        probs = F.softmax(lstm_out, dim=1).data\n",
    "    \n",
    "        if(model.use_gpu):\n",
    "            probs = probs.cpu()\n",
    "\n",
    "    # Getting the top 'k' for next char probs\n",
    "        probs, index_positions = probs.topk(k)        \n",
    "        index_positions = index_positions.numpy().squeeze()\n",
    "        probs = probs.numpy().flatten()\n",
    "        probs = probs/probs.sum()\n",
    "        char = np.random.choice(index_positions, p=probs)    \n",
    "    \n",
    "        return model.decoder[char], hidden\n",
    "    except Exception as e:\n",
    "            logger.error(f\"Error predicting next char: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, size, seed='The', k=1):\n",
    "    \"\"\"\n",
    "    Generates a sequence of text using the trained character-level language model.\n",
    "\n",
    "    Starting from a seed string, this method uses the model to predict the next character\n",
    "    one at a time, feeding each predicted character back into the model. It continues\n",
    "    this process until the desired output length is reached.\n",
    "\n",
    "    Args:\n",
    "        seed: The initial sequence of characters used to start the text generation.\n",
    "        size: The number of characters to generate after the seed.\n",
    "        k: Number of top character predictions to consider for sampling at each step.\n",
    "\n",
    "    Returns:\n",
    "        The full generated text including the seed and the newly predicted characters.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if(model.use_gpu):\n",
    "            model.to(device)\n",
    "        else:\n",
    "            model.cpu()\n",
    "            \n",
    "        model.eval()\n",
    "        output_chars = [c for c in seed]\n",
    "        hidden = model.hidden_state(1)\n",
    "        \n",
    "        for char in seed:\n",
    "            char, hidden = predict_next_char(model, char, hidden, k=k)\n",
    "\n",
    "        output_chars.append(char)\n",
    "        for i in range(size):\n",
    "            char, hidden = predict_next_char(model, output_chars[-1], hidden, k=k)\n",
    "            output_chars.append(char)\n",
    "            \n",
    "        return ''.join(output_chars)\n",
    "    except Exception as e:\n",
    "            logger.error(f\"Error making predictions: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating a text with 1000 chars starting with word 'Confidence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence of the chair\n",
      "  CHARLES, the Council, and Cornelius and Castius\n",
      "\n",
      "  PAROLLES, trumpets as I here, to be the worse to this thought\n",
      "    on me.\n",
      "  CLOWN. The soldiers of the command, I will stay to him, and we will be\n",
      "    the cap of the child in house of the soul, and tell thee a chance and stood\n",
      "    and send thee with a mother to my hand to th' cause. I have been\n",
      "    a state, and tell this war and the son that we show them the soul\n",
      "    of a soul, and white the chairs of man and to a power of the country that I have been\n",
      "    my lord.\n",
      "  COUNTESS. The merciless strong instantly should have thee that I heard\n",
      "    thee. I am a strain's child in the counterfeit of a power to tell you.\n",
      "                               Exeunt ARTHUR. A THALLOS and the KING, whose son\n",
      "                       a thorns, and they say and\n",
      "                                                          this tongues\n",
      "                  the trumpets and the state\n",
      "\n",
      "  CAESAR. That will I stay to have thee to her son,\n",
      "    Whom I will see \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, 1000, seed='Confidence ', k=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating a text with 1000 chars starting with word 'Love'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love and Caesar\n",
      "    That to my father shall be true an hour\n",
      "    And we hear him and the streets to the contrive.\n",
      "  KING JOHN. The man is so, the charge of the controll'd\n",
      "    With him that to their cheeks to stand on me.\n",
      "    I'll see the soldiers and a partian hard.\n",
      "    I will bear true that they are strong and thine,\n",
      "    Will this answer to-day and the soldiers\n",
      "    That, as you have that was a proud of my son\n",
      "    That we will stay to thee a stars to thine\n",
      "    Which I am sure that was never the worse\n",
      "    We shall strate me, and we will be a sentence\n",
      "    As we will see him.\n",
      "  CAESAR. What is the court?  \n",
      "  SURREY. What is the service of the sendent will\n",
      "    And honour what the service of my friends,\n",
      "    Wherefore their braces that will still be so?\n",
      "  KING. Why sees to this? To th' storm of him, to strong,\n",
      "    And, which I have been their and straight that shall\n",
      "    Think we to th' way. What, within that I shall have mine,\n",
      "    I shall, my Lord of Warwick! Where say you?\n",
      "  CLEOPATRA. I will see \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, 1000, seed='Love ', k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time: float = time.time()\n",
    "elapsed_time: float = end_time - start_time\n",
    "elapsed_minutes: int = int(elapsed_time // 60)\n",
    "elapsed_seconds: float = elapsed_time % 60\n",
    "\n",
    "logger.info(f\"‚è±Ô∏è Total execution time: {elapsed_minutes}m {elapsed_seconds:.2f}s\")\n",
    "logger.info(\"‚úÖ Notebook execution completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Built with ‚ù§Ô∏è using Z by HP AI Studio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
