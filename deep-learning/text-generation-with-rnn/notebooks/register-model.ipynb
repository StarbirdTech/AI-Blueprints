{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54cda062",
   "metadata": {},
   "source": [
    "<h1 style=\\\"text-align: center; font-size: 50px;\\\">  Text Generation with Neural Networks and Torch MLflow Integration</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd2b390",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "- Start Execution\n",
    "- User Constants\n",
    "- Install and Import Libraries\n",
    "- Configure Settings\n",
    "- Verify Assets\n",
    "- Logging Model to MLflow\n",
    "- Fetching the Latest Model Version from MLflow\n",
    "- Loading the Model and Running Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f121efc2",
   "metadata": {},
   "source": [
    "## Start Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f70cdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "# Configure logger\n",
    "logger: logging.Logger = logging.getLogger(\"register_model_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.propagate = False  # Prevent duplicate logs from parent loggers\n",
    "\n",
    "# Set formatter\n",
    "formatter: logging.Formatter = logging.Formatter(\n",
    "    fmt=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "# Configure and attach stream handler\n",
    "stream_handler: logging.StreamHandler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74b2b882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 14:20:21 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()  \n",
    "\n",
    "logger.info(\"Notebook execution started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bf0c8a",
   "metadata": {},
   "source": [
    "## User Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55c97850",
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_WORD = 'Love '\n",
    "SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc816f2b",
   "metadata": {},
   "source": [
    "## Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90c4a1c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7b622c55f450>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard Library Imports\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-Party Libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# MLflow for Experiment Tracking and Model Management\n",
    "import mlflow\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "from mlflow.models import ModelSignature\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d71adb",
   "metadata": {},
   "source": [
    "## Configure Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e33c1acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56c90710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ Define global experiment and run names to be used throughout the notebook ------------------------\n",
    "EXPERIMENT_SET = \"RNN text generation\"\n",
    "RUN_NAME = \"RNN Text Generation\"\n",
    "MODEL_NAME = \"dict_torch_rnn_model\"\n",
    "TORCH_MODEL = \"dict_torch_rnn_model.pt\"\n",
    "REGISTER_NAME = \"Shakespeare_Model\"\n",
    "EXPERIMENT_NAME = \"Shakespeare Text Generation\"\n",
    "\n",
    "# ------------------------ Paths ------------------------\n",
    "DATA_PATH = \"../data/shakespeare.txt\"\n",
    "MODEL_DECODER_PATH = \"models/decoder.pt\"\n",
    "MODEL_ENCODER_PATH = \"models/encoder.pt\"\n",
    "MODEL_PATH = 'models/dict_torch_rnn_model.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164d7100",
   "metadata": {},
   "source": [
    "## Verify Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6f6d862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 14:20:23 - INFO - Shakespeare text is properly configured. \n",
      "2025-07-16 14:20:23 - INFO - Decoder model is properly configured. \n",
      "2025-07-16 14:20:23 - INFO - Encoder model is properly configured. \n",
      "2025-07-16 14:20:23 - INFO - Rnn model is properly configured. \n"
     ]
    }
   ],
   "source": [
    "def log_asset_status(asset_path: str, asset_name: str, success_message: str, failure_message: str) -> None:\n",
    "    \"\"\"\n",
    "    Logs the status of a given asset based on its existence.\n",
    "\n",
    "    Parameters:\n",
    "        asset_path (str): File or directory path to check.\n",
    "        asset_name (str): Name of the asset for logging context.\n",
    "        success_message (str): Message to log if asset exists.\n",
    "        failure_message (str): Message to log if asset does not exist.\n",
    "    \"\"\"\n",
    "    if Path(asset_path).exists():\n",
    "        logger.info(f\"{asset_name} is properly configured. {success_message}\")\n",
    "    else:\n",
    "        logger.info(f\"{asset_name} is not properly configured. {failure_message}\")\n",
    "        \n",
    "log_asset_status(\n",
    "    asset_path=DATA_PATH,\n",
    "    asset_name=\"Shakespeare text\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please create and download the required assets in your project on AI Studio.\"\n",
    ")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=MODEL_DECODER_PATH ,\n",
    "    asset_name=\"Decoder model\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if model folder was properly downloaded in your project on AI Studio.\"\n",
    ")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=MODEL_ENCODER_PATH,\n",
    "    asset_name=\"Encoder model\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if model folder was properly downloaded in your project on AI Studio.\"\n",
    ")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=MODEL_PATH,\n",
    "    asset_name=\"Rnn model\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if model folder was properly downloaded in your project on AI Studio.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b916f206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77297a0",
   "metadata": {},
   "source": [
    "# Creating the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74473a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Text Data\n",
    "with open(DATA_PATH,'r',encoding='utf8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7002d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_characters = set(text) # creates a set of unique characters found in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21ca0611",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharModel(nn.Module):\n",
    "    def __init__(self, decoder, encoder, all_chars, num_hidden=256, num_layers=4,drop_prob=0.5, use_gpu=False):\n",
    "        \"\"\"Initializes CharModel\n",
    "\n",
    "        Args:\n",
    "            decoder: Assigns a unique integer to each character in a dictionary format\n",
    "            encoder : Reverses the decoder dictionary, providing a mapping from characters to their respective assigned integers.\n",
    "            all_chars: Set of unique characters found in the text.\n",
    "            num_hidden: Number of hidden layers. Defaults to 256.\n",
    "            num_layers: Number of layers. Defaults to 4.\n",
    "            drop_prob: Regularization technique to prevent overfitting. Defaults to 0.5.\n",
    "            use_gpu: If the model uses GPU. Defaults to False.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            super().__init__()\n",
    "            self.drop_prob = drop_prob\n",
    "            self.num_layers = num_layers\n",
    "            self.num_hidden = num_hidden\n",
    "            self.use_gpu = use_gpu\n",
    "            \n",
    "            self.all_chars = all_chars\n",
    "            self.decoder = torch.load(decoder)\n",
    "            self.encoder = torch.load(encoder)\n",
    "            \n",
    "            self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n",
    "            self.dropout = nn.Dropout(drop_prob)\n",
    "            self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
    "            logger.info(\"CharModel initialized successfully\")\n",
    "    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing CharModel: {str(e)}\")\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"Implementation of the CharModel logic, in which, the input passes through every step of the arquiteture\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor with shape (batch size and senquency length) containing character indices.\n",
    "            hidden: Tuple containing the inicial hidden states of the CharModel each with shape (batch size and senquency length).\n",
    "\n",
    "        Returns:\n",
    "            final_out: Output tensor representing the predicted logits for each character in the sequence.\n",
    "            hidden: Tuple containing the final hidden states of the CharModel.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            lstm_output, hidden = self.lstm(x, hidden)       \n",
    "            drop_output = self.dropout(lstm_output)\n",
    "            drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n",
    "            final_out = self.fc_linear(drop_output)\n",
    "            \n",
    "            return final_out, hidden\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error implementing CharModel logic: {str(e)}\")\n",
    "    \n",
    "    \n",
    "    def hidden_state(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initializes and returns the initial hidden state for a recurrent neural network (e.g., LSTM).\n",
    "\n",
    "        This method creates zero-filled tensors for the hidden state (h_0) and cell state (c_0), \n",
    "        supporting GPU execution if `self.use_gpu` is set to True.\n",
    "\n",
    "        Args:\n",
    "            batch_size: The number of sequences in the input batch, used to determine the tensor dimensions.\n",
    "\n",
    "        Returns:\n",
    "            Tuple: A tuple containing the hidden state and cell state tensors \n",
    "            with shape (num_layers, batch_size, num_hidden). Returns None if an exception occurs, and logs the error.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.use_gpu:\n",
    "                hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden).to(device),\n",
    "                        torch.zeros(self.num_layers,batch_size,self.num_hidden).to(device))\n",
    "            else:\n",
    "                hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden),\n",
    "                        torch.zeros(self.num_layers,batch_size,self.num_hidden))\n",
    "            \n",
    "            return hidden\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error Initializing and returning the initial hidden state: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082850f5",
   "metadata": {},
   "source": [
    "## Logging Model to MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62797b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        \"\"\"\n",
    "        Loads the model and associated artifacts (encoder, decoder) into memory.\n",
    "\n",
    "        Args:\n",
    "            context: MLflow context containing paths to model artifacts.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.model = CharModel(\n",
    "                            all_chars=all_characters,\n",
    "                            num_hidden=512,\n",
    "                            num_layers=3,\n",
    "                            drop_prob=0.5,\n",
    "                            use_gpu=False,\n",
    "                            decoder=context.artifacts['decoder'],\n",
    "                            encoder=context.artifacts['encoder']\n",
    "                                            \n",
    "                        )\n",
    "\n",
    "            self.model.load_state_dict(torch.load(context.artifacts['model_state_dict']))\n",
    "            self.model.eval()\n",
    "            logger.info(\"Loading context done successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading context: {str(e)}\")\n",
    "\n",
    "    def one_hot_encoder(self, encoded_text, num_uni_chars):\n",
    "        \"\"\"\n",
    "        Convert categorical data into a fixed-size vector of numerical values.\n",
    "\n",
    "        Args:\n",
    "            encoded_text: Batch of encoded text.\n",
    "            num_uni_chars: Number of unique characters\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            one_hot = np.zeros((encoded_text.size, num_uni_chars))\n",
    "            one_hot = one_hot.astype(np.float32)\n",
    "            one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0\n",
    "            one_hot = one_hot.reshape((*encoded_text.shape, num_uni_chars))\n",
    "            \n",
    "            return one_hot\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error converting categorical data: {str(e)}\")\n",
    "\n",
    "    def predict_next_char(self, char, hidden=None, k=3):\n",
    "        \"\"\"\n",
    "        Predicts the next character given an input character and the current hidden state.\n",
    "\n",
    "        This method encodes the input character, feeds it through the trained character-level \n",
    "        language model (e.g., LSTM), and samples from the top-k most probable characters \n",
    "        to determine the next one. It also returns the updated hidden state for sequential prediction.\n",
    "\n",
    "        Args:\n",
    "            char: The input character to start prediction from.\n",
    "            hidden: Current hidden state of the model. Each tensor has shape (num_layers, batch_size, num_hidden).\n",
    "                If None, a new hidden state should be initialized before calling this method.\n",
    "            k: Number of top predictions to sample from.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing the predicted next character and the updated hidden state.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            encoded_text = self.model.encoder[char]\n",
    "            encoded_text = np.array([[encoded_text]])\n",
    "            encoded_text = self.one_hot_encoder(encoded_text, len(self.model.all_chars))\n",
    "            inputs = torch.from_numpy(encoded_text)\n",
    "            inputs = inputs.cpu()\n",
    "                \n",
    "            hidden = tuple([state.data for state in hidden])\n",
    "            lstm_out, hidden = self.model(inputs, hidden)    \n",
    "            probs = F.softmax(lstm_out, dim=1).data\n",
    "            probs = probs.cpu()\n",
    "\n",
    "            \n",
    "            probs, index_positions = probs.topk(k)        \n",
    "            index_positions = index_positions.numpy().squeeze()\n",
    "            probs = probs.numpy().flatten()\n",
    "            probs = probs/probs.sum()\n",
    "            char = np.random.choice(index_positions, p=probs)\n",
    "\n",
    "            return self.model.decoder[char], hidden\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error predicting next char: {str(e)}\")\n",
    "\n",
    "    def generate_text(self, seed, size, k=3):\n",
    "        \"\"\"\n",
    "        Generates a sequence of text using the trained character-level language model.\n",
    "\n",
    "        Starting from a seed string, this method uses the model to predict the next character\n",
    "        one at a time, feeding each predicted character back into the model. It continues\n",
    "        this process until the desired output length is reached.\n",
    "\n",
    "        Args:\n",
    "            seed: The initial sequence of characters used to start the text generation.\n",
    "            size: The number of characters to generate after the seed.\n",
    "            k: Number of top character predictions to consider for sampling at each step.\n",
    "\n",
    "        Returns:\n",
    "            The full generated text including the seed and the newly predicted characters.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.model.cpu()\n",
    "                \n",
    "            self.model.eval()\n",
    "            output_chars = [c for c in seed]\n",
    "            hidden = self.model.hidden_state(1)\n",
    "            \n",
    "            for char in seed:\n",
    "                char, hidden = self.predict_next_char(char, hidden, k=k)\n",
    "        \n",
    "            output_chars.append(char)\n",
    "            for i in range(size):\n",
    "                char, hidden = self.predict_next_char(output_chars[-1], hidden, k=k)\n",
    "                output_chars.append(char)\n",
    "                \n",
    "            return ''.join(output_chars)\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating text: {str(e)}\")\n",
    "        \n",
    "    def predict(self, context, model_input):\n",
    "        \"\"\"\n",
    "        Runs inference using the loaded model and input data.\n",
    "\n",
    "        Args:\n",
    "            context: The MLflow context object.\n",
    "            model_input : A dictionary containing 'seed' and 'size' keys.\n",
    "\n",
    "        Returns:\n",
    "             The output from the model containing the predicted text.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            initial_word = model_input['initial_word'][0]\n",
    "            size = model_input['size'][0]\n",
    "            output = self.generate_text(seed=initial_word, size=size)\n",
    "            \n",
    "            return output\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error predicting text: {str(e)}\")\n",
    "\n",
    "    @classmethod\n",
    "    def log_model(cls, model_state_dict, decoder, encoder, demo_folder=\"../demo\", config_path=\"../configs/config.yaml\"): \n",
    "        \"\"\"\n",
    "        Logs the model to MLflow, including artifacts, dependencies, and input/output signatures.\n",
    "\n",
    "        Args:\n",
    "            model_state_dict: Path where the model is saved before logging.\n",
    "            decoder: Assigns a unique integer to each character in a dictionary format\n",
    "            encoder : Reverses the decoder dictionary, providing a mapping from characters to their respective assigned integers.\n",
    "            demo_folder: Path to the folder containing the compiled demo UI. Defaults to \"demo\".\n",
    "            config_path: Path to configuration file. Defaults to \"../configs/config.yaml\".\n",
    "        \"\"\"\n",
    "        try:\n",
    "            input_schema = Schema(\n",
    "                [\n",
    "                    ColSpec(\"string\", \"initial_word\"),\n",
    "                    ColSpec(\"long\", \"size\")\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            output_schema = Schema(\n",
    "                [\n",
    "                    ColSpec(\"string\", \"generated_text\")\n",
    "                ]\n",
    "            )\n",
    "        \n",
    "            signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "                \n",
    "            requirements = [\n",
    "                \"torch\",\n",
    "                \"numpy\"\n",
    "            ]\n",
    "            mlflow.pyfunc.log_model(\n",
    "                model_state_dict,\n",
    "                python_model=cls(),\n",
    "                artifacts={\n",
    "                    \"model_state_dict\": model_state_dict, \n",
    "                    'decoder': MODEL_DECODER_PATH, \n",
    "                    'encoder': MODEL_ENCODER_PATH, \n",
    "                    \"demo\": demo_folder,\n",
    "                    \"config\": config_path\n",
    "                },\n",
    "                signature=signature,\n",
    "                pip_requirements=requirements\n",
    "            )\n",
    "            logger.info(\"Logging model to MLflow done successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error logging model to MLflow: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dcff889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/phoenix/mlflow/920820733064782143', creation_time=1752673235935, experiment_id='920820733064782143', last_update_time=1752673235935, lifecycle_stage='active', name='Shakespeare Text Generation', tags={}>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri('/phoenix/mlflow')\n",
    "mlflow.set_experiment(experiment_name= EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "321851bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict = MODEL_PATH\n",
    "register_name = REGISTER_NAME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2dbda51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 14:20:24 - INFO - Run's Artifact URI: /phoenix/mlflow/920820733064782143/aefc55591c5f4f59b7e7449733733827/artifacts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce9e70da0aaf4db5a61b1334a57e65b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d19631f2c544d8b044db768e988c4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5932de26ca1342e0870fbd67fc26a72d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f4953098fd4a5ca9ea1a361dee3d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 14:20:25 - INFO - Logging model to MLflow done successfully\n",
      "Registered model 'Shakespeare_Model' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'Shakespeare_Model'.\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name = RUN_NAME) as run:\n",
    "    logger.info(f\"Run's Artifact URI: {run.info.artifact_uri}\")\n",
    "    RNNModel.log_model(model_state_dict, MODEL_DECODER_PATH, MODEL_ENCODER_PATH)\n",
    "    mlflow.register_model(model_uri = f\"runs:/{run.info.run_id}/{model_state_dict}\", name=register_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5241b5eb",
   "metadata": {},
   "source": [
    "## Fetching the Latest Model Version from MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec23af85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = mlflow.MlflowClient()\n",
    "model_metadata = client.get_latest_versions(register_name, stages=[\"None\"])\n",
    "latest_model_version = model_metadata[0].version\n",
    "latest_model_version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99271fda",
   "metadata": {},
   "source": [
    "## Loading the Model and Running Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a67fbba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 14:20:25 - INFO - CharModel initialized successfully\n",
      "2025-07-16 14:20:27 - INFO - Loading context done successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love and the Duke of Wentmoreland, and so\n",
      "    so strange to take the canners. The King and Sir John Caesar\n"
     ]
    }
   ],
   "source": [
    "model = mlflow.pyfunc.load_model(model_uri=f\"models:/{register_name}/{latest_model_version}\")\n",
    "print(model.predict({\"initial_word\": INITIAL_WORD, \"size\": SIZE}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "940d3428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 14:20:28 - INFO - ⏱️ Total execution time: 0m 6.13s\n",
      "2025-07-16 14:20:28 - INFO - ✅ Notebook execution completed successfully.\n"
     ]
    }
   ],
   "source": [
    "end_time: float = time.time()\n",
    "elapsed_time: float = end_time - start_time\n",
    "elapsed_minutes: int = int(elapsed_time // 60)\n",
    "elapsed_seconds: float = elapsed_time % 60\n",
    "\n",
    "logger.info(f\"⏱️ Total execution time: {elapsed_minutes}m {elapsed_seconds:.2f}s\")\n",
    "logger.info(\"✅ Notebook execution completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372c8d16",
   "metadata": {},
   "source": [
    "Built with ❤️ using Z by HP AI Studio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
