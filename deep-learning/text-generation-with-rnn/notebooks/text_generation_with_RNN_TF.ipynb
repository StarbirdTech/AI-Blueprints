{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ovpZyIhNIgoq"
   },
   "source": [
    "<h1 style=\\\"text-align: center; font-size: 50px;\\\"> 📜 Text Generation with Neural Networks and Tensorflow</h1>\n",
    "\n",
    "In this notebook our objective is to demonstrate how to generate text using a character-based RNN and Tensorflow working with a dataset of Shakespeare's  writing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "- Imports\n",
    "- Configurations\n",
    "- Generating Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WBd69MDEm4rF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 00:14:56.722889: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-13 00:14:56.731996: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749773696.741781     721 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749773696.744651     721 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-13 00:14:56.755964: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------ Standard Library Imports ------------------------\n",
    "import logging\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# ------------------------ Third-Party Libraries ------------------------\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, GRU, InputLayer\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import torch\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Create logger ===\n",
    "logger = logging.getLogger(\"text-generation-TF-notebook\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", \n",
    "                             datefmt=\"%Y-%m-%d %H:%M:%S\") \n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global experiment and run names to be used throughout the notebook\n",
    "MODEL_NAME = \"tf_rnn_model.h5\"\n",
    "\n",
    "# Set up the paths\n",
    "DATA_PATH = \"../data/shakespeare.txt\"\n",
    "TENSORBOARD_PATH = \"/phoenix/tensorboard/tensorlogs\"\n",
    "\n",
    "\n",
    "# Set up the chunk separator for text processing\n",
    "CHUNK_SEPARATOR = \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 00:14:59 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "logger.info('Notebook execution started.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_all_execution = datetime.now() # This variable is to help us to see in how much time this notebook will run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 00:14:59 - INFO - The Dataset is properly configured.\n"
     ]
    }
   ],
   "source": [
    "# Check whether the Dataset file exists\n",
    "is_dataset_available = Path(DATA_PATH).exists()\n",
    "\n",
    "# Log the configuration status of the dataset\n",
    "if is_dataset_available:\n",
    "    logger.info(\"The Dataset is properly configured.\")\n",
    "else:\n",
    "    logger.info(\n",
    "        \"The Dataset is not properly configured. Please check if Dataset was downloaded\"\n",
    "        \"in your project on AI Studio.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the text we'll use as a basis for our generations: let's try to generate 'Shakespearean' texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This text is from Shakespeare's Sonnet 1. It's one of the 154 sonnets written by William Shakespeare that were first published in 1609. This particular sonnet, like many others, discusses themes of beauty, procreation, and the transient nature of life, urging the beautiful to reproduce so their beauty can live on through their offspring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pD_55cOxLkAb"
   },
   "outputs": [],
   "source": [
    "path_to_file = DATA_PATH\n",
    "text = open(path_to_file, 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aavnuByVymwK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 00:14:59 - INFO - First 600 chars: \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bud buriest thy content,\n",
      "  And tender churl mak'st waste in niggarding:\n",
      "    Pity the world, or else th\n"
     ]
    }
   ],
   "source": [
    "logger.info('First 600 chars: \\n')\n",
    "print(text[:600])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rNnrKn_lL-IJ"
   },
   "source": [
    "## Preparing textual data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to encode our data to give the model a proper numerical representation of our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a set of unique characters found in the text\n",
    "vocab = sorted(set(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LFjSVAlWzf-N"
   },
   "source": [
    "### Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IalZLbvOzf-F"
   },
   "outputs": [],
   "source": [
    "char_to_int = {u:i for i, u in enumerate(vocab)}\n",
    "# assigns a unique integer to each character in a dictionary format, \n",
    "# creating a mapping that can later be used to transform encoded predictions back into characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "30ZYaWAOm4rt"
   },
   "outputs": [],
   "source": [
    "int_to_char = np.array(vocab)\n",
    "# reverses the decoder dictionary, providing a mapping from characters to their respective assigned integers, which is used to encode the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3fhOqV0lm4r2"
   },
   "outputs": [],
   "source": [
    "encoded_text = np.array([char_to_int[c] for c in text])\n",
    "# encodes the entire text as an array of integers, with each integer representing the character at that position\n",
    "# in the text according to the encoder dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bbmsf23Bymwe"
   },
   "source": [
    "## Creating Training Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hgsVvVxnymwf"
   },
   "source": [
    "Training batches are a way of dividing the dataset into smaller, manageable groups of data points that are fed into a machine learning model during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ciatnowvm4se"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1749773703.548590     721 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2268 MB memory:  -> device: 0, name: NVIDIA T600, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "seq_len = 120 # length of sequence for a training example\n",
    "total_num_seq = len(text)//(seq_len+1) # total number of training examples\n",
    "\n",
    "# Create Training Sequences\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
    "sequences = char_dataset.batch(seq_len+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9NGu-FkO_kYU"
   },
   "outputs": [],
   "source": [
    "def create_seq_targets(seq):\n",
    "    \"\"\"\n",
    "    Function that takes a sequence as input, duplicates, and shifts it to align the input and label. \n",
    "\n",
    "    Args:\n",
    "        seq: sequence of characters\n",
    "\n",
    "    Returns:\n",
    "        The text input and corresponding target.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        input_txt = seq[:-1]\n",
    "        target_txt = seq[1:]\n",
    "        return input_txt, target_txt\n",
    "    except Exception as e:\n",
    "            logger.error(f\"Error creating sequences of targets: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HszljTg8m4so"
   },
   "outputs": [],
   "source": [
    "dataset = sequences.map(create_seq_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p2pGotuNzf-S"
   },
   "outputs": [],
   "source": [
    "# Batch size\n",
    "batch_size = 128\n",
    "buffer_size = 10000\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r6oUuElIMgVx"
   },
   "source": [
    "## Creating the GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zHT8cLh7EAsg"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "# The embedding dimension\n",
    "embed_dim = 64\n",
    "# Number of RNN units\n",
    "rnn_neurons = 1026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FrOOK61Olm1C"
   },
   "outputs": [],
   "source": [
    "def sparse_cat_loss(y_true,y_pred):\n",
    "  return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MtCrdfzEI2N0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 00:15:04 - INFO - Model architecture created successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,376</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1026</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,361,176</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">84</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">86,268</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;34m128\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │         \u001b[38;5;34m5,376\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ (\u001b[38;5;34m128\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1026\u001b[0m)      │     \u001b[38;5;34m3,361,176\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m128\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m84\u001b[0m)        │        \u001b[38;5;34m86,268\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,452,820</span> (13.17 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,452,820\u001b[0m (13.17 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,452,820</span> (13.17 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,452,820\u001b[0m (13.17 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_model(vocab_size, embed_dim, rnn_neurons, batch_size):\n",
    "    \"\"\"Architecture to create the model.\n",
    "\n",
    "    Args:\n",
    "        vocab_size: Length of the vocabulary in chars.\n",
    "        embed_dim: Embedding dimension.\n",
    "        rnn_neurons: Number of RNN units.\n",
    "        batch_size: Size of the batchs.\n",
    "\n",
    "    Returns:\n",
    "        Model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = Sequential()\n",
    "        model.add(InputLayer(batch_shape=(batch_size, None)))\n",
    "        \n",
    "        model.add(Embedding(input_dim=vocab_size, output_dim=embed_dim))\n",
    "\n",
    "        model.add(GRU(rnn_neurons,\n",
    "                    return_sequences=True,\n",
    "                    stateful=True,\n",
    "                    recurrent_initializer='glorot_uniform'))\n",
    "\n",
    "        model.add(Dense(vocab_size))\n",
    "        model.compile(optimizer='adam', loss=sparse_cat_loss)\n",
    "        logger.info(\"Model architecture created successfully\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "            logger.error(f\"Error creating model architecture: {str(e)}\")\n",
    "\n",
    "model = create_model(vocab_size, embed_dim, rnn_neurons, batch_size)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wwsrpOik5zhv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 00:15:04 - INFO - Model architecture created successfully\n"
     ]
    }
   ],
   "source": [
    "model = create_model(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    rnn_neurons=rnn_neurons,\n",
    "    batch_size=batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard\n",
    "log_dir = TENSORBOARD_PATH\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LJL0Q0YPY6Ee"
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A4ygvfHn-wan"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 120, 84)  <=== (batch_size, sequence_length, vocab_size)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1749773705.301230     817 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-06-13 00:15:05.499968: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "\n",
    "  # Predict off some random batch\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "\n",
    "  # Display the dimensions of the predictions\n",
    "  print(example_batch_predictions.shape, \" <=== (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ld8z3LPBAuv"
   },
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "# Reformat to not be a lists of lists\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZYDQjKTlm4s8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 316ms/step - loss: 3.0007\n",
      "Epoch 2/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 290ms/step - loss: 1.6809\n",
      "Epoch 3/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 640ms/step - loss: 1.4274\n",
      "Epoch 4/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 626ms/step - loss: 1.3257\n",
      "Epoch 5/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 646ms/step - loss: 1.2699\n",
      "Epoch 6/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 630ms/step - loss: 1.2326\n",
      "Epoch 7/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 673ms/step - loss: 1.2072\n",
      "Epoch 8/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 684ms/step - loss: 1.1840\n",
      "Epoch 9/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 673ms/step - loss: 1.1658\n",
      "Epoch 10/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m513s\u001b[0m 1s/step - loss: 1.1479 \n",
      "Epoch 11/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 296ms/step - loss: 1.1350\n",
      "Epoch 12/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 290ms/step - loss: 1.1208\n",
      "Epoch 13/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 630ms/step - loss: 1.1074\n",
      "Epoch 14/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 639ms/step - loss: 1.0960\n",
      "Epoch 15/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 657ms/step - loss: 1.0858\n",
      "Epoch 16/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m247s\u001b[0m 702ms/step - loss: 1.0767\n",
      "Epoch 17/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 619ms/step - loss: 1.0670\n",
      "Epoch 18/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 630ms/step - loss: 1.0585\n",
      "Epoch 19/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 634ms/step - loss: 1.0482\n",
      "Epoch 20/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 295ms/step - loss: 1.0403\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f07a78e3590>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 20\n",
    "model.fit(dataset,epochs=epochs, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "2025-06-13 01:30:31 - INFO - Model saved\n"
     ]
    }
   ],
   "source": [
    "model.save(f'models/{model_name}') \n",
    "logger.info(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kKkD5M6eoSiN"
   },
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_iXG3VJvEXWM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 01:30:31 - INFO - Model architecture created successfully\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocab_size, embed_dim, rnn_neurons, batch_size=1)\n",
    "\n",
    "model.load_weights(f'models/{model_name}')\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WvuwZBX5Ogfd"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_seed=\"The \", gen_size=100, temp=1.0):\n",
    "    \"\"\"\n",
    "    Generates a sequence of text using the trained character-level language model.\n",
    "\n",
    "    Args:\n",
    "        model: Model created on function create_model\n",
    "        start_seed: Set of characters that will be the beginning of the text. \n",
    "        gen_size : Number of characters. Defaults to 100.\n",
    "        temp: Controls the randomness of the predictions made by the model.\n",
    "\n",
    "    Returns:\n",
    "        The full generated text including the seed and the newly predicted characters.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        num_generate = gen_size\n",
    "        input_eval = [char_to_int[s] for s in start_seed]\n",
    "        input_eval = tf.expand_dims(input_eval, 0)\n",
    "        text_generated = []\n",
    "        temperature = temp\n",
    "\n",
    "\n",
    "        for i in range(num_generate):\n",
    "            predictions = model(input_eval)\n",
    "            predictions = tf.squeeze(predictions, 0)\n",
    "            predictions = predictions / temperature\n",
    "            predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "            input_eval = tf.expand_dims([predicted_id], 0)\n",
    "            text_generated.append(int_to_char[predicted_id])\n",
    "\n",
    "        return start_seed + ''.join(text_generated)\n",
    "    except Exception as e:\n",
    "            logger.error(f\"Error making predictions: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating a text with 1000 chars starting with word 'Confidence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bS69SG5D5lwd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence with virtues, joyful mortals of peacef will set uhespection full.\n",
      "                                 Exeunt you at Criention, command, become\n",
      "    And set thee to thy youngest date; and like these course\n",
      "    I found it was with loyal scattering rare\n",
      "    With me to lip    Exit.\n",
      "  EMILIA. I wonder, sir, but tell; my lord:  \n",
      "  YORK. So was I have drunk new his victory; but I'll practice\n",
      "    You breathe together; I will grow to cold\n",
      "    And put the blessed earl outwear and cull thee,\n",
      "    But on that perish cannot burn and quite.\n",
      "\n",
      "\n",
      "                          86A. Wife, well?\n",
      "  HECTOR. I am in Roderis that loves Cassio.\n",
      "  PROSPERO. And, Emilia.\n",
      "  SEBASTIAN. Not God apprehends your thump.\n",
      "  BASSANIO. Ingin DALCO. I take the lady.\n",
      "  CRESSIDA. Then must accuse you!\n",
      "  ANTONIO. Nor I honor without the sad kept o' th' sea\n",
      "    As friends a dream in amongst the very\n",
      "    Obedience. Turn to counsel Cassio yourselves.\n",
      "    But come, sirs, answer me her reporting,\n",
      "    Although it be dead man?\n",
      "  AGAECHEEK. Na\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    if hasattr(layer, 'reset_states'):\n",
    "        layer.reset_states()\n",
    "print(generate_text(model, start_seed=\"Confidence \", gen_size=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating a text with 1000 chars starting with word 'Love'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love me one Nile; he was so iglant.\n",
      "    So high description, sent her funeral.\n",
      "  LEONTES. Lead up their kind'st tatting tears will I talk of you that thou mightst thou have.                 [Dives from me: then 'tis a gentleman, Lucions\n",
      "\n",
      "Enter LEONTES, TRINCUS, LAUNCENOT WIDOW. Heaven,\n",
      "    rancod. Best did lock us the charmiest of thy hand\n",
      "    The words they are already, not afraid of love.  \n",
      "    Two If thy hand commands thy love, look,\n",
      "    No prince!- as, good Craus! Pray you! let me see?\n",
      "  CURDINAL. Nay, hear me, Peter Henry's loving friends.\n",
      "  ANTONIO. How now, my lord, is't not that I have done.\n",
      "  DIANA. Hangworph is at all locks\n",
      "    in modesty; and endeavour, and yours are edre I lie to-night\n",
      "    By counting up the way to wife, so fast\n",
      "    When the rarent dragon that hath seore to bed.\n",
      "    Yet; dew we thought it persons to you;\n",
      "    Death in the bunch of heaven would come to ourselves\n",
      "    And you affract e'er undergrat'st my stheep.  \n",
      "    That we or a foully drunk be end.\n",
      "    Go tell th\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_seed=\"Love \", gen_size=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 01:30:44 - INFO -  1:15:45.479229\n"
     ]
    }
   ],
   "source": [
    "logger.info(f' {datetime.now() - start_time_all_execution}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 01:30:44 - INFO - Notebook execution completed.\n"
     ]
    }
   ],
   "source": [
    "logger.info('Notebook execution completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Built with ❤️ using Z by HP AI Studio."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "00-Generating-Text-with-RNNs.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
