{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\\\"text-align: center; font-size: 50px;\\\"> 📜 Text Generation with Neural Networks and Torch</h1>\n",
    "\n",
    "In this notebook our objective is to demonstrate how to generate text using a character-based RNN and Torch working with a dataset of Shakespeare's writing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "- Imports\n",
    "- Configurations\n",
    "- Get Text Data\n",
    "- Preparing textual data\n",
    "- One Hot Encoding\n",
    "- Creating Training Batches\n",
    "- Creating the LSTM Model\n",
    "- Training the Network\n",
    "- Generating Predictions\n",
    "- Logging Model to MLflow\n",
    "- Fetching the Latest Model Version from MLflow\n",
    "- Loading the Model and Running Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f94fd1781f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard Library Imports\n",
    "import logging\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Third-Party Libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# MLflow for Experiment Tracking and Model Management\n",
    "import mlflow\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "from mlflow.models import ModelSignature\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Create logger ===\n",
    "logger = logging.getLogger(\"text-generation-torch-notebook\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", \n",
    "                             datefmt=\"%Y-%m-%d %H:%M:%S\") \n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ Define global experiment and run names to be used throughout the notebook ------------------------\n",
    "EXPERIMENT_SET = \"RNN text generation\"\n",
    "RUN_NAME = \"RNN Text Generation\"\n",
    "MODEL_NAME = \"dict_torch_rnn_model\"\n",
    "TORCH_MODEL = \"dict_torch_rnn_model.pt\"\n",
    "REGISTER_NAME = \"Shakespeare_Model\"\n",
    "EXPERIMENT_NAME = \"Shakespeare Text Generation\"\n",
    "\n",
    "# ------------------------ Paths ------------------------\n",
    "DATA_PATH = \"../data/shakespeare.txt\"\n",
    "MODEL_DECODER_PATH = \"models/decoder.pt\"\n",
    "MODEL_ENCODER_PATH = \"models/encoder.pt\"\n",
    "MODEL_PATH = 'models/dict_torch_rnn_model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 18:07:50 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "logger.info('Notebook execution started.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_all_execution = datetime.now() # This variable is to help us to see in how much time this notebook will run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 18:07:51 - INFO - Shakespeare text is properly configured. \n",
      "2025-06-12 18:07:51 - INFO - Decoder model is properly configured. \n",
      "2025-06-12 18:07:51 - INFO - Encoder model is properly configured. \n",
      "2025-06-12 18:07:51 - INFO - Rnn model is properly configured. \n"
     ]
    }
   ],
   "source": [
    "def log_asset_status(asset_path: str, asset_name: str, success_message: str, failure_message: str) -> None:\n",
    "    \"\"\"\n",
    "    Logs the status of a given asset based on its existence.\n",
    "\n",
    "    Parameters:\n",
    "        asset_path (str): File or directory path to check.\n",
    "        asset_name (str): Name of the asset for logging context.\n",
    "        success_message (str): Message to log if asset exists.\n",
    "        failure_message (str): Message to log if asset does not exist.\n",
    "    \"\"\"\n",
    "    if Path(asset_path).exists():\n",
    "        logger.info(f\"{asset_name} is properly configured. {success_message}\")\n",
    "    else:\n",
    "        logger.info(f\"{asset_name} is not properly configured. {failure_message}\")\n",
    "        \n",
    "log_asset_status(\n",
    "    asset_path=DATA_PATH,\n",
    "    asset_name=\"Shakespeare text\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please create and download the required assets in your project on AI Studio.\"\n",
    ")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=MODEL_DECODER_PATH ,\n",
    "    asset_name=\"Decoder model\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if model folder was properly downloaded in your project on AI Studio.\"\n",
    ")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=MODEL_ENCODER_PATH,\n",
    "    asset_name=\"Encoder model\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if model folder was properly downloaded in your project on AI Studio.\"\n",
    ")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=MODEL_PATH,\n",
    "    asset_name=\"Rnn model\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if model folder was properly downloaded in your project on AI Studio.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the text we'll use as a basis for our generations: let's try to generate 'Shakespearean' texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This text is from Shakespeare's Sonnet 1. It's one of the 154 sonnets written by William Shakespeare that were first published in 1609. This particular sonnet, like many others, discusses themes of beauty, procreation, and the transient nature of life, urging the beautiful to reproduce so their beauty can live on through their offspring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH,'r',encoding='utf8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 18:07:53 - INFO - First 600 chars: \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bud buriest thy content,\n",
      "  And tender churl mak'st waste in niggarding:\n",
      "    Pity the world, or else th\n"
     ]
    }
   ],
   "source": [
    "logger.info('First 600 chars: \\n')\n",
    "print(text[:600])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing textual data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to encode our data to give the model a proper numerical representation of our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_characters = set(text) # creates a set of unique characters found in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = dict(enumerate(all_characters))\n",
    "# assigns a unique integer to each character in a dictionary format, \n",
    "# creating a mapping that can later be used to transform encoded predictions back into characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = {char: ind for ind, char in decoder.items()} \n",
    "# reverses the decoder dictionary, providing a mapping from characters to their respective assigned integers, which is used to encode the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(decoder, MODEL_DECODER_PATH)\n",
    "torch.save(encoder, MODEL_ENCODER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = np.array([encoder[char] for char in text])\n",
    "# encodes the entire text as an array of integers, with each integer representing the character at that position\n",
    "#in the text according to the encoder dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding is a way to convert categorical data into a fixed-size vector of numerical values.\n",
    "\n",
    "This encoding allows the model to treat input data uniformly and is particularly important for models that need to determine the presence or absence of a feature, such as a particular character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(encoded_text, num_uni_chars):\n",
    "    \"\"\"\n",
    "        Convert categorical data into a fixed-size vector of numerical values.\n",
    "\n",
    "        Args:\n",
    "            encoded_text: Batch of encoded text.\n",
    "            num_uni_chars: Number of unique characters\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a placeholder for zeros\n",
    "        one_hot = np.zeros((encoded_text.size, num_uni_chars))\n",
    "        \n",
    "        # Convert data type for later use with pytorch\n",
    "        one_hot = one_hot.astype(np.float32)\n",
    "\n",
    "        # Using indexing fill in the 1s at the correct index locations\n",
    "        one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0\n",
    "        \n",
    "        # Reshape it so it matches the batch shape\n",
    "        one_hot = one_hot.reshape((*encoded_text.shape, num_uni_chars))\n",
    "        \n",
    "        return one_hot\n",
    "    except Exception as e:\n",
    "            logger.error(f\"Error converting categorical data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Training Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training batches are a way of dividing the dataset into smaller, manageable groups of data points that are fed into a machine learning model during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(encoded_text, samp_per_batch=10, seq_len=50):\n",
    "    \n",
    "    '''\n",
    "    Generate (using yield) batches for training.\n",
    "    \n",
    "    X: Encoded Text of length seq_len\n",
    "    Y: Encoded Text shifted by one\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "    X:\n",
    "    \n",
    "    [[1 2 3]]\n",
    "    \n",
    "    Y:\n",
    "    \n",
    "    [[ 2 3 4]]\n",
    "    \n",
    "    encoded_text : Complete Encoded Text to make batches from\n",
    "    batch_size : Number of samples per batch\n",
    "    seq_len : Length of character sequence\n",
    "       \n",
    "    '''\n",
    "    try:\n",
    "        # Total number of characters per batch\n",
    "        # Example: If samp_per_batch is 2 and seq_len is 50, then 100\n",
    "        # characters come out per batch.\n",
    "        char_per_batch = samp_per_batch * seq_len\n",
    "        \n",
    "        # Number of batches available to make\n",
    "        # Use int() to roun to nearest integer\n",
    "        num_batches_avail = int(len(encoded_text)/char_per_batch)\n",
    "        \n",
    "        # Cut off end of encoded_text that\n",
    "        # won't fit evenly into a batch\n",
    "        encoded_text = encoded_text[:num_batches_avail * char_per_batch]\n",
    "        \n",
    "        # Reshape text into rows the size of a batch\n",
    "        encoded_text = encoded_text.reshape((samp_per_batch, -1))\n",
    "\n",
    "        # Go through each row in array.\n",
    "        for n in range(0, encoded_text.shape[1], seq_len):\n",
    "            # Grab feature characters\n",
    "            x = encoded_text[:, n:n+seq_len]\n",
    "            # y is the target shifted over by 1\n",
    "            y = np.zeros_like(x)\n",
    "            try:\n",
    "                y[:, :-1] = x[:, 1:]\n",
    "                y[:, -1]  = encoded_text[:, n+seq_len]\n",
    "            except:\n",
    "                y[:, :-1] = x[:, 1:]\n",
    "                y[:, -1] = encoded_text[:, 0]\n",
    "                \n",
    "            yield x, y\n",
    "    except Exception as e:\n",
    "            logger.error(f\"Error Generating batches: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharModel(nn.Module):\n",
    "    def __init__(self, decoder, encoder, all_chars, num_hidden=256, num_layers=4,drop_prob=0.5, use_gpu=False):\n",
    "        \"\"\"Initializes CharModel\n",
    "\n",
    "        Args:\n",
    "            decoder: Assigns a unique integer to each character in a dictionary format\n",
    "            encoder : Reverses the decoder dictionary, providing a mapping from characters to their respective assigned integers.\n",
    "            all_chars: Set of unique characters found in the text.\n",
    "            num_hidden: Number of hidden layers. Defaults to 256.\n",
    "            num_layers: Number of layers. Defaults to 4.\n",
    "            drop_prob: Regularization technique to prevent overfitting. Defaults to 0.5.\n",
    "            use_gpu: If the model uses GPU. Defaults to False.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            super().__init__()\n",
    "            self.drop_prob = drop_prob\n",
    "            self.num_layers = num_layers\n",
    "            self.num_hidden = num_hidden\n",
    "            self.use_gpu = use_gpu\n",
    "            \n",
    "            self.all_chars = all_chars\n",
    "            self.decoder = torch.load(decoder)\n",
    "            self.encoder = torch.load(encoder)\n",
    "            \n",
    "            self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n",
    "            self.dropout = nn.Dropout(drop_prob)\n",
    "            self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
    "            logger.info(\"CharModel initialized successfully\")\n",
    "    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing CharModel: {str(e)}\")\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"Implementation of the CharModel logic, in which, the input passes through every step of the arquiteture\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor with shape (batch size and senquency length) containing character indices.\n",
    "            hidden: Tuple containing the inicial hidden states of the CharModel each with shape (batch size and senquency length).\n",
    "\n",
    "        Returns:\n",
    "            final_out: Output tensor representing the predicted logits for each character in the sequence.\n",
    "            hidden: Tuple containing the final hidden states of the CharModel.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            lstm_output, hidden = self.lstm(x, hidden)       \n",
    "            drop_output = self.dropout(lstm_output)\n",
    "            drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n",
    "            final_out = self.fc_linear(drop_output)\n",
    "            \n",
    "            return final_out, hidden\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error implementing CharModel logic: {str(e)}\")\n",
    "    \n",
    "    \n",
    "    def hidden_state(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initializes and returns the initial hidden state for a recurrent neural network (e.g., LSTM).\n",
    "\n",
    "        This method creates zero-filled tensors for the hidden state (h_0) and cell state (c_0), \n",
    "        supporting GPU execution if `self.use_gpu` is set to True.\n",
    "\n",
    "        Args:\n",
    "            batch_size: The number of sequences in the input batch, used to determine the tensor dimensions.\n",
    "\n",
    "        Returns:\n",
    "            Tuple: A tuple containing the hidden state and cell state tensors \n",
    "            with shape (num_layers, batch_size, num_hidden). Returns None if an exception occurs, and logs the error.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.use_gpu:\n",
    "                hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden).to(device),\n",
    "                        torch.zeros(self.num_layers,batch_size,self.num_hidden).to(device))\n",
    "            else:\n",
    "                hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden),\n",
    "                        torch.zeros(self.num_layers,batch_size,self.num_hidden))\n",
    "            \n",
    "            return hidden\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error Initializing and returning the initial hidden state: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 18:08:00 - INFO - CharModel initialized successfully\n"
     ]
    }
   ],
   "source": [
    "model = CharModel(\n",
    "    all_chars=all_characters,\n",
    "    num_hidden=512,\n",
    "    num_layers=3,\n",
    "    drop_prob=0.5,\n",
    "    use_gpu=True,\n",
    "    encoder= MODEL_ENCODER_PATH,\n",
    "    decoder= MODEL_DECODER_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of data to be used for training\n",
    "train_percent = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2722804"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(len(encoded_text) * (train_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ind = int(len(encoded_text) * (train_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = encoded_text[:train_ind]\n",
    "val_data = encoded_text[train_ind:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs to train for\n",
    "epochs = 30\n",
    "# batch size \n",
    "batch_size = 128\n",
    "# Length of sequence\n",
    "seq_len = 100\n",
    "# for printing report purposes\n",
    "# always start at 0\n",
    "tracker = 0\n",
    "# number of characters in text\n",
    "num_char = max(encoded_text)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/phoenix/mlflow/382443359114083880', creation_time=1749696638886, experiment_id='382443359114083880', last_update_time=1749696638886, lifecycle_stage='active', name='Shakespeare Text Generation', tags={}>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri('/phoenix/mlflow')\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Step: 212 Val Loss: 2.6059865951538086\n",
      "Epoch: 1 Step: 424 Val Loss: 2.015252113342285\n",
      "Epoch: 2 Step: 636 Val Loss: 1.7928961515426636\n",
      "Epoch: 3 Step: 848 Val Loss: 1.6630843877792358\n",
      "Epoch: 4 Step: 1060 Val Loss: 1.58048415184021\n",
      "Epoch: 5 Step: 1272 Val Loss: 1.5260752439498901\n",
      "Epoch: 6 Step: 1484 Val Loss: 1.4877564907073975\n",
      "Epoch: 7 Step: 1696 Val Loss: 1.460910677909851\n",
      "Epoch: 8 Step: 1908 Val Loss: 1.4290704727172852\n",
      "Epoch: 9 Step: 2120 Val Loss: 1.4129059314727783\n",
      "Epoch: 10 Step: 2332 Val Loss: 1.4014630317687988\n",
      "Epoch: 11 Step: 2544 Val Loss: 1.3952082395553589\n",
      "Epoch: 12 Step: 2756 Val Loss: 1.388731837272644\n",
      "Epoch: 13 Step: 2968 Val Loss: 1.3830969333648682\n",
      "Epoch: 14 Step: 3180 Val Loss: 1.3756047487258911\n",
      "Epoch: 15 Step: 3392 Val Loss: 1.373688817024231\n",
      "Epoch: 16 Step: 3604 Val Loss: 1.3641048669815063\n",
      "Epoch: 17 Step: 3816 Val Loss: 1.3607993125915527\n",
      "Epoch: 18 Step: 4028 Val Loss: 1.3577864170074463\n",
      "Epoch: 19 Step: 4240 Val Loss: 1.354580044746399\n",
      "Epoch: 20 Step: 4452 Val Loss: 1.3593766689300537\n",
      "Epoch: 21 Step: 4664 Val Loss: 1.3559399843215942\n",
      "Epoch: 22 Step: 4876 Val Loss: 1.3566992282867432\n",
      "Epoch: 23 Step: 5088 Val Loss: 1.3522279262542725\n",
      "Epoch: 24 Step: 5300 Val Loss: 1.3469754457473755\n",
      "Epoch: 25 Step: 5512 Val Loss: 1.3485548496246338\n",
      "Epoch: 26 Step: 5724 Val Loss: 1.3471248149871826\n",
      "Epoch: 27 Step: 5936 Val Loss: 1.343714952468872\n",
      "Epoch: 28 Step: 6148 Val Loss: 1.3489716053009033\n",
      "Epoch: 29 Step: 6360 Val Loss: 1.3487138748168945\n"
     ]
    }
   ],
   "source": [
    "mlflow.start_run(run_name = RUN_NAME)\n",
    "\n",
    "mlflow.log_param(\"epochs\", epochs)\n",
    "mlflow.log_param(\"batch_size\", batch_size)\n",
    "\n",
    "# Set model to train\n",
    "model.train()\n",
    "\n",
    "# Check to see if using GPU\n",
    "if model.use_gpu:\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "    model.cuda()\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    hidden = model.hidden_state(batch_size)\n",
    "    \n",
    "    \n",
    "    for x,y in generate_batches(train_data, batch_size, seq_len):\n",
    "        \n",
    "        tracker += 1\n",
    "        \n",
    "        # One Hot Encode incoming data\n",
    "        x = one_hot_encoder(x, num_char)\n",
    "        \n",
    "        # Convert Numpy Arrays to Tensor\n",
    "        inputs = torch.from_numpy(x)\n",
    "        targets = torch.from_numpy(y)\n",
    "        \n",
    "        # Adjust for GPU if necessary\n",
    "        if model.use_gpu:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "        # Reset Hidden State\n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        lstm_output, hidden = model.forward(inputs, hidden)\n",
    "        loss = criterion(lstm_output, targets.view(batch_size*seq_len).long())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Clipping gradients to avoid explosion\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if tracker % 100 == 0:\n",
    "            val_hidden = model.hidden_state(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            \n",
    "            for x,y in generate_batches(val_data, batch_size, seq_len):\n",
    "                x = one_hot_encoder(x, num_char)\n",
    "                inputs = torch.from_numpy(x)\n",
    "                targets = torch.from_numpy(y)\n",
    "                \n",
    "                if model.use_gpu:\n",
    "                    inputs = inputs.to(device)\n",
    "                    targets = targets.to(device)\n",
    "                \n",
    "                val_hidden = tuple([state.data for state in val_hidden])\n",
    "                \n",
    "                lstm_output, val_hidden = model.forward(inputs, val_hidden)\n",
    "                val_loss = criterion(lstm_output, targets.view(batch_size*seq_len).long())\n",
    "        \n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "  \n",
    "            mlflow.log_metric(\"Val Loss\", val_loss.item(), step=tracker)\n",
    "        \n",
    "            model.train()\n",
    "            \n",
    "    print(f\"Epoch: {i} Step: {tracker} Val Loss: {val_loss.item()}\")\n",
    "\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = TORCH_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 19:22:16 - INFO - Model saved\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), f'models/{TORCH_MODEL}')\n",
    "logger.info(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharModel(\n",
       "  (lstm): LSTM(84, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc_linear): Linear(in_features=512, out_features=84, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f'models/{TORCH_MODEL}'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_char(model, char, hidden=None, k=1):\n",
    "    \"\"\"\n",
    "    Predicts the next character given an input character and the current hidden state.\n",
    "\n",
    "    This method encodes the input character, feeds it through the trained character-level \n",
    "    language model (e.g., LSTM), and samples from the top-k most probable characters \n",
    "    to determine the next one. It also returns the updated hidden state for sequential prediction.\n",
    "\n",
    "    Args:\n",
    "        char: The input character to start prediction from.\n",
    "        hidden: Current hidden state of the model. Each tensor has shape (num_layers, batch_size, num_hidden).\n",
    "            If None, a new hidden state should be initialized before calling this method.\n",
    "        k: Number of top predictions to sample from.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the predicted next character and the updated hidden state.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        encoded_text = model.encoder[char]\n",
    "        encoded_text = np.array([[encoded_text]])\n",
    "        encoded_text = one_hot_encoder(encoded_text, len(model.all_chars))\n",
    "        inputs = torch.from_numpy(encoded_text)\n",
    "    \n",
    "        if(model.use_gpu):\n",
    "            inputs = inputs.to(device)  \n",
    "\n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        lstm_out, hidden = model(inputs, hidden)        \n",
    "        probs = F.softmax(lstm_out, dim=1).data\n",
    "    \n",
    "        if(model.use_gpu):\n",
    "            probs = probs.cpu()\n",
    "\n",
    "    # Getting the top 'k' for next char probs\n",
    "        probs, index_positions = probs.topk(k)        \n",
    "        index_positions = index_positions.numpy().squeeze()\n",
    "        probs = probs.numpy().flatten()\n",
    "        probs = probs/probs.sum()\n",
    "        char = np.random.choice(index_positions, p=probs)    \n",
    "    \n",
    "        return model.decoder[char], hidden\n",
    "    except Exception as e:\n",
    "            logger.error(f\"Error predicting next char: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, size, seed='The', k=1):\n",
    "    \"\"\"\n",
    "    Generates a sequence of text using the trained character-level language model.\n",
    "\n",
    "    Starting from a seed string, this method uses the model to predict the next character\n",
    "    one at a time, feeding each predicted character back into the model. It continues\n",
    "    this process until the desired output length is reached.\n",
    "\n",
    "    Args:\n",
    "        seed: The initial sequence of characters used to start the text generation.\n",
    "        size: The number of characters to generate after the seed.\n",
    "        k: Number of top character predictions to consider for sampling at each step.\n",
    "\n",
    "    Returns:\n",
    "        The full generated text including the seed and the newly predicted characters.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if(model.use_gpu):\n",
    "            model.to(device)\n",
    "        else:\n",
    "            model.cpu()\n",
    "            \n",
    "        model.eval()\n",
    "        output_chars = [c for c in seed]\n",
    "        hidden = model.hidden_state(1)\n",
    "        \n",
    "        for char in seed:\n",
    "            char, hidden = predict_next_char(model, char, hidden, k=k)\n",
    "\n",
    "        output_chars.append(char)\n",
    "        for i in range(size):\n",
    "            char, hidden = predict_next_char(model, output_chars[-1], hidden, k=k)\n",
    "            output_chars.append(char)\n",
    "            \n",
    "        return ''.join(output_chars)\n",
    "    except Exception as e:\n",
    "            logger.error(f\"Error making predictions: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating a text with 1000 chars starting with word 'Confidence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence of a parley.\n",
      "  Pol. I am sorry too so sooth.\n",
      "  Fal. I am not a particular and any man to say you say 'tis a man\n",
      "    as you will not say that I am a part as the conscience, that I saw the\n",
      "    south this wild that that will say I should strive at his face and to\n",
      "    may be a man to tell the King.\n",
      "  Ham. When I should be thy sight and the such strangers and\n",
      "    save you to the with of him. If that the world is a stranger on\n",
      "    the sun, and the mark to make them a passerrant, that I will\n",
      "    come a stand of a part of a pretty wars.\n",
      "  Fal. That to you, that I say that I was too hand. I will be so too.\n",
      "  Ham. What's his thought?\n",
      "  Ham. Why, tell me what it says the man, I will say to make your son in\n",
      "    my son, and I am so to the streets, to have a soldier.\n",
      "  Prince. What's the man and the subties that there is the senate?\n",
      "  Hor. The sum of the stand of your best than the confederate thought\n",
      "    thou art not all, and then the merry of the store of the sea,\n",
      "    that I am so a monstrous and s\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, 1000, seed='Confidence ', k=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating a text with 1000 chars starting with word 'Love'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love hath some\n",
      "    commands that strikes the subject in his brain, and she\n",
      "    Than that the passion that the storm of their stand\n",
      "    Than though the sea are both of me.\n",
      "  PAROLLES. I had a power that I had been the\n",
      "    show of him, and then which I would not be such a matter. I have\n",
      "    been a man and the common thoughts, and thought I have so belov'd to\n",
      "    then are to be so strongly on the world as I had sent to thee.\n",
      "    Thou art the mountain word that to the sea are this and the state\n",
      "    and so the way of his access of a power of his.\n",
      "  PAGE. I will see the constant soul that I am a man and a state to\n",
      "    mark. Why, there's a man as I say the soul is not so soon and\n",
      "    the story. If I have said you will not to the wars of the chail.\n",
      "    I will be the world's tongue, I will not to the streets and hear me\n",
      "    alone.\n",
      "  COUNTESS. I am a more than thou art the war. To him well.\n",
      "  SECOND LORD. There's a market, and the strange of home.\n",
      "  CHARLES. I would not therefore shall not so, and tha\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, 1000, seed='Love ', k=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging Model to MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/mlflow/pyfunc/utils/data_validation.py:168: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
      "  color_warning(\n"
     ]
    }
   ],
   "source": [
    "class RNNModel(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        \"\"\"\n",
    "        Loads the model and associated artifacts (encoder, decoder) into memory.\n",
    "\n",
    "        Args:\n",
    "            context: MLflow context containing paths to model artifacts.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.model = CharModel(\n",
    "                            all_chars=all_characters,\n",
    "                            num_hidden=512,\n",
    "                            num_layers=3,\n",
    "                            drop_prob=0.5,\n",
    "                            use_gpu=False,\n",
    "                            decoder=context.artifacts['decoder'],\n",
    "                            encoder=context.artifacts['encoder']\n",
    "                                            \n",
    "                        )\n",
    "\n",
    "            self.model.load_state_dict(torch.load(context.artifacts['model_state_dict']))\n",
    "            self.model.eval()\n",
    "            logger.info(\"Loading context done successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading context: {str(e)}\")\n",
    "\n",
    "    def one_hot_encoder(self, encoded_text, num_uni_chars):\n",
    "        \"\"\"\n",
    "        Convert categorical data into a fixed-size vector of numerical values.\n",
    "\n",
    "        Args:\n",
    "            encoded_text: Batch of encoded text.\n",
    "            num_uni_chars: Number of unique characters\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            one_hot = np.zeros((encoded_text.size, num_uni_chars))\n",
    "            one_hot = one_hot.astype(np.float32)\n",
    "            one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0\n",
    "            one_hot = one_hot.reshape((*encoded_text.shape, num_uni_chars))\n",
    "            \n",
    "            return one_hot\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error converting categorical data: {str(e)}\")\n",
    "\n",
    "    def predict_next_char(self, char, hidden=None, k=3):\n",
    "        \"\"\"\n",
    "        Predicts the next character given an input character and the current hidden state.\n",
    "\n",
    "        This method encodes the input character, feeds it through the trained character-level \n",
    "        language model (e.g., LSTM), and samples from the top-k most probable characters \n",
    "        to determine the next one. It also returns the updated hidden state for sequential prediction.\n",
    "\n",
    "        Args:\n",
    "            char: The input character to start prediction from.\n",
    "            hidden: Current hidden state of the model. Each tensor has shape (num_layers, batch_size, num_hidden).\n",
    "                If None, a new hidden state should be initialized before calling this method.\n",
    "            k: Number of top predictions to sample from.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing the predicted next character and the updated hidden state.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            encoded_text = self.model.encoder[char]\n",
    "            encoded_text = np.array([[encoded_text]])\n",
    "            encoded_text = self.one_hot_encoder(encoded_text, len(self.model.all_chars))\n",
    "            inputs = torch.from_numpy(encoded_text)\n",
    "            inputs = inputs.cpu()\n",
    "                \n",
    "            hidden = tuple([state.data for state in hidden])\n",
    "            lstm_out, hidden = self.model(inputs, hidden)    \n",
    "            probs = F.softmax(lstm_out, dim=1).data\n",
    "            probs = probs.cpu()\n",
    "\n",
    "            \n",
    "            probs, index_positions = probs.topk(k)        \n",
    "            index_positions = index_positions.numpy().squeeze()\n",
    "            probs = probs.numpy().flatten()\n",
    "            probs = probs/probs.sum()\n",
    "            char = np.random.choice(index_positions, p=probs)\n",
    "\n",
    "            return self.model.decoder[char], hidden\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error predicting next char: {str(e)}\")\n",
    "\n",
    "    def generate_text(self, seed, size, k=3):\n",
    "        \"\"\"\n",
    "        Generates a sequence of text using the trained character-level language model.\n",
    "\n",
    "        Starting from a seed string, this method uses the model to predict the next character\n",
    "        one at a time, feeding each predicted character back into the model. It continues\n",
    "        this process until the desired output length is reached.\n",
    "\n",
    "        Args:\n",
    "            seed: The initial sequence of characters used to start the text generation.\n",
    "            size: The number of characters to generate after the seed.\n",
    "            k: Number of top character predictions to consider for sampling at each step.\n",
    "\n",
    "        Returns:\n",
    "            The full generated text including the seed and the newly predicted characters.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.model.cpu()\n",
    "                \n",
    "            self.model.eval()\n",
    "            output_chars = [c for c in seed]\n",
    "            hidden = self.model.hidden_state(1)\n",
    "            \n",
    "            for char in seed:\n",
    "                char, hidden = self.predict_next_char(char, hidden, k=k)\n",
    "        \n",
    "            output_chars.append(char)\n",
    "            for i in range(size):\n",
    "                char, hidden = self.predict_next_char(output_chars[-1], hidden, k=k)\n",
    "                output_chars.append(char)\n",
    "                \n",
    "            return ''.join(output_chars)\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating text: {str(e)}\")\n",
    "        \n",
    "    def predict(self, context, model_input):\n",
    "        \"\"\"\n",
    "        Runs inference using the loaded model and input data.\n",
    "\n",
    "        Args:\n",
    "            context: The MLflow context object.\n",
    "            model_input : A dictionary containing 'seed' and 'size' keys.\n",
    "\n",
    "        Returns:\n",
    "             The output from the model containing the predicted text.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            initial_word = model_input['initial_word'][0]\n",
    "            size = model_input['size'][0]\n",
    "            output = self.generate_text(seed=initial_word, size=size)\n",
    "            \n",
    "            return output\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error predicting text: {str(e)}\")\n",
    "\n",
    "    @classmethod\n",
    "    def log_model(cls, model_state_dict, decoder, encoder, demo_folder=\"../demo\"): \n",
    "        \"\"\"\n",
    "        Logs the model to MLflow, including artifacts, dependencies, and input/output signatures.\n",
    "\n",
    "        Args:\n",
    "            model_state_dict: Path where the model is saved before logging.\n",
    "            decoder: Assigns a unique integer to each character in a dictionary format\n",
    "            encoder : Reverses the decoder dictionary, providing a mapping from characters to their respective assigned integers.\n",
    "            demo_folder: Path to the folder containing the compiled demo UI. Defaults to \"demo\".\".\n",
    "        \"\"\"\n",
    "        try:\n",
    "            input_schema = Schema(\n",
    "                [\n",
    "                    ColSpec(\"string\", \"initial_word\"),\n",
    "                    ColSpec(\"long\", \"size\")\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            output_schema = Schema(\n",
    "                [\n",
    "                    ColSpec(\"string\", \"generated_text\")\n",
    "                ]\n",
    "            )\n",
    "        \n",
    "            signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "                \n",
    "            requirements = [\n",
    "                \"torch\",\n",
    "                \"numpy\"\n",
    "            ]\n",
    "            mlflow.pyfunc.log_model(\n",
    "                model_state_dict,\n",
    "                python_model=cls(),\n",
    "                artifacts={\n",
    "                    \"model_state_dict\": model_state_dict, \n",
    "                    'decoder': MODEL_DECODER_PATH, \n",
    "                    'encoder': MODEL_ENCODER_PATH, \n",
    "                    \"demo\": demo_folder},\n",
    "                signature=signature,\n",
    "                pip_requirements=requirements\n",
    "            )\n",
    "            logger.info(\"Logging model to MLflow done successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error logging model to MLflow: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/phoenix/mlflow/382443359114083880', creation_time=1749696638886, experiment_id='382443359114083880', last_update_time=1749696638886, lifecycle_stage='active', name='Shakespeare Text Generation', tags={}>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri('/phoenix/mlflow')\n",
    "mlflow.set_experiment(experiment_name= EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict = MODEL_PATH\n",
    "register_name = REGISTER_NAME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 19:23:13 - INFO - Run's Artifact URI: /phoenix/mlflow/382443359114083880/b5fb53392c984aeea12a7544343a7be0/artifacts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f52708b8b8495fb614f7a4d430ea2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "795feea981b24f7d91ea22b6522130d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42df3dd14b54461a5f66193f8140cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8850d7c538754d38b55db6f4eca81ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 19:23:20 - INFO - Logging model to MLflow done successfully\n",
      "Registered model 'Shakespeare_Model' already exists. Creating a new version of this model...\n",
      "Created version '7' of model 'Shakespeare_Model'.\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name = RUN_NAME) as run:\n",
    "    logger.info(f\"Run's Artifact URI: {run.info.artifact_uri}\")\n",
    "    RNNModel.log_model(model_state_dict, MODEL_DECODER_PATH, MODEL_ENCODER_PATH)\n",
    "    mlflow.register_model(model_uri = f\"runs:/{run.info.run_id}/{model_state_dict}\", name=register_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching the Latest Model Version from MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = mlflow.MlflowClient()\n",
    "model_metadata = client.get_latest_versions(register_name, stages=[\"None\"])\n",
    "latest_model_version = model_metadata[0].version\n",
    "latest_model_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Model and Running Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 19:23:21 - INFO - CharModel initialized successfully\n",
      "2025-06-12 19:23:21 - INFO - Loading context done successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love to the charge.\n",
      "  FALSTAFF. I am sorry to thy side. The court the world shall be.\n",
      "                    \n"
     ]
    }
   ],
   "source": [
    "model = mlflow.pyfunc.load_model(model_uri=f\"models:/{register_name}/{latest_model_version}\")\n",
    "print(model.predict({\"initial_word\": 'Love ', \"size\": 100}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 19:23:22 - INFO -  1:15:31.478280\n"
     ]
    }
   ],
   "source": [
    "logger.info(f' {datetime.now() - start_time_all_execution}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 19:23:22 - INFO - Notebook execution completed.\n"
     ]
    }
   ],
   "source": [
    "logger.info('Notebook execution completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Built with ❤️ using Z by HP AI Studio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
