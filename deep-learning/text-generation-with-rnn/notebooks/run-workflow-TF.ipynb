{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ovpZyIhNIgoq"
   },
   "source": [
    "<h1 style=\\\"text-align: center; font-size: 50px;\\\"> 📜 Text Generation with Neural Networks and Tensorflow</h1>\n",
    "\n",
    "In this notebook our objective is to demonstrate how to generate text using a character-based RNN and Tensorflow working with a dataset of Shakespeare's  writing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook Overview\n",
    "- Start Execution\n",
    "- Install and Import Libraries\n",
    "- Configure Settings\n",
    "- Verify Assets\n",
    "- Get Text Data\n",
    "- Preparing textual data\n",
    "- Text Vectorization\n",
    "- Creating Training Batches\n",
    "- Creating the GRU Model\n",
    "- Instance of the Model\n",
    "- Training the model\n",
    "- Saving the Model\n",
    "- Load Model\n",
    "- Generating Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "# Configure logger\n",
    "logger: logging.Logger = logging.getLogger(\"run_workflow_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.propagate = False  # Prevent duplicate logs from parent loggers\n",
    "\n",
    "# Set formatter\n",
    "formatter: logging.Formatter = logging.Formatter(\n",
    "    fmt=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "# Configure and attach stream handler\n",
    "stream_handler: logging.StreamHandler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 17:56:11 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()  \n",
    "\n",
    "logger.info(\"Notebook execution started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WBd69MDEm4rF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 17:56:18.804249: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-20 17:56:19.070750: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755712579.211481     302 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755712579.254066     302 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-20 17:56:19.528192: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11 s, sys: 5.94 s, total: 17 s\n",
      "Wall time: 17.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "%pip install -r ../requirements.txt --quiet\n",
    "\n",
    "# ------------------------ Standard Library Imports ------------------------\n",
    "import logging\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# ------------------------ Third-Party Libraries ------------------------\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, GRU, InputLayer\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import torch\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global experiment and run names to be used throughout the notebook\n",
    "MODEL_NAME = \"tf_rnn_model.h5\"\n",
    "\n",
    "# Set up the paths\n",
    "DATA_PATH = \"../data/shakespeare.txt\"\n",
    "TENSORBOARD_PATH = \"/phoenix/tensorboard/tensorlogs\"\n",
    "\n",
    "\n",
    "# Set up the chunk separator for text processing\n",
    "CHUNK_SEPARATOR = \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_all_execution = datetime.now() # This variable is to help us to see in how much time this notebook will run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 17:56:29 - INFO - The Dataset is properly configured.\n"
     ]
    }
   ],
   "source": [
    "# Check whether the Dataset file exists\n",
    "is_dataset_available = Path(DATA_PATH).exists()\n",
    "\n",
    "# Log the configuration status of the dataset\n",
    "if is_dataset_available:\n",
    "    logger.info(\"The Dataset is properly configured.\")\n",
    "else:\n",
    "    logger.info(\n",
    "        \"The Dataset is not properly configured. Please check if Dataset was downloaded\"\n",
    "        \"in your project on AI Studio.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the text we'll use as a basis for our generations: let's try to generate 'Shakespearean' texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This text is from Shakespeare's Sonnet 1. It's one of the 154 sonnets written by William Shakespeare that were first published in 1609. This particular sonnet, like many others, discusses themes of beauty, procreation, and the transient nature of life, urging the beautiful to reproduce so their beauty can live on through their offspring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pD_55cOxLkAb"
   },
   "outputs": [],
   "source": [
    "path_to_file = DATA_PATH\n",
    "text = open(path_to_file, 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aavnuByVymwK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 17:56:30 - INFO - First 600 chars: \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bud buriest thy content,\n",
      "  And tender churl mak'st waste in niggarding:\n",
      "    Pity the world, or else th\n"
     ]
    }
   ],
   "source": [
    "logger.info('First 600 chars: \\n')\n",
    "print(text[:600])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rNnrKn_lL-IJ"
   },
   "source": [
    "## Preparing textual data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to encode our data to give the model a proper numerical representation of our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a set of unique characters found in the text\n",
    "vocab = sorted(set(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LFjSVAlWzf-N"
   },
   "source": [
    "## Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IalZLbvOzf-F"
   },
   "outputs": [],
   "source": [
    "char_to_int = {u:i for i, u in enumerate(vocab)}\n",
    "# assigns a unique integer to each character in a dictionary format, \n",
    "# creating a mapping that can later be used to transform encoded predictions back into characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "30ZYaWAOm4rt"
   },
   "outputs": [],
   "source": [
    "int_to_char = np.array(vocab)\n",
    "# reverses the decoder dictionary, providing a mapping from characters to their respective assigned integers, which is used to encode the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3fhOqV0lm4r2"
   },
   "outputs": [],
   "source": [
    "encoded_text = np.array([char_to_int[c] for c in text])\n",
    "# encodes the entire text as an array of integers, with each integer representing the character at that position\n",
    "# in the text according to the encoder dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bbmsf23Bymwe"
   },
   "source": [
    "## Creating Training Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hgsVvVxnymwf"
   },
   "source": [
    "Training batches are a way of dividing the dataset into smaller, manageable groups of data points that are fed into a machine learning model during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ciatnowvm4se"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1755712591.299069     302 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13021 MB memory:  -> device: 0, name: NVIDIA RTX 5000 Ada Generation Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "seq_len = 120 # length of sequence for a training example\n",
    "total_num_seq = len(text)//(seq_len+1) # total number of training examples\n",
    "\n",
    "# Create Training Sequences\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
    "sequences = char_dataset.batch(seq_len+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9NGu-FkO_kYU"
   },
   "outputs": [],
   "source": [
    "def create_seq_targets(seq):\n",
    "    \"\"\"\n",
    "    Function that takes a sequence as input, duplicates, and shifts it to align the input and label. \n",
    "\n",
    "    Args:\n",
    "        seq: sequence of characters\n",
    "\n",
    "    Returns:\n",
    "        The text input and corresponding target.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        input_txt = seq[:-1]\n",
    "        target_txt = seq[1:]\n",
    "        return input_txt, target_txt\n",
    "    except Exception as e:\n",
    "            logger.error(f\"Error creating sequences of targets: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HszljTg8m4so"
   },
   "outputs": [],
   "source": [
    "dataset = sequences.map(create_seq_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p2pGotuNzf-S"
   },
   "outputs": [],
   "source": [
    "# Batch size\n",
    "batch_size = 128\n",
    "buffer_size = 10000\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r6oUuElIMgVx"
   },
   "source": [
    "## Creating the GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zHT8cLh7EAsg"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "# The embedding dimension\n",
    "embed_dim = 64\n",
    "# Number of RNN units\n",
    "rnn_neurons = 1026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FrOOK61Olm1C"
   },
   "outputs": [],
   "source": [
    "def sparse_cat_loss(y_true,y_pred):\n",
    "  return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MtCrdfzEI2N0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 17:56:33 - INFO - Model architecture created successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,376</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1026</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,361,176</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">84</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">86,268</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;34m128\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │         \u001b[38;5;34m5,376\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ (\u001b[38;5;34m128\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1026\u001b[0m)      │     \u001b[38;5;34m3,361,176\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m128\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m84\u001b[0m)        │        \u001b[38;5;34m86,268\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,452,820</span> (13.17 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,452,820\u001b[0m (13.17 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,452,820</span> (13.17 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,452,820\u001b[0m (13.17 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_model(vocab_size, embed_dim, rnn_neurons, batch_size):\n",
    "    \"\"\"Architecture to create the model.\n",
    "\n",
    "    Args:\n",
    "        vocab_size: Length of the vocabulary in chars.\n",
    "        embed_dim: Embedding dimension.\n",
    "        rnn_neurons: Number of RNN units.\n",
    "        batch_size: Size of the batchs.\n",
    "\n",
    "    Returns:\n",
    "        Model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = Sequential()\n",
    "        model.add(InputLayer(batch_shape=(batch_size, None)))\n",
    "        \n",
    "        model.add(Embedding(input_dim=vocab_size, output_dim=embed_dim))\n",
    "\n",
    "        model.add(GRU(rnn_neurons,\n",
    "                    return_sequences=True,\n",
    "                    stateful=True,\n",
    "                    recurrent_initializer='glorot_uniform'))\n",
    "\n",
    "        model.add(Dense(vocab_size))\n",
    "        model.compile(optimizer='adam', loss=sparse_cat_loss)\n",
    "        logger.info(\"Model architecture created successfully\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "            logger.error(f\"Error creating model architecture: {str(e)}\")\n",
    "\n",
    "model = create_model(vocab_size, embed_dim, rnn_neurons, batch_size)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wwsrpOik5zhv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 17:56:33 - INFO - Model architecture created successfully\n"
     ]
    }
   ],
   "source": [
    "model = create_model(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    rnn_neurons=rnn_neurons,\n",
    "    batch_size=batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard\n",
    "log_dir = TENSORBOARD_PATH\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LJL0Q0YPY6Ee"
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A4ygvfHn-wan"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1755712597.921392     449 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 120, 84)  <=== (batch_size, sequence_length, vocab_size)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 17:56:38.142572: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "\n",
    "  # Predict off some random batch\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "\n",
    "  # Display the dimensions of the predictions\n",
    "  print(example_batch_predictions.shape, \" <=== (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ld8z3LPBAuv"
   },
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "# Reformat to not be a lists of lists\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZYDQjKTlm4s8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 41ms/step - loss: 2.8689\n",
      "Epoch 2/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 43ms/step - loss: 1.6471\n",
      "Epoch 3/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 40ms/step - loss: 1.4022\n",
      "Epoch 4/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 38ms/step - loss: 1.3079\n",
      "Epoch 5/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 38ms/step - loss: 1.2562\n",
      "Epoch 6/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - loss: 1.2221\n",
      "Epoch 7/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - loss: 1.1953\n",
      "Epoch 8/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 37ms/step - loss: 1.1734\n",
      "Epoch 9/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 37ms/step - loss: 1.1541\n",
      "Epoch 10/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 37ms/step - loss: 1.1391\n",
      "Epoch 11/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 36ms/step - loss: 1.1246\n",
      "Epoch 12/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 37ms/step - loss: 1.1095\n",
      "Epoch 13/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 36ms/step - loss: 1.0975\n",
      "Epoch 14/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 37ms/step - loss: 1.0846\n",
      "Epoch 15/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 36ms/step - loss: 1.0762\n",
      "Epoch 16/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 36ms/step - loss: 1.0638\n",
      "Epoch 17/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 36ms/step - loss: 1.0540\n",
      "Epoch 18/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 36ms/step - loss: 1.0444\n",
      "Epoch 19/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 36ms/step - loss: 1.0367\n",
      "Epoch 20/20\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 36ms/step - loss: 1.0288\n",
      "CPU times: user 11min 12s, sys: 1min 5s, total: 12min 17s\n",
      "Wall time: 5min 12s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7ca326544ec0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 20\n",
    "model.fit(dataset,epochs=epochs, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "2025-08-20 18:01:50 - INFO - Model saved\n"
     ]
    }
   ],
   "source": [
    "model.save(f'models/{model_name}') \n",
    "logger.info(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kKkD5M6eoSiN"
   },
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_iXG3VJvEXWM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 18:01:51 - INFO - Model architecture created successfully\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocab_size, embed_dim, rnn_neurons, batch_size=1)\n",
    "\n",
    "model.load_weights(f'models/{model_name}')\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WvuwZBX5Ogfd"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_seed=\"The \", gen_size=100, temp=1.0):\n",
    "    \"\"\"\n",
    "    Generates a sequence of text using the trained character-level language model.\n",
    "\n",
    "    Args:\n",
    "        model: Model created on function create_model\n",
    "        start_seed: Set of characters that will be the beginning of the text. \n",
    "        gen_size : Number of characters. Defaults to 100.\n",
    "        temp: Controls the randomness of the predictions made by the model.\n",
    "\n",
    "    Returns:\n",
    "        The full generated text including the seed and the newly predicted characters.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        num_generate = gen_size\n",
    "        input_eval = [char_to_int[s] for s in start_seed]\n",
    "        input_eval = tf.expand_dims(input_eval, 0)\n",
    "        text_generated = []\n",
    "        temperature = temp\n",
    "\n",
    "\n",
    "        for i in range(num_generate):\n",
    "            predictions = model(input_eval)\n",
    "            predictions = tf.squeeze(predictions, 0)\n",
    "            predictions = predictions / temperature\n",
    "            predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "            input_eval = tf.expand_dims([predicted_id], 0)\n",
    "            text_generated.append(int_to_char[predicted_id])\n",
    "\n",
    "        return start_seed + ''.join(text_generated)\n",
    "    except Exception as e:\n",
    "            logger.error(f\"Error making predictions: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating a text with 1000 chars starting with word 'Confidence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bS69SG5D5lwd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence.\n",
      "  KING RICHARD. Thanks, gracious. Even here 'tis ript.\n",
      "  CASSIO. Do not you look, and bear you; much my mistress,\n",
      "    He hear the Queen's brains, not foolish that blow shielded thou know'st it, sir. Call\n",
      "    thus.  good brother,\n",
      "    Allow me,'t fast, and, spite, every one of child.\n",
      "  TROILUS. Be thou as fathors paying homage. Well, the uglye simplicity.\n",
      "    Now start-something fellow; but therefore\n",
      "    Was never your custom'd look each other\n",
      "    To man of mildly ignorant back,\n",
      "    None else now forc'd so God. What news, your plainness\n",
      "    finds not their age to bear'd?\n",
      "  VALENTINE. No, how the better door! Look of our journey,\n",
      "    No longer gnaw thee to out,\n",
      "    And in this kind of five for dreams on spur;\n",
      "    Peering her faction; pothey, and his point.\n",
      "  4. Watch. Note learning, brother, for all, Come, puth a madman himself.\n",
      "\n",
      "                  Enter CHIRON,  How now, Sir Thouchs a park him; and\n",
      "    no more than join'd, I must needs but bell retain you\n",
      "    against all. Marcus, what a\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    if hasattr(layer, 'reset_states'):\n",
    "        layer.reset_states()\n",
    "print(generate_text(model, start_seed=\"Confidence\", gen_size=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating a text with 1000 chars starting with word 'Love'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love! That cuttingly\n",
      "    say't by her thread as English toans, as false Lupitol, son.\n",
      "  HELENA. What news, how long bach way?.\n",
      "    I have sat for me, my lord among,\n",
      "    Thou unchanted from the bulk\n",
      "    That thou art torn and bring you first.  \n",
      "    Then thou art true-like the grief that wring you king.\n",
      "    Saddle which are these the fools some wealth INE. Tell me, my your company-hangings is in lume, yet sweetly lack'd. Here comes it, Trim. Unless I was to de\n",
      "    Which from the pretty overcale weak\n",
      "    The men that follows here, and you mistake\n",
      "    Betwixt torn furnish'd firstown that by A maid or sweetest flames,\n",
      "    But he himself is my pot's to this tomb,\n",
      "    I do not to the Tepera's cloak cants in thee,\n",
      "    O swill fills kindly walk, and song to Greeks,\n",
      "    Behold what voice may awake thee stone;\n",
      "\n",
      "                   Some the ladiest climbing!\n",
      "  Edg. [To VALENTIN]\n",
      "  KING RICHARK. My love. Believe will I come to Aufidemisonemay should stink a young venon and a familiar buy to lish and exe\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_seed=\"Love\", gen_size=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 18:02:17 - INFO - ⏱️ Total execution time: 6m 5.31s\n",
      "2025-08-20 18:02:17 - INFO - ✅ Notebook execution completed successfully.\n"
     ]
    }
   ],
   "source": [
    "end_time: float = time.time()\n",
    "elapsed_time: float = end_time - start_time\n",
    "elapsed_minutes: int = int(elapsed_time // 60)\n",
    "elapsed_seconds: float = elapsed_time % 60\n",
    "\n",
    "logger.info(f\"⏱️ Total execution time: {elapsed_minutes}m {elapsed_seconds:.2f}s\")\n",
    "logger.info(\"✅ Notebook execution completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Built with ❤️ using Z by HP AI Studio."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "00-Generating-Text-with-RNNs.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
