{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f6fbfc8-2cd1-4f92-b1f3-5bbcb2b471b2",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-size: 50px;\"> üåç Word Embeddings Generation</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181089eb-c770-45f3-95b7-67e91fa8a7d9",
   "metadata": {},
   "source": [
    "This Jupyter notebook demonstrates how to generate word embeddings from a given corpus using a pre-trained BERT model. These embeddings will be used to find semantically similar matches for a user query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4a84a6-5525-4e88-b294-bd9281fded03",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "- Imports\n",
    "- Configurations\n",
    "- Verify Assets\n",
    "- Load and Preprocess Data\n",
    "- Initialize BERT Tokenizer and Model\n",
    "- Generate Embeddings in Batches\n",
    "- Save Embeddings to File\n",
    "- Downloading the Bert Large Uncased Model\n",
    "- Defining the BERT Tourism Model Class\n",
    "- Logging Model to MLflow\n",
    "- Fetching the Latest Model Version from MLflow\n",
    "- Loading the Model and Running Inference\n",
    "- Displaying Results for the Input Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0992461b-63d7-479e-82fc-28723f297390",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed46e1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.14.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/deep_ep-1.0.0+a84a248-py3.12-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.26a0+c5e1555-py3.12-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required Python packages listed in requirements.txt silently\n",
    "%pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f56c9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[NeMo W 2025-06-13 15:22:17 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os  \n",
    "import logging\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Data manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Deep learning framework\n",
    "import torch  \n",
    "\n",
    "# NLP libraries\n",
    "import nltk  # Natural Language Toolkit\n",
    "from nemo.collections.nlp.models import BERTLMModel  # BERT Language Model from NVIDIA NeMo\n",
    "from transformers import AutoTokenizer  # Tokenizer for transformer-based models\n",
    "from transformers import logging as hf_logging\n",
    "\n",
    "# MLflow for Experiment Tracking and Model Management\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types.schema import Schema, ColSpec, TensorSpec, ParamSchema, ParamSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c61ad4-2cdf-443f-b6bc-f26184180536",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60988811-3eb5-4aff-9e04-245c2ec5cc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ Suppress Verbose Logs ------------------------\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Hugging Face Transformers logs\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "# NVIDIA NeMo logs\n",
    "logging.getLogger(\"nemo_logger\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7602f807-b638-4c44-b228-68a56c0b5a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logger\n",
    "logger = logging.getLogger(\"tourism_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", \n",
    "                              datefmt=\"%Y-%m-%d %H:%M:%S\")  \n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d21fc54d-eb42-432b-8a8a-9f0885fde221",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_PATH = \"../data/raw/corpus.csv\"\n",
    "TOKENIZER_DIR = \"../artifacts/tokenizer\"\n",
    "BERT_MODEL_NAME = \"bert-large-uncased\"\n",
    "BERT_MODEL_DATAFABRIC_PATH = \"/home/jovyan/datafabric/Bertlargeuncased/bertlargeuncased.nemo\"\n",
    "EMBEDDINGS_OUTPUT_PATH = \"../data/processed/\"\n",
    "BERT_MODEL_ONLINE_PATH = \"/root/.cache/torch/NeMo/NeMo_1.22.0/bertlargeuncased/ca4ebba9f05a8ffb79845249ca046983/bertlargeuncased.nemo\"\n",
    "DEMO_PATH = \"../demo\"\n",
    "EMBEDDINGS_PATH = \"../data/processed/embeddings.csv\"\n",
    "EXPERIMENT_NAME = \"BERT_Tourism_Experiment\"\n",
    "RUN_NAME = \"BERT_Tourism_Run\"\n",
    "MODEL_NAME = \"BERT_Tourism_Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eca84c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e97ae0bc-7bce-4948-9ed4-2ec6d0f4a90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 15:22:18 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "logger.info('Notebook execution started.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef823d05-3c57-4497-bf74-39f7e7d8a0a6",
   "metadata": {},
   "source": [
    "# Verify Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82e42f2a-529e-4d30-b570-2389d2767354",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 15:22:18 - INFO - BERT model is properly configured. \n",
      "2025-06-13 15:22:18 - INFO - Corpus data is properly configured. \n"
     ]
    }
   ],
   "source": [
    "def log_asset_status(asset_path: str, asset_name: str, success_message: str, failure_message: str) -> None:\n",
    "    \"\"\"\n",
    "    Logs the status of a given asset based on its existence.\n",
    "\n",
    "    Parameters:\n",
    "        asset_path (str): File or directory path to check.\n",
    "        asset_name (str): Name of the asset for logging context.\n",
    "        success_message (str): Message to log if asset exists.\n",
    "        failure_message (str): Message to log if asset does not exist.\n",
    "    \"\"\"\n",
    "    if Path(asset_path).exists():\n",
    "        logger.info(f\"{asset_name} is properly configured. {success_message}\")\n",
    "    else:\n",
    "        logger.info(f\"{asset_name} is not properly configured. {failure_message}\")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=BERT_MODEL_DATAFABRIC_PATH ,\n",
    "    asset_name=\"BERT model\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please create and download the required assets in your project on AI Studio.\"\n",
    ")\n",
    "\n",
    "log_asset_status(\n",
    "    asset_path=CORPUS_PATH,\n",
    "    asset_name=\"Corpus data\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please check if Corpus was properly downloaded in your project on AI Studio.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4635eb-9224-4a55-b8c5-a8d519c721c6",
   "metadata": {},
   "source": [
    "# Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d09b9bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the Punkt tokenizer data for sentence tokenization\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b4414a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 15:22:22 - INFO - First few entries of the DataFrame:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  Topic                                             Pledge\n",
      "0           0      1  Actually we as an association are still pretty...\n",
      "1           1      1  EFFAT welcomes the Commission Proposal for a R...\n",
      "2           2      1  HOTREC calls for a level playing field and fai...\n",
      "3           3      1  Estonia sees the need to synchronize and harmo...\n",
      "4           4      1  Sphere Travel Club contributes to a flourishin...\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset into a Pandas DataFrame\n",
    "corpus_df = pd.read_csv(CORPUS_PATH)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "logger.info(\"First few entries of the DataFrame:\")\n",
    "print(corpus_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a72e3471",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = corpus_df[\"Pledge\"].astype(str).tolist()  # Convert the column to a list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5b1b05-703e-477c-b68a-9ebcd963df99",
   "metadata": {},
   "source": [
    "# Initialize BERT Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f9e6430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 15:22:25 - INFO - Loading BERT model...\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Error while downloading from https://huggingface.co/bert-large-uncased/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "2025-06-13 15:25:43 - INFO - BERT model loaded successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.8 s, sys: 31.5 s, total: 48.3 s\n",
      "Wall time: 3min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Initialize the tokenizer with a pre-trained BERT model\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "tokenizer.save_pretrained(TOKENIZER_DIR)\n",
    "\n",
    "# Set device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "logger.info(\"Loading BERT model...\")\n",
    "\n",
    "# Ensure you have added the 'bertlargeuncased' model from the NVIDIA NGC model catalog.\n",
    "# If unavailable, use the alternative method below to download the model online.\n",
    "\n",
    "# Uncomment the following line to download the BERT model online:\n",
    "# bert_model = BERTLMModel.from_pretrained(model_name=\"bertlargeuncased\", strict=False).to(device)\n",
    "\n",
    "# Load the BERT model from a local .nemo file inside datafabric folder\n",
    "bert_model = BERTLMModel.restore_from(BERT_MODEL_DATAFABRIC_PATH, strict=False).to(device)\n",
    "\n",
    "logger.info(\"BERT model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cac586-cea0-4272-b959-d7c0e405e359",
   "metadata": {},
   "source": [
    "# Generate Embeddings in Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6010c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings_in_batches(texts, tokenizer, model, batch_size=32):\n",
    "    \"\"\"\n",
    "    Generates text embeddings using the NeMo BERT model in batches.\n",
    "    \n",
    "    Args:\n",
    "        texts (list of str): List of input texts.\n",
    "        tokenizer: Pretrained tokenizer.\n",
    "        model: Pretrained NeMo BERT model.\n",
    "        batch_size (int, optional): Batch size for processing. Default is 32.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Generated embeddings.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    all_embeddings = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        \n",
    "        # Tokenize batch with padding and truncation\n",
    "        encoded_input = tokenizer(\n",
    "            batch_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=128\n",
    "        )\n",
    "        encoded_input = {key: val.to(device) for key, val in encoded_input.items()}\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation for inference\n",
    "            output = model.bert_model(**encoded_input)\n",
    "        \n",
    "        # Extract the CLS token representation for embeddings\n",
    "        embeddings = output[:, 0, :].cpu().numpy()  # CLS token representation\n",
    "        all_embeddings.append(embeddings)\n",
    "\n",
    "    return np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373c7c77-7a19-469d-8962-862a4b9b7ce7",
   "metadata": {},
   "source": [
    "# Save Embeddings to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a03e94c9-6b53-4812-9226-a066896b95cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 15:29:42 - INFO - ‚úÖ Embedding completed and saved to: ../data/processed/embeddings.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 49s, sys: 2.71 s, total: 2min 51s\n",
      "Wall time: 3min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Generate embeddings using the pre-trained model\n",
    "embeddings = generate_embeddings_in_batches(documents, tokenizer, bert_model)\n",
    "\n",
    "# Convert embeddings into a DataFrame\n",
    "df_embeddings = pd.DataFrame(embeddings)\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(EMBEDDINGS_OUTPUT_PATH, exist_ok=True)\n",
    "    \n",
    "# Define output file path\n",
    "output_file = os.path.join(EMBEDDINGS_OUTPUT_PATH, \"embeddings.csv\")\n",
    "\n",
    "# Save embeddings\n",
    "df_embeddings.to_csv(output_file , index=False)\n",
    "\n",
    "logger.info(f\"‚úÖ Embedding completed and saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba27084",
   "metadata": {},
   "source": [
    "# Downloading the Bert Large Uncased Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "008a54b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you have added the 'bertlargeuncased' model from the NVIDIA NGC model catalog.\n",
    "# If unavailable, uncomment the following line and use the alternative method below to download the BERT model online.\n",
    "# bert_model = BERTLMModel.from_pretrained(model_name=\"bertlargeuncased\", strict=False).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd11732",
   "metadata": {},
   "source": [
    "# Defining the BERT Tourism Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55a65613",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTTourismModel(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        \"\"\"\n",
    "        Load precomputed embeddings, corpus, and the pre-trained BERT model.\n",
    "        \"\"\"\n",
    "        # Load precomputed embeddings and corpus data\n",
    "        self.embeddings_df = pd.read_csv(context.artifacts['embeddings_path'])\n",
    "        self.corpus_df = pd.read_csv(context.artifacts['corpus_path'])\n",
    "        \n",
    "        # Load tokenizer for BERT\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(context.artifacts[\"tokenizer_dir\"])\n",
    "        \n",
    "        # Set device to GPU if available, otherwise use CPU\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Load pre-trained BERT model\n",
    "        self.bert_model = BERTLMModel.restore_from(context.artifacts['bert_model_path'], strict=False).to(self.device)\n",
    "    \n",
    "    def generate_query_embedding(self, query):\n",
    "        \"\"\"\n",
    "        Generate BERT embeddings for the input query.\n",
    "        \"\"\"\n",
    "        self.bert_model.eval()  # Set model to evaluation mode\n",
    "        \n",
    "        # Tokenize the input query and move tensors to the selected device\n",
    "        encoded_input = self.tokenizer(query, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "        encoded_input = {key: val.to(self.device) for key, val in encoded_input.items()}\n",
    "        \n",
    "        # Get the model's output embedding\n",
    "        with torch.no_grad():\n",
    "            output = self.bert_model.bert_model(**encoded_input)\n",
    "        \n",
    "        # Return the [CLS] token embedding as a NumPy array\n",
    "        return output[:, 0, :].cpu().numpy()\n",
    "    \n",
    "    def predict(self, context, model_input, params):\n",
    "        \"\"\"\n",
    "        Compute similarity between query and precomputed embeddings,\n",
    "        then return the top 5 most similar results.\n",
    "        \"\"\"\n",
    "        # Extract the query string from model input\n",
    "        query = model_input[\"query\"][0]\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.generate_query_embedding(query)\n",
    "        \n",
    "        # Compute cosine similarity between query and precomputed embeddings\n",
    "        similarities = cosine_similarity(query_embedding, self.embeddings_df.values)\n",
    "        \n",
    "        # Get indices of top 5 most similar results\n",
    "        top_indices = np.argsort(similarities[0])[::-1][:5]\n",
    "        \n",
    "        # Retrieve corresponding results from the corpus\n",
    "        results = self.corpus_df.iloc[top_indices].copy()\n",
    "        results.loc[:, 'Similarity'] = similarities[0][top_indices]\n",
    "        \n",
    "        # Return results as a dictionary\n",
    "        return results.to_dict(orient=\"records\")\n",
    "    \n",
    "    @classmethod\n",
    "    def log_model(cls, model_name):\n",
    "        \"\"\"\n",
    "        Logs the model to MLflow with appropriate artifacts and schema.\n",
    "        \"\"\"\n",
    "        # Define input and output schema\n",
    "        input_schema = Schema([ColSpec(\"string\", \"query\")])\n",
    "        output_schema = Schema([\n",
    "            TensorSpec(np.dtype(\"object\"), (-1,), \"List of Pledges and Similarities\")\n",
    "        ])\n",
    "        params_schema = ParamSchema([ParamSpec(\"show_score\", \"boolean\", False)])\n",
    "        \n",
    "        # Define model signature\n",
    "        signature = ModelSignature(inputs=input_schema, outputs=output_schema, params=params_schema)\n",
    "        \n",
    "        # Log the model in MLflow\n",
    "        mlflow.pyfunc.log_model(\n",
    "            model_name,\n",
    "            python_model=cls(),\n",
    "            artifacts={\n",
    "                \"corpus_path\": CORPUS_PATH,\n",
    "                \"embeddings_path\": EMBEDDINGS_PATH, \n",
    "                \"tokenizer_dir\": TOKENIZER_DIR, \n",
    "                # If you are using the downloaded bert model then uncomment the line below and comment the other bert model line that uses nemo model from datafabric\n",
    "                #\"bert_model_path\": BERT_MODEL_ONLINE_PATH,            \n",
    "                \"bert_model_path\": BERT_MODEL_DATAFABRIC_PATH,\n",
    "                \"demo\": DEMO_PATH,\n",
    "            },\n",
    "            signature=signature\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca1787a",
   "metadata": {},
   "source": [
    "# Logging Model to MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "618cc00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 15:29:42 - INFO - Starting the experiment: BERT_Tourism_Experiment\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 50.84it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.85it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 310.32it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:11<00:00, 11.17s/it]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 705.07it/s]  \n",
      "2025/06/13 15:31:23 WARNING mlflow.utils.requirements_utils: The following packages were not found in the public PyPI package index as of 2025-03-04; if these packages are not present in the public PyPI index, you must install them manually before loading your model: {'pynvjitlink'}\n",
      "2025/06/13 15:31:23 WARNING mlflow.utils.requirements_utils: Found deep-ep version (1.0.0+a84a248) contains a local version label (+a84a248). MLflow logged a pip requirement for this package as 'deep-ep==1.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/06/13 15:31:23 WARNING mlflow.utils.requirements_utils: Found transformer-engine version (2.2.0.dev0+bee4649) contains a local version label (+bee4649). MLflow logged a pip requirement for this package as 'transformer-engine==2.2.0.dev0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/06/13 15:31:23 WARNING mlflow.utils.requirements_utils: Found pytorch-triton version (3.2.0+gitb2684bf3b.nvinternal) contains a local version label (+gitb2684bf3b.nvinternal). MLflow logged a pip requirement for this package as 'pytorch-triton==3.2.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "Successfully registered model 'BERT_Tourism_Model'.\n",
      "Created version '1' of model 'BERT_Tourism_Model'.\n",
      "2025-06-13 15:31:35 - INFO - Registered the model: BERT_Tourism_Model\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'Starting the experiment: {EXPERIMENT_NAME}')\n",
    "\n",
    "mlflow.set_tracking_uri('/phoenix/mlflow')\n",
    "# Set the MLflow experiment name\n",
    "mlflow.set_experiment(experiment_name=EXPERIMENT_NAME)\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "    # Print the artifact URI for reference\n",
    "    logging.info(f\"Run's Artifact URI: {run.info.artifact_uri}\")\n",
    "    \n",
    "    # Log the BERT similarity model to MLflow\n",
    "    BERTTourismModel.log_model(model_name=MODEL_NAME)\n",
    "\n",
    "    # Register the logged model in MLflow Model Registry\n",
    "    mlflow.register_model(\n",
    "        model_uri=f\"runs:/{run.info.run_id}/{MODEL_NAME}\", \n",
    "        name=MODEL_NAME\n",
    "    )\n",
    "\n",
    "logger.info(f'Registered the model: {MODEL_NAME}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fc2201",
   "metadata": {},
   "source": [
    "# Fetching the Latest Model Version from MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6adf42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest Model Version: 1\n",
      "Model Signature: inputs: \n",
      "  ['query': string (required)]\n",
      "outputs: \n",
      "  ['List of Pledges and Similarities': Tensor('object', (-1,))]\n",
      "params: \n",
      "  ['show_score': boolean (default: False)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the MLflow client\n",
    "client = MlflowClient()\n",
    "\n",
    "# Retrieve the latest version of the \"BERT_Tourism_Model\" model (not yet in a specific stage)\n",
    "model_metadata = client.get_latest_versions(MODEL_NAME, stages=[\"None\"])\n",
    "latest_model_version = model_metadata[0].version  # Extract the latest model version\n",
    "\n",
    "# Fetch model information, including its signature\n",
    "model_info = mlflow.models.get_model_info(f\"models:/{MODEL_NAME}/{latest_model_version}\")\n",
    "\n",
    "# Print the latest model version and its signature\n",
    "print(f\"Latest Model Version: {latest_model_version}\")\n",
    "print(f\"Model Signature: {model_info.signature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1979cad8",
   "metadata": {},
   "source": [
    "# Loading the Model and Running Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "792518db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained BERT similarity model from MLflow\n",
    "model = mlflow.pyfunc.load_model(model_uri=f\"models:/{MODEL_NAME}/{latest_model_version}\")\n",
    "\n",
    "# Define a sample query for testing\n",
    "query = \"Give me a resort budget vacation suggestion\"\n",
    "\n",
    "# Use the model to predict similar results based on the query\n",
    "result = model.predict({\"query\": [query]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daebb69",
   "metadata": {},
   "source": [
    "# Displaying Results for the Input Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6b19355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ïí‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïï\n",
      "‚îÇ    ‚îÇ Recommended Option                                                                                  ‚îÇ   Relevance Score ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ  0 ‚îÇ For a budget-friendly vacation, consider a resort with vacation options and cruise activities.      ‚îÇ          0.869167 ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ  1 ‚îÇ For a budget-friendly vacation, consider a getaway with beach options and vacation activities.      ‚îÇ          0.863822 ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ  2 ‚îÇ For a budget-friendly vacation, consider a getaway with hotel options and vacation activities.      ‚îÇ          0.862292 ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ  3 ‚îÇ For a budget-friendly vacation, consider a reservation with beach options and mountains activities. ‚îÇ          0.861587 ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ  4 ‚îÇ For a budget-friendly wildlife, consider a vacation with vacation options and holiday activities.   ‚îÇ          0.861586 ‚îÇ\n",
      "‚ïò‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïõ\n"
     ]
    }
   ],
   "source": [
    "# Convert the result into a pandas DataFrame\n",
    "df = pd.DataFrame(result)\n",
    "\n",
    "# Drop unnecessary columns if needed\n",
    "df = df.drop(columns=[\"Unnamed: 0\", \"Topic\"], errors=\"ignore\")\n",
    "\n",
    "# Rename columns for better readability\n",
    "df.rename(columns={\"Pledge\": \"Recommended Option\", \"Similarity\": \"Relevance Score\"}, inplace=True)\n",
    "\n",
    "# Display the DataFrame in a tabular format\n",
    "print(tabulate(df, headers=\"keys\", tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a24a3b99-3081-4559-8130-d8c5a401a20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 15:32:16 - INFO - Notebook execution completed.\n"
     ]
    }
   ],
   "source": [
    "logger.info('Notebook execution completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2315db8e-c3c8-4ca4-a458-fcae2b7d74d1",
   "metadata": {},
   "source": [
    "Built with ‚ù§Ô∏è using Z by HP AI Studio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aistudio",
   "language": "python",
   "name": "aistudio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
