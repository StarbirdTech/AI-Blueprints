{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5f8d3ca-e260-4d56-967d-dc4feb92e2e4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h1 style=\\\"text-align: center; font-size: 50px;\\\"> ðŸ¤– MLFlow Registration for Agentic RAG </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b7b5e0-2acd-4729-b8a2-03e81cf0df2d",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c533cc2-9ca5-442a-9409-bbad88be3eb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirements.txt --quiet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c2521b3-7544-4966-b0c2-c778eed7801b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-06-13 21:21:44,000 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorRT-LLM] TensorRT-LLM version: 0.18.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from collections import namedtuple\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Literal, Optional, TypedDict\n",
    "\n",
    "import pandas as pd\n",
    "import tensorrt_llm\n",
    "\n",
    "import mlflow.pyfunc\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.types import ColSpec, DataType, Schema\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Ensure the project root is on the Python path so we can import local modules\n",
    "project_root = Path('.').resolve().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.trt_llm_langchain import TensorRTLangchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "072a781b-9d7c-4b5b-bf58-bc0a11ae5d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_FORMAT = \"%(asctime)s [%(levelname)s] %(name)s: %(message)s\"\n",
    "LOG_DATEFMT = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=LOG_FORMAT,\n",
    "    datefmt=LOG_DATEFMT,\n",
    ")\n",
    "\n",
    "# Named logger for the Agentic RAG notebook\n",
    "logger = logging.getLogger(\"agentic_rag_notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92c8dc01-e50f-4077-9056-b966895ad8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 21:21:46 [INFO] agentic_rag_notebook: Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "logger.info('Notebook execution started.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25b8b8bb-262b-4892-aedb-7b2047895e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/mlflow/pyfunc/utils/data_validation.py:168: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
      "  color_warning(\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------\n",
    "# RagAgenticModel\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "class RagAgenticModel(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"\n",
    "    rag_mlflow_model.py\n",
    "    \n",
    "    This module defines an MLflow PythonModel subclass (`RagAgenticModel`) that faithfully reproduces\n",
    "    the Agentic RAG workflow defined in the Jupyter notebook. It uses LangGraph to replicate the exact\n",
    "    state graph, decision logic, and node functions (ingest_query, check_relevance, rewrite_query,\n",
    "    check_memory, retrieve_chunks, generate_answer, update_memory, output_answer).\n",
    "    \n",
    "    Artifacts expected when registering/logging:\n",
    "      - \"chroma_dir\": Persisted Chroma vectorstore directory\n",
    "      - \"memory_path\": Path to a JSON file (SimpleKVMemory)\n",
    "      \n",
    "    Usage:\n",
    "      RagAgenticModel.log_model(model_name=\"Agentic_RAG_Model\")\n",
    "      \n",
    "    After logging, you can load the model via:\n",
    "      mlflow.pyfunc.load_model(\"models:/Agentic_RAG_Model/Production\")\n",
    "      \n",
    "    and then call .predict({\"query\": \"<user question>\"}) to get a dict with keys:\n",
    "      - \"answer\": str\n",
    "      - \"retrieved_chunks\": List[str]\n",
    "      - \"messages\": List[Dict[str, Any]]\n",
    "    \"\"\"\n",
    "\n",
    "    TOPIC: str = \"AI Studio\"\n",
    "    CONTEXT_DIR: Path = Path(\"../data/context\")             \n",
    "    CHROMA_DIR: Path = Path(\"../data/chroma_store\")     \n",
    "    MEMORY_PATH: Path = Path(\"../data/memory/memory.json\")     \n",
    "    MANIFEST_PATH: Path = CHROMA_DIR / \"manifest.json\"\n",
    "\n",
    "    class SimpleKVMemory:\n",
    "        \"\"\"Very small persistent key-value store (JSON on disk).\"\"\"\n",
    "    \n",
    "        def __init__(self, file_path: Path) -> None:\n",
    "            self.file_path: Path = file_path\n",
    "            self._store: Dict[str, str] = self._load()\n",
    "    \n",
    "        # ---------- public ----------------------------------------------------\n",
    "        def get(self, key: str) -> Optional[str]:\n",
    "            \"\"\"Return answer if present, else None.\"\"\"\n",
    "            return self._store.get(key)\n",
    "    \n",
    "        def set(self, key: str, value: str) -> None:\n",
    "            \"\"\"Save answer and flush to disk.\"\"\"\n",
    "            self._store[key] = value\n",
    "            self._dump()\n",
    "    \n",
    "        # ---------- private ---------------------------------------------------\n",
    "        def _load(self) -> Dict[str, str]:\n",
    "            if self.file_path.exists():\n",
    "                try:\n",
    "                    with self.file_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "                        return json.load(f)\n",
    "                except Exception as exc:  \n",
    "                    logger.warning(\"Failed to load memory (%s). Starting fresh.\", exc)\n",
    "            return {}\n",
    "    \n",
    "        def _dump(self) -> None:\n",
    "            self.file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            with self.file_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(self._store, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "    class RAGState(TypedDict, total=False):\n",
    "        topic: str\n",
    "        query: str\n",
    "        is_relevant: Optional[bool]\n",
    "        rewritten_query: Optional[str]\n",
    "        retrieved_chunks: List[str]\n",
    "        answer: Optional[str]\n",
    "        from_memory: Optional[bool]\n",
    "        messages: List[Dict[str, Any]]  # full conversation with LLM\n",
    "\n",
    "    def load_context(self, context: mlflow.pyfunc.PythonModelContext) -> None:\n",
    "        \"\"\"\n",
    "        Load artifacts and initialize all components:\n",
    "          - Embedding model (HuggingFaceEmbeddings)\n",
    "          - Chroma vectorstore from artifact \"chroma_dir\"\n",
    "          - SimpleKVMemory from artifact \"memory_path\"\n",
    "          - Namedtuple Response (for LLM outputs)\n",
    "          - Build and compile the LangGraph state graph to self._compiled_graph\n",
    "        \"\"\"\n",
    "        self.TOPIC = RagAgenticModel.TOPIC\n",
    "        self._logger = logging.getLogger(\"RagAgenticModel\")\n",
    "        if not self._logger.handlers:\n",
    "            handler = logging.StreamHandler()\n",
    "            handler.setFormatter(\n",
    "                logging.Formatter(\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\")\n",
    "            )\n",
    "            self._logger.addHandler(handler)\n",
    "            self._logger.setLevel(logging.INFO)\n",
    "\n",
    "        # 1. Load embedding model\n",
    "        try:\n",
    "            self._embed_model = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "            encode_kwargs={\"normalize_embeddings\": True},\n",
    "            )\n",
    "        except:\n",
    "            self._embed_model = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "            encode_kwargs={\"normalize_embeddings\": True},\n",
    "            )\n",
    "            \n",
    "        # 2. Load persisted Chroma vectorstore\n",
    "        chroma_dir = Path(context.artifacts[\"chroma_dir\"])\n",
    "        self._vectorstore = Chroma(\n",
    "            collection_name=\"-\".join(self.TOPIC.split()),\n",
    "            persist_directory=str(chroma_dir),\n",
    "            embedding_function=self._embed_model,\n",
    "        )\n",
    "\n",
    "        # 3. Load LLM via TensorRTLangchain\n",
    "        sampling_params = tensorrt_llm.SamplingParams(\n",
    "            temperature=0.0,\n",
    "            top_k=1,\n",
    "            repetition_penalty=1.2,\n",
    "            stop_token_ids=[128009],\n",
    "        )\n",
    "        self._llm = TensorRTLangchain(model_path=\"nvidia/Llama-3.1-Nemotron-Nano-8B-v1\", sampling_params=sampling_params)\n",
    "\n",
    "        # 4. Initialize persistent memory\n",
    "        memory_path = Path(context.artifacts[\"memory_path\"])\n",
    "        memory_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        if not memory_path.exists():\n",
    "            memory_path.write_text(\"{}\", encoding=\"utf-8\")\n",
    "        self._memory = RagAgenticModel.SimpleKVMemory(memory_path)\n",
    "\n",
    "        # 5. Define a simple Response namedtuple (mirrors notebook)\n",
    "        self._LLMResponse = namedtuple(\"Response\", [\"content\"])\n",
    "\n",
    "        # 6. Build and compile the LangGraph state graph\n",
    "        self._build_state_graph()\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # Node Functions (each mirrors the notebook)\n",
    "    # ----------------------------------------\n",
    "    def ingest_query(self, state: RagAgenticModel.RAGState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Log the incoming user query and record it in the message history.\n",
    "        \"\"\"\n",
    "        user_query = state[\"query\"]\n",
    "        self._logger.info(\"Received user query: %s\", user_query)\n",
    "        previous_messages = state.get(\"messages\", [])\n",
    "        new_messages = previous_messages + [{\"role\": \"user\", \"content\": user_query}]\n",
    "        return {\"messages\": new_messages}\n",
    "\n",
    "    def check_relevance(self, state: RagAgenticModel.RAGState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Ask the LLM whether the query relates to our topic.\n",
    "        If not relevant, include a default apology answer.\n",
    "        \"\"\"\n",
    "        topic = state[\"topic\"]\n",
    "        user_query = state[\"query\"]\n",
    "\n",
    "        system_prompt = (\n",
    "            \"You are a strict classifier. Only respond with either \\\"yes\\\" or \\\"no\\\". \"\n",
    "            \"Do not include any additional words, explanations, or punctuation. \"\n",
    "            \"Answer based solely on whether the user's query is about the specified topic.\"\n",
    "        )\n",
    "        user_prompt = (\n",
    "            f\"The topic is: \\\"{topic}\\\"\\n\\n\"\n",
    "            f\"User query: \\\"{user_query}\\\"\\n\\n\"\n",
    "            \"Is this query related to the topic above? Respond with only 'yes' or 'no'.\"\n",
    "            \"Answer: \"\n",
    "        )\n",
    "\n",
    "        resp = self._get_response_from_llm(system_prompt, user_prompt)\n",
    "        is_relevant = \"yes\" in resp.strip().lower()\n",
    "        self._logger.info(\"Relevance check result: %s\", is_relevant)\n",
    "\n",
    "        messages = state.get(\"messages\", []) + [\n",
    "            {\"role\": \"developer\", \"content\": \"Relevance check result:\"},\n",
    "            {\"role\": \"assistant\", \"content\": resp},\n",
    "        ]\n",
    "        result: Dict[str, Any] = {\"is_relevant\": is_relevant, \"messages\": messages}\n",
    "        if not is_relevant:\n",
    "            result[\"answer\"] = f\"Sorry, I can only answer questions related to {topic}.\"\n",
    "        return result\n",
    "\n",
    "    def check_memory(self, state: RagAgenticModel.RAGState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Look up the exact user query in memory and return the cached answer if found.\n",
    "        \"\"\"\n",
    "        raw_query = state[\"query\"]\n",
    "        key = raw_query.strip().lower()\n",
    "        cached_answer = self._memory.get(key)\n",
    "        if cached_answer is not None:\n",
    "            self._logger.info(\"Cache hit for query: %s\", raw_query)\n",
    "            return {\"answer\": cached_answer, \"from_memory\": True}\n",
    "        self._logger.info(\"Cache miss for query: %s\", raw_query)\n",
    "        return {\"from_memory\": False}\n",
    "\n",
    "    def rewrite_query(self, state: RagAgenticModel.RAGState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Correct any grammar in the question and rewrite it as a clear statement\n",
    "        without altering its meaning, to improve retrieval.\n",
    "        \"\"\"\n",
    "        original = state[\"query\"]\n",
    "        system_prompt = (\n",
    "            \"You are a rewriting assistant. Your only task is to convert a question into a \"\n",
    "            \"grammatically correct statement. Do not change its meaning. \"\n",
    "            \"Output only the corrected statementâ€”no explanations or extra text.\"\n",
    "        )\n",
    "        user_prompt = (\n",
    "            \"Convert the following question into a grammatically correct statement \"\n",
    "            \"that preserves the original meaning exactly:\\n\\n\"\n",
    "            \"Note: Output only the corrected statementâ€”no explanations or extra text.\\n\"\n",
    "            f\"Question: \\\"{original}\\\"\\n\\n\"\n",
    "            \"Corrected Statement:\"\n",
    "        )\n",
    "\n",
    "        resp = self._get_response_from_llm(system_prompt, user_prompt).strip()\n",
    "        self._logger.info(\"Rewritten query: %s\", resp)\n",
    "\n",
    "        messages = state.get(\"messages\", []) + [\n",
    "            {\"role\": \"developer\", \"content\": \"Rewritten query:\"},\n",
    "            {\"role\": \"assistant\", \"content\": resp},\n",
    "        ]\n",
    "        return {\"rewritten_query\": resp, \"messages\": messages}\n",
    "\n",
    "    def retrieve_chunks(self, state: RagAgenticModel.RAGState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Fetch the top-k most relevant chunks for the rewritten query.\n",
    "        \"\"\"\n",
    "        statement = state[\"rewritten_query\"]\n",
    "        docs = self._vectorstore.similarity_search(statement, k=5)\n",
    "        chunks = [doc.page_content for doc in docs]\n",
    "        self._logger.info(\"Retrieved %d chunks for query.\", len(chunks))\n",
    "        return {\"retrieved_chunks\": chunks}\n",
    "\n",
    "    def generate_answer(self, state: RagAgenticModel.RAGState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Use the LLM to generate an answer based solely on retrieved context.\n",
    "        \"\"\"\n",
    "        topic = state[\"topic\"]\n",
    "        user_query = state[\"query\"]\n",
    "        context = \"\\n\\n---\\n\\n\".join(state[\"retrieved_chunks\"])\n",
    "\n",
    "        system_prompt = (\n",
    "            f\"You are a knowledgeable assistant specialized in {topic}. Your task is to answer \"\n",
    "            \"the user query using only the information found within the <context> block. \"\n",
    "            \"Ignore any external knowledge. If the context does not contain the answer, reply exactly with: \\\"I don't know.\\\" \"\n",
    "            \"Do not assume, infer, or add any extra information. \"\n",
    "            \"Respond with only the answerâ€”do not include any introductory or explanatory text.\"\n",
    "        )\n",
    "        user_prompt = (\n",
    "            f\"<context>\\n{context}\\n</context>\\n\\n\"\n",
    "            f\"User query: \\\"{user_query}\\\"\\n\\n\"\n",
    "            \"Based only on the context above, provide the exact answer to the query. \"\n",
    "            \"If the context does not contain the answer, respond exactly with: \\\"I don't know.\\\" \"\n",
    "            \"Give only the answerâ€”do not include any intro phrases such as 'The answer is' or 'Here it is'.\"\n",
    "            \"Answer: \"\n",
    "        )\n",
    "\n",
    "        resp = self._get_response_from_llm(system_prompt, user_prompt).strip()\n",
    "        self._logger.info(\"Generated answer (%d chars)\", len(resp))\n",
    "\n",
    "        messages = state.get(\"messages\", []) + [\n",
    "            {\"role\": \"developer\", \"content\": \"Generated answer:\"},\n",
    "            {\"role\": \"assistant\", \"content\": resp},\n",
    "        ]\n",
    "        return {\"answer\": resp, \"messages\": messages}\n",
    "\n",
    "    def update_memory(self, state: RagAgenticModel.RAGState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Store new query-answer pairs in memory for faster future lookup.\n",
    "        \"\"\"\n",
    "        if state.get(\"from_memory\"):\n",
    "            return {}\n",
    "        raw_query = state[\"query\"]\n",
    "        key = raw_query.strip().lower()\n",
    "        answer = state[\"answer\"]\n",
    "        if answer is not None:\n",
    "            self._memory.set(key, answer)\n",
    "            self._logger.info(\"Stored query-answer in memory for key: %s\", key)\n",
    "        return {}\n",
    "\n",
    "    def output_answer(self, state: RagAgenticModel.RAGState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        The final node. We do not print to STDOUT when serving via MLflow.\n",
    "        Just return an empty dict as this node does not add new state.\n",
    "        \"\"\"\n",
    "        return {}\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # Helper Methods\n",
    "    # ----------------------------------------\n",
    "    def _get_response_from_llm(self, system_prompt: str, user_prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Wrap the LLM call into the meta-prompt format and return the .content string.\n",
    "        \"\"\"\n",
    "        meta_llama_prompt = (\n",
    "            f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "            f\"{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "            f\"{user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "        )\n",
    "        raw = self._llm(meta_llama_prompt)\n",
    "        # TensorRTLangchain returns a raw string; we can wrap into Response if needed\n",
    "        return raw\n",
    "\n",
    "    # def _route_relevance(self, state: RagAgenticModel.RAGState) -> Literal[\"irrelevant\", \"relevant\"]:\n",
    "    def _route_relevance(self, state: RagAgenticModel.RAGState) -> str:\n",
    "        return \"relevant\" if state[\"is_relevant\"] else \"irrelevant\"\n",
    "\n",
    "    # def _route_memory(self, state: RagAgenticModel.RAGState) -> Literal[\"cached\", \"not_cached\"]:\n",
    "    def _route_memory(self, state: RagAgenticModel.RAGState) -> str:\n",
    "        return \"cached\" if state.get(\"from_memory\") else \"not_cached\"\n",
    "\n",
    "    def _build_state_graph(self) -> None:\n",
    "        \"\"\"\n",
    "        Construct and compile the LangGraph state graph exactly as in the notebook.\n",
    "        \"\"\"\n",
    "        rag_graph = StateGraph(RagAgenticModel.RAGState)\n",
    "\n",
    "        # Add nodes\n",
    "        rag_graph.add_node(\"ingest_query\", self.ingest_query)\n",
    "        rag_graph.add_node(\"check_relevance\", self.check_relevance)\n",
    "        rag_graph.add_node(\"rewrite_query\", self.rewrite_query)\n",
    "        rag_graph.add_node(\"check_memory\", self.check_memory)\n",
    "        rag_graph.add_node(\"retrieve_chunks\", self.retrieve_chunks)\n",
    "        rag_graph.add_node(\"generate_answer\", self.generate_answer)\n",
    "        rag_graph.add_node(\"update_memory\", self.update_memory)\n",
    "        rag_graph.add_node(\"output_answer\", self.output_answer)\n",
    "\n",
    "        # Add edges\n",
    "        rag_graph.add_edge(START, \"ingest_query\")\n",
    "        rag_graph.add_edge(\"ingest_query\", \"check_relevance\")\n",
    "\n",
    "        rag_graph.add_conditional_edges(\n",
    "            \"check_relevance\",\n",
    "            self._route_relevance,\n",
    "            {\n",
    "                \"irrelevant\": \"output_answer\",\n",
    "                \"relevant\": \"check_memory\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "        rag_graph.add_conditional_edges(\n",
    "            \"check_memory\",\n",
    "            self._route_memory,\n",
    "            {\n",
    "                \"cached\": \"output_answer\",\n",
    "                \"not_cached\": \"rewrite_query\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "        rag_graph.add_edge(\"rewrite_query\", \"retrieve_chunks\")\n",
    "        rag_graph.add_edge(\"retrieve_chunks\", \"generate_answer\")\n",
    "        rag_graph.add_edge(\"generate_answer\", \"update_memory\")\n",
    "        rag_graph.add_edge(\"update_memory\", \"output_answer\")\n",
    "        rag_graph.add_edge(\"output_answer\", END)\n",
    "\n",
    "        # Compile graph\n",
    "        self._compiled_graph = rag_graph.compile()\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # MLflow PythonModel Interface\n",
    "    # ----------------------------------------\n",
    "    def predict(self, context: mlflow.pyfunc.PythonModelContext, model_input):\n",
    "        \"\"\"\n",
    "        The MLflow inference entrypoint. Expects model_input = {\"query\": \"<user question>\"}.\n",
    "        Returns a dict with:\n",
    "          - \"answer\": str\n",
    "          - \"retrieved_chunks\": List[str]\n",
    "          - \"messages\": List[Dict[str, Any]]\n",
    "        \"\"\"\n",
    "        print('MODEL INPUT')\n",
    "        print(type(model_input))\n",
    "        print(model_input)\n",
    "        # If MLflow gave us a pandas DataFrame, extract the first row\n",
    "        if isinstance(model_input, pd.DataFrame):\n",
    "            if \"query\" not in model_input.columns:\n",
    "                raise Exception(\"DataFrame must contain a 'query' column.\")\n",
    "            # Take the first record in that column\n",
    "            raw_query = model_input[\"query\"].iloc[0]\n",
    "        else:\n",
    "            # Could be a plain dict or something else\n",
    "            if not isinstance(model_input, dict):\n",
    "                raise Exception(\n",
    "                    f\"Unexpected input type: {type(model_input)}. \"\n",
    "                    \"Expected pandas.DataFrame or dict with 'query'.\"\n",
    "                )\n",
    "            # If it's a dict, accept either string or single-element list\n",
    "            if \"query\" not in model_input:\n",
    "                raise Exception(\"Input dict must contain key 'query'.\")\n",
    "            raw_query = model_input[\"query\"]\n",
    "\n",
    "        # Initialize state with topic, query, and empty messages\n",
    "        initial_state: RagAgenticModel.RAGState = {\n",
    "            \"topic\": self.TOPIC,\n",
    "            \"query\": raw_query.strip(),\n",
    "            \"messages\": [],\n",
    "        }\n",
    "\n",
    "        # Invoke the compiled LangGraph\n",
    "        final_state = self._compiled_graph.invoke(input=initial_state)\n",
    "\n",
    "        # Extract elements to return\n",
    "        answer = final_state.get(\"answer\", \"\")\n",
    "        retrieved_chunks = final_state.get(\"retrieved_chunks\", [])\n",
    "        messages = final_state.get(\"messages\", [])\n",
    "\n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"retrieved_chunks\": retrieved_chunks,\n",
    "            \"messages\": messages,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def log_model(cls, model_name: str) -> None:\n",
    "        \"\"\"\n",
    "        Logs RagAgenticModel to MLflow and registers it in the Model Registry.\n",
    "\n",
    "        1. Assumes the following local directories exist relative to this file:\n",
    "             - data/chroma_store/       (persisted Chroma index)\n",
    "             - llm_weights/Llama-3.1-Nemotron-Nano-8B-v1/ (LLM weights folder)\n",
    "             - data/memory/memory.json  (initial memory file; created if missing)\n",
    "        2. Creates an MLflow run, logs the PyFunc model with the three artifacts, and registers it.\n",
    "\n",
    "        Args:\n",
    "          model_name (str): Name to register under in the MLflow Model Registry.\n",
    "        \"\"\"\n",
    "        # 1. Configure MLflow experiment & logging\n",
    "        logger = logging.getLogger(\"RagAgenticModel.log_model\")\n",
    "        if not logger.handlers:\n",
    "            h = logging.StreamHandler()\n",
    "            h.setFormatter(\n",
    "                logging.Formatter(\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\")\n",
    "            )\n",
    "            logger.addHandler(h)\n",
    "            logger.setLevel(logging.INFO)\n",
    "\n",
    "        # 2. Define local artifact paths (adjust if your folder structure differs)\n",
    "        project_root = Path.cwd().parent.resolve()\n",
    "        chroma_dir_local = project_root / \"data\" / \"chroma_store\"\n",
    "        memory_path_local = project_root / \"data\" / \"memory\" / \"memory.json\"\n",
    "\n",
    "        # 3. Validate local artifacts\n",
    "        if not chroma_dir_local.exists():\n",
    "            raise FileNotFoundError(f\"Chroma directory not found at {chroma_dir_local}\")\n",
    "        memory_path_local.parent.mkdir(parents=True, exist_ok=True)\n",
    "        if not memory_path_local.exists():\n",
    "            memory_path_local.write_text(\"{}\", encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "        # 4.a. Define input schema: a single column \"query\" of type string\n",
    "        input_schema = Schema([ColSpec(DataType.string, \"query\")])\n",
    "        # We omit output_schema (PyFunc can return arbitrary JSON), but we supply the input signature\n",
    "        signature = ModelSignature(inputs=input_schema)\n",
    "\n",
    "        # 4.b. Collect artifacts\n",
    "        artifacts: Dict[str, str] = {\n",
    "            \"chroma_dir\": str(RagAgenticModel.CHROMA_DIR),\n",
    "            \"memory_path\": str(RagAgenticModel.MEMORY_PATH),\n",
    "        }\n",
    "\n",
    "\n",
    "        # 4.c. Log the PyFunc model\n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=model_name,\n",
    "            python_model=cls(),\n",
    "            artifacts=artifacts,\n",
    "            signature=signature,\n",
    "            pip_requirements=\"../requirements.txt\",\n",
    "            code_paths=[\"../src\"],\n",
    "        )\n",
    "        logger.info(f\"Logged RagAgenticModel under artifact_path '{model_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a93c2489-381d-43be-b6bb-03fe0710710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Configuration: experiment, model name, run name\n",
    "MODEL_NAME = \"Agentic_RAG_Model\"\n",
    "RUN_NAME = f\"Register_{MODEL_NAME}\"\n",
    "EXPERIMENT_NAME = \"Agentic_RAG_Experiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0b18e64-3586-42cf-b46b-e8750cd34d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLflow tracking URI: /phoenix/mlflow\n",
      "Experiment: Agentic_RAG_Experiment\n"
     ]
    }
   ],
   "source": [
    "# 2. Set MLflow tracking URI and experiment\n",
    "mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\", \"/phoenix/mlflow\"))\n",
    "mlflow.set_experiment(experiment_name=EXPERIMENT_NAME)\n",
    "print(f\"Using MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"Experiment: {EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f46e1e7f-dbb5-489c-81f7-3baf48a7b177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/mlflow/pyfunc/__init__.py:3212: UserWarning: \u001b[1;33mAn input example was not provided when logging the model. To ensure the model signature functions correctly, specify the `input_example` parameter. See https://mlflow.org/docs/latest/model/signatures.html#model-input-example for more details about the benefits of using input_example.\u001b[0m\n",
      "  color_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started MLflow run: 4d6479d2e5614326ae847b856e1da63d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 144.15it/s]\n",
      "Downloading artifacts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 120.48it/s]\n",
      "2025-06-13 21:21:48,912 [INFO] RagAgenticModel.log_model: Logged RagAgenticModel under artifact_path 'Agentic_RAG_Model'\n",
      "2025-06-13 21:21:48 [INFO] RagAgenticModel.log_model: Logged RagAgenticModel under artifact_path 'Agentic_RAG_Model'\n",
      "Registered model 'Agentic_RAG_Model' already exists. Creating a new version of this model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.1 s, sys: 264 ms, total: 1.37 s\n",
      "Wall time: 3.34 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '7' of model 'Agentic_RAG_Model'.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 3. Start an MLflow run and log + register the model\n",
    "with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "    print(f\"Started MLflow run: {run.info.run_id}\")\n",
    "\n",
    "    # Log RagAgenticModel using the class method\n",
    "    RagAgenticModel.log_model(model_name=MODEL_NAME)\n",
    "\n",
    "    model_uri = f\"runs:/{run.info.run_id}/{MODEL_NAME}\"\n",
    "    mlflow.register_model(model_uri=model_uri, name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b07bed60-cb92-4842-9456-db84e69f35c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1266/3886261935.py:3: FutureWarning: ``mlflow.tracking.client.MlflowClient.get_latest_versions`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  versions = client.get_latest_versions(MODEL_NAME, stages=[\"None\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest registered version of 'Agentic_RAG_Model': 7\n",
      "Signature: inputs: \n",
      "  ['query': string (required)]\n",
      "outputs: \n",
      "  None\n",
      "params: \n",
      "  None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Retrieve the latest version from the Model Registry\n",
    "client = MlflowClient()\n",
    "versions = client.get_latest_versions(MODEL_NAME, stages=[\"None\"])\n",
    "if not versions:\n",
    "    raise RuntimeError(f\"No registered versions found for model '{MODEL_NAME}'.\")\n",
    "latest_version = versions[0].version\n",
    "\n",
    "model_info = mlflow.models.get_model_info(f\"models:/{MODEL_NAME}/{latest_version}\")\n",
    "print(f\"Latest registered version of '{MODEL_NAME}': {latest_version}\")\n",
    "print(f\"Signature: {model_info.signature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "251ee948-f6c4-4a2f-b31b-3f36f148df32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1266/1070438399.py:99: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self._embed_model = HuggingFaceEmbeddings(\n",
      "2025-06-13 21:21:51 [INFO] sentence_transformers.SentenceTransformer: Use pytorch device_name: cuda:0\n",
      "2025-06-13 21:21:51 [INFO] sentence_transformers.SentenceTransformer: Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "/tmp/ipykernel_1266/1070438399.py:111: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  self._vectorstore = Chroma(\n",
      "2025-06-13 21:21:54 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "Loading Model: \u001b[1;32m[1/3]\t\u001b[0mDownloading HF model\n",
      "\u001b[38;20mDownloaded model to /root/.cache/huggingface/hub/models--nvidia--Llama-3.1-Nemotron-Nano-8B-v1/snapshots/a22e1c57330633cd3522903f9bb82480bf3192a6\n",
      "\u001b[0m\u001b[38;20mTime: 53.574s\n",
      "\u001b[0mLoading Model: \u001b[1;32m[2/3]\t\u001b[0mLoading HF model to memory\n",
      "230it [00:00, 974.27it/s]\n",
      "\u001b[38;20mTime: 0.800s\n",
      "\u001b[0mLoading Model: \u001b[1;32m[3/3]\t\u001b[0mBuilding TRT-LLM engine\n",
      "\u001b[38;20mTime: 63.005s\n",
      "\u001b[0m\u001b[1;32mLoading model done.\n",
      "\u001b[0m\u001b[38;20mTotal latency: 117.380s\n",
      "\u001b[0m2025-06-13 21:24:28,253 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorRT-LLM] TensorRT-LLM version: 0.18.0\n",
      "[TensorRT-LLM][INFO] Engine version 0.18.0 found in the config file, assuming engine(s) built by new builder API.\n",
      "[TensorRT-LLM][INFO] Refreshed the MPI local session\n",
      "[TensorRT-LLM][INFO] MPI size: 1, MPI local size: 1, rank: 0\n",
      "[TensorRT-LLM][INFO] Rank 0 is using GPU 0\n",
      "[TensorRT-LLM][WARNING] Fix optionalParams : KV cache reuse disabled because model was not built with paged context FMHA support\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxNumSequences: 2048\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxBatchSize: 2048\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxBeamWidth: 1\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxSequenceLen: 131072\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxDraftLen: 0\n",
      "[TensorRT-LLM][INFO] TRTGptModel mMaxAttentionWindowSize: (131072) * 32\n",
      "[TensorRT-LLM][INFO] TRTGptModel enableTrtOverlap: 0\n",
      "[TensorRT-LLM][INFO] TRTGptModel normalizeLogProbs: 0\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxNumTokens: 8192\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxInputLen: 8192 = min(maxSequenceLen - 1, maxNumTokens) since context FMHA and usePackedInput are enabled\n",
      "[TensorRT-LLM][INFO] TRTGptModel If model type is encoder, maxInputLen would be reset in trtEncoderModel to maxInputLen: min(maxSequenceLen, maxNumTokens).\n",
      "[TensorRT-LLM][INFO] Capacity Scheduler Policy: GUARANTEED_NO_EVICT\n",
      "[TensorRT-LLM][INFO] Context Chunking Scheduler Policy: None\n",
      "[TensorRT-LLM][INFO] Loaded engine size: 15404 MiB\n",
      "[TensorRT-LLM][INFO] Inspecting the engine to identify potential runtime issues...\n",
      "[TensorRT-LLM][INFO] The profiling verbosity of the engine does not allow this analysis to proceed. Re-build the engine with 'detailed' profiling verbosity to get more diagnostics.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 1152.01 MiB for execution context memory.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 15380 (MiB)\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 6.98 GB GPU memory for runtime buffers.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 8.04 GB GPU memory for decoder.\n",
      "[TensorRT-LLM][INFO] Memory usage when calculating max tokens in paged kv cache: total: 47.99 GiB, available: 14.84 GiB\n",
      "[TensorRT-LLM][INFO] Number of blocks in KV cache primary pool: 1710\n",
      "[TensorRT-LLM][INFO] Number of blocks in KV cache secondary pool: 0, onboard blocks to primary memory before reuse: true\n",
      "[TensorRT-LLM][WARNING] maxAttentionWindow and maxSequenceLen are too large for at least one sequence to fit in kvCache. they are reduced to 109440\n",
      "[TensorRT-LLM][INFO] Number of tokens per block: 64.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 13.36 GiB for max tokens in paged KV cache (109440).\n",
      "Successfully loaded model 'Agentic_RAG_Model' version 7 for inference.\n",
      "CPU times: user 1min 27s, sys: 43.4 s, total: 2min 11s\n",
      "Wall time: 2min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 5. Load the model from the Model Registry\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri=f\"models:/{MODEL_NAME}/{latest_version}\")\n",
    "print(f\"Successfully loaded model '{MODEL_NAME}' version {latest_version} for inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98697b3f-6bf9-4729-ada9-cd87fa953428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 21:24:39,411 [INFO] RagAgenticModel: Received user query: What is the hardware requirement for AI Studio?\n",
      "2025-06-13 21:24:39 [INFO] RagAgenticModel: Received user query: What is the hardware requirement for AI Studio?\n",
      "/tmp/ipykernel_1266/1070438399.py:302: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  raw = self._llm(meta_llama_prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running Sample Inference ===\n",
      "MODEL INPUT\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "                                             query\n",
      "0  What is the hardware requirement for AI Studio?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed requests:   0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/tensorrt_llm/llmapi/utils.py:412: UserWarning: LLM API is running in async mode because you have a running event loop, but you are using sync API. This may lead to potential performance loss.\n",
      "  warnings.warn(\n",
      "Processed requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.51s/it]\n",
      "2025-06-13 21:24:41,934 [INFO] RagAgenticModel: Relevance check result: True\n",
      "2025-06-13 21:24:41 [INFO] RagAgenticModel: Relevance check result: True\n",
      "2025-06-13 21:24:41,938 [INFO] RagAgenticModel: Cache miss for query: What is the hardware requirement for AI Studio?\n",
      "2025-06-13 21:24:41 [INFO] RagAgenticModel: Cache miss for query: What is the hardware requirement for AI Studio?\n",
      "Processed requests:   0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/tensorrt_llm/llmapi/utils.py:412: UserWarning: LLM API is running in async mode because you have a running event loop, but you are using sync API. This may lead to potential performance loss.\n",
      "  warnings.warn(\n",
      "Processed requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.01s/it]\n",
      "2025-06-13 21:24:42,962 [INFO] RagAgenticModel: Rewritten query: The required hardware for AI Studio must have at least X GB of RAM and a multi-core processor. (Assuming specific technical details were missing in your note.)\n",
      "2025-06-13 21:24:42 [INFO] RagAgenticModel: Rewritten query: The required hardware for AI Studio must have at least X GB of RAM and a multi-core processor. (Assuming specific technical details were missing in your note.)\n",
      "2025-06-13 21:24:43,674 [INFO] RagAgenticModel: Retrieved 5 chunks for query.\n",
      "2025-06-13 21:24:43 [INFO] RagAgenticModel: Retrieved 5 chunks for query.\n",
      "Processed requests:   0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/tensorrt_llm/llmapi/utils.py:412: UserWarning: LLM API is running in async mode because you have a running event loop, but you are using sync API. This may lead to potential performance loss.\n",
      "  warnings.warn(\n",
      "Processed requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.14it/s]\n",
      "2025-06-13 21:24:44,565 [INFO] RagAgenticModel: Generated answer (75 chars)\n",
      "2025-06-13 21:24:44 [INFO] RagAgenticModel: Generated answer (75 chars)\n",
      "2025-06-13 21:24:44,576 [INFO] RagAgenticModel: Stored query-answer in memory for key: what is the hardware requirement for ai studio?\n",
      "2025-06-13 21:24:44 [INFO] RagAgenticModel: Stored query-answer in memory for key: what is the hardware requirement for ai studio?\n"
     ]
    }
   ],
   "source": [
    "# 6. Run a sample inference using the loaded model\n",
    "sample_query = \"What is the hardware requirement for AI Studio?\"\n",
    "input_payload = {\"query\": sample_query}\n",
    "\n",
    "print(\"\\n=== Running Sample Inference ===\")\n",
    "result = loaded_model.predict(input_payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30596601-ad9b-4631-a40e-318e9f9852b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "{sample_query}\n",
      "\n",
      "\n",
      "==============\n",
      "\n",
      "Answer:\n",
      "AMD Ryzenâ„¢ 9 processor, Intel Coreâ„¢ i5 12th generation processor, or higher \n",
      "\n",
      "\n",
      "==============\n",
      "\n",
      "Retrieved Chunks:\n",
      "  1. Technical Requirements\n",
      "\n",
      "Hardware:\n",
      "\n",
      "Windows 10 or 11 or Linux Ubuntu 22.04 LTS on a workstation\n",
      "\n",
      "GPU ...\n",
      "  2. Software:\n",
      "\n",
      "Windows 10 or 11 or Linux Ubuntu 22.04 LTS\n",
      "\n",
      "Windows OS requires Windows Subsystem for Lin...\n",
      "  3. title: 'System Requirements' sidebar_position: 1\n",
      "\n",
      "System Requirements\n",
      "\n",
      "Z by HP AI Studio currently r...\n",
      "  4. Distro selection modal\n",
      "\n",
      ":::tip\n",
      "\n",
      "If git is not already installed on your machine, the app will guide ...\n",
      "  5. title: 'Troubleshooting AI Studio' sidebar_position: 6\n",
      "\n",
      "AI Studio Troubleshooting Guide\n",
      "\n",
      "Find quick ...\n",
      "\n",
      "==============\n",
      "\n",
      "\n",
      "Message History:\n",
      "  [user]: What is the hardware requirement for AI Studio?\n",
      "  [developer]: Relevance check result:\n",
      "  [assistant]: Yes\n",
      "  [developer]: Rewritten query:\n",
      "  [assistant]: The required hardware for AI Studio must have at least X GB of RAM and a multi-core processor. (Assuming specific technical details were missing in your note.)\n",
      "  [developer]: Generated answer:\n",
      "  [assistant]: AMD Ryzenâ„¢ 9 processor, Intel Coreâ„¢ i5 12th generation processor, or higher\n"
     ]
    }
   ],
   "source": [
    "# 7. Print results\n",
    "print(f\"Query:\")\n",
    "print(\"{sample_query}\\n\")\n",
    "print(\"\\n==============\\n\")\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(result.get(\"answer\", \"<no answer>\"), \"\\n\")\n",
    "print(\"\\n==============\\n\")\n",
    "\n",
    "print(\"Retrieved Chunks:\")\n",
    "for idx, chunk in enumerate(result.get(\"retrieved_chunks\", []), start=1):\n",
    "    print(f\"  {idx}. {chunk[:100]}{'...' if len(chunk)>100 else ''}\")\n",
    "\n",
    "print(\"\\n==============\\n\")\n",
    "print(\"\\nMessage History:\")\n",
    "for msg in result.get(\"messages\", []):\n",
    "    role = msg.get(\"role\", \"<unknown>\")\n",
    "    content = msg.get(\"content\", \"\")\n",
    "    print(f\"  [{role}]: {content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
