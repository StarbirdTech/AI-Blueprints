{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5f8d3ca-e260-4d56-967d-dc4feb92e2e4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h1 style=\\\"text-align: center; font-size: 50px;\\\"> ðŸ¤– MLFlow Registration for Agentic RAG Model</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3047ff2",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "\n",
    "- Start Execution\n",
    "- Install and Import Libraries\n",
    "- Configure Settings\n",
    "- Define the Agentic RAG Model\n",
    "- Register the Model to MLFlow\n",
    "- Log Results to MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7867c9d9",
   "metadata": {},
   "source": [
    "# Start Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38d87aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "# Configure logger\n",
    "logger: logging.Logger = logging.getLogger(\"register_model_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.propagate = False  # Prevent duplicate logs from parent loggers\n",
    "\n",
    "# Set formatter\n",
    "formatter: logging.Formatter = logging.Formatter(\n",
    "    fmt=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "# Configure and attach stream handler\n",
    "stream_handler: logging.StreamHandler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1393ffe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 12:54:56 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()  \n",
    "logger.info(\"Notebook execution started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b7b5e0-2acd-4729-b8a2-03e81cf0df2d",
   "metadata": {},
   "source": [
    "# Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c533cc2-9ca5-442a-9409-bbad88be3eb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 32.6 ms, sys: 12.2 ms, total: 44.9 ms\n",
      "Wall time: 2.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "%pip install -r ../requirements.txt --quiet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2521b3-7544-4966-b0c2-c778eed7801b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorRT-LLM] TensorRT-LLM version: 0.18.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from collections import namedtuple\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Literal, Optional, TypedDict\n",
    "\n",
    "import pandas as pd\n",
    "import tensorrt_llm\n",
    "\n",
    "import mlflow.pyfunc\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.types import ColSpec, DataType, Schema\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM,AutoModel\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€ TRT-LLM â”€â”€â”€â”€â”€â”€â”€\n",
    "import tensorrt_llm\n",
    "parent_dir = os.path.dirname(os.path.abspath('.'))\n",
    "sys.path.append(parent_dir)\n",
    "from src.trt_llm_langchain import TensorRTLangchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf821052",
   "metadata": {},
   "source": [
    "# Configure Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af4a4231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ Suppress Verbose Logs ------------------------\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "072a781b-9d7c-4b5b-bf58-bc0a11ae5d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- MLflow Experiment Configuration -------------------------\n",
    "MODEL_NAME = \"Agentic_RAG_Model\"\n",
    "RUN_NAME = f\"Register_{MODEL_NAME}_Run\"\n",
    "EXPERIMENT_NAME = \"Agentic_RAG_Experiment\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c820d7a1",
   "metadata": {},
   "source": [
    "# Define the Agentic RAG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b8b8bb-262b-4892-aedb-7b2047895e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/mlflow/pyfunc/utils/data_validation.py:168: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
      "  color_warning(\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------\n",
    "# RagAgenticModel\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "class RagAgenticModel(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"\n",
    "    rag_mlflow_model.py\n",
    "    \n",
    "    This module defines an MLflow PythonModel subclass (`RagAgenticModel`) that faithfully reproduces\n",
    "    the Agentic RAG workflow defined in the Jupyter notebook. It uses LangGraph to replicate the exact\n",
    "    state graph, decision logic, and node functions (ingest_query, check_relevance, rewrite_query,\n",
    "    check_memory, retrieve_chunks, generate_answer, update_memory, output_answer).\n",
    "    \n",
    "    Artifacts expected when registering/logging:\n",
    "      - \"chroma_dir\": Persisted Chroma vectorstore directory\n",
    "      - \"memory_path\": Path to a JSON file (SimpleKVMemory)\n",
    "      \n",
    "    Usage:\n",
    "      RagAgenticModel.log_model(model_name=\"Agentic_RAG_Model\")\n",
    "      \n",
    "    After logging, you can load the model via:\n",
    "      mlflow.pyfunc.load_model(\"models:/Agentic_RAG_Model/Production\")\n",
    "      \n",
    "    and then call .predict({\"query\": \"<user question>\"}) to get a dict with keys:\n",
    "      - \"answer\": str\n",
    "      - \"retrieved_chunks\": List[str]\n",
    "      - \"messages\": List[Dict[str, Any]]\n",
    "    \"\"\"\n",
    "\n",
    "    TOPIC: str = \"AI Studio\"\n",
    "    CONTEXT_DIR: Path = Path(\"../data/context\")             \n",
    "    CHROMA_DIR: Path = Path(\"../data/chroma_store\")     \n",
    "    MEMORY_PATH: Path = Path(\"../data/memory/memory.json\")     \n",
    "    MANIFEST_PATH: Path = CHROMA_DIR / \"manifest.json\"\n",
    "\n",
    "    class SimpleKVMemory:\n",
    "        \"\"\"Very small persistent key-value store (JSON on disk).\"\"\"\n",
    "    \n",
    "        def __init__(self, file_path: Path) -> None:\n",
    "            self.file_path: Path = file_path\n",
    "            self._store: Dict[str, str] = self._load()\n",
    "    \n",
    "        # ---------- public ----------------------------------------------------\n",
    "        def get(self, key: str) -> Optional[str]:\n",
    "            \"\"\"Return answer if present, else None.\"\"\"\n",
    "            return self._store.get(key)\n",
    "    \n",
    "        def set(self, key: str, value: str) -> None:\n",
    "            \"\"\"Save answer and flush to disk.\"\"\"\n",
    "            self._store[key] = value\n",
    "            self._dump()\n",
    "    \n",
    "        # ---------- private ---------------------------------------------------\n",
    "        def _load(self) -> Dict[str, str]:\n",
    "            if self.file_path.exists():\n",
    "                try:\n",
    "                    with self.file_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "                        return json.load(f)\n",
    "                except Exception as exc:  \n",
    "                    logger.warning(\"Failed to load memory (%s). Starting fresh.\", exc)\n",
    "            return {}\n",
    "    \n",
    "        def _dump(self) -> None:\n",
    "            self.file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            with self.file_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(self._store, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "    class RAGState(TypedDict, total=False):\n",
    "        topic: str\n",
    "        query: str\n",
    "        is_relevant: Optional[bool]\n",
    "        rewritten_query: Optional[str]\n",
    "        retrieved_chunks: List[str]\n",
    "        answer: Optional[str]\n",
    "        from_memory: Optional[bool]\n",
    "        messages: List[Dict[str, Any]]  # full conversation with LLM\n",
    "\n",
    "    def load_context(self, context: mlflow.pyfunc.PythonModelContext) -> None:\n",
    "        \"\"\"\n",
    "        Load artifacts and initialize all components:\n",
    "          - Embedding model (HuggingFaceEmbeddings)\n",
    "          - Chroma vectorstore from artifact \"chroma_dir\"\n",
    "          - SimpleKVMemory from artifact \"memory_path\"\n",
    "          - Namedtuple Response (for LLM outputs)\n",
    "          - Build and compile the LangGraph state graph to self._compiled_graph\n",
    "        \"\"\"\n",
    "        self.TOPIC = RagAgenticModel.TOPIC\n",
    "        self._logger = logging.getLogger(\"RagAgenticModel\")\n",
    "        if not self._logger.handlers:\n",
    "            handler = logging.StreamHandler()\n",
    "            handler.setFormatter(\n",
    "                logging.Formatter(\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\")\n",
    "            )\n",
    "            self._logger.addHandler(handler)\n",
    "            self._logger.setLevel(logging.INFO)\n",
    "\n",
    "        # 1. Load embedding model\n",
    "        try:\n",
    "            self._embed_model = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "            encode_kwargs={\"normalize_embeddings\": True},\n",
    "            )\n",
    "        except:\n",
    "            self._embed_model = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "            encode_kwargs={\"normalize_embeddings\": True},\n",
    "            )\n",
    "            \n",
    "        # 2. Load persisted Chroma vectorstore\n",
    "        chroma_dir = Path(context.artifacts[\"chroma_dir\"])\n",
    "        self._vectorstore = Chroma(\n",
    "            collection_name=\"-\".join(self.TOPIC.split()),\n",
    "            persist_directory=str(chroma_dir),\n",
    "            embedding_function=self._embed_model,\n",
    "        )\n",
    "\n",
    "        # 3. Load LLM via TensorRTLangchain\n",
    "        sampling_params = tensorrt_llm.SamplingParams(\n",
    "            temperature=0.0,\n",
    "            top_k=1,\n",
    "            repetition_penalty=1.2,\n",
    "            stop_token_ids=[128009],\n",
    "        )\n",
    "        self._llm = TensorRTLangchain(model_path=\"nvidia/Llama-3.1-Nemotron-Nano-8B-v1\", sampling_params=sampling_params)\n",
    "\n",
    "        # 4. Initialize persistent memory\n",
    "        memory_path = Path(context.artifacts[\"memory_path\"])\n",
    "        memory_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        if not memory_path.exists():\n",
    "            memory_path.write_text(\"{}\", encoding=\"utf-8\")\n",
    "        self._memory = RagAgenticModel.SimpleKVMemory(memory_path)\n",
    "\n",
    "        # 5. Define a simple Response namedtuple (mirrors notebook)\n",
    "        self._LLMResponse = namedtuple(\"Response\", [\"content\"])\n",
    "\n",
    "        # 6. Build and compile the LangGraph state graph\n",
    "        self._build_state_graph()\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # Node Functions (each mirrors the notebook)\n",
    "    # ----------------------------------------\n",
    "    def ingest_query(self, state: RagAgenticModel.RAGState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Log the incoming user query and record it in the message history.\n",
    "        \"\"\"\n",
    "        user_query = state[\"query\"]\n",
    "        self._logger.info(\"Received user query: %s\", user_query)\n",
    "        previous_messages = state.get(\"messages\", [])\n",
    "        new_messages = previous_messages + [{\"role\": \"user\", \"content\": user_query}]\n",
    "        return {\"messages\": new_messages}\n",
    "\n",
    "    def check_relevance(self, state: RagAgenticModel.RAGState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Ask the LLM whether the query relates to our topic.\n",
    "        If not relevant, include a default apology answer.\n",
    "        \"\"\"\n",
    "        topic = state[\"topic\"]\n",
    "        user_query = state[\"query\"]\n",
    "\n",
    "        system_prompt = (\n",
    "            \"You are a strict classifier. Only respond with either \\\"yes\\\" or \\\"no\\\". \"\n",
    "            \"Do not include any additional words, explanations, or punctuation. \"\n",
    "            \"Answer based solely on whether the user's query is about the specified topic.\"\n",
    "        )\n",
    "        user_prompt = (\n",
    "            f\"The topic is: \\\"{topic}\\\"\\n\\n\"\n",
    "            f\"User query: \\\"{user_query}\\\"\\n\\n\"\n",
    "            \"Is this query related to the topic above? Respond with only 'yes' or 'no'.\"\n",
    "            \"Answer: \"\n",
    "        )\n",
    "\n",
    "        resp = self._get_response_from_llm(system_prompt, user_prompt)\n",
    "        is_relevant = \"yes\" in resp.strip().lower()\n",
    "        self._logger.info(\"Relevance check result: %s\", is_relevant)\n",
    "\n",
    "        messages = state.get(\"messages\", []) + [\n",
    "            {\"role\": \"developer\", \"content\": \"Relevance check result:\"},\n",
    "            {\"role\": \"assistant\", \"content\": resp},\n",
    "        ]\n",
    "        result: Dict[str, Any] = {\"is_relevant\": is_relevant, \"messages\": messages}\n",
    "        if not is_relevant:\n",
    "            result[\"answer\"] = f\"Sorry, I can only answer questions related to {topic}.\"\n",
    "        return result\n",
    "\n",
    "    def check_memory(self, state: RagAgenticModel.RAGState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Look up the exact user query in memory and return the cached answer if found.\n",
    "        \"\"\"\n",
    "        raw_query = state[\"query\"]\n",
    "        key = raw_query.strip().lower()\n",
    "        cached_answer = self._memory.get(key)\n",
    "        if cached_answer is not None:\n",
    "            self._logger.info(\"Cache hit for query: %s\", raw_query)\n",
    "            return {\"answer\": cached_answer, \"from_memory\": True}\n",
    "        self._logger.info(\"Cache miss for query: %s\", raw_query)\n",
    "        return {\"from_memory\": False}\n",
    "\n",
    "    def rewrite_query(self, state: RagAgenticModel.RAGState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Correct any grammar in the question and rewrite it as a clear statement\n",
    "        without altering its meaning, to improve retrieval.\n",
    "        \"\"\"\n",
    "        original = state[\"query\"]\n",
    "        system_prompt = (\n",
    "            \"You are a rewriting assistant. Your only task is to convert a question into a \"\n",
    "            \"grammatically correct statement. Do not change its meaning. \"\n",
    "            \"Output only the corrected statementâ€”no explanations or extra text.\"\n",
    "        )\n",
    "        user_prompt = (\n",
    "            \"Convert the following question into a grammatically correct statement \"\n",
    "            \"that preserves the original meaning exactly:\\n\\n\"\n",
    "            \"Note: Output only the corrected statementâ€”no explanations or extra text.\\n\"\n",
    "            f\"Question: \\\"{original}\\\"\\n\\n\"\n",
    "            \"Corrected Statement:\"\n",
    "        )\n",
    "\n",
    "        resp = self._get_response_from_llm(system_prompt, user_prompt).strip()\n",
    "        self._logger.info(\"Rewritten query: %s\", resp)\n",
    "\n",
    "        messages = state.get(\"messages\", []) + [\n",
    "            {\"role\": \"developer\", \"content\": \"Rewritten query:\"},\n",
    "            {\"role\": \"assistant\", \"content\": resp},\n",
    "        ]\n",
    "        return {\"rewritten_query\": resp, \"messages\": messages}\n",
    "\n",
    "    def retrieve_chunks(self, state: RagAgenticModel.RAGState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Fetch the top-k most relevant chunks for the rewritten query.\n",
    "        \"\"\"\n",
    "        statement = state[\"rewritten_query\"]\n",
    "        docs = self._vectorstore.similarity_search(statement, k=5)\n",
    "        chunks = [doc.page_content for doc in docs]\n",
    "        self._logger.info(\"Retrieved %d chunks for query.\", len(chunks))\n",
    "        return {\"retrieved_chunks\": chunks}\n",
    "\n",
    "    def generate_answer(self, state: RagAgenticModel.RAGState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Use the LLM to generate an answer based solely on retrieved context.\n",
    "        \"\"\"\n",
    "        topic = state[\"topic\"]\n",
    "        user_query = state[\"query\"]\n",
    "        context = \"\\n\\n---\\n\\n\".join(state[\"retrieved_chunks\"])\n",
    "\n",
    "        system_prompt = (\n",
    "            f\"You are a knowledgeable assistant specialized in {topic}. Your task is to answer \"\n",
    "            \"the user query using only the information found within the <context> block. \"\n",
    "            \"Ignore any external knowledge. If the context does not contain the answer, reply exactly with: \\\"I don't know.\\\" \"\n",
    "            \"Do not assume, infer, or add any extra information. \"\n",
    "            \"Respond with only the answerâ€”do not include any introductory or explanatory text.\"\n",
    "        )\n",
    "        user_prompt = (\n",
    "            f\"<context>\\n{context}\\n</context>\\n\\n\"\n",
    "            f\"User query: \\\"{user_query}\\\"\\n\\n\"\n",
    "            \"Based only on the context above, provide the exact answer to the query. \"\n",
    "            \"If the context does not contain the answer, respond exactly with: \\\"I don't know.\\\" \"\n",
    "            \"Give only the answerâ€”do not include any intro phrases such as 'The answer is' or 'Here it is'.\"\n",
    "            \"Answer: \"\n",
    "        )\n",
    "\n",
    "        resp = self._get_response_from_llm(system_prompt, user_prompt).strip()\n",
    "        self._logger.info(\"Generated answer (%d chars)\", len(resp))\n",
    "\n",
    "        messages = state.get(\"messages\", []) + [\n",
    "            {\"role\": \"developer\", \"content\": \"Generated answer:\"},\n",
    "            {\"role\": \"assistant\", \"content\": resp},\n",
    "        ]\n",
    "        return {\"answer\": resp, \"messages\": messages}\n",
    "\n",
    "    def update_memory(self, state: RagAgenticModel.RAGState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Store new query-answer pairs in memory for faster future lookup.\n",
    "        \"\"\"\n",
    "        if state.get(\"from_memory\"):\n",
    "            return {}\n",
    "        raw_query = state[\"query\"]\n",
    "        key = raw_query.strip().lower()\n",
    "        answer = state[\"answer\"]\n",
    "        if answer is not None:\n",
    "            self._memory.set(key, answer)\n",
    "            self._logger.info(\"Stored query-answer in memory for key: %s\", key)\n",
    "        return {}\n",
    "\n",
    "    def output_answer(self, state: RagAgenticModel.RAGState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        The final node. We do not print to STDOUT when serving via MLflow.\n",
    "        Just return an empty dict as this node does not add new state.\n",
    "        \"\"\"\n",
    "        return {}\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # Helper Methods\n",
    "    # ----------------------------------------\n",
    "    def _get_response_from_llm(self, system_prompt: str, user_prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Wrap the LLM call into the meta-prompt format and return the .content string.\n",
    "        \"\"\"\n",
    "        meta_llama_prompt = (\n",
    "            f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "            f\"{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "            f\"{user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "        )\n",
    "        raw = self._llm(meta_llama_prompt)\n",
    "        # TensorRTLangchain returns a raw string; we can wrap into Response if needed\n",
    "        return raw\n",
    "\n",
    "    # def _route_relevance(self, state: RagAgenticModel.RAGState) -> Literal[\"irrelevant\", \"relevant\"]:\n",
    "    def _route_relevance(self, state: RagAgenticModel.RAGState) -> str:\n",
    "        return \"relevant\" if state[\"is_relevant\"] else \"irrelevant\"\n",
    "\n",
    "    # def _route_memory(self, state: RagAgenticModel.RAGState) -> Literal[\"cached\", \"not_cached\"]:\n",
    "    def _route_memory(self, state: RagAgenticModel.RAGState) -> str:\n",
    "        return \"cached\" if state.get(\"from_memory\") else \"not_cached\"\n",
    "\n",
    "    def _build_state_graph(self) -> None:\n",
    "        \"\"\"\n",
    "        Construct and compile the LangGraph state graph exactly as in the notebook.\n",
    "        \"\"\"\n",
    "        rag_graph = StateGraph(RagAgenticModel.RAGState)\n",
    "\n",
    "        # Add nodes\n",
    "        rag_graph.add_node(\"ingest_query\", self.ingest_query)\n",
    "        rag_graph.add_node(\"check_relevance\", self.check_relevance)\n",
    "        rag_graph.add_node(\"rewrite_query\", self.rewrite_query)\n",
    "        rag_graph.add_node(\"check_memory\", self.check_memory)\n",
    "        rag_graph.add_node(\"retrieve_chunks\", self.retrieve_chunks)\n",
    "        rag_graph.add_node(\"generate_answer\", self.generate_answer)\n",
    "        rag_graph.add_node(\"update_memory\", self.update_memory)\n",
    "        rag_graph.add_node(\"output_answer\", self.output_answer)\n",
    "\n",
    "        # Add edges\n",
    "        rag_graph.add_edge(START, \"ingest_query\")\n",
    "        rag_graph.add_edge(\"ingest_query\", \"check_relevance\")\n",
    "\n",
    "        rag_graph.add_conditional_edges(\n",
    "            \"check_relevance\",\n",
    "            self._route_relevance,\n",
    "            {\n",
    "                \"irrelevant\": \"output_answer\",\n",
    "                \"relevant\": \"check_memory\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "        rag_graph.add_conditional_edges(\n",
    "            \"check_memory\",\n",
    "            self._route_memory,\n",
    "            {\n",
    "                \"cached\": \"output_answer\",\n",
    "                \"not_cached\": \"rewrite_query\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "        rag_graph.add_edge(\"rewrite_query\", \"retrieve_chunks\")\n",
    "        rag_graph.add_edge(\"retrieve_chunks\", \"generate_answer\")\n",
    "        rag_graph.add_edge(\"generate_answer\", \"update_memory\")\n",
    "        rag_graph.add_edge(\"update_memory\", \"output_answer\")\n",
    "        rag_graph.add_edge(\"output_answer\", END)\n",
    "\n",
    "        # Compile graph\n",
    "        self._compiled_graph = rag_graph.compile()\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # MLflow PythonModel Interface\n",
    "    # ----------------------------------------\n",
    "    def predict(self, context: mlflow.pyfunc.PythonModelContext, model_input):\n",
    "        \"\"\"\n",
    "        The MLflow inference entrypoint. Expects model_input = {\"query\": \"<user question>\"}.\n",
    "        Returns a dict with:\n",
    "          - \"answer\": str\n",
    "          - \"retrieved_chunks\": List[str]\n",
    "          - \"messages\": List[Dict[str, Any]]\n",
    "        \"\"\"\n",
    "        print('MODEL INPUT')\n",
    "        print(type(model_input))\n",
    "        print(model_input)\n",
    "        # If MLflow gave us a pandas DataFrame, extract the first row\n",
    "        if isinstance(model_input, pd.DataFrame):\n",
    "            if \"query\" not in model_input.columns:\n",
    "                raise Exception(\"DataFrame must contain a 'query' column.\")\n",
    "            # Take the first record in that column\n",
    "            raw_query = model_input[\"query\"].iloc[0]\n",
    "        else:\n",
    "            # Could be a plain dict or something else\n",
    "            if not isinstance(model_input, dict):\n",
    "                raise Exception(\n",
    "                    f\"Unexpected input type: {type(model_input)}. \"\n",
    "                    \"Expected pandas.DataFrame or dict with 'query'.\"\n",
    "                )\n",
    "            # If it's a dict, accept either string or single-element list\n",
    "            if \"query\" not in model_input:\n",
    "                raise Exception(\"Input dict must contain key 'query'.\")\n",
    "            raw_query = model_input[\"query\"]\n",
    "\n",
    "        # Initialize state with topic, query, and empty messages\n",
    "        initial_state: RagAgenticModel.RAGState = {\n",
    "            \"topic\": self.TOPIC,\n",
    "            \"query\": raw_query.strip(),\n",
    "            \"messages\": [],\n",
    "        }\n",
    "\n",
    "        # Invoke the compiled LangGraph\n",
    "        final_state = self._compiled_graph.invoke(input=initial_state)\n",
    "\n",
    "        # Extract elements to return\n",
    "        answer = final_state.get(\"answer\", \"\")\n",
    "        retrieved_chunks = final_state.get(\"retrieved_chunks\", [])\n",
    "        messages = final_state.get(\"messages\", [])\n",
    "\n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"retrieved_chunks\": retrieved_chunks,\n",
    "            \"messages\": messages,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def log_model(cls, model_name: str) -> None:\n",
    "        \"\"\"\n",
    "        Logs RagAgenticModel to MLflow and registers it in the Model Registry.\n",
    "\n",
    "        1. Assumes the following local directories exist relative to this file:\n",
    "             - data/chroma_store/       (persisted Chroma index)\n",
    "             - llm_weights/Llama-3.1-Nemotron-Nano-8B-v1/ (LLM weights folder)\n",
    "             - data/memory/memory.json  (initial memory file; created if missing)\n",
    "        2. Creates an MLflow run, logs the PyFunc model with the three artifacts, and registers it.\n",
    "\n",
    "        Args:\n",
    "          model_name (str): Name to register under in the MLflow Model Registry.\n",
    "        \"\"\"\n",
    "\n",
    "        sys.path.append(\"../src\")\n",
    "        from onnx_utils import ModelExportConfig,log_model\n",
    "\n",
    "        class TorchWrapper(torch.nn.Module):\n",
    "            def __init__(self, model):\n",
    "                super().__init__()\n",
    "                self.model = model\n",
    "\n",
    "            def forward(self, input_ids, attention_mask):\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    use_cache=False #ONNX doesn't support the cache\n",
    "                )\n",
    "                return outputs.logits  # Only export logits for ONNX\n",
    "\n",
    "        \n",
    "        # 1. Configure MLflow experiment & logging\n",
    "        logger = logging.getLogger(\"RagAgenticModel.log_model\")\n",
    "        if not logger.handlers:\n",
    "            h = logging.StreamHandler()\n",
    "            h.setFormatter(\n",
    "                logging.Formatter(\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\")\n",
    "            )\n",
    "            logger.addHandler(h)\n",
    "            logger.setLevel(logging.INFO)\n",
    "\n",
    "        # 2. Define local artifact paths (adjust if your folder structure differs)\n",
    "        project_root = Path.cwd().parent.resolve()\n",
    "        chroma_dir_local = project_root / \"data\" / \"chroma_store\"\n",
    "        memory_path_local = project_root / \"data\" / \"memory\" / \"memory.json\"\n",
    "\n",
    "        # 3. Validate local artifacts\n",
    "        if not chroma_dir_local.exists():\n",
    "            raise FileNotFoundError(f\"Chroma directory not found at {chroma_dir_local}\")\n",
    "        memory_path_local.parent.mkdir(parents=True, exist_ok=True)\n",
    "        if not memory_path_local.exists():\n",
    "            memory_path_local.write_text(\"{}\", encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "        # 4.a. Define input schema: a single column \"query\" of type string\n",
    "        input_schema = Schema([ColSpec(DataType.string, \"query\")])\n",
    "        # We omit output_schema (PyFunc can return arbitrary JSON), but we supply the input signature\n",
    "        signature = ModelSignature(inputs=input_schema)\n",
    "\n",
    "        # 4.b. Collect artifacts\n",
    "        artifacts: Dict[str, str] = {\n",
    "            \"chroma_dir\": str(RagAgenticModel.CHROMA_DIR),\n",
    "            \"memory_path\": str(RagAgenticModel.MEMORY_PATH),\n",
    "            \"config\": \"../configs/config.yaml\"\n",
    "        }\n",
    "\n",
    "        \n",
    "        # 5 Preparations to ONNX Conversion.\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        dtype = torch.float16 if device.type == \"cuda\" else torch.float32\n",
    "\n",
    "        embedding_model = AutoModel.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\", torch_dtype=dtype).to(device)\n",
    "\n",
    "        embedding_model.eval()\n",
    "\n",
    "        embedding_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "        embedding_inputs = embedding_tokenizer(\n",
    "            \"What is AI Studio?\", \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=128\n",
    "        )\n",
    "        embedding_input_sample = (\n",
    "            embedding_inputs[\"input_ids\"].to(device),\n",
    "            embedding_inputs[\"attention_mask\"].to(device)\n",
    "        )\n",
    "            \n",
    "        # Load nemotron model\n",
    "        nemotron_model_name = \"nvidia/Llama-3.1-Nemotron-Nano-8B-v1\"\n",
    "        nemotron_tokenizer = AutoTokenizer.from_pretrained(nemotron_model_name)\n",
    "        nemotronModel = AutoModelForCausalLM.from_pretrained(nemotron_model_name, torch_dtype=dtype).to(device)\n",
    "        nemotronModel.eval()\n",
    "\n",
    "        nemotron_tokenizer.pad_token = nemotron_tokenizer.eos_token\n",
    "        nemotron_inputs = nemotron_tokenizer(\n",
    "                            \"Hi\", \n",
    "                            return_tensors=\"pt\",\n",
    "                            padding=True,\n",
    "                            truncation=True,\n",
    "                            max_length=8 \n",
    "        )\n",
    "\n",
    "        nemotron_input_sample = (\n",
    "            nemotron_inputs[\"input_ids\"].to(device), \n",
    "            nemotron_inputs[\"attention_mask\"].to(device)\n",
    "        )\n",
    "\n",
    "        wrapped_model = TorchWrapper(nemotronModel) #Remove dynamic cache because it's not supported on ONNX\n",
    "\n",
    "        # 6. Create model configurations for ONNX conversion\n",
    "        logger.info(\"Creating model configurations for ONNX export...\")\n",
    "        \n",
    "\n",
    "        model_configs = [\n",
    "            ModelExportConfig(\n",
    "                model=embedding_model,\n",
    "                model_name=\"embedding_model\",\n",
    "                input_sample=embedding_input_sample, \n",
    "                task=\"feature-extraction\", \n",
    "                opset=17,\n",
    "                input_names=['input_ids', 'attention_mask'],\n",
    "                output_names=['last_hidden_state'], \n",
    "                dynamic_axes={\n",
    "                    'input_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "                    'attention_mask': {0: 'batch_size', 1: 'sequence_length'},\n",
    "                    'last_hidden_state': {0: 'batch_size', 1: 'sequence_length'}\n",
    "                }\n",
    "            ),\n",
    "            ModelExportConfig(\n",
    "                model=wrapped_model,\n",
    "                model_name=\"nemotron_model\",\n",
    "                input_sample=nemotron_input_sample,\n",
    "                task=\"text-generation\",\n",
    "                input_names=['input_ids', 'attention_mask'],\n",
    "                output_names=['logits'],\n",
    "                opset=17,\n",
    "                dynamic_axes={\n",
    "                    'input_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "                    'attention_mask': {0: 'batch_size', 1: 'sequence_length'},\n",
    "                    'logits': {0: 'batch_size', 1: 'sequence_length'}\n",
    "                }\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        \n",
    "        # 7. Logging MLFlow model and converting available models to ONNX file\n",
    "        log_model(\n",
    "            artifact_path=model_name,\n",
    "            python_model=cls(),\n",
    "            artifacts=artifacts,\n",
    "            signature=signature,\n",
    "            pip_requirements=\"../requirements.txt\",\n",
    "            code_paths=[\"../src\"],\n",
    "            models_to_convert_onnx=model_configs,\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Logged RagAgenticModel under artifact_path '{model_name}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1da6064",
   "metadata": {},
   "source": [
    "# Register the Model to MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0b18e64-3586-42cf-b46b-e8750cd34d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLflow tracking URI: /phoenix/mlflow\n",
      "Experiment: Agentic_RAG_Experiment\n"
     ]
    }
   ],
   "source": [
    "# 1. Set MLflow tracking URI and experiment\n",
    "mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\", \"/phoenix/mlflow\"))\n",
    "mlflow.set_experiment(experiment_name=EXPERIMENT_NAME)\n",
    "print(f\"Using MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"Experiment: {EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f46e1e7f-dbb5-489c-81f7-3baf48a7b177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started MLflow run: 6c5bc12f0b144fba8c0dfa7ad23c2b1c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [23:41<00:00, 355.39s/it] \n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.47it/s]\n",
      "2025-08-18 13:19:09,270 [INFO] RagAgenticModel.log_model: Creating model configurations for ONNX export...\n",
      "2025-08-18 13:19:09 - INFO - ðŸ”§ Generating ONNX model(s) for specified models...\n",
      "2025-08-18 13:19:09 - INFO - ðŸ”„ Converting transformers model: embedding_model\n",
      "2025-08-18 13:19:09 - INFO - ðŸ“ Model directory: embedding_model\n",
      "2025-08-18 13:19:09 - INFO - ðŸ” Model identified as: transformers\n",
      "2025-08-18 13:19:09 - INFO - ðŸ¤— Converting loaded Transformers model for task: feature-extraction with opset 17\n",
      "2025-08-18 13:19:15 - INFO - âœ… Transformers model exported to: embedding_model/embedding_model.onnx\n",
      "2025-08-18 13:19:15 - INFO - âœ… Converted embedding_model to directory: embedding_model\n",
      "2025-08-18 13:19:15 - INFO - ðŸ”„ Converting pytorch model: nemotron_model\n",
      "2025-08-18 13:19:15 - INFO - ðŸ“ Model directory: nemotron_model\n",
      "2025-08-18 13:19:15 - INFO - ðŸ” Model identified as: pytorch\n",
      "2025-08-18 13:19:15 - INFO - ðŸ”„ Exporting loaded PyTorch model with opset 17...\n",
      "2025-08-18 13:24:35 - INFO - âœ… PyTorch model exported to: nemotron_model/nemotron_model.onnx\n",
      "2025-08-18 13:24:35 - INFO - âœ… Converted nemotron_model to directory: nemotron_model\n",
      "2025-08-18 13:24:35 - INFO - ðŸ“¦ Added model directory artifact: model_embedding_model -> embedding_model\n",
      "2025-08-18 13:24:35 - INFO - ðŸ“¦ Added model directory artifact: model_nemotron_model -> nemotron_model\n",
      "2025-08-18 13:24:35 - INFO -   No Triton structure requested, using model directories as-is\n",
      "Downloading artifacts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 99.71it/s] \n",
      "Downloading artifacts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 131.84it/s]\n",
      "Downloading artifacts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 54.61it/s]\n",
      "Downloading artifacts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.81s/it]\n",
      "Downloading artifacts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 551/551 [01:52<00:00,  4.90it/s] \n",
      "2025-08-18 13:32:17 - INFO - Model logged with artifacts: ['chroma_dir', 'memory_path', 'config', 'model_embedding_model', 'model_nemotron_model']\n",
      "2025-08-18 13:32:17 - INFO - âœ… Model logged with 2 model directories created!\n",
      "2025-08-18 13:32:17,777 [INFO] RagAgenticModel.log_model: Logged RagAgenticModel under artifact_path 'Agentic_RAG_Model'\n",
      "Registered model 'Agentic_RAG_Model' already exists. Creating a new version of this model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model 'Agentic_RAG_Model' successfully logged and registered under experiment 'Agentic_RAG_Experiment'.\n",
      "CPU times: user 2min 26s, sys: 5min 30s, total: 7min 57s\n",
      "Wall time: 37min 6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '2' of model 'Agentic_RAG_Model'.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 2. Start an MLflow run and log + register the model\n",
    "with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "    print(f\"Started MLflow run: {run.info.run_id}\")\n",
    "\n",
    "    # Log RagAgenticModel using the class method\n",
    "    RagAgenticModel.log_model(model_name=MODEL_NAME)\n",
    "\n",
    "    model_uri = f\"runs:/{run.info.run_id}/{MODEL_NAME}\"\n",
    "    mlflow.register_model(model_uri=model_uri, name=MODEL_NAME)\n",
    "\n",
    "# ------------------------- Success Confirmation -------------------------\n",
    "\n",
    "print(f\"âœ… Model '{MODEL_NAME}' successfully logged and registered under experiment '{EXPERIMENT_NAME}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b07bed60-cb92-4842-9456-db84e69f35c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest registered version of 'Agentic_RAG_Model': 2\n",
      "Signature: inputs: \n",
      "  ['query': string (required)]\n",
      "outputs: \n",
      "  None\n",
      "params: \n",
      "  None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Retrieve the latest version from the Model Registry\n",
    "client = MlflowClient()\n",
    "versions = client.get_latest_versions(MODEL_NAME, stages=[\"None\"])\n",
    "if not versions:\n",
    "    raise RuntimeError(f\"No registered versions found for model '{MODEL_NAME}'.\")\n",
    "latest_version = versions[0].version\n",
    "\n",
    "model_info = mlflow.models.get_model_info(f\"models:/{MODEL_NAME}/{latest_version}\")\n",
    "print(f\"Latest registered version of '{MODEL_NAME}': {latest_version}\")\n",
    "print(f\"Signature: {model_info.signature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a40b1ee",
   "metadata": {},
   "source": [
    "# Log Results to MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "251ee948-f6c4-4a2f-b31b-3f36f148df32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TensorRTLangchain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m<timed exec>:2\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/mlflow/tracing/provider.py:422\u001b[39m, in \u001b[36mtrace_disabled.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    420\u001b[39m disable()\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m     is_func_called, result = \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    424\u001b[39m     enable()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/mlflow/pyfunc/__init__.py:1125\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(model_uri, suppress_warnings, dst_path, model_config)\u001b[39m\n\u001b[32m   1123\u001b[39m         model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path, model_config)\n\u001b[32m   1124\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1125\u001b[39m         model_impl = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mMAIN\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_load_pyfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1127\u001b[39m     \u001b[38;5;66;03m# This error message is particularly for the case when the error is caused by module\u001b[39;00m\n\u001b[32m   1128\u001b[39m     \u001b[38;5;66;03m# \"databricks.feature_store.mlflow_model\". But depending on the environment, the offending\u001b[39;00m\n\u001b[32m   1129\u001b[39m     \u001b[38;5;66;03m# module might be \"databricks\", \"databricks.feature_store\" or full package. So we will\u001b[39;00m\n\u001b[32m   1130\u001b[39m     \u001b[38;5;66;03m# raise the error with the following note if \"databricks\" presents in the error. All non-\u001b[39;00m\n\u001b[32m   1131\u001b[39m     \u001b[38;5;66;03m# databricks module errors will just be re-raised.\u001b[39;00m\n\u001b[32m   1132\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m conf[MAIN] == _DATABRICKS_FS_LOADER_MODULE \u001b[38;5;129;01mand\u001b[39;00m e.name.startswith(\u001b[33m\"\u001b[39m\u001b[33mdatabricks\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/mlflow/pyfunc/model.py:1040\u001b[39m, in \u001b[36m_load_pyfunc\u001b[39m\u001b[34m(model_path, model_config)\u001b[39m\n\u001b[32m   1039\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_load_pyfunc\u001b[39m(model_path: \u001b[38;5;28mstr\u001b[39m, model_config: Optional[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]] = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1040\u001b[39m     context, python_model, signature = \u001b[43m_load_context_model_and_signature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1041\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _PythonModelPyfuncWrapper(\n\u001b[32m   1042\u001b[39m         python_model=python_model,\n\u001b[32m   1043\u001b[39m         context=context,\n\u001b[32m   1044\u001b[39m         signature=signature,\n\u001b[32m   1045\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/mlflow/pyfunc/model.py:1034\u001b[39m, in \u001b[36m_load_context_model_and_signature\u001b[39m\u001b[34m(model_path, model_config)\u001b[39m\n\u001b[32m   1029\u001b[39m     artifacts[saved_artifact_name] = os.path.join(\n\u001b[32m   1030\u001b[39m         model_path, saved_artifact_info[CONFIG_KEY_ARTIFACT_RELATIVE_PATH]\n\u001b[32m   1031\u001b[39m     )\n\u001b[32m   1033\u001b[39m context = PythonModelContext(artifacts=artifacts, model_config=model_config)\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m \u001b[43mpython_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1036\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m context, python_model, signature\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 124\u001b[39m, in \u001b[36mRagAgenticModel.load_context\u001b[39m\u001b[34m(self, context)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# 3. Load LLM via TensorRTLangchain\u001b[39;00m\n\u001b[32m    118\u001b[39m sampling_params = tensorrt_llm.SamplingParams(\n\u001b[32m    119\u001b[39m     temperature=\u001b[32m0.0\u001b[39m,\n\u001b[32m    120\u001b[39m     top_k=\u001b[32m1\u001b[39m,\n\u001b[32m    121\u001b[39m     repetition_penalty=\u001b[32m1.2\u001b[39m,\n\u001b[32m    122\u001b[39m     stop_token_ids=[\u001b[32m128009\u001b[39m],\n\u001b[32m    123\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m \u001b[38;5;28mself\u001b[39m._llm = \u001b[43mTensorRTLangchain\u001b[49m(model_path=\u001b[33m\"\u001b[39m\u001b[33mnvidia/Llama-3.1-Nemotron-Nano-8B-v1\u001b[39m\u001b[33m\"\u001b[39m, sampling_params=sampling_params)\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# 4. Initialize persistent memory\u001b[39;00m\n\u001b[32m    127\u001b[39m memory_path = Path(context.artifacts[\u001b[33m\"\u001b[39m\u001b[33mmemory_path\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'TensorRTLangchain' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 4. Load the model from the Model Registry\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri=f\"models:/{MODEL_NAME}/{latest_version}\")\n",
    "print(f\"Successfully loaded model '{MODEL_NAME}' version {latest_version} for inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98697b3f-6bf9-4729-ada9-cd87fa953428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running Sample Inference ===\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loaded_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m input_payload = {\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m: sample_query}\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Running Sample Inference ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m result = \u001b[43mloaded_model\u001b[49m.predict(input_payload)\n",
      "\u001b[31mNameError\u001b[39m: name 'loaded_model' is not defined"
     ]
    }
   ],
   "source": [
    "# 5. Run a sample inference using the loaded model\n",
    "sample_query = \"What is the hardware requirement for AI Studio?\"\n",
    "input_payload = {\"query\": sample_query}\n",
    "\n",
    "print(\"\\n=== Running Sample Inference ===\")\n",
    "result = loaded_model.predict(input_payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30596601-ad9b-4631-a40e-318e9f9852b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Print results\n",
    "print(f\"Query:\")\n",
    "print(\"{sample_query}\\n\")\n",
    "print(\"\\n==============\\n\")\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(result.get(\"answer\", \"<no answer>\"), \"\\n\")\n",
    "print(\"\\n==============\\n\")\n",
    "\n",
    "print(\"Retrieved Chunks:\")\n",
    "for idx, chunk in enumerate(result.get(\"retrieved_chunks\", []), start=1):\n",
    "    print(f\"  {idx}. {chunk[:100]}{'...' if len(chunk)>100 else ''}\")\n",
    "\n",
    "print(\"\\n==============\\n\")\n",
    "print(\"\\nMessage History:\")\n",
    "for msg in result.get(\"messages\", []):\n",
    "    role = msg.get(\"role\", \"<unknown>\")\n",
    "    content = msg.get(\"content\", \"\")\n",
    "    print(f\"  [{role}]: {content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667f99ae-f6ac-41f2-91d3-6d59a7a7f1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time: float = time.time()\n",
    "elapsed_time: float = end_time - start_time\n",
    "elapsed_minutes: int = int(elapsed_time // 60)\n",
    "elapsed_seconds: float = elapsed_time % 60\n",
    "\n",
    "logger.info(f\"â±ï¸ Total execution time: {elapsed_minutes}m {elapsed_seconds:.2f}s\")\n",
    "logger.info(\"âœ… Notebook execution completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e51bf82-ddc0-41c9-a713-98b7180aa8a0",
   "metadata": {},
   "source": [
    "Built with â¤ï¸ using [**HP AI Studio**](https://hp.com/ai-studio)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
