{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\\\"text-align: center; font-size: 50px;\\\"> Register Model </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "\n",
    "- Start Execution\n",
    "- Install and Import Libraries\n",
    "- Configure Settings\n",
    "- Register the Model Log Results to MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "# Configure logger\n",
    "logger: logging.Logger = logging.getLogger(\"register_model_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.propagate = False  # Prevent duplicate logs from parent loggers\n",
    "\n",
    "# Set formatter\n",
    "formatter: logging.Formatter = logging.Formatter(\n",
    "    fmt=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "# Configure and attach stream handler\n",
    "stream_handler: logging.StreamHandler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-09 19:45:06 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()  \n",
    "logger.info(\"Notebook execution started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 400 ms, sys: 232 ms, total: 632 ms\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "%pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-08-09 19:46:26 nemo_logging:349] /opt/conda/envs/aistudio/lib/python3.10/site-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import nemo                             # NVIDIA NeMo core package\n",
    "import nemo.collections.asr as nemo_asr # Speech Recognition (ASR) collection\n",
    "import nemo.collections.tts as nemo_tts # Text-to-Speech (TTS) collection\n",
    "\n",
    "# ------------------------- Transformers -------------------------\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# ------------------------- Audio Processing Utilities -------------------------\n",
    "import IPython                          # For playing audio inside Jupyter Notebooks\n",
    "import soundfile                        # For reading and writing audio files\n",
    "from pathlib import Path                # Filesystem path management\n",
    "\n",
    "# ------------------------- System Utilities -------------------------\n",
    "\n",
    "import os                               # Operating system interfaces\n",
    "import shutil                           # High-level file operations\n",
    "import uuid                             # Unique ID generation\n",
    "import io                               # Input/Output core tools\n",
    "import base64                           # Encoding and decoding base64 strings\n",
    "import json                             # JSON serialization and deserialization\n",
    "import warnings                         # Suppressing and managing warnings\n",
    "import numpy as np                      # Numerical array operations\n",
    "np.float_ = np.float64\n",
    "import torch\n",
    "\n",
    "# ------------------------- MLflow Integration -------------------------\n",
    "\n",
    "import mlflow                           # MLflow experiment tracking and model management\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "from mlflow.types import ParamSchema, ParamSpec\n",
    "from mlflow.models import ModelSignature\n",
    "\n",
    "# ------------------------ Utils Import ------------------------\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from onnx_utils import ModelExportConfig\n",
    "from utils import load_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ Suppress Verbose Logs ------------------------\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Suppress NeMo internal logging\n",
    "logging.getLogger('nemo_logger').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- Model File Paths -------------------------\n",
    "MT_MODEL = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "ASR_MODEL_PATH = \"/home/jovyan/datafabric/STT_En_Citrinet_1024_Gamma_0.25/stt_en_citrinet_1024_gamma_0_25.nemo\"                  # Speech-to-Text (ASR) model\n",
    "SPECTROGRAM_GENERATOR_PATH = \"/home/jovyan/datafabric/TTS_Es_Multispeaker_FastPitch_HiFiGAN/tts_es_fastpitch_multispeaker.nemo\"  # Spectrogram generator model (FastPitch)\n",
    "VOCODER_PATH = \"/home/jovyan/datafabric/TTS_Es_Multispeaker_FastPitch_HiFiGAN/tts_es_hifigan_ft_fastpitch_multispeaker.nemo\"     # Vocoder model (HiFiGAN)\n",
    "\n",
    "AUDIO_SAMPLE_PATH = \"../data/ForrestGump.mp3\"      # Path to the input English audio sample\n",
    "\n",
    "# ------------------------- MLflow Experiment Configuration -------------------------\n",
    "\n",
    "EXPERIMENT_NAME = \"NeMo_Translation_Experiment\"    # MLflow experiment name\n",
    "RUN_NAME = \"NeMo_en_es_Translation_Run\"            # Specific run name inside the experiment\n",
    "MODEL_NAME = \"nemo_en_es\"                          # Registered model name in MLflow\n",
    "DEMO_PATH = \"../demo\"                              # Path to save demo outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register the Model and Log Results to MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NemoTranslationModel(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"\n",
    "    A custom MLflow pyfunc model for performing end-to-end audio translation using NVIDIA NeMo models.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_context(self, context):\n",
    "        \"\"\"Load NeMo models and prepare the temporary working directory.\"\"\"\n",
    "        model_dir = context.artifacts[\"model\"]\n",
    "\n",
    "        self.asr_model = nemo_asr.models.EncDecCTCModel.restore_from(f\"{model_dir}/enc_dec_CTC.nemo\")\n",
    "        self.mt_tokenizer = MarianTokenizer.from_pretrained(MT_MODEL)\n",
    "        self.mt_model = MarianMTModel.from_pretrained(MT_MODEL)\n",
    "        self.spectrogram_generator = nemo_tts.models.FastPitchModel.restore_from(f\"{model_dir}/fast_pitch.nemo\")\n",
    "        self.vocoder = nemo_tts.models.HifiGanModel.restore_from(f\"{model_dir}/hifi_gan.nemo\")\n",
    "\n",
    "        self.framerate = 41000\n",
    "\n",
    "        os.makedirs(\"/phoenix/mlflow/tmp\", exist_ok=True)\n",
    "\n",
    "    def transcribe_audio(self, model_input):\n",
    "        \"\"\"Deserialize base64-encoded audio, save it temporarily, and perform speech-to-text.\"\"\"\n",
    "        serialized_audio = model_input['source_serialized_audio'][0]\n",
    "        audio_buffer = io.BytesIO(base64.b64decode(serialized_audio))\n",
    "        audio_array, self.framerate = soundfile.read(audio_buffer)\n",
    "\n",
    "        # Ensure mono-channel audio\n",
    "        if audio_array.ndim > 1:\n",
    "            audio_array = audio_array[:, 0]\n",
    "\n",
    "        temp_wave_path = f\"/phoenix/mlflow/tmp/{self.file_id}.wav\"\n",
    "        soundfile.write(temp_wave_path, audio_array, self.framerate)\n",
    "\n",
    "        # Perform ASR\n",
    "        transcribed_text = self.asr_model.cuda().transcribe([temp_wave_path])\n",
    "        return transcribed_text\n",
    "\n",
    "    def text_to_audio(self, text: str):\n",
    "        \"\"\"Generate audio waveform from text using TTS models.\"\"\"\n",
    "        parsed_tokens = self.spectrogram_generator.cuda().parse(text)\n",
    "        spectrogram = self.spectrogram_generator.cuda().generate_spectrogram(tokens=parsed_tokens, speaker=2)\n",
    "        audio_tensor = self.vocoder.cuda().convert_spectrogram_to_audio(spec=spectrogram)\n",
    "\n",
    "        return audio_tensor.to('cpu').detach().numpy()\n",
    "\n",
    "    def serialize_audio(self, audio_array: np.ndarray):\n",
    "        \"\"\"Serialize a NumPy audio array into a base64-encoded WAV file.\"\"\"\n",
    "\n",
    "        \n",
    "        wave_path = f\"/phoenix/mlflow/tmp/out_{self.file_id}.wav\"\n",
    "        soundfile.write(wave_path, audio_array, samplerate=self.framerate, format='WAV')\n",
    "\n",
    "        with io.BytesIO() as buffer:\n",
    "            soundfile.write(buffer, audio_array, samplerate=self.framerate, format='WAV')\n",
    "            buffer.seek(0)\n",
    "            audio_base64 = base64.b64encode(buffer.read()).decode('utf-8')\n",
    "\n",
    "        return audio_base64\n",
    "\n",
    "    def predict(self, context, model_input, params=None):\n",
    "        \"\"\"\n",
    "        Perform inference:\n",
    "        1. Transcribe audio (if input is audio)\n",
    "        2. Translate text using Hugging Face MarianMT\n",
    "        3. Synthesize translated text into speech\n",
    "        4. Serialize the audio if needed\n",
    "        \"\"\"\n",
    "\n",
    "        self.file_id = uuid.uuid1()\n",
    "        use_audio = params.get(\"use_audio\", False)\n",
    "\n",
    "        if use_audio:\n",
    "            source_text = self.transcribe_audio(model_input)[0]\n",
    "        else:\n",
    "            source_text = model_input['source_text'][0]\n",
    "\n",
    "        # Move model to GPU if available\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.mt_model = self.mt_model.to(device)\n",
    "\n",
    "        # Tokenize and move inputs to device\n",
    "        inputs = self.mt_tokenizer(source_text, return_tensors=\"pt\", padding=True)\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "        # Generate translation\n",
    "        translated = self.mt_model.generate(**inputs)\n",
    "        translated_text = self.mt_tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "        translated_audio_base64 = \"\"\n",
    "        if use_audio:\n",
    "            audio_array = self.text_to_audio(translated_text)\n",
    "            translated_audio_base64 = self.serialize_audio(audio_array[0])\n",
    "\n",
    "        return {\n",
    "            \"original_text\": source_text,\n",
    "            \"translated_text\": translated_text,\n",
    "            \"translated_serialized_audio\": translated_audio_base64,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def log_model(cls, model_name: str, nemo_models: dict, demo_folder: str, config_path: str, pip_requirements: str | list[str] | None = None,):\n",
    "        \"\"\"\n",
    "        Log the translation model to MLflow with model artifacts and signatures.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name under which to register the model.\n",
    "            nemo_models: Dictionary mapping component names to their local .nemo file paths.\n",
    "            demo_folder: Path to the demo files folder.\n",
    "        \"\"\"\n",
    "        sys.path.append(\"../src\")\n",
    "        from onnx_utils import ModelExportConfig,log_model\n",
    "        \n",
    "        input_schema = Schema([\n",
    "            ColSpec(\"string\", \"source_text\"),\n",
    "            ColSpec(\"string\", \"source_serialized_audio\"),\n",
    "        ])\n",
    "\n",
    "        output_schema = Schema([\n",
    "            ColSpec(\"string\", \"original_text\"),\n",
    "            ColSpec(\"string\", \"translated_text\"),\n",
    "            ColSpec(\"string\", \"translated_serialized_audio\"),\n",
    "        ])\n",
    "\n",
    "        params_schema = ParamSchema([\n",
    "            ParamSpec(\"use_audio\", \"boolean\", False)\n",
    "        ])\n",
    "\n",
    "        signature = ModelSignature(\n",
    "            inputs=input_schema,\n",
    "            outputs=output_schema,\n",
    "            params=params_schema\n",
    "        )\n",
    "\n",
    "        os.makedirs(model_name, exist_ok=True)\n",
    "\n",
    "        # Copy NeMo model artifacts\n",
    "        if \"enc_dec_CTC\" in nemo_models:\n",
    "            shutil.copyfile(nemo_models[\"enc_dec_CTC\"], f\"{model_name}/enc_dec_CTC.nemo\")\n",
    "        if \"fast_pitch\" in nemo_models:\n",
    "            shutil.copyfile(nemo_models[\"fast_pitch\"], f\"{model_name}/fast_pitch.nemo\")\n",
    "        if \"hifi_gan\" in nemo_models:\n",
    "            shutil.copyfile(nemo_models[\"hifi_gan\"], f\"{model_name}/hifi_gan.nemo\")\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Loading Models in memory to convert to onnx\n",
    "        mt_model = MarianMTModel.from_pretrained(MT_MODEL)\n",
    "        asr_model = nemo_asr.models.EncDecCTCModel.restore_from(nemo_models[\"enc_dec_CTC\"])\n",
    "        fast_pitch_model = nemo_tts.models.FastPitchModel.restore_from(nemo_models[\"fast_pitch\"])\n",
    "        hifi_gan_model = nemo_tts.models.HifiGanModel.restore_from(nemo_models[\"hifi_gan\"])\n",
    "      \n",
    "\n",
    "        # 🎯 Create ModelExportConfig objects with loaded models (manual configuration)\n",
    "        model_configs = [ \n",
    "            ModelExportConfig(\n",
    "                model=mt_model,                         # 🚀 Pre-loaded Transformers model!\n",
    "                model_name=\"Helsinki-NLP\",              # ONNX file naming\n",
    "                task=\"translation\",                     # Model task\n",
    "            ),\n",
    "            # NeMo ASR model\n",
    "            ModelExportConfig(\n",
    "                model=asr_model.to(device),                        # 🚀 Pre-loaded NeMo ASR model!\n",
    "                model_name=\"enc_dec_CTC\",               # ONNX file naming\n",
    "            ),\n",
    "            # NeMo FastPitch model\n",
    "            ModelExportConfig(\n",
    "                model=fast_pitch_model.to(device),                 # 🚀 Pre-loaded NeMo TTS model!\n",
    "                model_name=\"fast_pitch\",                # ONNX file naming\n",
    "            ),\n",
    "            # NeMo HifiGAN model\n",
    "            ModelExportConfig(\n",
    "                model=hifi_gan_model.to(device),                   # 🚀 Pre-loaded NeMo Vocoder model!\n",
    "                model_name=\"hifi_gan\",                  # ONNX file naming\n",
    "            ),\n",
    "        ] \n",
    "            \n",
    "            \n",
    "        log_model(\n",
    "            artifact_path=model_name,\n",
    "            python_model=cls(),\n",
    "            artifacts={\"model\": model_name, \"demo\": demo_folder, \"config\": config_path},\n",
    "            signature=signature,\n",
    "            models_to_convert_onnx=model_configs,\n",
    "            pip_requirements=pip_requirements\n",
    "        )\n",
    "\n",
    "         # Clean up temporary files\n",
    "        shutil.rmtree(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-09 19:51:26 - INFO - 🔧 Generating ONNX model(s) for specified models...\n",
      "2025-08-09 19:51:26 - INFO - 🔄 Converting transformers model (from memory) -> Helsinki-NLP.onnx\n",
      "2025-08-09 19:51:26 - INFO - 🔍 Model identified as: transformers\n",
      "2025-08-09 19:51:26 - INFO - 🤗 Converting loaded Transformers model for task: translation\n",
      "2025-08-09 19:51:52 - INFO - ✅ Transformers model exported to: Helsinki-NLP.onnx\n",
      "2025-08-09 19:51:52 - INFO - ✅ Converted Helsinki-NLP: Helsinki-NLP.onnx\n",
      "2025-08-09 19:51:52 - INFO - 🔄 Converting nemo model (from memory) -> enc_dec_CTC.onnx\n",
      "2025-08-09 19:51:52 - INFO - 🔍 Model identified as: nemo\n",
      "2025-08-09 19:51:52 - INFO - 🔄 Exporting loaded NeMo model...\n",
      "2025-08-09 19:51:52 - INFO - 🔄 Using official NVIDIA export() method\n",
      "2025-08-09 19:53:02 - INFO - Model saved to ONNX: enc_dec_CTC.onnx\n",
      "2025-08-09 19:53:02 - INFO - ✅ Converted enc_dec_CTC: enc_dec_CTC.onnx\n",
      "2025-08-09 19:53:02 - INFO - 🔄 Converting nemo model (from memory) -> fast_pitch.onnx\n",
      "2025-08-09 19:53:02 - INFO - 🔍 Model identified as: nemo\n",
      "2025-08-09 19:53:02 - INFO - 🔄 Exporting loaded NeMo model...\n",
      "2025-08-09 19:53:02 - INFO - 🔄 Using official NVIDIA export() method\n",
      "2025-08-09 19:53:18 - INFO - Model saved to ONNX: fast_pitch.onnx\n",
      "2025-08-09 19:53:18 - INFO - ✅ Converted fast_pitch: fast_pitch.onnx\n",
      "2025-08-09 19:53:18 - INFO - 🔄 Converting nemo model (from memory) -> hifi_gan.onnx\n",
      "2025-08-09 19:53:18 - INFO - 🔍 Model identified as: nemo\n",
      "2025-08-09 19:53:18 - INFO - 🔄 Exporting loaded NeMo model...\n",
      "2025-08-09 19:53:18 - INFO - 🔄 Using official NVIDIA export() method\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing weight norm...\n",
      "Removing weight norm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-09 19:53:23 - INFO - Model saved to ONNX: hifi_gan.onnx\n",
      "2025-08-09 19:53:23 - INFO - ✅ Converted hifi_gan: hifi_gan.onnx\n",
      "2025-08-09 19:53:23 - INFO - 📦 Added ONNX artifact: onnx_Helsinki-NLP -> Helsinki-NLP.onnx\n",
      "2025-08-09 19:53:23 - INFO - 📦 Added ONNX artifact: onnx_enc_dec_CTC -> enc_dec_CTC.onnx\n",
      "2025-08-09 19:53:23 - INFO - 📦 Added ONNX artifact: onnx_fast_pitch -> fast_pitch.onnx\n",
      "2025-08-09 19:53:23 - INFO - 📦 Added ONNX artifact: onnx_hifi_gan -> hifi_gan.onnx\n",
      "2025-08-09 19:53:23 - INFO -   Creating individual model directories for artifacts...\n",
      "2025-08-09 19:53:23 - INFO -   Copying ONNX models for 4 models: ['Helsinki-NLP', 'enc_dec_CTC', 'fast_pitch', 'hifi_gan']\n",
      "2025-08-09 19:53:23 - INFO - 📄 Model file already has correct name: Helsinki-NLP.onnx\n",
      "2025-08-09 19:53:23 - INFO - 📄 Model file already has correct name: enc_dec_CTC.onnx\n",
      "2025-08-09 19:53:23 - INFO - 📄 Model file already has correct name: fast_pitch.onnx\n",
      "2025-08-09 19:53:23 - INFO - 📄 Model file already has correct name: hifi_gan.onnx\n",
      "2025-08-09 19:53:23 - INFO - 📦 Added model file artifact: model_Helsinki-NLP -> Helsinki-NLP.onnx\n",
      "2025-08-09 19:53:23 - INFO - 📦 Added model file artifact: model_enc_dec_CTC -> enc_dec_CTC.onnx\n",
      "2025-08-09 19:53:23 - INFO - 📦 Added model file artifact: model_fast_pitch -> fast_pitch.onnx\n",
      "2025-08-09 19:53:23 - INFO - 📦 Added model file artifact: model_hifi_gan -> hifi_gan.onnx\n",
      "2025-08-09 19:53:23 - INFO - ✅ Copied 4 model files!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db444e12ff9467c8207f1a018f44e17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c0ab549de45432cba0051eb347df2f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df1125b601e4917b422b6d8ec8ea5f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f217c7b01747b6bd790eaf380062e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273c87e34b1746a39f23c9450497e52f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c0faa4d41c74c6c8b7fc4beb49c7351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccedfe2391de43ddbf1d431df88207e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b847aa703624466bb5efe2b44c939e9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65248cce15e34636af0721c022da0d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a587e9d66e03432f9a36b6e26be24c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c8b968bd51c4d2bb02f6d7814509edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-09 19:57:39 - INFO - Model logged with artifacts: ['model', 'demo', 'config', 'onnx_Helsinki-NLP', 'onnx_enc_dec_CTC', 'onnx_fast_pitch', 'onnx_hifi_gan', 'model_Helsinki-NLP', 'model_enc_dec_CTC', 'model_fast_pitch', 'model_hifi_gan']\n",
      "2025-08-09 19:57:39 - INFO - ✅ Model logged with 4 model directories created!\n",
      "Registered model 'nemo_en_es' already exists. Creating a new version of this model...\n",
      "Created version '3' of model 'nemo_en_es'.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------- MLflow Model Logging and Registration -------------------------\n",
    "\n",
    "mlflow.set_tracking_uri('/phoenix/mlflow')\n",
    "# Set the MLflow experiment\n",
    "mlflow.set_experiment(experiment_name=EXPERIMENT_NAME)\n",
    "\n",
    "# Start a new MLflow run\n",
    "with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "    # Define the set of NeMo model components to be logged\n",
    "    nemo_model_artifacts = {\n",
    "        \"enc_dec_CTC\": ASR_MODEL_PATH,\n",
    "        \"fast_pitch\": SPECTROGRAM_GENERATOR_PATH,\n",
    "        \"hifi_gan\": VOCODER_PATH,\n",
    "    }\n",
    "\n",
    "    # Log the custom translation model with specified artifacts and demo folder\n",
    "    NemoTranslationModel.log_model(\n",
    "        model_name=MODEL_NAME,\n",
    "        nemo_models=nemo_model_artifacts,\n",
    "        demo_folder=\"../demo\",\n",
    "        config_path=\"../configs/config.yaml\",\n",
    "        pip_requirements=\"../requirements.txt\"\n",
    "    )\n",
    "\n",
    "    # Register the logged model in MLflow Model Registry\n",
    "    mlflow.register_model(\n",
    "        model_uri=f\"runs:/{run.info.run_id}/{MODEL_NAME}\",\n",
    "        name=MODEL_NAME\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model 'nemo_en_es' successfully logged and registered under experiment 'NeMo_Translation_Experiment'.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------- Success Confirmation -------------------------\n",
    "\n",
    "print(f\"✅ Model '{MODEL_NAME}' successfully logged and registered under experiment '{EXPERIMENT_NAME}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-09 19:57:42 - INFO - ⏱️ Total execution time: 12m 36.65s\n",
      "2025-08-09 19:57:42 - INFO - ✅ Notebook execution completed successfully.\n"
     ]
    }
   ],
   "source": [
    "end_time: float = time.time()\n",
    "elapsed_time: float = end_time - start_time\n",
    "elapsed_minutes: int = int(elapsed_time // 60)\n",
    "elapsed_seconds: float = elapsed_time % 60\n",
    "\n",
    "logger.info(f\"⏱️ Total execution time: {elapsed_minutes}m {elapsed_seconds:.2f}s\")\n",
    "logger.info(\"✅ Notebook execution completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Built with ❤️ using [**HP AI Studio**](https://hp.com/ai-studio)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aistudio",
   "language": "python",
   "name": "aistudio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
