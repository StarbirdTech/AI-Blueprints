# üéôÔ∏è Audio Translation with NeMo Models

# üìö Contents

* [üß† Overview](#overview)
* [üóÇ Project Structure](#project-structure)
* [‚öôÔ∏è Setup](#setup)
* [üöÄ Usage](#usage)
* [üìû Contact and Support](#contact-and-support)

---

## Overview

This project demonstrates an end-to-end **audio translation pipeline** using **NVIDIA NeMo models**. It takes an English audio sample and performs:

1. **Speech-to-Text (STT)** conversion using Citrinet  
2. **Text Translation (TT)** from English to Spanish using NMT  
3. **Text-to-Speech (TTS)** synthesis in Spanish using FastPitch and HiFiGAN  

All steps are GPU-accelerated, and the full workflow is integrated with **MLflow** for experiment tracking and model registration.

---

## Project Structure

```
‚îú‚îÄ‚îÄ data                                                                    # Data assets used in the project
‚îÇ   ‚îú‚îÄ‚îÄ ForrestGump.mp3
‚îÇ   ‚îî‚îÄ‚îÄ June18.mp3
‚îú‚îÄ‚îÄ demo                                                                    # UI-related files
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ docs
‚îÇ   ‚îú‚îÄ‚îÄ react_ui_for_audio_translation.png                                  # React UI screenshot 
‚îÇ   ‚îú‚îÄ‚îÄ streamlit_ui_for_audio_translation.png                              # Streamlit UI screenshot screenshot    
‚îÇ   ‚îú‚îÄ‚îÄ successful react ui result for audio translation.pdf                # React UI screenshot 
‚îÇ   ‚îî‚îÄ‚îÄ successful streamlit ui result for audio translation. pdf           # Streamlit UI screenshot
‚îú‚îÄ‚îÄ notebooks
‚îÇ   ‚îî‚îÄ‚îÄ english_to_spanish.ipynb                                            # Main notebook for the project
‚îú‚îÄ‚îÄ README.md                                                               # Project documentation
‚îî‚îÄ‚îÄ requirements.txt                                                        # Python dependencies (used with pip install)
```

---

## Setup

### 0 ‚ñ™ Minimum Hardware Requirements

Ensure your environment meets the minimum hardware requirements for smooth model inference:

- RAM: 16 GB  
- VRAM: 8 GB  
- GPU: NVIDIA GPU

### 1 ‚ñ™ Create an AI Studio Project

- Create a new project in [Z by HP AI Studio](https://zdocs.datascience.hp.com/docs/aistudio/overview).

### 2 ‚ñ™ Set Up a Workspace

- Choose the **NeMo Framework** image from the **NVIDIA NGC Catalog** in AI Studio during workspace setup.

### 3 ‚ñ™ Clone the Repository

```bash
https://github.com/HPInc/AI-Blueprints.git
```

- Ensure all files are available after workspace creation.

### 4 ‚ñ™ Add Required NeMo Models

From the **Models** tab, add the following models from the model catalog in AI Studio:

1. **Speech-to-Text (STT)**  
   - Model: `stt_en_citrinet_1024_gamma_0_25-1.0.0`  
   - Asset Name: `STT En Citrinet 1024 Gamma 0.25`

2. **Neural Machine Translation (NMT)**  
   - Model: `nmt_en_es_transformer12x2-1.0.0rc1`  
   - Asset Name: `NMT En Es Transformer12x2`

3. **Text-to-Speech (TTS)**  
   - Model: `tts_es_multispeaker_fastpitchhifigan-1.15.0`  
   - Asset Name: `TTS Es Multispeaker FastPitch HiFiGAN`

Make sure these models are downloaded and available in the `datafabric` folder inside your workspace.

---

## Usage

### 1 ‚ñ™ Run the Notebook

Open and run the notebook located at:

```bash
notebooks/english_to_spanish.ipynb
```

This will:

- Load STT, NMT, and TTS models from the NGC assets  
- Convert an English audio file to English text  
- Translate the text into Spanish  
- Synthesize spoken Spanish audio from the translated text  
- Log the entire workflow as a composite model in **MLflow**

### 2 ‚ñ™ Deploy the Nemo Translation Service

- In AI Studio, navigate to **Deployments > New Service**.  
- Give your service a name (e.g. ‚ÄúNemoTranslation‚Äù), then select the registered NemoTranslationModel.  
- Pick the desired model version and enable **GPU acceleration** for best performance.  
- Click **Deploy** to launch the service.

### 3 ‚ñ™ Swagger / Raw API

Once your service is running, open the **Swagger UI** from its Service URL.  

#### Example payload for text-only translation:
```jsonc
{
  "dataframe_records": [
    {
      "source_text": "Hello, world!",
      "source_serialized_audio": ""
    }
  ],
  "parameters": {
    "use_audio": false
  }
}
````

Paste that into the Swagger ‚Äú/invocations‚Äù endpoint and click **Try it out** to see the raw JSON response.

### 4 ‚ñ™ Use the HTML Demo

From the Swagger page, click the **‚ÄúDemo‚Äù** link to interact via a simple web form:

* Enter your source text.
* Click **Translate**.
* View the translated text right in the browser.

### 5 ‚ñ™ Launch the Streamlit UI

1. To launch the Streamlit UI, follow the instructions in the README file located in the `demo/streamlit-webapp` folder.
2. Enter text to translate, hit **Translate**, and enjoy the live results!



### Successful UI demo

- React
![Automated Evaluation React UI](docs/react_ui_for_audio_translation.png)  

- Streamlit
![Automated Evaluation Streamlit UI](docs/streamlit_ui_for_audio_translation.png)  

---

## Contact and Support

- Issues: Open a new issue in our [**AI-Blueprints GitHub repo**](https://github.com/HPInc/AI-Blueprints).

- Docs: Refer to the **[AI Studio Documentation](https://zdocs.datascience.hp.com/docs/aistudio/overview)** for detailed guidance and troubleshooting. 

- Community: Join the [**HP AI Creator Community**](https://community.datascience.hp.com/) for questions and help.

---

> Built with ‚ù§Ô∏è using [**HP AI Studio**](https://www.hp.com/us-en/workstations/ai-studio.html).
