{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcF9ZWvjSybR"
   },
   "source": [
    "<h1 style=\"text-align: center; font-size: 50px;\"> Stock Analysis with Pandas </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lv0guwQAcpgX"
   },
   "source": [
    "In this notebook, we run a series of database operations using standard Pandas, running on CPU. These values will be displayed and logged to be used as reference to compare with the GPU accelerated version using CUDF. \n",
    "\n",
    "The data we will be working with is a subset of the [USA 514 Stocks Prices NASDAQ NYSE dataset](https://www.kaggle.com/datasets/olegshpagin/usa-stocks-prices-ohlcv) from Kaggle. This was segmented in differently sized samples, with 5M, 10M, 15M and 20M data entries, and should be set up as an asset (Dataset) called USA_Stocks on the AI Studio project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "- Start Execution\n",
    "- Install and Import Libraries\n",
    "- Configure Settings\n",
    "- Verify Assets\n",
    "- Perform Analysis with Standard Pandas\n",
    "- Run Analysis and Log Results to MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging  # For application-level logging\n",
    "import time     # For runtime measurement (wall clock)\n",
    "\n",
    "# Configure logger\n",
    "logger: logging.Logger = logging.getLogger(\"run_workflow_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.propagate = False  # Prevent duplicate logs from parent loggers\n",
    "\n",
    "# Set formatter\n",
    "formatter: logging.Formatter = logging.Formatter(\n",
    "    fmt=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "# Configure and attach stream handler\n",
    "stream_handler: logging.StreamHandler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-10 16:26:13 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()  \n",
    "\n",
    "logger.info(\"Notebook execution started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 61.9 ms, sys: 22.8 ms, total: 84.6 ms\n",
      "Wall time: 4.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "%pip install -r ../../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Standard Library Imports\n",
    "# =============================\n",
    "import os\n",
    "import sys\n",
    "import types\n",
    "import nbformat\n",
    "import importlib.util\n",
    "import warnings           # To manage and filter Python warnings\n",
    "from pathlib import Path  # For object-oriented filesystem paths\n",
    "\n",
    "# =============================\n",
    "# Third-Party Library Imports\n",
    "# =============================\n",
    "import pandas as pd       # Data manipulation and analysis\n",
    "import mlflow             # Experiment tracking and model logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ Suppress Verbose Logs ------------------------\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the USA stock parquet datasets\n",
    "DATASET_DIR = Path(\"/home/jovyan/datafabric/USA_Stocks/\")\n",
    "\n",
    "# Sample sizes (in millions of rows) to evaluate during the analysis\n",
    "SAMPLE_SIZES_TO_TEST = [5, 10]\n",
    "\n",
    "# Rolling window size (in days) used for time-series statistical operations\n",
    "ROLLING_WINDOW_SIZE = 7\n",
    "\n",
    "# Name of the MLflow experiment for tracking performance and metrics\n",
    "MLFLOW_EXPERIMENT_NAME = \"USA Stock Analysis with Pandas\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WmOguzNUcw4F",
    "outputId": "dbe56095-403b-4527-8809-472c0561d403"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-10 16:26:20 - INFO - Dataset is properly configured\n"
     ]
    }
   ],
   "source": [
    "# Define required dataset filenames\n",
    "dataset_filenames = [\n",
    "    \"usa_stocks_5m.parquet\",\n",
    "    \"usa_stocks_10m.parquet\",\n",
    "    \"usa_stocks_15m.parquet\",\n",
    "    \"usa_stocks_20m.parquet\",\n",
    "]\n",
    "\n",
    "# Construct full dataset file paths using pathlib\n",
    "dataset_paths = [DATASET_DIR / filename for filename in dataset_filenames]\n",
    "\n",
    "# Check if all dataset files exist\n",
    "all_files_exist = all(path.exists() for path in dataset_paths)\n",
    "\n",
    "# Output the dataset configuration status\n",
    "if all_files_exist:\n",
    "    logger.info(\"Dataset is properly configured\")\n",
    "else:\n",
    "    logger.info(\"Dataset is not properly configured. Please, create and download the assets on your project on AI Studio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pq01z9FvJjxR"
   },
   "source": [
    "# Perform Analysis with Standard Pandas    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cells, we will define functions to run different operations in datasets:\n",
    "  * A function to describe the dataset\n",
    "  * A function to aggregate results grouped by \"ticker\" (the identifier of each stock)\n",
    "  * A function to aggregate by ticker, year and week\n",
    "  * A function to retrieve a rolling window with a given number of days for each ticker\n",
    "\n",
    "For each of these functions, the result will be logged and displayed in the notebook. These functions will then be applied to the given set of samples in sample_sizes (e.g. [5, 10]). Bigger samples (15M and 20M) might be too heavy depending on the setup of your computer, so we recommend to configure the desired sample sizes according to the available resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "DqqjcfcfJnvy",
    "outputId": "2592a7f6-8777-4515-9737-4e0c48228dc8"
   },
   "outputs": [],
   "source": [
    "def describe_dataframe(df):\n",
    "    \"\"\"\n",
    "    Compute basic descriptive statistics for the input DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (elapsed_time_in_seconds, descriptive_statistics)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    descriptive_stats = df.describe()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return elapsed_time, descriptive_stats\n",
    "\n",
    "\n",
    "def aggregate_by_ticker(df):\n",
    "    \"\"\"\n",
    "    Perform simple aggregation grouped by ticker.\n",
    "\n",
    "    Aggregates:\n",
    "        - Minimum datetime\n",
    "        - Maximum datetime\n",
    "        - Count of records\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (elapsed_time_in_seconds, aggregated_dataframe)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    aggregation_result = df.groupby(\"ticker\").agg({\n",
    "        \"datetime\": [\"min\", \"max\", \"count\"]\n",
    "    })\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return elapsed_time, aggregation_result\n",
    "\n",
    "\n",
    "def aggregate_by_ticker_week(df):\n",
    "    \"\"\"\n",
    "    Perform composite aggregation grouped by ticker, year, and week.\n",
    "\n",
    "    Aggregates:\n",
    "        - Minimum closing price\n",
    "        - Maximum closing price\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (elapsed_time_in_seconds, aggregated_dataframe)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    df[[\"year\", \"week\", \"day\"]] = df[\"datetime\"].dt.isocalendar()\n",
    "    aggregation_result = df.groupby([\"ticker\", \"year\", \"week\"]).agg({\n",
    "        \"close\": [\"min\", \"max\"]\n",
    "    })\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return elapsed_time, aggregation_result\n",
    "\n",
    "\n",
    "def compute_rolling_mean(df, window_days):\n",
    "    \"\"\"\n",
    "    Calculate rolling window mean for each ticker over a given number of days.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        window_days (int): Number of days for the rolling window.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (elapsed_time_in_seconds, result_dataframe)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    result = (\n",
    "        df.set_index(\"datetime\")\n",
    "          .sort_index()\n",
    "          .groupby(\"ticker\")\n",
    "          .rolling(f\"{window_days}D\")\n",
    "          .mean()\n",
    "          .reset_index()\n",
    "    )\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return elapsed_time, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Analysis and Log Results to MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/10 16:26:20 INFO mlflow.tracking.fluent: Experiment with name 'USA Stock Analysis with Pandas' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Analysis for 5M Rows ---\n",
      "Description Time      : 0.7597 seconds\n",
      "Simple Aggregation    : 0.1838 seconds\n",
      "Composite Aggregation : 0.4242 seconds\n",
      "Rolling Window (7D) : 3.2777 seconds\n",
      "\n",
      "--- Running Analysis for 10M Rows ---\n",
      "Description Time      : 1.3147 seconds\n",
      "Simple Aggregation    : 0.4355 seconds\n",
      "Composite Aggregation : 0.8405 seconds\n",
      "Rolling Window (7D) : 8.3399 seconds\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_tracking_uri('/phoenix/mlflow')\n",
    "# Set the MLflow experiment to track runs\n",
    "mlflow.set_experiment(experiment_name=MLFLOW_EXPERIMENT_NAME)\n",
    "\n",
    "# Loop through each dataset sample size and run analysis\n",
    "for sample_size in SAMPLE_SIZES_TO_TEST:\n",
    "    run_name = f\"Standard Analysis - {sample_size}M\"\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        # Log configuration parameters\n",
    "        mlflow.log_param(\"Computing\", \"cpu\")\n",
    "        mlflow.log_param(\"Dataset size in millions of rows\", sample_size)\n",
    "        \n",
    "        # Load dataset corresponding to the current sample size\n",
    "        dataset_path = f\"/home/jovyan/datafabric/USA_Stocks/usa_stocks_{sample_size}m.parquet\"\n",
    "        df = pd.read_parquet(dataset_path)\n",
    "\n",
    "        print(f\"\\n--- Running Analysis for {sample_size}M Rows ---\")\n",
    "        \n",
    "        # Description\n",
    "        description_time, _ = describe_dataframe(df)\n",
    "        mlflow.log_metric(\"Description_time_seconds\", description_time)\n",
    "        print(f\"Description Time      : {description_time:.4f} seconds\")\n",
    "        \n",
    "        # Simple Aggregation\n",
    "        simple_agg_time, _ = aggregate_by_ticker(df)\n",
    "        mlflow.log_metric(\"Simple_aggregation_time_seconds\", simple_agg_time)\n",
    "        print(f\"Simple Aggregation    : {simple_agg_time:.4f} seconds\")\n",
    "        \n",
    "        # Composite Aggregation\n",
    "        composite_agg_time, _ = aggregate_by_ticker_week(df)\n",
    "        mlflow.log_metric(\"Composite_aggregation_time_seconds\", composite_agg_time)\n",
    "        print(f\"Composite Aggregation : {composite_agg_time:.4f} seconds\")\n",
    "        \n",
    "        # Rolling Window\n",
    "        rolling_time, _ = compute_rolling_mean(df, ROLLING_WINDOW_SIZE)\n",
    "        mlflow.log_metric(f\"Rolling_window_{ROLLING_WINDOW_SIZE}D_time_seconds\", rolling_time)\n",
    "        print(f\"Rolling Window ({ROLLING_WINDOW_SIZE}D) : {rolling_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-10 16:26:40 - INFO - ⏱️ Total execution time: 0m 27.33s\n",
      "2025-09-10 16:26:40 - INFO - ✅ Notebook execution completed successfully.\n"
     ]
    }
   ],
   "source": [
    "end_time: float = time.time()\n",
    "elapsed_time: float = end_time - start_time\n",
    "elapsed_minutes: int = int(elapsed_time // 60)\n",
    "elapsed_seconds: float = elapsed_time % 60\n",
    "\n",
    "logger.info(f\"⏱️ Total execution time: {elapsed_minutes}m {elapsed_seconds:.2f}s\")\n",
    "logger.info(\"✅ Notebook execution completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Built with ❤️ using Z by HP AI Studio."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "aistudio",
   "language": "python",
   "name": "aistudio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
