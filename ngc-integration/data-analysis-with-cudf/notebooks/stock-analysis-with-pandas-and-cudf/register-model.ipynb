{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\\\"text-align: center; font-size: 50px;\\\"> Register Model </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "\n",
    "- Start Execution\n",
    "- Install and Import Libraries\n",
    "- Define User Constants\n",
    "- Define Workflow Methods Loaded with cuDF\n",
    "- Log Results to MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Configure logger\n",
    "logger: logging.Logger = logging.getLogger(\"register_model_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.propagate = False  # Prevent duplicate logs from parent loggers\n",
    "\n",
    "# Set formatter\n",
    "formatter: logging.Formatter = logging.Formatter(\n",
    "    fmt=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "# Configure and attach stream handler\n",
    "stream_handler: logging.StreamHandler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15 07:41:32 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()  \n",
    "logger.info(\"Notebook execution started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 24.5 ms, sys: 22.8 ms, total: 47.3 ms\n",
      "Wall time: 1.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "%pip install -r ../../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# Load cuDF Pandas extension (required for GPU acceleration)\n",
    "# ========================================================\n",
    "%load_ext cudf.pandas\n",
    "\n",
    "# =============================\n",
    "# Standard Library Imports\n",
    "# =============================\n",
    "import os\n",
    "import sys\n",
    "import types\n",
    "import nbformat\n",
    "import importlib.util\n",
    "import warnings           # To manage and filter Python warnings\n",
    "from pathlib import Path  # For object-oriented filesystem paths\n",
    "\n",
    "# =============================\n",
    "# Third-Party Library Imports\n",
    "# =============================\n",
    "import pandas as pd       # Data manipulation and analysis\n",
    "import mlflow             # Experiment tracking and model logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define User Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ Suppress Verbose Logs ------------------------\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the USA stock parquet datasets\n",
    "DATASET_DIR = Path(\"/home/jovyan/datafabric/USA_Stocks/\")\n",
    "\n",
    "# Sample sizes (in millions of rows) to evaluate during the analysis\n",
    "SAMPLE_SIZES_TO_TEST = [5, 10]\n",
    "\n",
    "# Rolling window size (in days) used for time-series statistical operations\n",
    "ROLLING_WINDOW_SIZE = 7\n",
    "\n",
    "# Name of the MLflow experiment for tracking performance and metrics\n",
    "MLFLOW_EXPERIMENT_NAME = \"USA Stock Analysis with Pandas and cuDF\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Workflow Methods Loaded With cuDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_dataframe(df):\n",
    "    \"\"\"\n",
    "    Compute basic descriptive statistics for the input DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (elapsed_time_in_seconds, descriptive_statistics)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    descriptive_stats = df.describe()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return elapsed_time, descriptive_stats\n",
    "\n",
    "\n",
    "def aggregate_by_ticker(df):\n",
    "    \"\"\"\n",
    "    Perform simple aggregation grouped by ticker.\n",
    "\n",
    "    Aggregates:\n",
    "        - Minimum datetime\n",
    "        - Maximum datetime\n",
    "        - Count of records\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (elapsed_time_in_seconds, aggregated_dataframe)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    aggregation_result = df.groupby(\"ticker\").agg({\n",
    "        \"datetime\": [\"min\", \"max\", \"count\"]\n",
    "    })\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return elapsed_time, aggregation_result\n",
    "\n",
    "\n",
    "def aggregate_by_ticker_week(df):\n",
    "    \"\"\"\n",
    "    Perform composite aggregation grouped by ticker, year, and week.\n",
    "\n",
    "    Aggregates:\n",
    "        - Minimum closing price\n",
    "        - Maximum closing price\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (elapsed_time_in_seconds, aggregated_dataframe)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    df[[\"year\", \"week\", \"day\"]] = df[\"datetime\"].dt.isocalendar()\n",
    "    aggregation_result = df.groupby([\"ticker\", \"year\", \"week\"]).agg({\n",
    "        \"close\": [\"min\", \"max\"]\n",
    "    })\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return elapsed_time, aggregation_result\n",
    "\n",
    "\n",
    "def compute_rolling_mean(df, window_days):\n",
    "    \"\"\"\n",
    "    Calculate rolling window mean for each ticker over a given number of days.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        window_days (int): Number of days for the rolling window.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (elapsed_time_in_seconds, result_dataframe)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    result = (\n",
    "        df.set_index(\"datetime\")\n",
    "          .sort_index()\n",
    "          .groupby(\"ticker\")\n",
    "          .rolling(f\"{window_days}D\")\n",
    "          .mean()\n",
    "          .reset_index()\n",
    "    )\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return elapsed_time, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Results to MLFlow\n",
    "\n",
    "In this section, we will log the dataset analysis results into MLFlow, particularly the necessary time it took for each dataset to run successfully and the different operations performed on them. We will be calling the functions defined in the workflow notebook which will be applied to the given set of samples in sample_sizes (e.g. [5, 10])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/15 07:41:42 INFO mlflow.tracking.fluent: Experiment with name 'USA Stock Analysis with Pandas and cuDF' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Analysis for 5M Rows ---\n",
      "\n",
      "--- Running Analysis for 10M Rows ---\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_tracking_uri('/phoenix/mlflow')\n",
    "# Set the MLflow experiment to track runs\n",
    "mlflow.set_experiment(experiment_name=MLFLOW_EXPERIMENT_NAME)\n",
    "\n",
    "# Loop through each dataset sample size and run analysis\n",
    "for sample_size in SAMPLE_SIZES_TO_TEST:\n",
    "    run_name = f\"Standard Analysis - {sample_size}M\"\n",
    "    \n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        # Log configuration parameters\n",
    "        mlflow.log_param(\"Computing\", \"cpu\")\n",
    "        mlflow.log_param(\"Dataset size in millions of rows\", sample_size)\n",
    "        \n",
    "        # Load dataset corresponding to the current sample size\n",
    "        dataset_path = f\"/home/jovyan/datafabric/USA_Stocks/usa_stocks_{sample_size}m.parquet\"\n",
    "        df = pd.read_parquet(dataset_path)\n",
    "\n",
    "        print(f\"\\n--- Running Analysis for {sample_size}M Rows ---\")\n",
    "        \n",
    "        # Description\n",
    "        description_time, _ = describe_dataframe(df)\n",
    "        mlflow.log_metric(\"Description_time_seconds\", description_time)\n",
    "        \n",
    "        # Simple Aggregation\n",
    "        simple_agg_time, _ = aggregate_by_ticker(df)\n",
    "        mlflow.log_metric(\"Simple_aggregation_time_seconds\", simple_agg_time)\n",
    "        \n",
    "        # Composite Aggregation\n",
    "        composite_agg_time, _ = aggregate_by_ticker_week(df)\n",
    "        mlflow.log_metric(\"Composite_aggregation_time_seconds\", composite_agg_time)\n",
    "        \n",
    "        # Rolling Window\n",
    "        rolling_time, _ = compute_rolling_mean(df, ROLLING_WINDOW_SIZE)\n",
    "        mlflow.log_metric(f\"Rolling_window_{ROLLING_WINDOW_SIZE}D_time_seconds\", rolling_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15 07:42:02 - INFO - ⏱️ Total execution time: 0m 29.43s\n",
      "2025-07-15 07:42:02 - INFO - ✅ Notebook execution completed successfully.\n"
     ]
    }
   ],
   "source": [
    "end_time: float = time.time()\n",
    "elapsed_time: float = end_time - start_time\n",
    "elapsed_minutes: int = int(elapsed_time // 60)\n",
    "elapsed_seconds: float = elapsed_time % 60\n",
    "\n",
    "logger.info(f\"⏱️ Total execution time: {elapsed_minutes}m {elapsed_seconds:.2f}s\")\n",
    "logger.info(\"✅ Notebook execution completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Built with ❤️ using [**HP AI Studio**](https://hp.com/ai-studio)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aistudio",
   "language": "python",
   "name": "aistudio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
