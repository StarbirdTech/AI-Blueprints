[{"file": "README1.md", "original": "<<PH66>><<SEP>>In this folder, we move forward with valuable examples Deep Learning applications on AI Studio, focusing mainly in Computer Vision with Convolutional Neural Networks and Natural Language with Transformers.", "corrected": "<<PH66>><<SEP>>In this folder, we move forward with valuable examples of Deep Learning applications on AI Studio, focusing mainly on Computer Vision with Convolutional Neural Networks and Natural Language with Transformers."}, {"file": "README1.md", "original": "Currently, we are working with three examples, all of them requiring Deep Learning workspaces (preferably with GPU) to run.<<PH67>><<SEP>>The repository is organized into the following structure:<<PH68>><<SEP>>This experiment shows a simple BertQA experiment, providing code to train a model, and other to load a trained model from Hugging Face, deploying a service in MLFlow to perform the inference<<PH69>><<SEP>>This experiment shows how to create a simple text generation, one character per time.", "corrected": "Currently, we are working with three examples, all of them requiring Deep Learning workspaces (preferably with a GPU) to run.<<PH67>><<SEP>>The repository is organized into the following structure:<<PH68>><<SEP>>This experiment shows a simple BERT QA experiment, providing code to train a model, and others to load a trained model from Hugging Face, deploying a service in MLFlow to perform the inference<<PH69>><<SEP>>This experiment shows how to create a simple text generation, one character at a time."}, {"file": "README1.md", "original": "This example uses a dataset of Shakespeare's texts.<<PH70>><<SEP>>This is a Computer Vision experiment that uses convolutional networks for image transformation - more specifically improving the resolution of an image.", "corrected": "This example uses a dataset of Shakespeare's texts.<<PH70>><<SEP>>This is a Computer Vision experiment that uses convolutional networks for image transformation - more specifically, improving the resolution of an image."}, {"file": "README1.md", "original": "This experiment requires the DIV2K dataset to run, that should be downloaded from s3://dsp-demo-bucket/div2k-data into an assset called DIV2K.<<PH71>>[[BULLET28]] Issues: Open a new issue in our [**AI-Blueprints GitHub repo**](<<PH9>>).<<PH72>>[[BULLET30]] Docs: Refer to the **[AI Studio Documentation](<<PH10>>)** for detailed guidance and troubleshooting. <<PH73>>[[BULLET32]] Community: Join the [**HP AI Creator Community**](<<PH11>>) for questions and help.<<PH74>>> <<PH8>>", "corrected": "This experiment requires the DIV2K dataset to run, which should be downloaded from s3://dsp-demo-bucket/div2k-data into an asset called DIV2K.<<PH71>>[[BULLET28]] Issues: Open a new issue in our [**AI-Blueprints GitHub repo**](<<PH9>>).<<PH72>>[[BULLET30]] Docs: Refer to the **[AI Studio Documentation](<<PH10>>)** for detailed guidance and troubleshooting. <<PH73>>[[BULLET32]] Community: Join the [**HP AI Creator Community**](<<PH11>>) for questions and help.<<PH74>>> <<PH8>>"}, {"file": "README2.md", "original": "<<PH442>>[[BULLET55]] [Overview](<<PH32>>)<<PH220>>- <<PH443>>- <<PH444>>- <<PH445>>- <<PH446>>- <<PH447>>[[BULLET61]] [Troubleshooting](<<PH38>>) <<PH226>>[[BULLET62]] [Contact and Support](<<PH39>>)<<PH448>><<SEP>>This repository contains a collection of sample projects that you can run quickly and effortlessly, designed to integrate seamlessly with [**HP AI Studio**](<<PH40>>).", "corrected": "<<PH442>>[[BULLET55]] [Overview](<<PH32>>)<<PH220>>- <<PH443>>- <<PH444>>- <<PH445>>- <<PH446>>- <<PH447>>[[BULLET61]] [Troubleshooting](<<PH38>>) <<PH226>>[[BULLET62]] [Contact and Support](<<PH39>>)<<PH448>><<SEP>>This repository contains a collection of sample projects that you can run quickly and effortlessly, designed to integrate seamlessly with [**HP AI Studio**](<<PH40>>)."}, {"file": "README2.md", "original": "Each project runs end-to-end, offering out-of-the-box, ready-to-use solutions across various domains, including data science, machine learning, deep learning, and generative AI.<<PH449>><<SEP>>The projects leverage local open-source models such as **LLaMA** (Meta), **BERT** (Google), and **Nemotron** (NVIDIA), alongside selected online models accessible via **Hugging Face**.", "corrected": "Each project runs end-to-end, offering out-of-the-box, ready-to-use solutions across various domains, including data science, machine learning, deep learning, and generative AI.<<PH449>><<SEP>>The projects leverage local open-source models such as **LLaMA** (Meta), **BERT** (Google), and **Nemotron** (NVIDIA), alongside selected online models accessible via **Hugging Face**."}, {"file": "README2.md", "original": "These examples cover a wide range of use cases, including **data visualization**, **stock analysis**, **audio translation**, **agentic RAG applications**, and much more.<<PH450>><<SEP>>We are continuously expanding this collection with new projects.", "corrected": "These examples cover a wide range of use cases, including **data visualization**, **stock analysis**, **audio translation**, **agentic RAG applications**, and much more.<<PH450>><<SEP>>We are continuously expanding this collection with new projects."}, {"file": "README2.md", "original": "If you have suggestions or would like to see a specific sample project integrated with [**HP AI Studio**](<<PH41>>), please feel free to open a new issue in this repository \u2014 we welcome your feedback!<<PH451>><<SEP>>The sample projects in this folder demonstrate how to build data science applications with [**HP AI Studio**](<<PH42>>).<<PH452>><<SEP>>We provide **2 blueprint projects**, each designed for quick and easy use to help you get started efficiently.<<PH453>><<SEP>>This project is a simple **classification** experiment focused on predicting species of **Iris flowers**.<<PH454>><<SEP>>It runs on the **Data Science Workspace**, demonstrating basic supervised learning techniques for multi-class classification tasks.<<PH455>><<SEP>>This project explores a **regression** experiment using **mobility data** collected during the COVID-19 pandemic.<<PH456>><<SEP>>It highlights how city-level movement patterns changed during the crisis.", "corrected": "If you have suggestions or would like to see a specific sample project integrated with [**HP AI Studio**](<<PH41>>), please feel free to open a new issue in this repository \u2014 we welcome your feedback!<<PH451>><<SEP>>The sample projects in this folder demonstrate how to build data science applications with [**HP AI Studio**](<<PH42>>).<<PH452>><<SEP>>We provide **2 blueprint projects**, each designed for quick and easy use to help you get started efficiently.<<PH453>><<SEP>>This project is a simple **classification** experiment focused on predicting species of **iris flowers**.<<PH454>><<SEP>>It runs on the **Data Science Workspace**, demonstrating basic supervised learning techniques for multi-class classification tasks.<<PH455>><<SEP>>This project explores a **regression** experiment using **mobility data** collected during the COVID-19 pandemic.<<PH456>><<SEP>>It highlights how city-level movement patterns changed during the crisis."}, {"file": "README2.md", "original": "The experiment runs on the **Data Science Workspace**.<<PH457>><<SEP>>The sample projects in this folder demonstrate how to build deep learning applications with [**HP AI Studio**](<<PH43>>).<<PH458>><<SEP>>We provide **6 blueprint projects**, each designed for quick and easy use to help you get started efficiently.<<PH459>><<SEP>>This project performs basic **image classification** using the **TensorFlow** framework.<<PH460>><<SEP>>It trains a model to classify handwritten digits from the **MNIST** dataset and runs on the **Deep Learning Workspace**.<<PH461>><<SEP>>This project demonstrates a simple **BERT Question Answering (QA)** experiment.", "corrected": "The experiment runs on the **Data Science Workspace**.<<PH457>><<SEP>>The sample projects in this folder demonstrate how to build deep learning applications with [**HP AI Studio**](<<PH43>>).<<PH458>><<SEP>>We provide **6 blueprint projects**, each designed for quick and easy use to help you get started efficiently.<<PH459>><<SEP>>This project performs basic **image classification** using the **TensorFlow** framework.<<PH460>><<SEP>>It trains a model to classify handwritten digits from the **MNIST** dataset and runs on the **Deep Learning Workspace**.<<PH461>><<SEP>>This project demonstrates a simple **BERT Question Answering (QA)** experiment."}, {"file": "README2.md", "original": "It provides code to train a BERT-based model, as well as instructions to load a pretrained model from **Hugging Face**.<<PH462>><<SEP>>The model is deployed using **MLflow** to expose an inference service capable of answering questions based on input text.<<PH463>><<SEP>>This project builds a simple **recommender system** for movies using **TensorFlow**.<<PH464>><<SEP>>It trains on user-item interaction data to predict movie preferences and runs on the **Deep Learning Workspace**.<<PH465>><<SEP>>This project implements a **text classification** system to detect **spam** messages.<<PH466>><<SEP>>It uses deep learning techniques and requires the **Deep Learning Workspace** for training and inference.<<PH467>><<SEP>>This project showcases a **Computer Vision** experiment that applies convolutional neural networks for **image super-resolution** \u2014 enhancing the quality and resolution of input images.<<PH468>><<SEP>>This project illustrates how to build a simple **character-by-character text generation** model.<<PH469>><<SEP>>It trains on a dataset containing **Shakespeare's texts**, demonstrating the fundamentals of text generation by predicting one character at a time.<<PH470>><<SEP>>The sample projects in this folder demonstrate how to build generative AI applications with [**HP AI Studio**](<<PH44>>).<<PH471>><<SEP>>We provide **7 blueprint projects**, each designed for quick and easy use to help you get started efficiently.<<PH472>>**Automated\u202fEvaluation\u202fwith\u202fStructured\u202fOutputs** turns a local **Meta\u2011Llama\u20113** model into an MLflow\u2011served scorer that rates any batch of texts (e.g., project abstracts) against arbitrary rubric criteria.<<PH473>>[[BULLET129]] Generates scores locally via\u202f`<<PH45>>` (no data leaves your machine)<<PH319>>[[BULLET130]] Registers the evaluator as a **pyfunc** model in MLflow<<PH320>>[[BULLET131]] Exposes a REST `<<PH46>>` endpoint<<PH321>>[[BULLET132]] Ships two front\u2011ends \u2014 a **Streamlit** dashboard and a pure **HTML/JS** UI \u2014 for instant human\u2011friendly interaction and CSV download.<<PH474>><<SEP>>This notebook performs automatic code explanation by extracting code snippets from Jupyter notebooks and generating natural language descriptions using LLMs.", "corrected": "It provides code to train a BERT-based model, as well as instructions to load a pretrained model from **Hugging Face**.<<PH462>><<SEP>>The model is deployed using **MLflow** to expose an inference service capable of answering questions based on input text.<<PH463>><<SEP>>This project builds a simple **recommender system** for movies using **TensorFlow**.<<PH464>><<SEP>>It trains on user-item interaction data to predict movie preferences and runs on the **Deep Learning Workspace**.<<PH465>><<SEP>>This project implements a **text classification** system to detect **spam** messages.<<PH466>><<SEP>>It uses deep learning techniques and requires the **Deep Learning Workspace** for training and inference.<<PH467>><<SEP>>This project showcases a **Computer Vision** experiment that applies convolutional neural networks for **image super-resolution** \u2014 enhancing the quality and resolution of input images.<<PH468>><<SEP>>This project illustrates how to build a simple **character-by-character text generation** model.<<PH469>><<SEP>>It trains on a dataset containing **Shakespeare's texts**, demonstrating the fundamentals of text generation by predicting one character at a time.<<PH470>><<SEP>>The sample projects in this folder demonstrate how to build generative AI applications with [**HP AI Studio**](<<PH44>>).<<PH471>><<SEP>>We provide **7 blueprint projects**, each designed for quick and easy use to help you get started efficiently.<<PH472>>**Automated Evaluation with Structured Outputs** turns a local **Meta-Llama-3** model into an MLflow-served scorer that rates any batch of texts (e.g., project abstracts) against arbitrary rubric criteria.<<PH473>>[[BULLET129]] Generates scores locally via `<<PH45>>` (no data leaves your machine)<<PH319>>[[BULLET130]] Registers the evaluator as a **pyfunc** model in MLflow<<PH320>>[[BULLET131]] Exposes a REST `<<PH46>>` endpoint<<PH321>>[[BULLET132]] Ships two front-ends \u2014 a **Streamlit** dashboard and a pure **HTML/JS** UI \u2014 for instant human-friendly interaction and CSV download.<<PH474>><<SEP>>This notebook performs automatic code explanation by extracting code snippets from Jupyter notebooks and generating natural language descriptions using LLMs."}, {"file": "README2.md", "original": "It supports contextual enrichment based on adjacent markdown cells, enables configurable prompt templating, and integrates with PromptQuality and Galileo for evaluation and tracking. The pipeline is modular, supports local or hosted model inference, and is compatible with LLaMA, Mistral, and Hugging Face-based models.", "corrected": "It supports contextual enrichment based on adjacent markdown cells, enables configurable prompt templating, and integrates with PromptQuality and Galileo for evaluation and tracking. The pipeline is modular, supports local or hosted model inference, and is compatible with LLaMA, Mistral, and Hugging Face-based models."}, {"file": "README2.md", "original": "It also includes GitHub notebook crawling, metadata structuring, and vector store integration for downstream tasks like RAG and semantic search.<<PH475>><<SEP>>This project demonstrates a full-stack LLM fine-tuning experiment using ORPO (Open-Source Reinforcement Pretraining Objective) to align a base language model with human preference data.", "corrected": "It also includes GitHub notebook crawling, metadata structuring, and vector store integration for downstream tasks like RAG and semantic search.<<PH475>><<SEP>>This project demonstrates a full-stack LLM fine-tuning experiment using ORPO (Open-Source Reinforcement Pretraining Objective) to align a base language model with human preference data."}, {"file": "README2.md", "original": "It leverages the Z by HP AI Studio Local GenAI environment, and uses models such as LLaMA 3, Gemma 1B, and Mistral 7B as foundations.<<PH476>><<SEP>>We incorporate:<<PH477>><<SEP>>Galileo PromptQuality for evaluating model responses with human-like scorers (e.g., context adherence)<<PH334>><<SEP>>TensorBoard for human feedback visualization before fine-tuning<<PH335>><<SEP>>A flexible model selector and inference runner architecture<<PH336>><<SEP>>A comparative setup to benchmark base vs fine-tuned models on the same prompts<<PH478>><<SEP>>This notebook performs image generation inference using the Stable Diffusion architecture, with support for both standard and DreamBooth fine-tuned models.", "corrected": "It leverages the Z by HP AI Studio Local GenAI environment, and uses models such as LLaMA 3, Gemma 1B, and Mistral 7B as foundations.<<PH476>><<SEP>>We incorporate:<<PH477>><<SEP>>Galileo PromptQuality for evaluating model responses with human-like scorers (e.g., context adherence)<<PH334>><<SEP>>TensorBoard for human feedback visualization before fine-tuning<<PH335>><<SEP>>A flexible model selector and inference runner architecture<<PH336>><<SEP>>A comparative setup to benchmark base vs fine-tuned models on the same prompts<<PH478>><<SEP>>This notebook performs image generation inference using the Stable Diffusion architecture, with support for both standard and DreamBooth fine-tuned models."}, {"file": "README2.md", "original": "It loads configuration and secrets from YAML files, enables local or deployed inference execution, and calculates custom image quality metrics such as entropy and complexity.", "corrected": "It loads configuration and secrets from YAML files, enables local or deployed inference execution, and calculates custom image quality metrics, such as entropy and complexity."}, {"file": "README2.md", "original": "The pipeline is modular, supports Hugging Face model loading, and integrates with PromptQuality for evaluation and tracking.<<PH479>><<SEP>>This notebook implements a full Retrieval-Augmented Generation (RAG) pipeline for automatically generating a scientific presentation script.", "corrected": "The pipeline is modular, supports Hugging Face model loading, and integrates with PromptQuality for evaluation and tracking.<<PH479>><<SEP>>This notebook implements a full Retrieval-Augmented Generation (RAG) pipeline for automatically generating a scientific presentation script."}, {"file": "README2.md", "original": "It integrates paper retrieval from arXiv, text extraction and chunking, embedding generation with HuggingFace, vector storage with ChromaDB, and context-aware generation using LLMs.", "corrected": "It integrates paper retrieval from arXiv, text extraction and chunking, embedding generation with HuggingFace, vector storage with ChromaDB, and context-aware generation using LLMs."}, {"file": "README2.md", "original": "It also integrates Galileo Prompt Quality for evaluation and logging, and supports multi-source model loading including local Llama.cpp, HuggingFace-hosted, and HuggingFace-cloud models like Mistral or DeepSeek.<<PH480>><<SEP>>This project demonstrates how to build a semantic chunking and summarization pipeline for texts using LangChain, Sentence Transformers, and Galileo for model evaluation, protection, and observability.", "corrected": "It also integrates Galileo Prompt Quality for evaluation and logging, and supports multi-source model loading, including local Llama.cpp, HuggingFace-hosted, and HuggingFace-cloud models like Mistral or DeepSeek.<<PH480>><<SEP>>This project demonstrates how to build a semantic chunking and summarization pipeline for texts using LangChain, Sentence Transformers, and Galileo for model evaluation, protection, and observability."}, {"file": "README2.md", "original": "It leverages the Z by HP AI Studio Local GenAI image and the Meta Llama 3.1 model with 8B parameters to generate concise and contextually accurate summaries from text data.<<PH481>><<SEP>>This project is an AI-powered vanilla RAG (Retrieval-Augmented Generation) chatbot built using LangChain and Galileo for model evaluation, protection, and observability.", "corrected": "It leverages the Z by HP AI Studio Local GenAI image and the Meta Llama 3.1 model with 8B parameters to generate concise and contextually accurate summaries from text data.<<PH481>><<SEP>>This project is an AI-powered vanilla RAG (Retrieval-Augmented Generation) chatbot built using LangChain and Galileo for model evaluation, protection, and observability."}, {"file": "README2.md", "original": "It leverages the Z by HP AI Studio Local GenAI image and the Meta Llama 3.1 model with 8B parameters to generate contextual and document-grounded answers to user queries about Z by HP AI Studio.<<PH482>><<SEP>>The sample projects in this folder demonstrate how to integrate **NVIDIA NGC (NVIDIA GPU Cloud)** resources with [**HP AI Studio**](<<PH47>>).<<PH483>><<SEP>>We provide **5 blueprint projects**, each designed for quick and easy use to help you get started efficiently.<<PH484>><<SEP>>This project contains a single integrated pipeline\u2014Agentic RAG for AI Studio with TRT-LLM and LangGraph\u2014that implements a Retrieval-Augmented Generation (RAG) workflow using:<<PH485>><<SEP>>TensorRT-backed Llama-3.1-Nano (TRT-LLM): for fast, GPU-accelerated inference.<<PH367>><<SEP>>LangGraph: to orchestrate an agentic, multi-step decision flow (relevance check, memory lookup, query rewriting, retrieval, answer generation, and memory update).<<PH368>><<SEP>>ChromaDB: as a local vector store over Markdown context files (about AI Studio).<<PH369>><<SEP>>SimpleKVMemory: a lightweight on-disk key-value store to cache query-answer pairs.<<PH486>><<SEP>>This project demonstrates an end-to-end **audio translation pipeline** using **NVIDIA NeMo models**.", "corrected": "It leverages the Z by HP AI Studio Local GenAI image and the Meta Llama 3.1 model with 8B parameters to generate contextual and document-grounded answers to user queries about Z by HP AI Studio.<<PH482>><<SEP>>The sample projects in this folder demonstrate how to integrate **NVIDIA NGC (NVIDIA GPU Cloud)** resources with [**HP AI Studio**](<<PH47>>).<<PH483>><<SEP>>We provide **5 blueprint projects**, each designed for quick and easy use to help you get started efficiently.<<PH484>><<SEP>>This project contains a single integrated pipeline\u2014Agentic RAG for AI Studio with TRT-LLM and LangGraph\u2014that implements a Retrieval-Augmented Generation (RAG) workflow using:<<PH485>><<SEP>>TensorRT-backed Llama-3.1-Nano (TRT-LLM): for fast, GPU-accelerated inference.<<PH367>><<SEP>>LangGraph: to orchestrate an agentic, multi-step decision flow (relevance check, memory lookup, query rewriting, retrieval, answer generation, and memory update).<<PH368>><<SEP>>ChromaDB: as a local vector store over Markdown context files (about AI Studio).<<PH369>><<SEP>>SimpleKVMemory: a lightweight on-disk key-value store to cache query-answer pairs.<<PH486>><<SEP>>This project demonstrates an end-to-end **audio translation pipeline** using **NVIDIA NeMo models**."}, {"file": "README2.md", "original": "It takes an English audio sample and performs:<<PH487>>[[BULLET167]] **Speech-to-Text (STT)** conversion using Citrinet<<PH376>>[[BULLET168]] **Text Translation (TT)** from English to Spanish using NMT<<PH377>>[[BULLET169]] **Text-to-Speech (TTS)** synthesis in Spanish using FastPitch and HiFiGAN<<PH488>><<SEP>>All steps are GPU-accelerated, and the full workflow is integrated with **MLflow** for experiment tracking and model registration.<<PH489>><<SEP>>In this project, we provide notebooks to compare the execution time of dataset operations using traditional **Pandas** (CPU) versus **NVIDIA\u2019s cuDF**, a GPU-accelerated drop-in replacement for Pandas.", "corrected": "It takes an English audio sample and performs:<<PH487>>[[BULLET167]] **Speech-to-Text (STT)** conversion using Citrinet<<PH376>>[[BULLET168]] **Text Translation (TT)** from English to Spanish using NMT<<PH377>>[[BULLET169]] **Text-to-Speech (TTS)** synthesis in Spanish using FastPitch and HiFiGAN<<PH488>><<SEP>>All steps are GPU-accelerated, and the full workflow is integrated with **MLflow** for experiment tracking and model registration.<<PH489>><<SEP>>In this project, we provide notebooks to compare the execution time of dataset operations using traditional **Pandas** (CPU) versus **NVIDIA\u2019s cuDF**, a GPU-accelerated drop-in replacement for Pandas."}, {"file": "README2.md", "original": "This example is presented in two different formats:<<PH490>>[[BULLET175]] **Original Example Notebook**: This version, created by NVIDIA, runs the entire evaluation within a single notebook.", "corrected": "This example is presented in two different formats:<<PH490>>[[BULLET175]] **Original Example Notebook**: This version, created by NVIDIA, runs the entire evaluation within a single notebook."}, {"file": "README2.md", "original": "It includes downloading the data and restarting the kernel to activate the cuDF extension.<<PH491>>[[BULLET177]] **Data Analysis Notebooks**: These notebooks use preprocessed datasets of varying sizes from **datafabric** folder in AI Studio.", "corrected": "It includes downloading the data and restarting the kernel to activate the cuDF extension.<<PH491>>[[BULLET177]] **Data Analysis Notebooks**: These notebooks use preprocessed datasets of varying sizes from the **datafabric** folder in AI Studio."}, {"file": "README2.md", "original": "The evaluation is split across two notebooks\u2014one using Pandas (CPU) and the other using cuDF (GPU)\u2014with performance metrics logged to **MLflow**.<<PH492>><<SEP>>This project is a GPU-accelerated, interactive **exploratory data analysis (EDA)** dashboard for the [OpenCellID](<<PH48>>) dataset.", "corrected": "The evaluation is split across two notebooks\u2014one using Pandas (CPU) and the other using cuDF (GPU)\u2014with performance metrics logged to **MLflow**.<<PH492>><<SEP>>This project is a GPU-accelerated, interactive **exploratory data analysis (EDA)** dashboard for the [OpenCellID](<<PH48>>) dataset."}, {"file": "README2.md", "original": "It uses **Panel** and **cuDF** to deliver lightning-fast geospatial analysis and visualization.<<PH493>><<SEP>>You can explore cell tower distributions by radio type, operator, country, and time window \u2014 rendered live on an interactive map with full GPU acceleration.<<PH494>><<SEP>>This project implements an **AI-powered recommendation agent** that delivers personalized travel suggestions based on user queries.<<PH495>><<SEP>>It leverages the **NVIDIA NeMo Framework** and **BERT embeddings** to understand user intent and generate highly relevant, tailored vacation recommendations.<<PH496>><<SEP>>This section provides solutions for common issues users may encounter when working with AI Blueprint projects in HP AI Studio:<<PH497>><<SEP>>1. <<PH498>>   Each project\u2019s README includes recommended minimum hardware specifications (e.g., RAM, VRAM).", "corrected": "It uses **Panel** and **cuDF** to deliver lightning-fast geospatial analysis and visualization.<<PH493>><<SEP>>You can explore cell tower distributions by radio type, operator, country, and time window \u2014 rendered live on an interactive map with full GPU acceleration.<<PH494>><<SEP>>This project implements an **AI-powered recommendation agent** that delivers personalized travel suggestions based on user queries.<<PH495>><<SEP>>It leverages the **NVIDIA NeMo Framework** and **BERT embeddings** to understand user intent and generate highly relevant, tailored vacation recommendations.<<PH496>><<SEP>>This section provides solutions for common issues users may encounter when working with AI Blueprint projects in HP AI Studio:<<PH497>><<SEP>>1. <<PH498>>   Each project\u2019s README includes recommended minimum hardware specifications (e.g., RAM, VRAM)."}, {"file": "README2.md", "original": "Make sure your system meets these requirements\u2014especially when working with large models or during deployment, as insufficient resources can cause failures.<<PH499>><<SEP>>2. <<PH500>>   If you download models or datasets while your workspace is running, they might not appear in the workspace.", "corrected": "Make sure your system meets these requirements\u2014especially when working with large models or during deployment, as insufficient resources can cause failures.<<PH499>><<SEP>>2. <<PH500>>   If you download models or datasets while your workspace is running, they might not appear in the workspace."}, {"file": "README2.md", "original": "In such cases, restart your workspace to ensure they are properly recognized.<<PH501>>[[BULLET196]] **Connection or SSL Errors in Notebooks**<<PH414>>   If you encounter SSL certificate or connection errors while accessing websites from notebooks (especially on restricted networks), verify your network settings.", "corrected": "In such cases, restart your workspace to ensure they are properly recognized.<<PH501>>[[BULLET196]] **Connection or SSL Errors in Notebooks**<<PH414>>   If you encounter SSL certificate or connection errors while accessing websites from notebooks (especially on restricted networks), verify your network settings."}, {"file": "README2.md", "original": "Consider using a proxy to bypass restrictive network constraints.<<PH502>><<SEP>>4. <<PH503>>   Ensure that all required files and directories are correctly placed as specified in the project\u2019s README.", "corrected": "Consider using a proxy to bypass restrictive network constraints.<<PH502>><<SEP>>4. <<PH503>>   Ensure that all required files and directories are correctly placed as specified in the project\u2019s README."}, {"file": "README2.md", "original": "If any paths or files are missing, create or move them to the correct locations.<<PH504>><<SEP>>5. <<PH505>>   For projects requiring NVIDIA GPUs, verify GPU availability by running `<<PH49>>` in the terminal.", "corrected": "If any paths or files are missing, create or move them to the correct locations.<<PH504>><<SEP>>5. <<PH505>>   For projects requiring NVIDIA GPUs, verify GPU availability by running `<<PH49>>` in the terminal."}, {"file": "README2.md", "original": "Ensure that a compatible GPU is accessible and has sufficient free memory to run the project.<<PH506>><<SEP>>6. <<PH507>>   Even if your hardware meets the specs, limited available RAM or VRAM can cause deployment issues.", "corrected": "Ensure that a compatible GPU is accessible and has sufficient free memory to run the project.<<PH506>><<SEP>>6. <<PH507>>   Even if your hardware meets the specs, limited available RAM or VRAM can cause deployment issues."}, {"file": "README2.md", "original": "Close other running workspaces or programs to free up memory.<<PH508>><<SEP>>7. <<PH509>>   API requests triggered through user interfaces have a response timeout limit (usually 30 seconds).", "corrected": "Close other running workspaces or programs to free up memory.<<PH508>><<SEP>>7. <<PH509>>   API requests triggered through user interfaces have a response timeout limit (usually 30 seconds)."}, {"file": "README2.md", "original": "For long-running tasks or large inputs, use the provided notebooks instead of the UI to avoid timeout errors.<<PH510>>[[BULLET210]] Issues: Open a new issue in our [**AI-Blueprints GitHub repo**](<<PH50>>).<<PH511>>[[BULLET212]] Docs: Refer to the **[AI Studio Documentation](<<PH51>>)** for detailed guidance and troubleshooting.<<PH512>>[[BULLET214]] Community: Join the [**HP AI Creator Community**](<<PH52>>) for questions and help.<<PH513>>> <<PH31>>", "corrected": "For long-running tasks or large inputs, use the provided notebooks instead of the UI to avoid timeout errors.<<PH510>>[[BULLET210]] Issues: Open a new issue in our [**AI-Blueprints GitHub repo**](<<PH50>>).<<PH511>>[[BULLET212]] Docs: Refer to the **[AI Studio Documentation](<<PH51>>)** for detailed guidance and troubleshooting.<<PH512>>[[BULLET214]] Community: Join the [**HP AI Creator Community**](<<PH52>>) for questions and help.<<PH513>>> <<PH31>>"}, {"file": "README3.md", "original": "<<PH271>>![Python](<<PH24>>)<<PH153>>![Jupyter](<<PH25>>)<<PH154>>![TensorFlow](<<PH26>>)<<PH155>>![PyTorch](<<PH27>>)<<PH156>>![React UI](<<PH28>>)<<PH272>>[[BULLET49]] [\ud83e\udde0 Overview](<<PH29>>)<<PH161>>* <<PH273>>[[BULLET51]] [\u2699\ufe0f Setup](<<PH31>>)<<PH163>>[[BULLET52]] [\ud83d\ude80 Usage](<<PH32>>)<<PH164>>[[BULLET53]] [\ud83d\udcde Contact and Support](<<PH33>>)<<PH274>><<SEP>>The objective of this template is to show how to create a simple text generation with trained models from Hugging Face, one character per time using a dataset of Shakespeare's texts.<<PH275>><<SEP>>Ensure your environment meets the minimum compute requirements for smooth performance:<<PH276>>- <<PH277>>- <<PH278>>- <<PH279>>[[BULLET73]] Create a new project in [Z by HP AI Studio](<<PH34>>).<<PH280>>[[BULLET77]] Choose **Deep Learning** as the base image.<<PH281>>[[BULLET83]] Ensure all files are available after workspace creation.<<PH282>><<SEP>>Run the following notebook `<<PH35>>`:<<PH204>>[[BULLET90]] Obtain Text Data from the shakespeare.txt.<<PH205>>[[BULLET91]] Prepare the textual data.", "corrected": "<<PH271>>![Python](<<PH24>>)<<PH153>>![Jupyter](<<PH25>>)<<PH154>>![TensorFlow](<<PH26>>)<<PH155>>![PyTorch](<<PH27>>)<<PH156>>![React UI](<<PH28>>)<<PH272>>[[BULLET49]] [\ud83e\udde0 Overview](<<PH29>>)<<PH161>>* <<PH273>>[[BULLET51]] [\u2699\ufe0f Setup](<<PH31>>)<<PH163>>[[BULLET52]] [\ud83d\ude80 Usage](<<PH32>>)<<PH164>>[[BULLET53]] [\ud83d\udcde Contact and Support](<<PH33>>)<<PH274>><<SEP>>The objective of this template is to show how to create a simple text generation with trained models from Hugging Face, one character at a time using a dataset of Shakespeare's texts.<<PH275>><<SEP>>Ensure your environment meets the minimum compute requirements for smooth performance:<<PH276>>- <<PH277>>- <<PH278>>- <<PH279>>[[BULLET73]] Create a new project in [Z by HP AI Studio](<<PH34>>).<<PH280>>[[BULLET77]] Choose **Deep Learning** as the base image.<<PH281>>[[BULLET83]] Ensure all files are available after workspace creation.<<PH282>><<SEP>>Run the following notebook `<<PH35>>`:<<PH204>>[[BULLET90]] Obtain Text Data from the shakespeare.txt.<<PH205>>[[BULLET91]] Prepare the textual data."}, {"file": "README3.md", "original": "It's needed to encode the data to provide the model a proper numerical representation of the text.<<PH206>>[[BULLET92]] Create Training Batches for divide the dataset into smaller, manageable groups of data points that are fed into a machine learning model during the training process.<<PH207>><<SEP>>4. <<PH283>>[[BULLET94]] Train the model.<<PH209>>[[BULLET95]] Train the model with the selected epochs.<<PH284>>[[BULLET97]] Generate the Predictions with the words 'Confidence' and 'Love'.", "corrected": "It's needed to encode the data to provide the model a proper numerical representation of the text.<<PH206>>[[BULLET92]] Create Training Batches for dividing the dataset into smaller, manageable groups of data points that are fed into a machine learning model during the training process.<<PH207>><<SEP>>4. <<PH283>>[[BULLET94]] Train the model.<<PH209>>[[BULLET95]] Train the model with the selected epochs.<<PH284>>[[BULLET97]] Generate the predictions with the words 'Confidence' and 'Love'."}, {"file": "README3.md", "original": "The words can be changed.<<PH285>><<SEP>>Run the following notebook `<<PH36>>`:<<PH215>>[[BULLET100]] Obtain Text Data from the shakespeare.txt.<<PH216>>[[BULLET101]] Prepare the textual data.", "corrected": "The words can be changed.<<PH285>><<SEP>>Run the following notebook `<<PH36>>`:<<PH215>>[[BULLET100]] Obtain text data from the Shakespeare.txt.<<PH216>>[[BULLET101]] Prepare the textual data."}, {"file": "README3.md", "original": "It's needed to decode and encode the data to give the model a proper numerical representation of the text.<<PH217>>[[BULLET102]] One Hot Encoding to convert categorical data into a fixed-size vector of numerical values.<<PH218>>[[BULLET103]] Create Training Batches for divide the dataset into smaller, manageable groups of data points that are fed into a machine learning model during the training process.<<PH219>>[[BULLET104]] Create the LSTM Model with the decoder and encoder files<<PH220>>[[BULLET105]] Train the Network to do the Predictions<<PH221>>[[BULLET106]] Generate the Predictions with the words 'Confidence' and 'Love'.", "corrected": "It's needed to decode and encode the data to give the model a proper numerical representation of the text.<<PH217>>[[BULLET102]] One Hot Encoding to convert categorical data into a fixed-size vector of numerical values.<<PH218>>[[BULLET103]] Create training batches for dividing the dataset into smaller, manageable groups of data points that are fed into a machine learning model during the training process.<<PH219>>[[BULLET104]] Create the LSTM model with the decoder and encoder files.<<PH220>>[[BULLET105]] Train the network to do the predictions.<<PH221>>[[BULLET106]] Generate the predictions with the words 'Confidence' and 'Love'."}, {"file": "README3.md", "original": "The words can be changed.<<PH286>>[[BULLET109]] Execute `<<PH37>>` to register the model in MLflow and create the API logic.  <<PH225>><<SEP>>2. <<PH287>>[[BULLET111]] Name the service and select the registered model.  <<PH227>>[[BULLET112]] Choose an available model version and configure it with **GPU acceleration**.  <<PH228>>[[BULLET113]] Start the deployment.  <<PH229>>[[BULLET114]] Once deployed, click on the **Service URL** to access the Swagger API page.  <<PH230>>[[BULLET115]] At the top of the Swagger API page, follow the provided link to open the demo UI for interacting with the locally deployed model.<<PH288>><<SEP>>Once deployed, access the **Swagger UI** via the Service URL.<<PH289>><<SEP>>Paste a payload like:<<PH290>><<SEP>>And as response:<<PH291>><<SEP>>From the Swagger page, click the **\u201cDemo\u201d** link to interact via a simple web form:<<PH292>>[[BULLET129]] Enter your source text.<<PH249>>* <<PH293>>[[BULLET131]] View the generated text right in the browser.<<PH294>>[[BULLET135]] HTML<<PH255>>![Automated Evaluation React UI](<<PH38>>)  <<PH295>>[[BULLET141]] Issues: Open a new issue in our [**AI-Blueprints GitHub repo**](<<PH39>>).<<PH296>>[[BULLET143]] Docs: Refer to the **[AI Studio Documentation](<<PH40>>)** for detailed guidance and troubleshooting. <<PH297>>[[BULLET145]] Community: Join the [**HP AI Creator Community**](<<PH41>>) for questions and help.<<PH298>>> <<PH23>>", "corrected": "The words can be changed.<<PH286>>[[BULLET109]] Execute `<<PH37>>` to register the model in MLflow and create the API logic.  <<PH225>><<SEP>>2. <<PH287>>[[BULLET111]] Name the service and select the registered model.  <<PH227>>[[BULLET112]] Choose an available model version and configure it with **GPU acceleration**.  <<PH228>>[[BULLET113]] Start the deployment.  <<PH229>>[[BULLET114]] Once deployed, click on the **Service URL** to access the Swagger API page.  <<PH230>>[[BULLET115]] At the top of the Swagger API page, follow the provided link to open the demo UI for interacting with the locally deployed model.<<PH288>><<SEP>>Once deployed, access the **Swagger UI** via the Service URL.<<PH289>><<SEP>>Paste a payload like:<<PH290>><<SEP>>And as response:<<PH291>><<SEP>>From the Swagger page, click the **\u201cDemo\u201d** link to interact via a simple web form:<<PH292>>[[BULLET129]] Enter your source text.<<PH249>>* <<PH293>>[[BULLET131]] View the generated text right in the browser.<<PH294>>[[BULLET135]] HTML<<PH255>>![Automated Evaluation React UI](<<PH38>>)  <<PH295>>[[BULLET141]] Issues: Open a new issue in our [**AI-Blueprints GitHub repo**](<<PH39>>).<<PH296>>[[BULLET143]] Docs: Refer to the **[AI Studio Documentation](<<PH40>>)** for detailed guidance and troubleshooting. <<PH297>>[[BULLET145]] Community: Join the [**HP AI Creator Community**](<<PH41>>) for questions and help.<<PH298>>> <<PH23>>"}, {"file": "README4.md", "original": "<<PH294>>![Python](<<PH24>>)<<PH170>>![Jupyter](<<PH25>>)<<PH171>>![LangChain](<<PH26>>)<<PH172>>![Streamlit UI](<<PH27>>)<<PH173>>![React UI](<<PH28>>)<<PH295>>[[BULLET58]] [\ud83e\udde0 Overview](<<PH29>>)<<PH178>>- <<PH296>>[[BULLET60]] [\u2699\ufe0f Configuration](<<PH31>>)<<PH180>>[[BULLET61]] [\ud83d\udd27 Setup](<<PH32>>)<<PH181>>[[BULLET62]] [\ud83d\ude80 Usage](<<PH33>>)<<PH182>>[[BULLET63]] [\ud83d\udcde Contact and Support](<<PH34>>)<<PH297>><<SEP>>This project demonstrates how to build a semantic chunking and summarization pipeline for texts using **LangChain** and **Sentence Transformers**.", "corrected": "<<PH294>>![Python](<<PH24>>)<<PH170>>![Jupyter](<<PH25>>)<<PH171>>![LangChain](<<PH26>>)<<PH172>>![Streamlit UI](<<PH27>>)<<PH173>>![React UI](<<PH28>>)<<PH295>>[[BULLET58]] [\ud83e\udde0 Overview](<<PH29>>)<<PH178>>- <<PH296>>[[BULLET60]] [\u2699\ufe0f Configuration](<<PH31>>)<<PH180>>[[BULLET61]] [\ud83d\udd27 Setup](<<PH32>>)<<PH181>>[[BULLET62]] [\ud83d\ude80 Usage](<<PH33>>)<<PH182>>[[BULLET63]] [\ud83d\udcde Contact and Support](<<PH34>>)<<PH297>><<SEP>>This project demonstrates how to build a semantic chunking and summarization pipeline for texts using **LangChain** and **Sentence Transformers**."}, {"file": "README4.md", "original": "It leverages the **Z by HP AI Studio Local GenAI image** and the Meta Llama 3.1 model with 8B parameters to generate concise and contextually accurate summaries from text data.<<PH298>><<SEP>>The blueprint uses a centralized configuration system through `<<PH35>>`:<<PH299>><<SEP>>To ensure smooth execution and reliable model deployment, make sure your system meets the following minimum hardware specifications:<<PH300>>- <<PH301>>- <<PH302>>- <<PH303>>[[BULLET96]] Create a new project in [Z by HP AI Studio](<<PH36>>).<<PH219>>[[BULLET97]] (Optional) Add a description and relevant tags.<<PH304>>[[BULLET101]] Choose **Local GenAI** as the base image.<<PH305>>[[BULLET105]] Clone the GitHub repository:<<PH306>>[[BULLET109]] Ensure all files are available after workspace creation..<<PH307>>[[BULLET113]] Download the Meta Llama 3.1 model with 8B parameters via Models\u202ftab:<<PH308>>- <<PH309>>- <<PH310>>- <<PH311>>- <<PH312>>[[BULLET119]] Make sure that the model is in the `<<PH41>>` folder inside your workspace.<<PH313>>[[BULLET123]] Add your API keys to the `<<PH42>>` file under the `<<PH43>>` folder:<<PH314>>[[BULLET125]] Edit `<<PH45>>` with relevant configuration details.<<PH315>><<SEP>>Execute the notebook inside the `<<PH46>>` folder:<<PH316>><<SEP>>This will:<<PH317>>[[BULLET137]] Set up the semantic chunking pipeline<<PH262>>[[BULLET138]] Create the summarization chain with LangChain<<PH263>>[[BULLET139]] Register the model in MLflow<<PH318>>- <<PH319>>[[BULLET144]] Name the service and select the registered model.<<PH269>>[[BULLET145]] Choose a model version and enable **GPU acceleration**.<<PH270>>[[BULLET146]] Start the deployment.<<PH271>>[[BULLET147]] Once deployed, access the **Swagger UI** via the Service URL.<<PH272>>[[BULLET148]] Use the API endpoints to generate summaries from your text data.<<PH320>>![text Summarization Demo UI](<<PH47>>)<<PH321>>:warning: Current implementation of deployed model **do not** perform the chunking steps: summarization is run directly by the LLM model.", "corrected": "It leverages the **Z by HP AI Studio Local GenAI image** and the Meta Llama 3.1 model with 8B parameters to generate concise and contextually accurate summaries from text data.<<PH298>><<SEP>>The blueprint uses a centralized configuration system through `<<PH35>>`:<<PH299>><<SEP>>To ensure smooth execution and reliable model deployment, make sure your system meets the following minimum hardware specifications:<<PH300>>- <<PH301>>- <<PH302>>- <<PH303>>[[BULLET96]] Create a new project in [Z by HP AI Studio](<<PH36>>).<<PH219>>[[BULLET97]] (Optional) Add a description and relevant tags.<<PH304>>[[BULLET101]] Choose **Local GenAI** as the base image.<<PH305>>[[BULLET105]] Clone the GitHub repository:<<PH306>>[[BULLET109]] Ensure all files are available after workspace creation..<<PH307>>[[BULLET113]] Download the Meta Llama 3.1 model with 8B parameters via Models\u202ftab:<<PH308>>- <<PH309>>- <<PH310>>- <<PH311>>- <<PH312>>[[BULLET119]] Make sure that the model is in the `<<PH41>>` folder inside your workspace.<<PH313>>[[BULLET123]] Add your API keys to the `<<PH42>>` file under the `<<PH43>>` folder:<<PH314>>[[BULLET125]] Edit `<<PH45>>` with relevant configuration details.<<PH315>><<SEP>>Execute the notebook inside the `<<PH46>>` folder:<<PH316>><<SEP>>This will:<<PH317>>[[BULLET137]] Set up the semantic chunking pipeline<<PH262>>[[BULLET138]] Create the summarization chain with LangChain<<PH263>>[[BULLET139]] Register the model in MLflow<<PH318>>- <<PH319>>[[BULLET144]] Name the service and select the registered model.<<PH269>>[[BULLET145]] Choose a model version and enable **GPU acceleration**.<<PH270>>[[BULLET146]] Start the deployment.<<PH271>>[[BULLET147]] Once deployed, access the **Swagger UI** via the Service URL.<<PH272>>[[BULLET148]] Use the API endpoints to generate summaries from your text data.<<PH320>>![text Summarization Demo UI](<<PH47>>)<<PH321>>:warning: Current implementation of deployed model **does not** perform the chunking steps: summarization is run directly by the LLM model."}, {"file": "README4.md", "original": "In the case of suggested local model (i.e. Llama3.1-8b), texts with more than 1000 words may cause instabilities when summarization is triggered on the UI.", "corrected": "In the case of the suggested local model (i.e., Llama 3.1-8b), texts with more than 1000 words may cause instabilities when summarization is triggered on the UI."}, {"file": "README4.md", "original": "We recommend using different models or smaller texts to avoid these problems.<<PH322>>[[BULLET158]] Issues & Bugs: Open a new issue in our [**AI-Blueprints GitHub repo**](<<PH48>>).<<PH323>>- <<PH324>>[[BULLET162]] Community: Join the [**HP AI Creator Community**](<<PH50>>) for questions and help.<<PH325>>> <<PH23>>", "corrected": "We recommend using different models or smaller texts to avoid these problems.<<PH322>>[[BULLET158]] Issues & Bugs: Open a new issue in our [**AI-Blueprints GitHub repo**](<<PH48>>).<<PH323>>- <<PH324>>[[BULLET162]] Community: Join the [**HP AI Creator Community**](<<PH50>>) for questions and help.<<PH325>>> <<PH23>>"}, {"file": "README5.md", "original": "<<PH288>>![Python](<<PH26>>)<<PH162>>![Jupyter](<<PH27>>)<<PH163>>![Keras](<<PH28>>)<<PH164>>![TensorFlow](<<PH29>>)<<PH165>>![Streamlit UI](<<PH30>>)<<PH289>>[[BULLET51]] [\ud83e\udde0 Overview](<<PH31>>)<<PH170>>- <<PH290>>[[BULLET53]] [\u2699\ufe0f Setup](<<PH33>>)<<PH172>>[[BULLET54]] [\ud83d\ude80 Usage](<<PH34>>)<<PH173>>[[BULLET55]] [\ud83d\udcde Contact and Support](<<PH35>>)<<PH291>><<SEP>>This project shows how to do a image classification, specifically digits of handwritten images, using TensorFlow and MNIST(Modified National Institute of Standards and Technology) dataset of handwritten digits.", "corrected": "<<PH288>>![Python](<<PH26>>)<<PH162>>![Jupyter](<<PH27>>)<<PH163>>![Keras](<<PH28>>)<<PH164>>![TensorFlow](<<PH29>>)<<PH165>>![Streamlit UI](<<PH30>>)<<PH289>>[[BULLET51]] [\ud83e\udde0 Overview](<<PH31>>)<<PH170>>- <<PH290>>[[BULLET53]] [\u2699\ufe0f Setup](<<PH33>>)<<PH172>>[[BULLET54]] [\ud83d\ude80 Usage](<<PH34>>)<<PH173>>[[BULLET55]] [\ud83d\udcde Contact and Support](<<PH35>>)<<PH291>> This project shows how to do an image classification, specifically digits of handwritten images, using TensorFlow and the MNIST (Modified National Institute of Standards and Technology) dataset of handwritten digits."}, {"file": "README5.md", "original": "The MNIST dataset consists of a collection of handwritten digits from 0 to 9. <<PH292>><<SEP>>Ensure your environment meets the minimum compute requirements for smooth image classification performance:<<PH293>>- <<PH294>>- <<PH295>>- <<PH296>>[[BULLET81]] Create a new project in [Z by HP AI Studio](<<PH36>>).<<PH297>>[[BULLET85]] Choose **Deep Learning** as the base image.<<PH298>>[[BULLET89]] Clone the GitHub repository:  <<PH299>>[[BULLET92]] Ensure all files are available after workspace creation.<<PH300>><<SEP>>Execute the notebook inside the `<<PH37>>` folder:<<PH301>><<SEP>>This will:<<PH302>>[[BULLET104]] Load and preprocess the MNIST data <<PH227>>[[BULLET105]] Create the model architecture  <<PH228>>[[BULLET106]] Train the model<<PH303>><<SEP>>Execute the notebook inside the `<<PH38>>` folder:<<PH304>><<SEP>>This will:<<PH305>>- <<PH306>>[[BULLET114]] Fetch the Latest Model Version from MLflow<<PH239>>[[BULLET115]] Load the Model and Running Inference<<PH307>>- <<PH308>>[[BULLET120]] Name the service and select the registered model.<<PH245>>[[BULLET121]] Choose a model version and **GPU** it's **not necessary**.<<PH246>>[[BULLET122]] Choose the workspace.<<PH247>>[[BULLET123]] Start the deployment.<<PH248>>[[BULLET124]] Note: This is a local deployment running on your machine.", "corrected": "The MNIST dataset consists of a collection of handwritten digits from 0 to 9. <<PH292>><<SEP>>Ensure your environment meets the minimum compute requirements for smooth image classification performance:<<PH293>>- <<PH294>>- <<PH295>>- <<PH296>>[[BULLET81]] Create a new project in [Z by HP AI Studio](<<PH36>>).<<PH297>>[[BULLET85]] Choose **Deep Learning** as the base image.<<PH298>>[[BULLET89]] Clone the GitHub repository:  <<PH299>>[[BULLET92]] Ensure all files are available after workspace creation.<<PH300>><<SEP>>Execute the notebook inside the `<<PH37>>` folder:<<PH301>><<SEP>>This will:<<PH302>>[[BULLET104]] Load and preprocess the MNIST data <<PH227>>[[BULLET105]] Create the model architecture  <<PH228>>[[BULLET106]] Train the model<<PH303>><<SEP>>Execute the notebook inside the `<<PH38>>` folder:<<PH304>><<SEP>>This will:<<PH305>>- <<PH306>>[[BULLET114]] Fetch the Latest Model Version from MLflow<<PH239>>[[BULLET115]] Load the Model and Running Inference<<PH307>>- <<PH308>>[[BULLET120]] Name the service and select the registered model.<<PH245>>[[BULLET121]] Choose a model version and **GPU** it's **not necessary**.<<PH246>>[[BULLET122]] Choose the workspace.<<PH247>>[[BULLET123]] Start the deployment.<<PH248>>[[BULLET124]] Note: This is a local deployment running on your machine."}, {"file": "README5.md", "original": "As a result, if API processing takes more than a few minutes, it may return a timeout error.", "corrected": "As a result, if API processing takes more than a few minutes, it may return a timeout error."}, {"file": "README5.md", "original": "If you need to work with inputs that require longer processing times, we recommend using the provided notebook in the project files instead of accessing the API via Swagger or the web app UI.<<PH309>><<SEP>>Once deployed, access the **Swagger UI** via the Service URL.<<PH310>><<SEP>>Paste a payload like:<<PH311>><<SEP>>And as response:<<PH312>>[[BULLET138]] To launch the Streamlit UI, follow the instructions in the README file located in the `<<PH39>>` folder.<<PH313>>[[BULLET140]] Navigate to the shown URL and view the handwritten classification.<<PH314>>[[BULLET144]] Streamlit<<PH272>>![Handwritten Digit Classification Streamlit UI](<<PH40>>)<<PH315>>[[BULLET150]] Issues: Open a new issue in our [**AI-Blueprints GitHub repo**](<<PH41>>).<<PH316>>[[BULLET152]] Docs: Refer to the **[AI Studio Documentation](<<PH42>>)** for detailed guidance and troubleshooting. <<PH317>>[[BULLET154]] Community: Join the [**HP AI Creator Community**](<<PH43>>) for questions and help.<<PH318>>> <<PH25>>", "corrected": "If you need to work with inputs that require longer processing times, we recommend using the provided notebook in the project files instead of accessing the API via Swagger or the web app UI.<<PH309>><<SEP>>Once deployed, access the **Swagger UI** via the Service URL.<<PH310>><<SEP>>Paste a payload like:<<PH311>><<SEP>>And as response:<<PH312>>[[BULLET138]] To launch the Streamlit UI, follow the instructions in the README file located in the `<<PH39>>` folder.<<PH313>>[[BULLET140]] Navigate to the shown URL and view the handwritten classification.<<PH314>>[[BULLET144]] Streamlit<<PH272>>![Handwritten Digit Classification Streamlit UI](<<PH40>>)<<PH315>>[[BULLET150]] Issues: Open a new issue in our [**AI-Blueprints GitHub repo**](<<PH41>>).<<PH316>>[[BULLET152]] Docs: Refer to the **[AI Studio Documentation](<<PH42>>)** for detailed guidance and troubleshooting. <<PH317>>[[BULLET154]] Community: Join the [**HP AI Creator Community**](<<PH43>>) for questions and help.<<PH318>>> <<PH25>>"}, {"file": "README6.md", "original": "<<PH239>>![Python](<<PH23>>)<<PH137>>![Jupyter](<<PH24>>)<<PH138>>![HuggingFace](<<PH25>>)<<PH139>>![BERT](<<PH26>>)<<PH140>>![TensorFlow](<<PH27>>)<<PH141>>![React UI](<<PH28>>)<<PH240>>[[BULLET48]] [\ud83e\udde0 Overview](<<PH29>>)<<PH146>>* <<PH241>>[[BULLET50]] [\u2699\ufe0f Setup](<<PH31>>)<<PH148>>[[BULLET51]] [\ud83d\ude80 Usage](<<PH32>>)<<PH149>>[[BULLET52]] [\ud83d\udcde Contact and Support](<<PH33>>)<<PH242>> The Bidirectional Encoder Representations from Transformers (BERT) is based on a deep learning model in which every output is connected to every input, and the weightings between them are dynamically calculated based upon their connection.", "corrected": "<<PH239>>![Python](<<PH23>>)<<PH137>>![Jupyter](<<PH24>>)<<PH138>>![HuggingFace](<<PH25>>)<<PH139>>![BERT](<<PH26>>)<<PH140>>![TensorFlow](<<PH27>>)<<PH141>>![React UI](<<PH28>>)<<PH240>>[[BULLET48]] [\ud83e\udde0 Overview](<<PH29>>)<<PH146>>* <<PH241>>[[BULLET50]] [\u2699\ufe0f Setup](<<PH31>>)<<PH148>>[[BULLET51]] [\ud83d\ude80 Usage](<<PH32>>)<<PH149>>[[BULLET52]] [\ud83d\udcde Contact and Support](<<PH33>>)<<PH242>> The Bidirectional Encoder Representations from Transformers (BERT) is based on a deep learning model in which every output is connected to every input, and the weightings between them are dynamically calculated based upon their connection."}, {"file": "README6.md", "original": "BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.<<PH243>><<SEP>>Ensure your environment meets the minimum compute requirements for smooth performance:<<PH244>>- <<PH245>>- <<PH246>>- <<PH247>>[[BULLET73]] Create a new project in [Z by HP AI Studio](<<PH34>>).<<PH248>>[[BULLET77]] Choose **Deep Learning** as the base image.<<PH249>>[[BULLET83]] Ensure all files are available after workspace creation.<<PH250>><<SEP>>Run the following notebook `<<PH35>>`:<<PH190>>[[BULLET90]] Download the dataset from the HuggingFace datasets repository.<<PH191>>[[BULLET91]] Tokenize, preparing the inputs for the model.<<PH192>>[[BULLET92]] Load metrics and transforms the output model(Logits) to numbers.<<PH193>>[[BULLET93]] Train, using the model:<<PH251>>[[BULLET95]] Complete the training evaluation of the model.<<PH196>>[[BULLET96]] Create a question-answering pipeline from transformers and pass the model to it.<<PH252>><<SEP>>Run the following notebook `<<PH36>>`:<<PH200>><<SEP>>1. <<PH253>>[[BULLET100]] Fetch the Latest Model Version from MLflow<<PH202>>[[BULLET101]] Load the Model and Run Inference<<PH254>>[[BULLET104]] Run the following notebook `<<PH37>>`: <<PH206>><<SEP>>2. <<PH255>>[[BULLET106]] Name the service and select the registered model.  <<PH208>>[[BULLET107]] Choose an available model version and configure it with **GPU acceleration**.  <<PH209>>[[BULLET108]] Start the deployment.  <<PH210>>[[BULLET109]] Once deployed, click on the **Service URL** to access the Swagger API page.  <<PH211>>[[BULLET110]] At the top of the Swagger API page, follow the provided link to open the demo UI for interacting with the locally deployed model.  <<PH256>><<SEP>>Once deployed, access the **Swagger UI** via the Service URL.<<PH257>><<SEP>>Paste a payload like:<<PH258>><<SEP>>And as response:<<PH259>>[[BULLET125]] Issues: Open a new issue in our [**AI-Blueprints GitHub repo**](<<PH38>>).<<PH260>>[[BULLET127]] Docs: Refer to the **[AI Studio Documentation](<<PH39>>)** for detailed guidance and troubleshooting. <<PH261>>[[BULLET129]] Community: Join the [**HP AI Creator Community**](<<PH40>>) for questions and help.<<PH262>>> <<PH22>>", "corrected": "BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.<<PH243>><<SEP>>Ensure your environment meets the minimum compute requirements for smooth performance:<<PH244>>- <<PH245>>- <<PH246>>- <<PH247>>[[BULLET73]] Create a new project in [Z by HP AI Studio](<<PH34>>).<<PH248>>[[BULLET77]] Choose **Deep Learning** as the base image.<<PH249>>[[BULLET83]] Ensure all files are available after workspace creation.<<PH250>><<SEP>>Run the following notebook `<<PH35>>`:<<PH190>>[[BULLET90]] Download the dataset from the HuggingFace datasets repository.<<PH191>>[[BULLET91]] Tokenize, preparing the inputs for the model.<<PH192>>[[BULLET92]] Load metrics and transform the output model (Logits) to numbers.<<PH193>>[[BULLET93]] Train, using the model:<<PH251>>[[BULLET95]] Complete the training evaluation of the model.<<PH196>>[[BULLET96]] Create a question-answering pipeline from transformers and pass the model to it.<<PH252>><<SEP>>Run the following notebook `<<PH36>>`:<<PH200>><<SEP>>1. <<PH253>>[[BULLET100]] Fetch the latest model version from MLflow.<<PH202>>[[BULLET101]] Load the model and run inference.<<PH254>>[[BULLET104]] Run the following notebook `<<PH37>>`: <<PH206>><<SEP>>2. <<PH255>>[[BULLET106]] Name the service and select the registered model.  <<PH208>>[[BULLET107]] Choose an available model version and configure it with **GPU acceleration**.  <<PH209>>[[BULLET108]] Start the deployment.  <<PH210>>[[BULLET109]] Once deployed, click on the **Service URL** to access the Swagger API page.  <<PH211>>[[BULLET110]] At the top of the Swagger API page, follow the provided link to open the demo UI for interacting with the locally deployed model.  <<PH256>><<SEP>>Once deployed, access the **Swagger UI** via the Service URL.<<PH257>><<SEP>>Paste a payload like:<<PH258>><<SEP>>And as response:<<PH259>>[[BULLET125]] Issues: Open a new issue in our [**AI-Blueprints GitHub repo**](<<PH38>>).<<PH260>>[[BULLET127]] Docs: Refer to the **[AI Studio Documentation](<<PH39>>)** for detailed guidance and troubleshooting. <<PH261>>[[BULLET129]] Community: Join the [**HP AI Creator Community**](<<PH40>>) for questions and help.<<PH262>>> <<PH22>>"}]